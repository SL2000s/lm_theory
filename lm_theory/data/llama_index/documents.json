[
    {
        "text": "\\begin{definition}\\label{Lipschitz_definition}\nGiven two metric spaces $(\\mathcal{X}, d_{\\mathcal{X}})$ and $(\\mathcal{Y}, d_{\\mathcal{Y}})$, a function $f:\\mathcal{X} \\rightarrow \\mathcal{Y}$ is called \\textit{Lipschitz continuous} (or $K$-\\textit{Lipschitz}) if there exists a constant $K\\geq 0$ such that \n\\begin{equation}\nd_{\\mathcal{Y}}(f(\\mathbf{x}),f(\\mathbf{x'})) \\leq K d_{\\mathcal{X}}(\\mathbf{x},\\mathbf{x'}) \\spaces\\spaces \\text{for all } \\mathbf{x},\\mathbf{x'} \\in \\mathcal{X}.\n\\end{equation}\nThe smallest such $K$ is the \\textit{Lipschitz constant} of $f$, denoted $\\lip(f)$.\n\\end{definition}",
        "metadata": {
            "Statement label": "Definition 1",
            "Statement title": "Lipschitz Continuity"
        }
    },
    {
        "text": "\\begin{lemma}[\\citealp{federer1969geometric}]\nLet $g,h$ be two composable Lipschitz functions. Then $g \\circ h$ is also Lipschitz with $\\lip(g \\circ h) \\leq \\lip(g) \\lip(h)$.\n\\end{lemma}\nProof:\nSee proof at:\nFederer, \\textit{H. Geometric Measure Theory}. Classics in Mathematics. Springer Berlin Heidelberg, 1969. ISBN 9783642620102.",
        "metadata": {
            "Statement label": "Lemma 1",
            "Statement title": "Lipschitz Composition Lemma"
        }
    },
    {
        "text": "\\begin{lemma} \\label{lemma:tie_weights}\nIf $W^K \\in \\mathbb{R}^{D \\times D/H}$ is full rank (i.e.~full column rank), and $W^K \\neq W^Q$, then $J_{ij}$ has terms that are unbounded for $i \\neq j$, hence $\\tilde{f}$ is \\emph{not} Lipschitz. \n\\end{lemma}\nProof:\n\\begin{proof}\n\nLet us investigate the expression $\\tilde{K}_{ij} \\coloneqq P_{ij} W^{K^\\top}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k)(\\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K) \\in \\mathbb{R}^{\\frac{D}{H} \\times \\frac{D}{H}}$ for $i\\neq j$, which is related to $\\tilde{J}_{ij}$ as follows by Equation \\eqref{eq:jij}:\n\n\\begin{equation*}\n    W^{K^\\top} \\tilde{J}_{ij} = \\left(\\frac{2}{\\sqrt{D/H}} \\tilde{K}_{ij} + P_{ij}I \\right)\n    W^{K^\\top}.\n\\end{equation*}\n\nIt suffices to show that $\\tilde{K}_{ij}$ is unbounded to show that $\\tilde{J}_{ij}$ is unbounded, since $W^K$ is full rank and $P_{ij} \\in [0,1]$. \n\nLet $\\mathbf{y}_j^\\top = \\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K$. \nThen we have:\n\\begin{align*}\n    \\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k \n    &= W^{Q^\\top}\\mathbf{x}_i - W^{K^\\top}\\mathbf{x}_j - \\sum_k P_{ik} (W^{Q^\\top}\\mathbf{x}_i - W^{K^\\top}\\mathbf{x}_k)\\\\\n    &= W^{Q^\\top}\\mathbf{x}_i - W^{K^\\top}\\mathbf{x}_j - (W^{Q^\\top}\\mathbf{x}_i - \\sum_k P_{ik} W^{K^\\top}\\mathbf{x}_k) \\\\\n    &= - W^{K^\\top}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k).\n\\end{align*}\nHence $\\tilde{K}_{ij} = - P_{ij} (\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k) \\mathbf{y}_j^\\top$.\nNote $\\mathbf{y}_i$ can take an arbitrary value in $\\mathbb{R}^{D/H}$, since $W^K \\neq W^Q$ and $W^K$ is full-rank.\n\nFor all $j \\neq i$, let us choose $\\mathbf{x}_j$ such that $\\mathbf{y}_j = -\\mathbf{y}_i$. This is possible for any value of $\\mathbf{y}_i$ since $W^K$ is full-rank.\nNote $\\mathbf{y}_j = - \\mathbf{y}_i$ and not $\\mathbf{y}_i$.\nWe then have that $\\|\\mathbf{y}_j\\|_2^2$ is equal for all $j$, hence $P_{ij} \\coloneqq \\frac{\\exp(-\\|\\mathbf{y}_j\\|_2^2)}{\\sum_k \\exp(-\\|\\mathbf{y}_k\\|_2^2)} = \\frac{1}{N}$ for all $j$. \nThen for $i \\neq j$, $\\tilde{K}_{ij}$ simplifies to\n\\begin{equation*}\n\\tilde{K}_{ij} = - \\frac{1}{N} \\left(-\\mathbf{y}_i - \\frac{1}{N} (N-2) (-\\mathbf{y}_i)\\right) (-\\mathbf{y}_i)^\\top  = - \\frac{2N-2}{N^2} \\mathbf{y}_i \\mathbf{y}_i^\\top \n\\end{equation*}\nwhose entries are unbounded since $\\mathbf{y}_i$ can be any vector in $\\mathbb{R}^{D/H}$ (note we assume $N \\geq 2$ for self-attention to be well-defined, hence $2N-2 \\neq 0$).\n\\end{proof}",
        "metadata": {
            "Statement label": "Lemma 2",
            "Statement title": "Non-Lipschitz Condition for Distinct Full Rank Matrices"
        }
    },
    {
        "text": "\\begin{lemma} \\label{lemma:key}\n$\\Tr(\\mathrm{Cov}(\\mathbb{Y})) = \\sum_j P_{ij}\\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_2^2 \\leq \\sum_j P_{ij}\\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2 \\leq \\phi^{-1}(N-1)$ where $\\phi(c) = c \\exp(c+1)$ is a one-dimensional invertible function on $\\mathbb{R}_{\\geq 0}$.\n\\end{lemma}\nProof:\n\\begin{proof}\nThe first equality holds since $\\Tr(\\mathrm{Cov}(\\mathbb{Y})) = \\sum_j \\mathrm{Cov}(\\mathbb{Y})_{jj} = \\sum_j \\mathrm{Var}(\\mathbb{Y}_j) = \\sum_j \\mathbb{E}[(\\mathbb{Y}_j -\\mathbb{E}[\\mathbb{Y}_j])^2]$. \nThe next inequality holds since $\\mathrm{Var}(\\mathbb{Y}_j) = \\mathrm{Var}(\\overline{\\mathbb{Y}}_j) = \\mathbb{E}[\\overline{\\mathbb{Y}}_j^2] -\\mathbb{E}[\\overline{\\mathbb{Y}}_j]^2 \\leq \\mathbb{E}[\\overline{\\mathbb{Y}}_j^2]$ where $\\overline{\\mathbb{Y}}= \\mathbb{Y} - y_i$. \nThe final inequality can be proved as follows.\n\nWe would like to bound \n\\begin{equation}\n    \\sum_j P_{ij}\\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2 = \\frac{\\sum_j \\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2 \\exp(-\\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2)}{\\sum_k \\exp(-\\|\\mathbf{y}_k-\\mathbf{y}_i\\|_2^2)}  = \\frac{\\sum_j z_j^2 \\exp(-z_j^2)}{\\sum_k \\exp(-z_k^2)}\n\\end{equation}\nwhere $z_j \\coloneqq \\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2$ (hence $z_i=0$). \nDefine:\n\\begin{equation}\n    g(\\mathbf{z}) \\coloneqq \\frac{\\sum_j z_j^2 \\exp(-z_j^2)}{\\sum_k \\exp(-z_k^2)} = \\frac{\\sum_{j \\neq i} z_j^2 \\exp(-z_j^2)}{1 + \\sum_{k \\neq i} \\exp(-z_k^2)}.\n\\end{equation}\nFirst note that as $z_j \\rightarrow \\infty$, $\\exp(-z_j^2) \\rightarrow 0$ exponentially fast, causing the product $z_j^2 \\exp(-z_j^2) \\rightarrow 0$.\nHence we expect the above quantity to be bounded and attain its maximum.\n\nLet $h(z_j) \\coloneqq \\exp(-z_j^2)$ for notational conciseness, and note $h(z_j) > 0$. By taking partial derivatives with the chain rule, we have that for $j \\neq i$\n\\begin{equation}\n\\frac{\\partial g(\\mathbf{z})}{\\partial z_j} = \\frac{2z_j h(z_j)}{(\\sum_k h(z_k))^2}\\left[(1-z_j^2)\\sum_k h(z_k) + \\sum_k h(z_k)z_k^2\\right].\n\\end{equation}\nHence the derivative is $0$ if and only if $z_j = 0$ or $(1-z_j^2)\\sum_k h(z_k) + \\sum_k h(z_k)z_k^2 = 0$, the latter being equivalent to $z_j^2 = 1 + \\frac{\\sum_k h(z_k)z_k^2}{\\sum_k h(z_k)} = 1 + g(\\mathbf{z})$. \nHence at the maximum, the non-zero values among $\\{z_j\\}_{j=1}^N$ must be equal to one another.\nIt is clear now that the maximum value $c$ is attained when $z_j^2 = 1 + c$ for $j \\neq i$ (and recall $z_i = 0$). \nSo $h(z_j) = \\exp(-1-c)$ for $j \\neq i$.\nSubstituting this into $g(z)$, and rearranging, we obtain $c \\exp(c+1) = N - 1$. Note $\\phi(x) \\coloneqq x \\exp(x+1)$ is increasing for $x > 0$ hence $c = \\phi^{-1}(N-1)$.\n\\end{proof}",
        "metadata": {
            "Statement label": "Lemma 3",
            "Statement title": "Covariance Trace Bound"
        }
    },
    {
        "text": "\\begin{lemma}\\label{lemma:f3}\n$\\|Y^\\top P^{(i)}Y\\|_\\infty \\leq  \\phi^{-1}(N-1) \\sqrt{D/H} $ ($\\phi$ defined as in Lemma \\ref{lemma:key}).\n\\end{lemma}\nProof:\n\\begin{proof}\nRecall that $Y^\\top P^{(i)}Y = \\mathrm{Cov}(\\mathbb{Y})$. Let $\\sigma(\\mathbb{Y}_m)$ denote the standard deviation of $\\mathbb{Y}_m$. Then $[\\mathrm{Cov}(\\mathbb{Y})]_{lm} \\leq \\sigma(\\mathbb{Y}_l)\\sigma(\\mathbb{Y}_m)$.\nHence \n\\begin{align*}\n\\|\\mathrm{Cov}(\\mathbb{Y})\\|_{\\infty} = \\max_l \\sum_m \\left|[\\mathrm{Cov}(\\mathbb{Y})]_{lm}\\right| \n& \\leq  \\max_l \\sigma(\\mathbb{Y}_l) \\sum_m \\sigma(\\mathbb{Y}_m) \\\\\n& \\leq \\sqrt{\\frac{D}{H}} \\sum_m \\sigma^2(\\mathbb{Y}_m)  = \\sqrt{\\frac{D}{H}} \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\\\\n& \\leq \\sqrt{\\frac{D}{H}} \\phi^{-1}(N-1),\n\\end{align*}\nsince $\\sum_m \\sigma(\\mathbb{Y}_m) \\leq \\sqrt{\\frac{D}{H}} \\sqrt{\\sum_m \\sigma^2(\\mathbb{Y}_m)}$ (by e.g.~using the Cauchy--Schwartz inequality on $[\\sigma(\\mathbb{Y}_1), \\ldots, \\sigma(\\mathbb{Y}_{D/H})]$ and $\\mathds{1}$) and $\\max_l \\sigma(\\mathbb{Y}_l) \\leq \\sqrt{\\sum_m \\sigma^2(\\mathbb{Y}_m)}$, and the last inequality is from Lemma \\ref{lemma:key}. \n\\end{proof}",
        "metadata": {
            "Statement label": "Lemma 4",
            "Statement title": "Matrix Norm Bound Lemma"
        }
    },
    {
        "text": "\\begin{lemma} \\label{lemma:low_rank}\n$\\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty \\leq  \\phi^{-1}(N-1) \\sqrt{D/H}$.\n\\end{lemma}\nProof:\n\\begin{proof} \\label{lemma:f4}\nNote $\\|\\mathbf{u}\\mathbf{v}^\\top\\|_{\\infty} = \\|\\mathbf{u}\\|_{\\infty} \\|\\mathbf{v}\\|_1$ for real vectors $\\mathbf{u},\\mathbf{v}$. Hence\n\\begin{align*}\n    \\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty & = \\sum_j P_{ij} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_\\infty \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_1 \\\\\n    & = \\mathbf{a}^\\top \\mathbf{b} \\leq \\|\\mathbf{a}\\|_2 \\|\\mathbf{b}\\|_2,\n\\end{align*}\nwhere $a_j = \\sqrt{P_{ij}} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_\\infty$, $b_j = \\sqrt{P_{ij}} \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_1$.\n\nNote $a_j \\leq c_j \\coloneqq  \\sqrt{P_{ij}} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_2$ since $\\|\\mathbf{u}\\|_\\infty \\leq \\|\\mathbf{u}\\|_2$ for vector $\\mathbf{u}$. Hence $\\|\\mathbf{a}\\|_2 \\leq \\|\\mathbf{c}\\|_2$.\n\nAlso $b_j \\leq \\sqrt{\\frac{D}{H}} d_j \\coloneqq  \\sqrt{\\frac{D}{H}} \\sqrt{P_{ij}} \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_2$ since $\\|\\mathbf{u}\\|_1 \\leq \\sqrt{\\frac{D}{H}}\\|\\mathbf{u}\\|_2$ for $\\mathbf{u} \\in \\mathbb{R}^{D/H}$ (e.g.~by the Cauchy--Schwartz inequality on $[|\\mathbf{u}_1|, \\ldots, |\\mathbf{u}_{D/H}|]$ and $\\mathds{1}$). Hence $\\|b\\|_2 \\leq \\sqrt{\\frac{D}{H}}\\|d\\|_2$.\n\nNote $\\|c\\|_2^2 = \\sum_j P_{ij} \\|y_j - \\sum_k P_{ik} y_k\\|_2^2 = \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\phi^{-1}(N-1)$ from Lemma \\ref{lemma:key},\nand $\\|d\\|_2^2 =  \\sum_j P_{ij} \\|y_i - y_j\\|_2^2 \\leq  \\phi^{-1}(N-1)$ also from Lemma \\ref{lemma:key}.\nHence $\\|a\\|_2 \\|b\\|_2 \\leq \\sqrt{\\frac{D}{H}} \\|c\\|_2 \\|d\\|_2 \\leq \\sqrt{\\frac{D}{H}} \\phi^{-1}(N-1)$.\n\\end{proof}",
        "metadata": {
            "Statement label": "Lemma 5",
            "Statement title": "Low-Rank Approximation Bound"
        }
    },
    {
        "text": "\\begin{lemma} \\label{lemma:block_rows}\nLet A be a block matrix with block rows $A_1, \\ldots, A_N$. Then $\\|A\\|_2 \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2}$, and equality holds if and only if the first right singular vectors of the $A_i$ align.\n\\end{lemma}\nProof:\n\\begin{proof}\n\\begin{equation*}\n\\|A\\|_2^2 = \\left\\Vert \\begin{bmatrix} A_1 \\\\ \\vdots \\\\ A_N \\\\ \\end{bmatrix}\\right\\Vert_2^2 = \\sup_{\\|\\mathbf{x}\\|_2=1} \\left\\Vert\\begin{bmatrix} A_1 \\\\ \\vdots \\\\ A_N \\\\ \\end{bmatrix} \\mathbf{x}\\right\\Vert_2^2 = \\sup_{\\|\\mathbf{x}\\|_2=1} \\sum_i \\|A_i \\mathbf{x}\\|_2^2 \\leq \\sum_i \\sup_{\\|\\mathbf{x}\\|_2=1} \\|A_i \\mathbf{x}\\|_2^2 = \\sum_i \\|A_i\\|_2^2.\n\\end{equation*}\nNote that equality holds if and only if the first right singular vectors of the $A_i$ align.\n\\end{proof}",
        "metadata": {
            "Statement label": "Lemma 6",
            "Statement title": "Block Row Norm Inequality"
        }
    },
    {
        "text": "\\begin{lemma} \\label{lemma:f6}\n$\\|Y^\\top P^{(i)}Y\\|_2 \\leq \\phi^{-1}(N-1)$\n\\end{lemma}\nProof:\n\\begin{proof}\n$\\|Y^\\top P^{(i)}Y\\|_2=\\|\\mathrm{Cov}(\\mathbb{Y})\\|_2 = \\lambda_{\\max}(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\phi^{-1}(N-1)$, where the first equality holds by symmetry of $\\mathrm{Cov}(\\mathbb{Y})$ and the next holds by $\\mathrm{Cov}(\\mathbb{Y})$ being positive semi-definite, so all its eigenvalues are non-negative, and hence the maximal eigenvalue is bounded by the sum of the eigenvalues, equal to its trace. The final inequality is from Lemma \\ref{lemma:key}.\n\\end{proof}",
        "metadata": {
            "Statement label": "Lemma 7",
            "Statement title": "Matrix Norm Bound"
        }
    },
    {
        "text": "\\begin{lemma} \\label{lemma:f7}\n$\\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_2 \\leq  \\phi^{-1}(N-1)$\n\\end{lemma}\nProof:\n\\begin{proof}\nDirectly use Cauchy--Schwartz on $c$ and $d$ in the proof of Lemma \\ref{lemma:low_rank}. \n\\end{proof}",
        "metadata": {
            "Statement label": "Lemma 8",
            "Statement title": "Bounding Sum of Projected Differences"
        }
    },
    {
        "text": "\\begin{lemma} \\label{lemma:block_cols}\nLet A be a block matrix with block columns $A_1, \\ldots, A_N$. Then $\\|A\\|_2 \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2}$.\n\\end{lemma}\nProof:\n\\begin{proof}\n\\begin{align*}\n\\|A\\|_2 &= \\|[A_1, \\ldots, A_N]\\|_2 = \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1}  \\left\\Vert [A_1, \\ldots, A_N] \\begin{bmatrix} \\mathbf{x}_1\\\\ \\vdots \\\\ \\mathbf{x}_N \\\\ \\end{bmatrix} \\right\\Vert_2^2 = \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1} \\|\\sum_i A_i \\mathbf{x}_i\\|_2 \\\\ \n& \\leq \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1} \\sum_i \\|A_i \\mathbf{x}_i\\|_2 = \\sup_{\\|\\mathbf{e}_i\\|_2=1, \\sum_i \\lambda_i^2 =1} \\sum_i \\lambda_i \\|A_i \\mathbf{e}_i\\|_2 = \\sup_{\\sum_i \\lambda_i^2 =1} \\sum_i \\lambda_i \\|A_i\\|_2 \\\\\n& \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2},\n\\end{align*}\nwhere we are using the substitution $\\mathbf{x}_i = \\lambda_i \\mathbf{e}_i$, and the last inequality holds by e.g.~Cauchy--Schwartz inequality on $[\\lambda_1, \\ldots, \\lambda_N]$ and $[\\|A_1\\|_2, \\ldots, \\|A_N\\|_2]$.\n\\end{proof}",
        "metadata": {
            "Statement label": "Lemma 9",
            "Statement title": "Block Column Norm Inequality"
        }
    },
    {
        "text": "\\begin{theorem}[\\citealp{federer1969geometric}] \\label{thm:jacobian} Let $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ be differentiable and Lipschitz continuous under a choice of $p$-norm $\\|\\cdot\\|_p$. \nLet $J_f(x)$ denote its total derivative (Jacobian) at $x$. Then $\\lip_p(f) = \\sup_{\\mathbf{x}\\in \\mathbb{R}^n} \\|J_f(\\mathbf{x})\\|_p$ where $\\|J_f(\\mathbf{x})\\|_p$ is the induced operator norm on $J_f(\\mathbf{x})$. \n\\end{theorem}\nProof:\nSee proof at:\nFederer, \\textit{H. Geometric Measure Theory}. Classics in Mathematics. Springer Berlin Heidelberg, 1969. ISBN 9783642620102.",
        "metadata": {
            "Statement label": "Theorem 1",
            "Statement title": "Jacobian Lipschitz Norm Theorem"
        }
    },
    {
        "text": "\\begin{theorem} \\label{thm:dp_not_lipschitz}\n\\verb!DP-MHA! is not Lipschitz for any vector $p$-norm $\\|\\cdot\\|_p$ with $p \\in [1, \\infty]$.\n\\end{theorem}\nProof:\n\\begin{proof}\nThe mapping $f$ can be written as\n\\vspace{-5mm}\n\\begin{equation}\nf(X) = PX = \\softmax{X A^\\top X^\\top} X = \\begin{bmatrix}\n    f_1(X)^\\top \\\\\n    \\vdots \\\\\n    f_N(X)^\\top\n\\end{bmatrix} \\in \\mathbb{R}^{N \\times D},\n\\end{equation}\nwhere $A = W^K W^{Q^\\top} / \\sqrt{D/H} \\in \\mathbb{R}^{D \\times D}$ and\n$f_i(X) = \\sum_{j=1}^N P_{ij}\\mathbf{x}_j$ with $P_{i:}^\\top = \\softmax{XA\\mathbf{x}_i}$.\nHence $f$ can be interpreted as a map of each $\\mathbf{x}_i$ to a point in the convex hull of ${\\mathbf{x}_1,...,\\mathbf{x}_N}$.\nSince $f$ is a map from $\\mathbb{R}^{N \\times D}$ to $\\mathbb{R}^{N \\times D}$, its Jacobian is\n\\begin{equation}\n    J_f = \\begin{bmatrix}\n    J_{11} & \\dots & J_{1N} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    J_{N1} & \\dots & J_{NN} \\\\\n    \\end{bmatrix}\\in \\mathbb{R}^{ND \\times ND},\n\\end{equation}\nwhere $J_{ij} = \\frac{\\partial f_i(X)}{\\partial \\mathbf{x}_j} \\in \\mathbb{R}^{D \\times D}$. \nBy taking partial derivatives we can show that $J_{ij} = X^\\top P^{(i)} \\left[E_{ji}XA^\\top + XA\\delta_{ij}\\right] + P_{ij}I$\nwhere $E_{ij} \\in \\mathbb{R}^{N \\times N}$ is a binary matrix with zeros everywhere except the $(i,j)$th entry, $\\delta_{ij}$ is the Kronecker delta, and $P^{(i)} \\coloneqq \\diag(P_{i:}) - P_{i:}^\\top P_{i:}$.\nSo for $i=j$:\n\\begin{align}\nJ_{ii} &=X^\\top P^{(i)}E_{ii}XA^\\top + X^\\top P^{(i)}XA + P_{ii}I \\nonumber \\\\\n&= P_{ii}\\left(\\mathbf{x}_i - \\textstyle\\sum_k P_{ik} \\mathbf{x}_k\\right)\\mathbf{x}_i^\\top A^\\top + X^\\top P^{(i)}XA + P_{ii}I. \\label{eq:jac_dot_general}\n\\end{align}\nFor the last equality, note $E_{ii}X$ has all rows equal to zero except for the $i$th row given by $\\mathbf{x}_i^\\top$. We can then verify that $X^\\top P^{(i)}E_{ii}X$ simplifies to $P_{ii}(\\mathbf{x}_i - \\sum_k P_{ik} \\mathbf{x}_k)\\mathbf{x}_i^\\top$.\n\nFor vector $p$-norms, $\\|J_f\\|_p$ is bounded if and only if its entries are bounded, by definition of the operator norm. \nThe entries of $X^\\top P^{(i)}XA$ are bounded for arbitrary $A$ only if the entries of $X^\\top P^{(i)}X$ are bounded.\nSo let us investigate the entries of this $D\\times D$ matrix. \nWriting out each term of the matrix, we observe that it is in fact a covariance matrix of a discrete distribution. Specifically:\n\\begin{equation} \\label{eq:cov_general}\n    [X^\\top P^{(i)}X]_{lm}  = \\textstyle\\sum_k P_{ik} x_{kl} x_{km} - \\left(\\textstyle\\sum_k P_{ik}  x_{kl}\\right)\\left(\\textstyle\\sum_k P_{ik} x_{km}\\right) = \\mathrm{Cov}(\\mathbb{X}_l,\\mathbb{X}_m),\n\\end{equation}\nwhere $\\mathbb{X}$ is a discrete distribution with support at the inputs $\\{\\mathbf{x}_1,\\ldots,\\mathbf{x}_N \\}$ and probability mass function given by their softmax probabilities $\\mathbb{P}(\\mathbb{X}=\\mathbf{x}_j)=P_{ij}$. \nA consequence of this interpretation is that $P^{(i)}$ is \\textit{positive semi-definite} (PSD) since for $D=1$, Equation \\eqref{eq:cov_general} becomes $X^\\top P^{(i)} X = \\mathrm{Var}(\\mathbb{X}) \\geq 0$, with equality if and only if the $\\mathbf{x}_j$ are all equal.\n\nWe use this observation to show that the terms of $J_{ii}$ are unbounded, and so \\verb!DP-MHA! is \\emph{not} Lipschitz.\nConsider the case $\\mathbf{x}_i=0$. Then $P_{i:}^\\top = \\softmax{XA\\mathbf{x}_i} = \\frac{1}{N} \\mathds{1}$, i.e.\\ we have uniform attention regardless of $\\mathbf{x}_{ \\neq i}$. \nThe first term of $J_{ii}$ in Equation \\eqref{eq:jac_dot_general} disappears since $\\mathbf{x}_i=\\mathbf{0}$, and the last term becomes $\\frac{1}{N} I$. For the second term, the entries $[X^\\top P^{(i)}X]_{ll} = \\mathrm{Var}(\\mathbb{X}_l)$ are unbounded since the latter is equal to the sample variance of ${x_{1l},\\ldots,x_{Nl}}$, which can be arbitrarily large.\n\nNote that we have shown that single head dot-product self-atttention ($H=1$) is not Lipschitz, but it is clear that this implies multihead self-attention \\verb!DP-MHA! is also not Lipschitz, since the output of multihead attention is a linear combination of the outputs of each head.\n\\end{proof}",
        "metadata": {
            "Statement label": "Theorem 2",
            "Statement title": "Non-Lipschitzness of DP-MHA"
        }
    },
    {
        "text": "\\begin{theorem} \\label{thm:main}\n\\verb!L2-MHA! is Lipschitz, with the following bound on $\\lip_{\\infty}(F)$:\n\\begin{align*}\n    \\lip_{\\infty}(F)  \\leq &\\left(4 \\phi^{-1}(N-1) + \\frac{1}{\\sqrt{D/H}}\\right) \\|W^{O^\\top}\\|_{\\infty} \\\\\n    &\\max_h \\|W^{Q,h}\\|_{\\infty} \\|W^{{Q,h}^\\top}\\|_{\\infty} \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty} \n\\end{align*}\nand the following bound on $\\lip_{2}(F)$:\n\\begin{align*}\n    \\lip_2(F) \\leq & \\frac{\\sqrt{N}}{\\sqrt{D/H}}\n    \\left(4 \\phi^{-1}(N-1) + 1 \\right) \\\\ \n    & \\left(\\sqrt{\\textstyle\\sum_h \\|W^{Q,h}\\|_2^2\\, \\|W^{V,h}\\|_2^2}\\right) \\|W^O\\|_2 \n\\end{align*}\nwhere $\\phi(x) \\coloneqq x\\exp(x+1)$ is an invertible univariate function on $x > 0$, and $N$ is the input sequence length.\n\nSpecifically, $\\phi^{-1}(N-1) = W_0(\\frac{N}{e})$ where $W_0$ is the Lambert $W$-function, which grows sub-logarithmically as $O(\\log N - \\log \\log N)$ \\citep{corless1996lambertw}. Hence the above bounds can be simplified to $O(\\log N)$ for $p=\\infty$ and $O(\\sqrt{N} \\log N)$ for $p=2$.\n\\end{theorem}\nProof:\n\\subsubsection{Upper bound on $\\boldsymbol{\\lip_{\\infty}(F)}$ for L2-MHA}\n\nConsider the choice $p=\\infty$, where $\\|J_f\\|_\\infty$ is the maximum absolute row sum of $J_f$. \nA key observation is that if we can bound the $\\infty$-norm of the Jacobian of $f_i$, a single output of $f$, (i.e.~a single block row $\\|[J_{i1},...,J_{iN}]\\|_\\infty$ of $J_f$) then this is also a bound on $\\|J_f\\|_{\\infty}$ due to permutation equivariance of self-attention; all block rows have the same maximal $\\|\\cdot\\|_\\infty$ when each is optimised over the input $X$. \nUsing this, we can prove that $\\|J_f\\|_\\infty$ admits an upper bound that is $O(\\log N - \\log \\log N)$. Below we state and prove lemmas that lead to the proof of this upper bound.\n\nFirst we analyse the term $\\sqrt{A}^\\top X^\\top P^{(i)}X \\sqrt{A}$, that appears in the first term of $J_{ii}$. \nNote that for $Y \\coloneqq X \\sqrt{A}$, so that the rows of $Y$ are $\\mathbf{y}_i^\\top \\coloneqq \\mathbf{x}_i^\\top \\sqrt{A}$, we have \n\\begin{equation}\n    \\sqrt{A}^\\top X^\\top P^{(i)}X \\sqrt{A}= Y^\\top P^{(i)} Y =  \\mathrm{Cov}(\\mathbb{Y})\n\\end{equation}\nwhere $\\mathbb{P}(\\mathbb{Y}=\\mathbf{y}_j)=P_{ij} = \\exp(-\\|\\mathbf{y}_j - \\mathbf{y}_i\\|^2_2)/\\sum_k \\exp(-\\|\\mathbf{y}_k - \\mathbf{y}_i\\|^2_2)$.\nThe last equality uses the observation in Equation \\eqref{eq:cov}.\n\nThe central inequality used throughout the proof of the main theorem is the following:\n[Lemma \\ref{lemma:key}]\n\nNote $\\phi(\\log N) = (\\log N) \\exp(\\log N + 1) \\geq N \\log N \\geq N -1$ for $N \\geq 3$. Since $\\phi$ is increasing, we have $\\phi^{-1}(N-1) \\leq \\log(N)$ for $N \\geq 3$. In fact, it is known that $\\phi^{-1}(N-1) = O(\\log N - \\log \\log N)$ \\citep{corless1996lambertw}.\n\nNote the $A$ term in $f(X) = \\tilde{f}(X) A$ allows us to use the above inequality, since $Y^\\top P^{(i)}Y = \\mathrm{Cov}(\\mathbb{Y})$ now appears in the terms of $J_f$:\n\\begin{align}\n    J_{ii}\n    &= 2 \\sqrt{A} [Y^\\top P^{(i)}Y]\\sqrt{A}^\\top + P_{ii} A,  \\\\ \n    J_{ij},\n    &= 2 \\sqrt{A} P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top \\sqrt{A}^\\top + P_{ij} A  \\hspace{2mm} \\text{for $i \\neq j$}.\n\\end{align}\n\nUsing the inequalities $\\|BC\\| \\leq \\|B\\| \\|C\\|$, $\\|B + C\\| \\leq \\|B\\| + \\|C\\|$ and $\\|[A_1, \\ldots, A_N]\\| \\leq \\sum_i \\|A_i\\|$, we have: \n\\begin{align*}\n\\|[J_{i1} &, \\ldots, J_{iN}]\\|_{\\infty}  \\\\\n\\leq & \\|J_{ii}\\|_{\\infty} + \\sum_{j \\neq i} \\|J_{ij}\\|_{\\infty} \\\\\n  \\leq & 2 \\|\\sqrt{A}\\|_{\\infty} \\|Y^\\top P^{(i)}Y\\|_{\\infty} \\|\\sqrt{A}^\\top\\|_{\\infty} + P_{ii} \\|A\\|_{\\infty} \\\\\n & + 2 \\sum_{j \\neq i} \\|\\sqrt{A}\\|_{\\infty} \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty \\|\\sqrt{A}^\\top\\|_{\\infty} + P_{ij} \\|A\\|_{\\infty}\\\\\n  = & 2  \\|\\sqrt{A}\\|_{\\infty}\\|\\sqrt{A}^\\top\\|_{\\infty} \n\\bigg(\\|Y^\\top P^{(i)}Y\\|_\\infty \n + \\sum_{j\\neq i} \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty\\bigg) + \\|A\\|_{\\infty} \\\\\n = & 2  \\frac{\\|W^{Q}\\|_{\\infty}\\|W^{Q^\\top}\\|_{\\infty}}{\\sqrt{D/H}} \n\\bigg(\\|Y^\\top P^{(i)}Y\\|_\\infty \n + \\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty\\bigg) + \\frac{\\|W^Q W^{Q^\\top}\\|_{\\infty}}{\\sqrt{D/H}}.\n\\end{align*}\nFor the first equality, note that $\\sum_j P_{ij}=1$. For the second equality, note that the summand for $j=i$ is $0$ because the term $\\mathbf{y}_i - \\mathbf{y}_j=\\mathbf{0}$. \nEach of the terms in the brackets are bounded by the following lemmas:\n[Lemma \\ref{lemma:f3}]\n[Lemma \\ref{lemma:f4}]\n\nPutting the above lemmas altogether, with the observation $\\sup_X \\|J_f(X)\\|_\\infty = \\sup_X \\|[J_{i1}(X), \\ldots, J_{iN}(X)]\\|_\\infty$ by permutation invariance of $\\|J_f\\|_\\infty$ (since $f$ is permutation equivariant and $\\|\\cdot\\|_\\infty$ is the maximum absolute row sum), we have\n\\begin{align}\n\\|J_f\\|_{\\infty}\n& \\leq 4\\|W^Q\\|_{\\infty}\\|W^{Q^\\top}\\|_{\\infty} \\phi^{-1}(N-1)\n+ \\frac{\\|W^Q W^{Q^\\top}\\|_{\\infty}}{\\sqrt{D/H}} \\nonumber\\\\\n& \\leq \\|W^Q\\|_{\\infty}\\|W^{Q^\\top}\\|_{\\infty} \\left(4\\phi^{-1}(N-1) + \\frac{1}{\\sqrt{D/H}}\\right) \\label{ineq:infty}\\\\\n& \\leq \\|W^Q\\|_{\\infty} \\|W^{Q^\\top}\\|_{\\infty} \\left(4\\log N + \\frac{1}{\\sqrt{D/H}}\\right), \\nonumber\n\\end{align}\nwhere the last inequality holds for $N \\geq 3$.\n\nThe full multihead attention map that combines the heads $f^h(X)$ is:\n\\begin{equation*}\nF: X \\mapsto \\left[f^1(X)W^{V,1}, \\ldots f^H(X)W^{V,H}\\right] W^O = g(X) W^V W^O\n\\end{equation*}\nwhere $g:X \\mapsto [f^1(X),\\ldots,f^H(X)]$, $W^O \\in \\mathbb{R}^{D \\times D}$ and\n\\begin{equation*}\n    W^V = \\begin{bmatrix}\n    W^{V,1} & \\dots & 0 \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & W^{V,H} \\\\\n    \\end{bmatrix} \\in \\mathbb{R}^{DH \\times D}.\n\\end{equation*}\nNote the Jacobian $J_g$ is a block matrix whose rows are $J_{f^h}$, hence $\\|J_g\\|_{\\infty} = \\max_h \\|J_{f^h}\\|_{\\infty}$, and similarly $\\|W^{V^\\top}\\|_{\\infty} = \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty}$. Hence we have\n\\begin{equation*}\n    \\lip_{\\infty}(F) \\leq \\max_h \\|J_{f^h}\\|_{\\infty} \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty} \\|W^{O^\\top}\\|_{\\infty}.\n\\end{equation*}\n\nCombining this with Inequality (\\ref{ineq:infty}), we have:\n\\begin{equation*}\n    \\lip_{\\infty}(F)  \\leq \\left(4 \\phi^{-1}(N-1) + \\frac{1}{\\sqrt{D/H}}\\right) \\max_h \\|W^{Q,h}\\|_{\\infty} \\|W^{{Q,h}^\\top}\\|_{\\infty} \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty} \\ \\|W^{O^\\top}\\|_{\\infty}.\n\\end{equation*}\n\n\n\\subsubsection{Upper bound on $\\boldsymbol{\\lip_2(F)}$ for L2-MHA}\nFor $p=2$, we use the following lemma:\n[\ref{lemma:block_rows}]\nHence a bound on the spectral norm of each block row of $J_f$ can give us an $O(\\sqrt{N})$ bound on $\\|J_f\\|_2$, which may be loose, and it remains an open question as to whether this bound can be tightened.\n\nTo bound the $\\|\\cdot\\|_2$ norm of each row of $J_f$, we use the following lemmas:\n[\ref{lemma:f6}]\n[\ref{lemma:f7}]\n\nAgain using the inequalities $\\|BC\\| \\leq \\|B\\| \\|C\\|$, $\\|B + C\\| \\leq \\|B\\| + \\|C\\|$ and $\\|[A_1, \\ldots, A_N]\\| \\leq \\sum_i \\|A_i\\|$, with the additional equality $\\|B^\\top\\|_2 = \\|B\\|_2$, we have the bound: \n\\begin{align*}\n&\\|[J_{i1}, \\ldots, J_{iN}]\\|_2 \\\\\n & \\leq  2  \\frac{\\|W^Q\\|_2\\|W^{Q^\\top}\\|_2}{\\sqrt{D/H}} \n\\bigg(\\|Y^\\top P^{(i)}Y\\|_2\n + \\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_2 \\bigg) + \\frac{\\|W^Q W^{Q^\\top}\\|_2}{\\sqrt{D/H}} \\\\\n & \\leq  4\\phi^{-1}(N-1) \\frac{\\|W^Q\\|_2^2}{\\sqrt{D/H}}  + \\frac{\\|W^Q W^{Q^\\top}\\|_2}{\\sqrt{D/H}} \\\\\n & \\leq  \\frac{\\|W^Q\\|_2^2}{\\sqrt{D/H}} \\bigg(4\\phi^{-1}(N-1)+1 \\bigg).\n\\end{align*}\nUsing Lemma \\ref{lemma:block_rows}, we have that\n\\begin{align}\n    \\|J_f\\|_2 & \\leq \\frac{\\sqrt{N}\\|W^Q\\|_2^2}{\\sqrt{D/H}} \\bigg(4\\phi^{-1}(N-1)+1 \\bigg) \\label{ineq:2} \\\\\n    & \\leq \\frac{\\sqrt{N}\\|W^Q\\|_2^2}{\\sqrt{D/H}}(4\\log N+1). \\nonumber\n\\end{align}\nTo obtain the final result for the full multihead self-attention $F$, we need a final lemma:\n[\ref{{lemma:block_cols}}]\nRecall that \n\\begin{equation*}\nF: X \\mapsto \\left[f^1(X)W^{V,1}, \\ldots, f^H(X)W^{V,H}\\right] W^O.\n\\end{equation*}\nSince $\\|f^h(X)W^{V,h}\\|_2 \\leq \\|J_{f^h}\\|_2 \\|W^{V,h}\\|_2$, by Lemma \\ref{lemma:block_cols} we have that\n\\begin{equation*}\n    \\left\\|[f^1(X)W^{V,1}, \\ldots, f^H(X)W^{V,H}]\\right\\|_2 \\leq \\sqrt{\\sum_h \\|J_{f^h}\\|_2^2 \\|W^{V,h}\\|_2^2}\n\\end{equation*} and hence\n\\begin{equation}\n    \\lip_2(F) \n    \\leq \\left(\\sqrt{\\sum_h \\|J_{f^h}\\|_2^2 \\|W^{V,h}\\|_2^2}\\right) \\|W^O\\|_2.\n\\end{equation}\nCombining this with Inequality (\\ref{ineq:2}), we have:\n\\begin{equation*}\n    \\lip_2(F) \\leq \\frac{\\sqrt{N}}{\\sqrt{D/H}}\n    \\left(4 \\phi^{-1}(N-1) + 1 \\right) \\left(\\sqrt{\\textstyle\\sum_h \\|W^{Q,h}\\|_2^2\\, \\|W^{V,h}\\|_2^2}\\right) \\|W^O\\|_2.\n\\end{equation*}\n",
        "metadata": {
            "Statement label": "Theorem 3",
            "Statement title": "Lipschitz Bound for L2-MHA"
        }
    },
    {
        "text": "\\begin{corollary} \\label{cor:lip_conv}\nFor a fully-connected network (\\verb!FCN!) or a convolutional neural network (\\verb!CNN!) $f=W_K \\circ \\rho_{K-1} \\circ W_{K-1} \\circ \\ldots \\circ \\rho_1 \\circ W_1$, we have $\\lip_p(f) \\leq \\prod_k \\|W_k\\|_p$ under a choice of $p$-norm with $1$-Lipschitz non-linearities $\\rho_k$.\n\\end{corollary}",
        "metadata": {
            "Statement label": "Corollary 1",
            "Statement title": "Lipschitz Bound for Neural Networks"
        }
    },
    {
        "text": "\\begin{mdframed}[backgroundcolor=red!5]\n\\textbf{[Definition] ~ Semantic subspace:} any independent $N_\\alpha$-dimensional subspace $\\mathbb{S}_\\alpha^{N_\\alpha} \\subset \\mathbb{S}^N$ for which every element may be uniquely identified by some parameters $\\theta_\\alpha$, such that it is possible for the attention scores $w_t$ to be fully specified by $\\theta_\\alpha$.\n\n\\end{mdframed}",
        "metadata": {
            "Statement label": "Definition 2",
            "Statement title": "Semantic Subspace"
        }
    },
    {
        "text": "\\begin{mdframed}[backgroundcolor=red!5]\n\\textbf{[Definition] ~ Semantic separability:} ability for parallel heads to be fully specified by different semantic subspaces.\n\n\\end{mdframed}",
        "metadata": {
            "Statement label": "Definition 3",
            "Statement title": "Semantic Separability"
        }
    },
    {
        "text": "\\begin{mdframed}[backgroundcolor=red!5]\n\\textbf{[Definition] ~ Sparse attention:} the low-temperature limit $a_t \\approx \\delta_{tt^*}$ and $\\Delta x = m_{t^*}$, where $\\delta$ is the Kronecker delta. This occurs when there is a large difference between the top two scores: $t^* = \\mathrm{argmax}_t w_t$ and $w_{t^*} - \\max_{t\\neq t^*} w_t \\gg 1$.\\end{mdframed}",
        "metadata": {
            "Statement label": "Definition 4",
            "Statement title": "Sparse Attention"
        }
    },
    {
        "text": "\\begin{mdframed}[backgroundcolor=red!5]\n\\textbf{[Definition] ~ Isotropic attention:} the high-temperature limit $a_t = \\frac{1}{T}$ and $\\Delta x = \\langle m_t\\rangle_t$. This occurs when $w_t$ is constant, requiring $q=0$ or constant $k_t$.\n\\end{mdframed}",
        "metadata": {
            "Statement label": "Definition 5",
            "Statement title": "Isotropic Attention"
        }
    },
    {
        "text": "\\begin{mdframed}[backgroundcolor=red!5]\n\\textbf{[Definition] ~ Circuit collapse:} spontaneous phase transition in which a sparse attention distribution selects a different token due to noise on $\\{q, k_t\\}$. Let $\\epsilon^w_t = k_t^T\\epsilon^q + q^T\\epsilon^k_t + \\mathcal{O}({\\epsilon^q}^T\\epsilon^k_t)$ be perturbations on $w_t$ that result from $\\epsilon^q$ and $\\epsilon^k_t$. Circuit collapse occurs when there exists a $t \\neq t^*$ for which $w_{t^*} - w_t < \\epsilon^w_t - \\epsilon^w_{t^*}$.\n\\end{mdframed}",
        "metadata": {
            "Statement label": "Definition 6",
            "Statement title": "Circuit Collapse"
        }
    },
    {
        "text": "\\begin{theorem}\n    \\texttt{No-Norm}: If two heads with finite non-zero temperature attend to different semantic subspaces, the subspaces must be linearly independent $\\mathbb{S}^{N_\\alpha}_\\alpha \\equiv \\mathbb{R}^{N_\\alpha}$. Corollary: $W_{QK}$ is a low-rank matrix with (left and right) null-spaces that span all non-attended information.\n\\label{theorem: structure: no-norm}\n\\end{theorem}\nProof:\n\\textit{Proof.} ~\nLet $\\theta_A$ and $\\theta_B$ be co-ordinates for the subspaces of $x$ attended to by heads A and B respectively, and $\\phi$ be all other information. Let $\\theta_A\\perp\\theta_B\\perp\\phi$ and $x \\perp y_t$, where $\\perp$ denotes independence. Without loss of generality, write\n\\begin{equation}\n    x(\\theta_A,\\theta_B,\\phi) ~=~ x_A(\\theta_A) ~+~ x_B(\\theta_B) ~+~ x_{other}(\\theta_A,\\theta_B,\\phi)\n\\end{equation}\nThen write\n\\begin{equation}\n\\begin{split}\n    w_t^{(A)}(\\theta_A) ~&=~ \\left(W_{QK}^{(A)} y_t\\right)^T x(\\theta_A,\\theta_B,\\phi)  \\\\\n    &=~ \\left(W_{QK}^{(A)} y_t\\right)^T x_A(\\theta_A) ~+~ \\left(W_{QK}^{(A)} y_t\\right)^T x_B(\\theta_B) ~+~ \\left(W_{QK}^{(A)} y_t\\right)^T x_{other}(\\theta_A,\\theta_B,\\phi) \\\\\n\\end{split}\n\\end{equation}\nwhich requires $\\left(W_{QK}^{(A)} y_t\\right)^T x_B(\\theta_B)=0$ and $\\left(W_{QK}^{(A)} y_t\\right)^T x_{other}(\\theta_A,\\theta_B,\\phi)=0$, since any cancellation between the two terms must be independent of $\\theta_A,\\phi$ and so can be absorbed entirely into the function $x_{B}(\\theta_B)$. This means that $x_B(\\theta_B)$ and $x_{other}(\\theta_A,\\theta_B,\\phi)$ must both be orthogonal to $W_{QK}^{(A)} y_t$, meaning that they reside on the \\textit{left null space} of $W_{QK}^{(A)}$, or are projected by ${W_{QK}^{(A)}}^T$onto a null space of $y_t$.\n\nHead A can only attend to $\\theta_A$ if $x_A(\\theta_A)$ it is not on either of these null spaces, meaning that $x_A(\\theta_A)$ is linearly independent of $x_{B}(\\theta_B)$ and $x_{other}(\\theta_A,\\theta_B,\\phi)$. Likewise for head B\n\\begin{equation}\n\\begin{split}\n    w_t^{(B)}(\\theta_B) ~&=~ \\left(W_{QK}^{(B)} y_t\\right)^T x(\\theta_A,\\theta_B,\\phi)  \\\\\n    &=~ \\left(W_{QK}^{(B)} y_t\\right)^T x_A(\\theta_A) ~+~ \\left(W_{QK}^{(B)} y_t\\right)^T x_B(\\theta_B) ~+~ \\left(W_{QK}^{(B)} y_t\\right)^T x_{other}(\\theta_A,\\theta_B,\\phi) \\\\\n\\end{split}\n\\end{equation}\nrequires that $x_{B}(\\theta_B)$ is linearly independent of both $x_A(\\theta_A)$ and $x_{other}(\\theta_A,\\theta_B,\\phi)$. Since $x_{other}$ resides on both null spaces, it is linearly independent of both $x_A(\\theta_A)$ and $x_B(\\theta_B)$, and may be seen as a third subspace that passes information through to subsequent layers.\n\nWe can also write $w_t = \\left(W_{QK}^T x\\right)^T y_t$, and so the same argument also holds for subspaces on $y_t$. In this case, non-attended subspaces are spanned by the \\textit{right null space} of $W_{QK}$.\n",
        "metadata": {
            "Statement label": "Theorem 4",
            "Statement title": "Orthogonal Attention Subspaces"
        }
    },
    {
        "text": "\\begin{theorem}\n    \\texttt{Pre-Norm}: Semantic subspaces must be represented as orthogonal spheres $\\mathbb{S}^{N_\\alpha}\\equiv\\mathcal{S}^{N_\\alpha-1}$ defined using the $L_2$-norm. Corollary: if either orthogonality or constant-norm are violated, semantic subspaces interfere through a multiplicative factor on $w_t$.\n\\label{theorem: structure: pre-norm}\n\\end{theorem}\nProof:\n\\textit{Proof.} ~Write \n\\begin{equation}\nx(\\theta_A,\\theta_B,\\phi) ~=~ x_A(\\theta_A) ~+~ x_B(\\theta_B) ~+~ x_{AB}(\\theta_A,\\theta_B) ~+~ x_{other}(\\theta_A,\\theta_B,\\phi)\n\\end{equation}\nThen for head A we have\n\\begin{equation}\n    w_t^{(A)}(\\theta_A) ~=~ \\frac{1}{\\left|y_t\\right|\\left|x(\\theta_A,\\theta_B,\\phi) \\right|} {w^*_t}^{(A)}(\\theta_A)\n\\end{equation}\nwhere $w^*_t$ are the attention scores from the \\texttt{No-Norm} case, which requires $x_A(\\theta_A)$ and $x_B(\\theta_B)$ to be linearly independent. Now we additionally require $\\left|x(\\theta_A,\\theta_B,\\phi) \\right| \\perp \\theta_B,\\phi$, with\n\\begin{equation}\n|x| ~=~ \\sqrt{|x_A|^2 ~+~ |x_B ~+~ x_{AB} ~+~ x_{other}|^2 ~+~ 2 x_A^T \\left(x_B ~+~ x_{AB} ~+~ x_{other}\\right)}\n\\end{equation}\nwhere we suppress parameter dependence for readability. Since $\\sqrt{\\cdot}$ is a monotonic function, this can only be satisfied if\n\\begin{equation}\n|x_A|^2 ~+~ |x_B ~+~ x_{AB} ~+~ x_{other}|^2 ~+~ 2 x_A^T \\left(x_B ~+~ x_{AB} ~+~ x_{other}\\right) ~\\perp~ \\theta_B,\\phi\n\\end{equation}\nRepeating this process for head B gives\n\\begin{equation}\n|x_B|^2 ~+~ |x_A ~+~ x_{AB} ~+~ x_{other}|^2 ~+~ 2 x_B^T \\left(x_A ~+~ x_{AB} ~+~ x_{other}\\right) ~\\perp~ \\theta_A,\\phi\n\\end{equation}\nCombining and collecting dependencies, we then have\n\\begin{align}\n    |x_A|^2 ~=~ const ~~~&\\forall~~~ \\theta_A \\\\\n    |x_B|^2 ~=~ const ~~~&\\forall~~~ \\theta_B \\\\\n    \n    \\left( x_{AB} ~+~ 2x_A ~+~ 2x_B \\right)^T x_{AB} ~+~ 2x_A^Tx_B ~=~ const ~~~&\\forall~~~ \\theta_A,\\theta_B \\\\\n    \\left(x_{other} + 2x_A + 2x_B + 2x_{AB}\\right)^T x_{other} ~=~ const ~~~&\\forall~~~ \\theta_A,\\theta_B,\\phi\n\\end{align}\nWe can go one step further, noticing that each individual term carries a different functional dependence, and so must independently be constant\\footnote{N.B. If $|x_{AB}|^2 \\propto x_A^Tx_B$ then $|x_{AB}|^2=const$ reduces to $x_A^Tx_B=const$, which is already required.}. We then have $\\forall~~\\mu,\\nu\\in\\{A,B,AB,other\\}$\n\\begin{equation}\n    |x_\\mu|=const   ~~~~~~\\mathrm{and}~~~~~~  x_\\mu^Tx_\\nu=const \n\\end{equation}\nThe requirements $|x_A(\\theta_A)|=const ~\\forall~\\theta_A$ and $|x_B(\\theta_B)|=const ~\\forall~\\theta_B$ mean that the semantic subspaces have a spherical structure defined by the $L_2$-norm $|\\cdot|$.\n\nNow consider the requirement $x_A(\\theta_A)^Tx_B(\\theta_B)=const$. Say that $\\theta_A$ and $\\theta_B$ have $N_A$ and $N_B$ degrees of freedom, meaning that $x_A$ and $x_B$ have $N_A-1$ and $N_B-1$ respectively, since they each lose one by confinement to the sphere. Say that the constant is nonzero such that $x_A^Tx_B \\neq 0$. This means that there must be some direction $i$ for which $x_{Ai}x_{Bi} \\neq0$. If we know all $N_A-1$ coordinates of $x_A$, and all $N_B - 2$ coordinates of $x_B$ except for direction $i$, then we also know the value of $x_{Bi}$, because it is fixed by the constant. However, this would mean that $x_A$ and $x_B$ are not independent, violating the condition $\\theta_A \\perp \\theta_B$. The only way to satisfy independence is if $x_{Ai}x_{Bi}=0~\\forall~i$, ensuring that degrees of freedom on $x_A$ and $x_B$ never become entangled. Therefore, to satisfy semantic independence, we must have $x_A(\\theta_A)^Tx_B(\\theta_B)=0 ~\\forall~\\theta_A,\\theta_B$. This means that the subspaces are not just linearly independent, but orthogonal.\n\nWe have shown the proof for semantic subspaces of $x$. As for Theorem~\\ref{theorem: structure: no-norm}, the same structure must be true for $y_t$ by symmetry.",
        "metadata": {
            "Statement label": "Theorem 5",
            "Statement title": "Orthogonal Spheres Theorem"
        }
    },
    {
        "text": "\\begin{theorem}\n    \\texttt{QKV-Norm}: Semantic subspaces must be linearly independent.\n\\label{theorem: structure: qkv-norm}\n\\end{theorem}\nProof:\n\\textit{Proof.} ~We have\n\\begin{equation}\n    w_t^{(A)}(\\theta_A) ~=~ \\frac{1}{\\left|k_t^{(A)}\\right|\\left|q^{(A)}\\right|} {w^*_t}^{(A)}(\\theta_A)\n\\end{equation}\nwhere $w^*_t$ are the attention scores from the \\texttt{No-Norm} case, which requires $x_A(\\theta_A)$ and $x_B(\\theta_B)$ to be linearly independent. Use\n\\begin{equation}\nx(\\theta_A,\\theta_B,\\phi) ~=~ x_A(\\theta_A) ~+~ x_B(\\theta_B) ~+~ x_{other}(\\theta_A,\\theta_B,\\phi)\n\\end{equation}\nand \n\\begin{equation}\n\\begin{split}\n    q^{(A)}(\\theta_A) ~&=~ W_Q^{(A)} x(\\theta_A,\\theta_B,\\phi)  \\\\\n    &=~ W_Q^{(A)} x_A(\\theta_A) ~+~ W_Q^{(A)} x_B(\\theta_B) ~+~ W_Q^{(A)} x_{other}(\\theta_A,\\theta_B,\\phi) \\\\\n\\end{split}\n\\end{equation}\nSince we already have the condition of linearly independent $x_A,x_B$, there must exist a linear projection operator $P_A$ such that $P_A x_A = x_A$. Defining $W_Q^{(A)}=P_A$, we then have\n\\begin{equation}\n    q^{(A)}(\\theta_A) ~=~ W_Q^{(A)} x_A(\\theta_A) \n\\end{equation}\nThis demonstrates that it is possible to separate linearly independent semantic subspaces on $x$. By symmetry of $w_t^{(A)}(\\theta_A)$, the same must be true for $y_t$.\n",
        "metadata": {
            "Statement label": "Theorem 6",
            "Statement title": "Semantic Orthogonality Theorem"
        }
    },
    {
        "text": "\\begin{theorem}\n    Consider independent infinitesimal perturbations on queries $\\epsilon^q \\in \\mathbb{R}^{N_{qkv}}$, keys $\\epsilon^k_t \\in \\mathbb{R}^{N_{qkv}}$, and messages $\\epsilon^m_t \\in \\mathbb{R}^{N_{qkv}}$. These propagate onto $\\Delta x = \\sum_{t}a_tm_t$ as\n    \\begin{align}\n        \\epsilon^{\\Delta x(q)} ~~&\\xrightarrow[\\epsilon^q\\rightarrow0]{\\mathrm{~~~~perturb~q~~~~}}~~ \\mathop{\\mathbb{E}}_{a_t} \\Big[ m_t {\\tilde k}_t^T \\Big] \\epsilon^q ~~~~~~~~~~~~~~~~~  {\\tilde k}_t ~\\triangleq~ k_t ~- \\mathop{\\mathbb{E}}_{a_t} \\Big[ k_t \\Big] \\\\\n        \\epsilon^{\\Delta x(k)} ~~&\\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{~~~~perturb~k~~~~}}~~ \\mathop{\\mathbb{E}}_{a_t} \\Big[ {\\tilde m}_t {\\epsilon^k_t}^T \\Big] q ~~~~~~~~~~~~~~~~~  {\\tilde m}_t ~\\triangleq~ m_t ~- \\mathop{\\mathbb{E}}_{a_t} \\Big[ m_t \\Big] \\\\\n        \\epsilon^{\\Delta x(m)} ~~&\\xrightarrow[\\epsilon^m_t\\rightarrow0]{\\mathrm{~~~~perturb~m~~~~}}~~ \\mathop{\\mathbb{E}}_{a_t} \\Big[ \\epsilon^m_t \\Big]\n    \\end{align}\n    where ${\\tilde z}_t$ is the value of $z_t$ measured from the attention-weighted centroid $\\mathbb{E}_{a_t}[z_t] = \\sum_t a_t z_t$.\n\\label{theorem: stability: general}\n\\end{theorem}\nProof:\n\\textit{Proof.} ~Consider $q\\rightarrow q+\\epsilon^q$ where $\\epsilon^q$ are infinitesimal perturbations on $q$. Then $\\Delta x \\rightarrow \\Delta x + \\epsilon^{\\Delta x(q)}$ where by Taylor expansion we find\n\\begin{equation}\n    \\epsilon^{\\Delta x(q)} ~=~ \\frac{\\partial \\Delta x}{\\partial q}\\epsilon^q ~+~ \\mathcal{O}\\left({\\epsilon^q}^2\\right)\n\\end{equation}\nwhere the leading term is a matrix $\\frac{\\partial \\Delta x}{\\partial q}$ acting on a vector $\\epsilon^q$. Differentiating gives\n\\begin{equation}\n    \\frac{\\partial\\Delta x}{\\partial q} ~=~ \\sum_{ij} m_i \\frac{\\partial a_i}{\\partial w_j} \\frac{\\partial w_j}{\\partial q}\n\\end{equation}\nwith $a_i = \\texttt{softmax}_i(w_i)$ and $w_i=k_i^Tq$, and we are using $i,j,k$ etc to index over tokens instead of $t,t',t''$ etc, because this is more readable when we have many summations. Then\n\n\\begin{equation}\n\\begin{split}\n    \\frac{\\partial a_i}{\\partial w_j} ~&=~ \\frac{\\partial}{\\partial w_j} ~ \\frac{e^{w_i}}{\\sum_k e^{w_k}} \\\\\n    &=~ \\frac{\\delta_{ij}e^{w_i}}{\\sum_k e^{w_k}} ~+~ e^{w_i}\\left(-\\frac{e^{w_j}}{\\left(\\sum_ke^{w_k}\\right)^2}\\right) \\\\\n    &=~ \\frac{e^{w_i}}{\\sum_k e^{w_k}}\\left( 1 ~-~ \\frac{e^{w_j}}{\\sum_le^{w_l}}\\right) \\\\\n    &=~ a_i\\left(\\delta_{ij} ~-~ a_j\\right) \\\\\n\\end{split}\n\\end{equation}\nand $\\frac{\\partial w_i}{\\partial q} = k_i^T$, where we retain the transpose to indicate that this is an element of the dual vector space (i.e. covector). Inserting these results into our expression for $\\epsilon^{\\Delta x(q)}$ gives\n\\begin{equation}\n\\begin{split}\n    \\epsilon^{\\Delta x(q)} ~&=~ \\sum_{ij} m_i a_i\\left(\\delta_{ij} ~-~ a_j\\right) k_j^T \\epsilon^q \\\\\n    &=~ \\sum_{i} m_i a_i \\left(k_i ~-~ \\sum_j a_jk_j \\right)^T \\epsilon^q \\\\\n    &=~ \\sum_{i} m_i a_i {\\tilde k}_i^T \\epsilon^q \\\\\n    &=~ \\mathop{\\mathbb{E}}_{a_i} \\Big[m_i {\\tilde k}_i^T \\Big] \\epsilon^q \\\\\n\\end{split}\n\\end{equation}\nThis is the result for Eq.~\\ref{eq: stability: general q}. Repeating the process for perturbations on $k_i$, we have\n\\begin{equation}\n    \\epsilon^{\\Delta x(k)} ~=~ \\sum_i\\frac{\\partial \\Delta x}{\\partial k_i}\\epsilon^k_i ~+~ \\mathcal{O}\\left({\\epsilon^k}^2\\right)\n\\end{equation}\nand\n\\begin{equation}\n\\begin{split}\n    \\frac{\\partial\\Delta x}{\\partial k_i} ~&=~ \\sum_{jk} m_j \\frac{\\partial a_j}{\\partial w_k} \\frac{\\partial w_k}{\\partial k_i} \\\\\n    &=~ \\sum_{jk} m_j a_j \\left(\\delta_{jk} ~-~ a_k\\right) \\delta_{ki} q^T \\\\\n    &=~ \\sum_{j} m_j a_j \\left(\\delta_{ji} ~-~ a_i\\right) q^T \\\\\n    &=~ a_i {\\tilde m}_i q^T\n\\end{split}\n\\end{equation}\nTherefore\n\\begin{equation}\n    \\epsilon^{\\Delta x(k)} ~=~ \\sum_i a_i {\\tilde m}_i q^T \\epsilon^k_i ~=~ \\mathop{\\mathbb{E}}_{a_i} \\Big[{\\tilde m}_i {\\epsilon^k_i}^T \\Big] q\n\\end{equation}\nwhich is the result for Eq.~\\ref{eq: stability: general k}. Finally,\n\\begin{equation}\n\\begin{split}\n    \\epsilon^{\\Delta x(m)} ~&=~ \\sum_i \\frac{\\partial \\Delta x}{\\partial m_i}\\epsilon^m_i \\\\\n    &=~ \\sum_{i} a_i \\epsilon^m_i \\\\\n    &=~ \\mathop{\\mathbb{E}}_{a_i} \\Big[ \\epsilon^m_i \\Big]\n\\end{split}\n\\end{equation}\nusing $\\frac{\\partial\\Delta x}{\\partial m_i} = \\frac{\\partial}{\\partial m_i}\\sum_j a_j m_j = \\sum_j a_j \\delta_{ij} = a_i$. This is the result for Eq.~\\ref{eq: stability: general m}.\n",
        "metadata": {
            "Statement label": "Theorem 7",
            "Statement title": "Propagation of Infinitesimal Perturbations in Attention Mechanisms"
        }
    },
    {
        "text": "\\begin{theorem}\n    For sparse attention:\n    \\begin{equation}\n        \\epsilon^{\\Delta x(q)} \\xrightarrow[\\epsilon^q\\rightarrow0]{\\mathrm{~~perturb~q~~}} 0   ~~~~~~~~~~\n        \\epsilon^{\\Delta x(k)} \\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{~~perturb~k~~}} 0   ~~~~~~~~~~\n        \\epsilon^{\\Delta x(m)} \\xrightarrow[\\epsilon^m_t\\rightarrow0]{\\mathrm{~~perturb~m~~}} \\epsilon^m_{t^*}\n    \\end{equation}\n    i.e. the message is stable with respect to small interference in the queries and keys. Interference in the selected value is linearly transferred onto the message.\n\\label{theorem: stability: sparse}\n\\end{theorem}\nProof:\n\\textit{Proof.} ~For sparse attention we have $a_t = \\delta_{tt^*}$ for some $t^*$. For perturbations of $q$, the RHS of Eq.~\\ref{eq: stability: general q} becomes\n\\begin{equation}\n\\begin{split}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ m_t {\\tilde k}_t^T \\Big] \\epsilon^q ~&=~ \\sum_{t} a_t m_t {\\tilde k}_t^T \\epsilon^q \\\\\n    &=~ \\sum_{t} \\delta_{tt^*} m_t {\\tilde k}_t^T \\epsilon^q \\\\\n    &=~ m_{t^*} {\\tilde k}_{t^*}^T \\epsilon^q \\\\\n    &=~ 0 \\\\\n\\end{split}\n\\end{equation}\nwhere the final step is because ${\\tilde k}_{t^*} = k_{t^*} - \\mathbb{E}_{a_t}[k_t] = k_{t^*} - \\sum_t \\delta_{tt^*} k_t = k_{t^*}-k_{t^*} = 0$. For perturbations of $k_t$, the RHS of Eq.~\\ref{eq: stability: general k} evaluates to $0$ because \n\\begin{equation}\n\\begin{split}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ {\\tilde m}_t {\\epsilon^k_t}^T \\Big] q ~&=~ \\sum_t a_t {\\tilde m}_t q^T \\epsilon^k_t \\\\\n    &=~ \\sum_t \\delta_{tt^*} {\\tilde m}_t q^T \\epsilon^k_t \\\\\n    &=~ {\\tilde m}_{t^*} q^T \\epsilon^k_{t^*} \\\\\n    &=~ 0 \\\\\n\\end{split}\n\\end{equation}\nwhere the final step is because ${\\tilde m}_{t^*} = m_{t^*} - \\sum_t \\delta_{tt^*} m_t = m_{t^*}-m_{t^*} = 0$. For perturbations of $m_t$, the RHS of Eq.~\\ref{eq: stability: general m} evaluates to\n\\begin{equation}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ \\epsilon^m_t \\Big] ~=~ \\sum_{t} a_t \\epsilon^m_t ~=~ \\sum_{t} \\delta_{tt^*} \\epsilon^m_t ~=~ \\epsilon^m_{t^*}\n\\end{equation}\n",
        "metadata": {
            "Statement label": "Theorem 8",
            "Statement title": "Stability of Sparse Attention"
        }
    },
    {
        "text": "\\begin{theorem}\n    For isotropic attention:\n    \\begin{equation}\n        \\epsilon^{\\Delta x(q)} \\xrightarrow[\\epsilon^q\\rightarrow0]{\\mathrm{perturb~q}} \\langle m_t {\\tilde k}_t^T \\rangle_t \\epsilon^q ~~~~~~~~\n        \n        \\epsilon^{\\Delta x(k)} \\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{perturb~k}} \\langle {\\tilde m}_t {\\epsilon^k_t}^T \\rangle_t ~q   ~~~~~~~~\n        \\epsilon^{\\Delta x(m)} \\xrightarrow[\\epsilon^m_t\\rightarrow0]{\\mathrm{perturb~m}} \\langle \\epsilon^m_t \\rangle_t\n    \\end{equation}\n    \n    \n    \n    \n    \n    N.B. isotropy requires $k_t=const$ or $q=0$. Lemma 1: the update is stable to noisy $q$ when $k_t=const$, or when $m_t \\perp k_t$ (c.f. keys and messages from independent subspaces). Lemma 2: the update is stable to noisy $k_t$ when $q=0$, or when $m_t \\perp \\epsilon_t^k$. Lemma 3: the update is stable to noisy $m_t$ when $\\langle \\epsilon^m_t \\rangle_t=0$. Other cases propagate linearly.\n\\label{theorem: stability: isotropic}\n\\end{theorem}\nProof:\n\\textit{Proof.} ~For isotropic attention we have $a_t = \\frac{1}{T}$. For perturbations of $q$, the RHS of Eq.~\\ref{eq: stability: general q} is\n\\begin{equation}\n\\begin{split}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ m_t {\\tilde k}_t^T \\Big] \\epsilon^q ~&=~ \\sum_{t} a_t m_t {\\tilde k}_t^T \\epsilon^q \\\\\n    &=~ \\frac{1}{T} \\sum_{t=1}^T m_t {\\tilde k}_t^T \\epsilon^q \\\\\n    &=~ \\langle m_t {\\tilde k}_t^T \\rangle_t \\epsilon^q \\\\\n\\end{split}\n\\end{equation}\nFor lemma 1, we note that $k_t=const$ implies ${\\tilde k}_t=0$, and if $m_t \\perp k_t$ then $\\langle m_t {\\tilde k}_t^T \\rangle_t = \\langle m_t k_t \\rangle_t - \\langle m_t \\rangle_t \\langle k_t\\rangle_t = Cov(m_t,k_t) = 0$.\n\nFor perturbations of $k_t$, the RHS of Eq.~\\ref{eq: stability: general k} is\n\\begin{equation}\n\\begin{split}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ {\\tilde m}_t {\\epsilon^k_t}^T \\Big] q ~&=~ \\frac{1}{T} \\sum_{t=1}^T {\\tilde m}_t {\\epsilon^k_t}^T q \\\\\n    &=~ \\langle {\\tilde m}_t {\\epsilon^k_t}^T \\rangle_t q \\\\\n\\end{split}\n\\end{equation}\nFor lemma 2, this expression evaluates to $0$ if $q=0$, and if $m_t \\perp \\epsilon_t^k$ then $\\langle {\\tilde m}_t {\\epsilon^k_t}^T \\rangle_t = \\langle m_t {\\epsilon^k_t}^T \\rangle_t - \\langle m_t \\rangle_t \\langle {\\epsilon^k_t}^T\\rangle_t = Cov(m_t,{\\epsilon^k_t}^T) = 0$.\n\nFor perturbations of $m_t$, the RHS of Eq.~\\ref{eq: stability: general m} evaluates to\n\\begin{equation}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ \\epsilon^m_t \\Big] ~=~ \\frac{1}{T}\\sum_{t=1}^T \\epsilon^m_t ~=~ \\langle \\epsilon^m_t \\rangle_t\n\\end{equation}",
        "metadata": {
            "Statement label": "Theorem 9",
            "Statement title": "Stability of Isotropic Attention"
        }
    },
    {
        "text": "\\begin{theorem}\n    Sensitivity of sparse attention to multiplicative perturbations $\\epsilon^q = \\kappa^q q$ and $\\epsilon^k = \\kappa^k_t k_t$ with $\\kappa^q,\\kappa^k_t\\ll1$. Circuit collapse occurs when $\\exists~ t \\neq t^*$ for which:\n    \\begin{equation}\n        \\frac{w_{t^*}}{w_t} ~\\begin{cases} ~<~ \\lambda_w & \\mathrm{if}~ w_t \\left(1 + \\kappa^q + \\kappa^k_{t^*}\\right) > 0 \\\\\n        ~>~ \\lambda_w & \\mathrm{otherwise} \\\\ \\end{cases}\n        ~~~~~~~~~~~~~ \\lambda_w ~\\triangleq~ \\frac{1 + \\kappa^q + \\kappa^k_t}{1 + \\kappa^q + \\kappa^k_{t^*}}\n    \\end{equation}\n    where temperature cancels in the fraction. \\textbf{Attention is fully stable above the critical transition point $\\lambda_w$} (c.f. $w_t \\left(1 + \\kappa^q + \\kappa^k_{t^*}\\right) > 0$). We see that query perturbations alone are insufficient, as they result in $\\lambda_w=1$. Lemma: consider the special case when all keys have similar length $k_t \\approx const$, the attended token has $\\theta_{t^*}\\approx0$, the keys are far-from-orthogonal s.t. $\\theta_t \\ll 1$, and $\\kappa^q\\approx0$. Using $w_t \\triangleq |q| |k_t| \\cos\\theta_t$, circuit collapse occurs when $\\exists~ t \\neq t^*$ for which:\n    \\begin{equation}\n            \\frac{1}{2}\\theta_t^2 ~\\lesssim~ \\kappa^k_t - \\kappa^k_{t^*}   ~~~~~~~~~~~ \\mathrm{if}~ w_t \\left(1  + \\kappa^k_{t^*}\\right) > 0 ~\\text{, otherwise reverse}\n    \\label{eq: sparse circuit collapse result}\n    \\end{equation}\n    i.e. stability requires either well-separated keys s.t. $\\theta_t \\gg 0$, or small perturbations $\\kappa_t-\\kappa^*_t \\ll 1$.\n\\label{theorem: multiplicative stability: sparse}\n\\end{theorem}\nProof:\n\\textit{Proof.} ~ Apply $q\\rightarrow q+\\epsilon^q$ and $k_t\\rightarrow k_t+\\epsilon_t^k$ to $w_t = q^Tk_t$, then we have $w_t \\rightarrow w_t + \\epsilon_w$ such that $\\epsilon^w_t = q^T\\epsilon_t^k + {\\epsilon^q}^Tk_t + {\\epsilon^q}^T\\epsilon_t^k$. For multiplicative perturbations we have  $\\epsilon^q = \\kappa^q q$ and $\\epsilon^k = \\kappa^k_t k_t$, and so $\\epsilon^w_t = \\kappa^k_t q^Tk_t + \\kappa^q q^Tk_t + \\kappa^k_t\\kappa^qq^Tk_t$. Each term recovers a factor of $w_t=q^Tk_t$, which we factor out to give $\\epsilon^w_t = \\left(\\kappa^q  + \\kappa^k_t + \\kappa^k_t\\kappa^q\\right)w_t$. The final term is subleading in the limit of small perturbations, and so\n\\begin{equation}\n    \\epsilon^w_t ~\\xrightarrow[~\\kappa^q,\\kappa^k_t\\rightarrow0~]{}~ \\left(\\kappa^q  ~+~ \\kappa^k_t\\right)w_t ~+~ \\mathcal{O}\\left(\\kappa^q\\kappa^k_t\\right)\n\\end{equation}\nCircuit collapse occurs when $w_{t^*} - w_t < \\epsilon^w_t - \\epsilon^w_{t^*}$ for some $t$. Substituting our limit for $\\epsilon^w_t$ gives\n\\begin{equation}\n    w_{t^*} - w_t ~<~ \\left(\\kappa^q  ~+~ \\kappa^k_t\\right)w_t - \\left(\\kappa^q  ~+~ \\kappa^k_{t^*}\\right)w_{t^*}\n\\end{equation}\nand collecting terms gives\n\\begin{equation}\n    \\left(1 ~+~ \\kappa^q ~+~ \\kappa^k_{t^*}\\right) w_{t^*} ~<~ \\left(1 ~+~ \\kappa^q ~+~ \\kappa^k_t\\right)w_t\n\\end{equation}\nWe then divide each side by $w_t (1 + \\kappa^q + \\kappa^k_{t^*})$, taking care to reverse the sign of the inequality when this factor is negative, to give\n\\begin{equation}\n    \\frac{w_{t^*}}{w_t} ~\\begin{cases} ~<~ \\lambda_w & \\mathrm{if}~ w_t \\left(1 + \\kappa^q + \\kappa^k_{t^*}\\right) &gt; 0 \\\\\n    ~&gt;~ \\lambda_w & \\mathrm{otherwise} \\\\ \\end{cases}\n    ~~~~~~~~~~~~~ \\lambda_w ~\\triangleq~ \\frac{1 + \\kappa^q + \\kappa^k_t}{1 + \\kappa^q + \\kappa^k_{t^*}}\n\\end{equation}\nwhich is the first expression in the theorem.  We note that any temperature parameter cancels in the fraction, which means that the attention head cannot become more stable by reducing its temperature to become more sparse. $\\lambda_w$ has the limits\n\\begin{equation}\n    \\lambda_w ~\\xrightarrow[\\kappa^q\\rightarrow0]{~~\\mathrm{keys~only}~~}~ \\frac{1+\\kappa^k_t}{1+\\kappa^k_{t^*}}\n    ~~~~~~~~~~~~~~~~~\n    \\lambda_w ~\\xrightarrow[\\kappa^k_t,\\kappa^k_{t^*}\\rightarrow0]{~~\\mathrm{query~only}~~}~ \\frac{1 + \\kappa_q}{1 + \\kappa_q} = 1\n\\end{equation}\nmeaning that query perturbations alone are insufficient, contributing only when they co-occur with perturbations on the keys. Write $w_t = |q| |k_t| \\cos\\theta_t$ with $\\theta_t = q \\wedge k_t$, and the approximation of identical key norms $k_{t^*}=k_t\\equiv k$ turns this into $w_t = |q| |k| \\cos\\theta_t$. Then\n\\begin{equation}\n    \\frac{w_{t^*}}{w_t} ~=~ \\frac{|q| |k| \\cos\\theta_{t^*}}{|q| |k| \\cos\\theta_t} ~=~ \\frac{\\cos\\theta_{t^*}}{\\cos\\theta_t}\n\\end{equation}\nThen $\\theta_{t^*}=0$ means that $\\cos\\theta_{t^*} = \\cos0=1$, and so $\\frac{\\cos\\theta_{t^*}}{\\cos\\theta_t} = \\frac{1}{\\cos\\theta_t} = \\sec \\theta_t$. We perform a Taylor expansion in $\\theta_t$ to obtain\n\\begin{equation}\n    \\frac{w_{t^*}}{w_t} ~\\approx~ \\sec\\theta_t ~\\approx~ 1 ~+~ \\frac{1}{2}\\theta_t^2 ~+~\\mathcal{O}\\left(\\theta_t^4\\right)\n\\end{equation}\nwhich is valid when $\\theta_t \\ll 1$. This is true for any $t\\neq t^*$ for which $k_t$ is far from orthogonal with $k_{t^*}$. Substituting this into our circuit collapse condition, we have\n\\begin{equation}\n    1 ~+~ \\frac{1}{2}\\theta_t^2 ~<~ \\frac{1 + \\kappa^k_t}{1 + \\kappa^k_{t^*}} ~~~~~~~~~~~~~~ \\mathrm{if}~ w_t \\left(1 + \\kappa^k_{t^*}\\right) &gt; 0 \n\\end{equation}\nwhere we consider the case of $\\kappa_q\\approx0$ for readability. Re-arranging gives\n\\begin{equation}\n    \\frac{1}{2}\\theta_t^2 ~\\lesssim~  \\kappa^k_t - \\kappa^k_{t^*} ~~~~~~~~~~~~~~~~\\text{Circuit~collapse~when~}k_t~\\text{similar}\n\\label{eq: app: sparse circuit collapse result duplicate}\n\\end{equation}\nif $w_t(1 + \\kappa^k_{t^*}) &gt; 0$, and we reverse the inequality otherwise. We have approximated the denominator on the RHS as $1 + \\kappa^k_{t^*} \\approx 1$ for $\\kappa^k_{t^*}\\rightarrow0$.\n\nWhen $\\theta_t \\ll 1$, the LHS of Eq.~\\ref{eq: app: sparse circuit collapse result duplicate} is small. This means that the attention head can tolerate only very small perturbations $\\{\\kappa^k_t,\\kappa^k_{t^*}\\}$. Therefore semantic subspaces must either have a highly orthogonal substructure s.t. $\\theta_t \\gtrsim 1 ~\\forall~t\\neq t^*$, or be orthogonal s.t. $\\kappa_t\\ll1 ~\\forall~ t$.",
        "metadata": {
            "Statement label": "Theorem 10",
            "Statement title": "Sensitivity of Sparse Attention to Multiplicative Perturbations"
        }
    },
    {
        "text": "\\begin{theorem}\n    Sensitivity of isotropic attention to multiplicative perturbations. Say $\\epsilon^k = \\kappa^k_t k_t$ with $\\kappa^k_t\\ll1$ where $\\{\\kappa_t\\}$ have comparable amplitudes. Then\n    \\begin{equation}\n        \\epsilon^{\\Delta x(k)} \n        ~\\approx~\n        \\begin{cases}\n        0 ~&~ \\text{if~$\\kappa_t$~independent~of~${\\tilde m}_t$,~by~symmetry} \\\\\n        0 ~&~ \\text{if~$\\kappa_t\\equiv\\kappa$~for~constant~$\\kappa$} \\\\\n        0 ~&~ \\text{if~$q=0$} \\\\\n        w \\langle {\\tilde m}_t \\kappa^k_t \\rangle_t  ~&~ \\text{otherwise}\n        \\end{cases}\n    \\end{equation}\n\\label{theorem: multiplicative stability: isotropic}\n\\end{theorem}\nProof:\n\\textit{Proof.} ~We begin with the following result from Theorem~\\ref{theorem: stability: isotropic}:\n\\begin{equation}\n\\epsilon^{\\Delta x(k)} ~\\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{~~perturb~k~~}}~ \\langle {\\tilde m}_t {\\epsilon^k_t}^T \\rangle_t ~q\n\\end{equation}\nSubstituting $\\epsilon^k = \\kappa^k_t k_t$ and taking $q$ inside the brackets gives\n\\begin{equation}\n\\langle{{\\tilde m}_t  \\epsilon^k_t}^T \\rangle_t ~q ~=~ \n\\langle {\\tilde m}_t \\kappa_t {k_t}^T \\rangle_t q ~=~\n ~ \\langle {\\tilde m}_t \\kappa_t w_t \\rangle_t\n\\end{equation}\nWe then notice that isotropic attention requires that $w_t$ is a constant, which we call $w$. Then\n\\begin{equation}\n\\epsilon^{\\Delta x(k)} ~\\approx~ w \\langle {\\tilde m}_t \\kappa_t \\rangle_t\n\\end{equation}\nis our general result. We then note three special cases, each resulting in $\\epsilon^{\\Delta x(k)}=0$:\n\\begin{enumerate}\n    \\item If $\\kappa_t \\perp {\\tilde m}_t$ then $\\langle {\\tilde m}_t \\kappa_t \\rangle_t = \\langle m_t \\kappa_t \\rangle_t - \\langle m_t \\rangle_t \\langle \\kappa_t \\rangle_t = Cov(m_t,\\kappa_t) = 0$. This is case when interference $\\kappa_t^k$ on the keys is not dominated by the same semantic subspace as the message $m_t$.\n    \\item If all keys are perturbed by the same factor $\\kappa_t\\equiv\\kappa$, then $\\langle {\\tilde m}_t \\kappa_t \\rangle_t = \\kappa \\langle {\\tilde m}_t \\rangle_t =0$ because $\\langle {\\tilde m}_t \\rangle_t=0$.\n    \\item Isotropic attention can be achieved by either $q=0$ or $k_t=const$. If the case is $q=0$ then this implies $w=0$ also.\n\\end{enumerate}",
        "metadata": {
            "Statement label": "Theorem 11",
            "Statement title": "Multiplicative Stability in Isotropic Attention"
        }
    },
    {
        "text": "\\begin{theorem}\n\tShifting attention scores $w_t$ by a constant offset does not affect the attention distribution. Therefore attention is fully determined by differences in scores.\n \\label{theorem: att shift operator}\n\\end{theorem}\nProof:\n\\textit{Proof.} ~Applying the shift $w_t \\xrightarrow[\\mathrm{offset~w}]{} w_t + \\delta w ~\\forall~t$ with fixed $\\delta w$, we have\n\\begin{equation}\n\\begin{split}\na_t ~&=~ \\frac{e^{w_t}}{\\sum_{t'} e^{w_{t'}}} \\\\\n&\\xrightarrow[\\mathrm{offset~w}]{} ~ \\frac{e^{\\delta w}e^{w_t}}{\\sum_{t'} e^{\\delta w}e^{w_{t'}}} ~=~ \\frac{e^{\\delta w}}{e^{\\delta w}}\\frac{e^{w_t}}{\\sum_{t'} e^{w_{t'}}} ~=~ 1 \\cdot a_t~=~ a_t\n\\end{split}\n\\end{equation}\nAlternatively we may write\n\\begin{equation}\na_t ~=~ \\frac{e^{w_t}}{\\sum_{t'} e^{w_{t'}}} ~=~ \\frac{e^{w_t}}{e^{w_{t'}}\\sum_{t'} e^{w_{t'}-w_t}} ~=~ \\frac{1}{\\sum_{t'} e^{w_{t'}-w_t}}\n\\end{equation}\nwhere $\\left(w_{t'}+\\delta t\\right)-\\left(w_t+\\delta t\\right) = w_{t'}-w_t$.",
        "metadata": {
            "Statement label": "Theorem 12",
            "Statement title": "Shift Invariance of Attention"
        }
    },
    {
        "text": "\\begin{theorem}\n\tMultiplying attention scores by a positive factor changes the inverse-temperature of the attention distribution, modulating its sparsity (low temperature = less entropy = more sparse). Corollary: In the sparse limit, attention is fully determined by the order of $w_t$.\n \\label{theorem: att scale operator}\n\\end{theorem}\nProof:\n\textit{Proof.} ~Applying the scaling $w_t \\xrightarrow[\\mathrm{scale~w}]{} \\kappa w_t ~\\forall~ t$ with fixed $\\kappa > 0$, we have\n\\begin{equation}\n\\begin{split}\n\\frac{e^{\\kappa w_t}}{\\sum_{t'} e^{\\kappa w_{t'}}} ~~&=~~ \\frac{1}{\\sum_{t'} e^{\\kappa (w_{t'}-w_t)}}   \\\\\n~~&\\xrightarrow[\\kappa\\rightarrow 0]{} ~~~~ \\frac{1}{\\sum_{t'} e^0} ~=~ \\frac{1}{T} ~\\forall~ t ~~~~~~~~~~~~~~~~~~~~~\\text{[fully isotropic distribution]} \\\\\n~~&\\xrightarrow[\\kappa\\rightarrow \\infty]{} ~~ \n\\begin{cases}\n1 ~~ ~\\mathrm{for}~ t = \\mathrm{argmax}_{t'} w_{t'} \\\\\n0 ~~ ~~~\\forall~ t \\neq \\mathrm{argmax}_{t'} w_{t'} \\\\\n\\end{cases}\n~~~\\text{[fully sparse distribution]}\n\\end{split}\n\\end{equation}\nwhere the \\texttt{argmax} operator is fully determined by the order of $w_t$.",
        "metadata": {
            "Statement label": "Theorem 13",
            "Statement title": "Attention Scaling Theorem"
        }
    },
    {
        "text": "\\begin{theorem}\n    In the \\texttt{No-Norm} case, the attention distribution $a_t$ is defined by the projection of $y_t$ onto a fixed vector $y_x$ for a given $x$. The length of $y_x$ is an inverse-temperature parameter.\n\\end{theorem}\nProof:\n\textit{Proof.} ~Write $w_t = x^TW_{QK}y_t = (W_{QK}^Tx)^T y_t \\equiv y_x^T y_t$ where $y_x \\triangleq W_{QK}^T x\\in\\mathbb{R}^{N_y}$, which is the dot-product between $y_t$ and a fixed vector $y_x$ on the row space of $W_{QK}$. Then, re-writing in terms of the vector lengths and the enclosing angle $\\theta_{y_t} = y_x\\wedge y_t$, we have $w = |y_x||y_t|\\cos\\theta_{y_t}$. The factor $|y_x|$ is identical for all $t$, making it an inverse-temperature.",
        "metadata": {
            "Statement label": "Theorem 14",
            "Statement title": "Inverse-Temperature Projection Theorem"
        }
    },
    {
        "text": "\\begin{theorem}\nIn the \\texttt{No-Norm} case, bias parameters in the construction of query and key vectors are nullified by the \\texttt{softmax}, or only contribute terms that may be recovered if $x$ contains a constant direction.\n\\label{theorem: decomposition}\n\\end{theorem}\nProof:\n\textit{Proof.} ~Consider a modification to the construction of query and key vectors that uses the affine transformations $q = W_Qx+b_Q$ and $k_t = W_Ky_t+b_K$, with $W_{Q}\\in\\mathbb{R}^{N_{qkv}\\times N_x}$, $W_{K}\\in\\mathbb{R}^{N_{qkv}\\times N_y}$, $W_{QK}\\triangleq W_Q^TW_K$, and $b_Q,b_K\\in\\mathbb{R}^{N_{qkv}}$. The dot-product attention scores are then:\n\\begin{equation}\n\\begin{split}\n    w_t ~&=~ q^T k_t \\\\\n       &=~ \\left(W_Qx + b_Q\\right)^T \\left(W_Ky_t + b_K\\right)  \\\\\n       &=~ x^TW_{QK}y_t ~+~ ({W_Q}^Tb_K)^Tx ~+~ ({W_K}^Tb_Q)^Ty_t ~+~ b_Q^Tb_K \\\\\n    w_t ~+~ const~ &=~ x^TW_{QK}y_t ~+~ ({W_Q}^Tb_K)^Tx ~+~ ({W_K}^Tb_Q)^Ty_t  \\\\\n       &\\triangleq~ x^TW_{QK}y_t ~+~ \\rho_x^Tx ~+~ \\rho_y^Ty_t  ~~~~~~~~~~~~~\\rightarrow ~\\rho_x^Tx =const ~\\mathrm{given}~x\\rightarrow \\\\\n       &=~ x^TW_{QK}y_t ~+~ \\rho_y^Ty_t  ~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\rightarrow ~W_{QK} \\triangleq \\Omega^T \\Lambda \\Sigma~\\text{via SVD}~\\rightarrow  \\\\\n       &=~ x^T\\Omega^T\\Lambda\\Sigma y_t ~+~ \\rho_y^Ty_t ~~~~~~~~~~~~~~~~~~~~~~~~~~ \\rightarrow ~x' \\triangleq \\Omega x, ~~y'_t \\triangleq \\Sigma y_t ~\\rightarrow \\\\\n       &=~ {x'}^T\\Lambda y'_t ~+~ \\rho_y^Ty_t\n\\end{split}\n\\end{equation}\nAfter expanding the terms, we find an additive constant $b_Q^Tb_K$, and move this onto the LHS. Theorem~\\ref{theorem: att shift operator} states that this has no impact on the output of the \\texttt{softmax} operator. We identify $\\rho_x\\triangleq W_Q^Tb_k$ and $\\rho_y\\triangleq W_K^Tb_q$ as vectors on the \\textbf{row-spaces of $W_Q$ and $W_K$ respectively}, defined as linear maps of the special directions $b_K$ and $b_Q$. Since $x$ is constant for each \\texttt{softmax}, $\\rho_x^Tx$ is constant, and we absorb it into the LHS. We perform the singular value decomposition $W_{QK} \\triangleq \\Omega^T \\Lambda \\Sigma$ where $\\{\\Omega\\in\\mathbb{R}^{N_x\\times N_x},~\\Sigma\\in\\mathbb{R}^{N_y\\times N_y}\\}$ are orthonormal matrices and $\\Lambda \\in \\mathbb{R}^{N_x\\times N_y}$ is a diagonal matrix of positive-semidefinite singular values with maximum rank $\\min(N_x,N_y,N_{qkv})$. Orthonormal matrices apply a basis change to the embedding space using rotations and reflections. We write the transformed embeddings as $x' \\triangleq \\Omega x$ and $y_t' \\triangleq \\Sigma y_t$. The dot-product then has two terms:\n\\begin{enumerate}\n    \\item ${x'}^T \\Lambda y'_t = \\sum_i \\Lambda_{ii} x'_i y'_{ti}$ sculpts the attention distribution according to \\textit{pairwise relationships} between embeddings. We can say that $\\{\\Omega,\\Sigma\\}$ align the bases of $x$ and $y_t$, mapping them onto a common orthonormal coordinate system. $\\Lambda_{ii}$ then assigns an importance weight to each coordinate $i$, determining the contribution of $x'_iy'_{ti}$.\n    %\\footnote{``Every direction'' is true when $M=N_x=N_y\\equiv N$, otherwise the rank of $Q^TK$ is $min(M,N_x,N_y)$.}\n    %\\item $\\rho_x^Tx$ means ``$(s)$ receives from all senders when $x \\parallel \\rho_x$'', where $\\rho_x$ must be a vector on the column-space of $Q$.\n    \\item $\\rho_y^Ty$ means ``token $t$ sends to all receivers when $y_t \\parallel \\rho_y$'', where $\\rho_y$ must be a vector on the row-space of $W_K$. This may be recovered in the expansion of ${x'}^T\\Lambda y'_t$ if there exists a direction $i$ for which $x'_i=const$.\n\\end{enumerate}",
        "metadata": {
            "Statement label": "Theorem 15",
            "Statement title": "Bias Nullification Theorem"
        }
    },
    {
        "text": "\\begin{definition}\\label{def:linear_representation_orig}\n  A vector $\\bar{\\ell}_W$ is a linear representation of a binary concept $W$ if for all contexts $\\ell$, and all concept variables $Z$ that are causally separable with $W$, we have, for all $\\alpha>0$,\n  \\begin{align}\n    \\Pr(W = 1 \\given \\ell + \\alpha \\bar{\\ell}_W) &> \\Pr(W = 1 \\given \\ell), \\text{ and} \\\\\n    \\Pr(Z \\given \\ell + \\alpha \\bar{\\ell}_W) &= \\Pr(Z\\given \\ell).\n  \\end{align} \n\\end{definition}",
        "metadata": {
            "Statement label": "Definition 7",
            "Statement title": "Linear Concept Separability"
        }
    },
    {
        "text": "\\begin{definition}\\label{def:hierarchical_relation}\n  A value $z$ is \\defnphrase{subordinate} to a value $w$ (denoted by $z \\prec w$) if $\\yquad(z) \\subseteq \\yquad(w)$.\n  We say a categorical concept $Z \\in_R \\{z_0, \\dots, z_{k-1}\\}$ is subordinate to a categorical concept $W \\in_R \\{w_0, \\dots, w_{n-1}\\}$ if there exists a value $w_Z$ of $W$ such that each value $z_i$ of $Z$ is subordinate to $w_Z$.\n\\end{definition}",
        "metadata": {
            "Statement label": "Definition 8",
            "Statement title": "Subordination Hierarchy"
        }
    },
    {
        "text": "\\begin{definition}\\label{def:linear_representation}\n    A vector $\\bar{\\ell}_W$ is a linear representation of a binary concept $W$ if\n    \\begin{align}\n      \\Pr(W = 1 \\given \\ell + \\alpha \\bar{\\ell}_W) &> \\Pr(W = 1 \\given \\ell) \\text{, and} \\label{eq:linear_cond1} \\\\\n      \\Pr(Z \\given \\ell + \\alpha\\bar{\\ell}_W) &= \\Pr(Z \\given \\ell), \\label{eq:linear_cond2}\n    \\end{align}   \n    for all contexts $\\ell$, all $\\alpha>0$, and all concept variables $Z$ that are either subordinate or causally separable with $W$.\n    Here, if $W$ is a binary feature for an attribute $w$, $W=1$ denotes $W = \\ConceptValue{is\\_w}$.\n\\end{definition}",
        "metadata": {
            "Statement label": "Definition 9",
            "Statement title": "Linear Concept Representation"
        }
    },
    {
        "text": "\\begin{definition}\\label{def:vector_representation}\n  We say that binary feature $W$ for an attribute $w$ has a \\emph{vector representation} $\\bar{\\ell}_w \\in \\Reals^d$ if $\\bar{\\ell}_w$ satisfies \\cref{def:linear_representation} and $\\|\\bar{\\ell}_w\\|_2=b_w$ in \\cref{thm:magnitude}.\n  If the vector representation of a binary feature is not unique, we say $\\bar\\ell_w$ is the vector representation that maximizes $b_w$.\n\\end{definition}",
        "metadata": {
            "Statement label": "Definition 10",
            "Statement title": "Vector Magnitude Maximization"
        }
    },
    {
        "text": "\\begin{definition}\n  The \\emph{polytope representation} of a categorical concept $Z = \\{z_0, \\dots, z_{k-1}\\}$ is the convex hull of the vector representations of the elements of the concept.\n\\end{definition}",
        "metadata": {
            "Statement label": "Definition 11",
            "Statement title": "Polytope Representation of Categorical Concepts"
        }
    },
    {
        "text": "\\begin{theorem}[Magnitudes of Linear Representations]\n\\label{thm:magnitude}\n  Suppose there exists a linear representation (normalized direction) $\\bar\\ell_W$ of a binary feature $W$ for an attribute $w$.  \n  Then, there is a constant $b_w>0$ and a choice of unembedding space origin $\\bar{\\gamma}_0^w$ in \\cref{eq:transformation} such that\n  \\begin{equation}\\label{eq:magnitude}\n    \\begin{cases}\n      \\bar\\ell_W^\\top g(y) = b_w & \\text{if } y\\in \\yquad(w)\\\\\n      \\bar\\ell_W^\\top g(y) = 0 & \\text{if } y \\not\\in \\yquad(w).\n    \\end{cases}\n  \\end{equation}\n  Further, if there are $d$ causally separable attributes $\\{w_0, \\dots, w_{d-1}\\}$ with linear representations, we can choose a canonical origin $\\bar\\gamma_0$ in \\cref{eq:transformation} as\n  \\begin{equation}\n    \\bar\\gamma_0 = \\sum_i \\bar{\\gamma}_0^{w_i}.\n  \\end{equation}\n\n\\end{theorem}\nProof:\n\\begin{proof}\n  For any $y_1, y_0 \\in \\yquad(w)$ or $y_1, y_0 \\not\\in \\yquad(w)$, let $Z$ be a binary concept where $\\yquad(Z = 0) = \\{y_0\\}$ and $\\yquad(Z = 1) = \\{y_1\\}$.\n  Since $Z$ is subordinate to $W$, \\cref{eq:linear_cond2} implies that\n  \\begin{align}\n    & \\logit \\Pr(Y = y_1 \\given Y \\in \\{y_0, y_1\\} , \\ell +  \\bar\\ell_W )= \\logit \\Pr(Y = y_1 \\given Y \\in \\{y_0, y_1\\} , \\ell)\\\\\n    & \\iff  \\bar\\ell_W ^\\top  (g(y_1) - g(y_0))=\\bar\\ell_W ^\\top  A(\\gamma(y_1) - \\gamma(y_0)) = 0\n  \\end{align}\n  where $A$ is the invertible matrix in \\cref{eq:transformation}.\n  This means that $\\bar\\ell_W^\\top A \\gamma(y)$ is the same for all $y\\in \\yquad(w)$, and it is also the same for all $y\\not\\in \\yquad(w)$.\n\n  Furthermore, for any $y_1 \\in \\yquad(w)$ and $y_0 \\not\\in \\yquad(w)$, \\cref{eq:linear_cond1} implies that\n  \\begin{align}\n    & \\logit \\Pr(Y = y_1 \\given Y \\in \\{y_0, y_1\\} , \\ell + \\bar\\ell_W )> \\logit \\Pr(Y = y_1 \\given Y \\in \\{y_0, y_1\\} , \\ell)\\\\\n    & \\iff  \\bar\\ell_W ^\\top  (g(y_1) - g(y_0)) =  \\bar\\ell_W ^\\top  A(\\gamma(y_1) - \\gamma(y_0))> 0.\n  \\end{align}\n\n  Thus, by setting $b_w^0 = \\bar\\ell_W^\\top A \\gamma(y)$ for any $y \\not\\in \\yquad(w)$, and $b_w = \\bar\\ell_W^\\top A \\gamma(y_1) - \\bar\\ell_W^\\top A \\gamma(y_0) > 0$ for any $y_1 \\in \\yquad(w)$ and $y_0 \\not\\in \\yquad(w)$, we get\n  \\begin{equation}\\label{eq:feature_vector}\n    \\begin{cases}\n      \\bar\\ell_W^\\top A \\gamma(y) = b_w^0 + b_w & \\text{if } y\\in \\yquad(w)\\\\\n      \\bar\\ell_W^\\top A \\gamma(y) = b_w^0 & \\text{if } y \\not\\in \\yquad(w).\n    \\end{cases}\n  \\end{equation}\n  Then, we can choose an origin as\n  \\begin{equation}\n    \\bar\\gamma_0^w = b_w^0 A^{-1} \\bar\\ell_W\n  \\end{equation}\n  satisfying \\cref{eq:magnitude}.\n\n  On the other hand, if there exist $\\bar\\ell_W$ and $\\bar\\ell_Z$ for causally separable attributes $w$ and $z$, then $\\bar\\ell_W$ and $\\bar\\ell_Z$ are orthogonal by the property of the causal inner product.\n  If they are not orthogonal, adding $\\bar\\ell_Z$ can change the other concept $W$, and it is a contradiction.\n  Now if there exist the linear representation for $d$ binary features for causally separable attributes $\\{w_0, \\dots, w_{d-1}\\}$, we can choose a canonical $\\bar\\gamma_0$ in \\cref{eq:transformation} as\n  \\begin{equation}\n    \\bar\\gamma_0 = \\sum_i \\bar{\\gamma}_0^{w_i}.\n  \\end{equation}\n  with \\cref{eq:magnitude} satisfied.\n\\end{proof}",
        "metadata": {
            "Statement label": "Theorem 16",
            "Statement title": "Magnitude Consistency Theorem"
        }
    },
    {
        "text": "\\begin{theorem}[Hierarchical Orthogonality]\n\\label{thm:orthogonality}\n  Suppose there exist the vector representations for all the following binary features.\n  Then, we have that\n  \\begin{enumerate}[label=(\\alph*)]\n    \\item $\\bar\\ell_{w_1} - \\bar\\ell_{w_0}$ is a linear representation $\\bar{\\ell}_{\\ConceptDirMath{w_0}{w_1}}$ defined in \\cref{def:linear_representation}; \\label{item:a}\n    \\item $\\bar\\ell_{w} \\perp \\bar\\ell_{z} - \\bar\\ell_w$ for $z \\prec w$; \\label{item:left}\n    \\item $\\bar{\\ell}_{w} \\perp \\bar\\ell_{z_1} - \\bar\\ell_{z_0}$ for $Z \\in_R \\{z_0, z_1\\}$ subordinate to $W \\in_R \\{\\ConceptValue{not\\_w}, \\ConceptValue{is\\_w}\\}$; \\label{item:middle}\n    \\item $\\bar{\\ell}_{w_1} - \\bar{\\ell}_{w_0}  \\perp \\bar\\ell_{z_1} - \\bar\\ell_{z_0}$ for $Z \\in_R \\{z_0, z_1\\}$ subordinate to $W \\in_R \\{w_0, w_1\\}$; and \\label{item:right}\n    \\item $\\bar\\ell_{w_1}-\\bar\\ell_{w_0} \\perp \\bar\\ell_{w_2} - \\bar\\ell_{w_1}$ for $w_2 \\prec w_1 \\prec w_0$.\n  \\end{enumerate}\n\n\\end{theorem}\nProof:\n\\begin{proof}\n  \\begin{enumerate}[label=(\\alph*)]\n    \\item For $\\bar\\ell_{w_1}$ and $\\bar\\ell_{w_0}$, by \\Cref{thm:magnitude}, we have\n    \\begin{equation}\n      \\begin{cases}\n        (\\bar\\ell_{w_1} - \\bar\\ell_{w_0})^\\top g(y) = 0-b_{w_0} = -b_{w_0} & \\text{if } y\\in \\yquad(w_0)\\\\\n        (\\bar\\ell_{w_1} - \\bar\\ell_{w_0})^\\top g(y) = b_{w_1} - 0 = b_{w_1} & \\text{if } y \\in \\yquad(w_1)\\\\\n        (\\bar\\ell_{w_1} - \\bar\\ell_{w_0})^\\top g(y) = 0 - 0 = 0 & \\text{if } y\\not\\in \\yquad(w_0)\\cup \\yquad(w_1).\n      \\end{cases}\n    \\end{equation}\n    Since $\\bar\\ell_{w_1} - \\bar\\ell_{w_0}$ can change the target concept $\\ConceptDirMath{w_0}{w_1}$ without changing any other concept subordinate or causally separable to the target concept, $\\bar\\ell_{w_1} - \\bar\\ell_{w_0}$ is the linear representation $\\bar{\\ell}_{w_0\\Rightarrow w_1}$.\n\n    \\item For $\\bar\\ell_{w}$ and $\\bar\\ell_{z}$ where $z \\prec w$, by \\Cref{thm:magnitude}, we have\n    \\begin{equation}\n      \\begin{cases}\n        (\\bar\\ell_{z} - \\bar\\ell_{w})^\\top g(y) = b_z - b_w & \\text{if } y\\in \\yquad(z)\\\\\n        (\\bar\\ell_{z} - \\bar\\ell_{w})^\\top g(y) =  0 -b_w =  -b_w & \\text{if } y \\in \\yquad(w)\\setminus \\yquad(z)\\\\\n        (\\bar\\ell_{z} - \\bar\\ell_{w})^\\top g(y) = 0 - 0 = 0 & \\text{if } y\\not\\in \\yquad(w).\n      \\end{cases}\n    \\end{equation}\n    When $w\\setminus z$ denotes an attribute defined by $\\yquad(w)\\setminus \\yquad(z)$, $\\bar\\ell_{z} - \\bar\\ell_{w}$ can change the target concept $\\ConceptDirMath{w\\setminus z}{z}$ without changing any other concept subordinate or causally separable to the target concept.\n    Thus, $\\bar\\ell_{z} - \\bar\\ell_{w}$ is the linear representation $\\bar{\\ell}_{w\\setminus z \\Rightarrow z}$.\n    This concept means $\\ConceptValue{not\\_z}  \\Rightarrow \\ConceptValue{is\\_z}$ conditioned on $w$, and hence it is subordinate to $w$.\n    \n    Therefore, $\\bar\\ell_w$ is orthogonal to the linear representation $\\bar{\\ell}_{w\\setminus z \\Rightarrow z} = \\bar\\ell_z - \\bar\\ell_w$ by the property of the causal inner product.\n    If they are not orthogonal, adding $\\bar\\ell_w$ can change the other concept $\\ConceptDirMath{w\\setminus z}{z}$, and it is a contradiction.\n\n    \\item By the above result \\ref{item:left}, $\\bar\\ell_w^\\top (\\bar\\ell_{z_1} - \\bar\\ell_w) = \\bar\\ell_w^\\top (\\bar\\ell_{z_0} - \\bar\\ell_w) = 0$.\n    Therefore, $\\bar\\ell_w^\\top (\\bar\\ell_{z_1} - \\bar\\ell_{z_0}) = 0$.\n\n    \\item Let's say that $w_1$ is $w_Z$ defined in \\Cref{def:hierarchical_relation}.\n    The binary contrast $\\ConceptDirMath{z_0}{z_1}$ is subordinate to the binary feature for the attribute $w_0$.\n    By the property of the causal inner product, $\\bar\\ell_{w_0}$ is orthogonal to the linear representation $\\bar{\\ell}_{z_0\\Rightarrow z_1} = \\bar\\ell_{z_1} - \\bar\\ell_{z_0}$ (by \\ref{item:a}).\n    Then, with the above result \\ref{item:middle}, we have $(\\bar\\ell_{w_1} - \\bar\\ell_{w_0})^\\top (\\bar\\ell_{z_1} - \\bar\\ell_{z_0})$.\n\n    \\item By the above result \\ref{item:left}, we have\n    \\begin{equation}\n      \\begin{cases}\n        \\|\\bar\\ell_{w_1} - \\bar\\ell_{w_0}\\|_2^2 = \\|\\bar\\ell_{w_1}\\|_2^2  - \\|\\bar\\ell_{w_0}\\|_2^2\\\\\n        \\|\\bar\\ell_{w_2} - \\bar\\ell_{w_1}\\|_2^2 = \\|\\bar\\ell_{w_2}\\|_2^2  - \\|\\bar\\ell_{w_1}\\|_2^2\\\\\n        \\|\\bar\\ell_{w_2} - \\bar\\ell_{w_0}\\|_2^2 = \\|\\bar\\ell_{w_2}\\|_2^2  - \\|\\bar\\ell_{w_0}\\|_2^2.\n      \\end{cases}\n    \\end{equation}\n    Then,\n    \\begin{align}\n      &\\|\\bar\\ell_{w_1} - \\bar\\ell_{w_0}\\|_2^2 + \\|\\bar\\ell_{w_2} - \\bar\\ell_{w_1}\\|_2^2\\\\\n      &= \\|\\bar\\ell_{w_1}\\|_2^2  - \\|\\bar\\ell_{w_0}\\|_2^2 + \\|\\bar\\ell_{w_2}\\|_2^2  - \\|\\bar\\ell_{w_1}\\|_2^2\\\\\n      &= \\|\\bar\\ell_{w_2}\\|_2^2  - \\|\\bar\\ell_{w_0}\\|_2^2\\\\\n      & = \\|\\bar\\ell_{w_2} - \\bar\\ell_{w_0}\\|_2^2.\n    \\end{align}\n    Therefore, $\\bar\\ell_{w_1} - \\bar\\ell_{w_0}$ is orthogonal to $\\bar\\ell_{w_2} - \\bar\\ell_{w_1}$.\n  \\end{enumerate}\n\\end{proof}",
        "metadata": {
            "Statement label": "Theorem 17",
            "Statement title": "Hierarchical Orthogonality Theorem"
        }
    },
    {
        "text": "\\begin{theorem}[Categorical Concepts are Represented as Simplices]\n\\label{thm:simplex}\n  Suppose that $\\{w_0, \\dots, w_{k-1}\\}$ is a collection of $k$ mutually exclusive attributes such that for every joint distribution $Q(w_0, \\dots w_{k-1})$ there is some $\\ell_{i}$ such that $\\Pr(W = w_i \\given \\ell_{i}) = Q(W=w_i)$ for every $i$.\n  Then, the vector representations $\\bar\\ell_{w_0}, \\dots, \\bar\\ell_{w_{k-1}}$ form a $(k-1)$-simplex in the representation space. In this case, we take the simplex to be the representation of the categorical concept $W = \\{w_0, \\dots, w_{k-1}\\}$.\n\n\\end{theorem}\nProof:\n\\begin{proof}\n  If we can represent arbitrary joint distributions, this means, in particular, that we can change the probability of one attribute without changing the relative probability between a pair of other attributes.\n  Consider the case where $k=3$, as illustrated in \\Cref{fig:proof}.\n  If $\\bar\\ell_{w_0}, \\bar\\ell_{w_1}, \\bar\\ell_{w_2}$ are on a line, then there is no direction in that line (to change the value in the categorical concept) such that adding the direction can change the probability of $w_2$ without changing the relative probabilities between $w_0$ and $w_1$.\n  However, if $\\bar\\ell_{w_0}, \\bar\\ell_{w_1}, \\bar\\ell_{w_2}$ are not on a line, they form a triangle.\n  Then, there exists a line that is toward $\\bar\\ell_{w_2}$ and perpendicular to the opposite side of the triangle.\n  Now adding the direction $\\tilde{\\ell}$ can manipulate the probability of $w_2$ without changing the relative probabilities between $w_0$ and $w_1$.\n  That is, for any $\\alpha > 0$ and context embedding $\\ell$,\n  \\begin{equation}\n    \\begin{cases}\n      \\Pr(W = w_2 \\given \\ell + \\alpha \\tilde{\\ell}) > \\Pr(W = w_2 \\given \\ell), \\text{ and}\\\\\n      \\frac{\\Pr(W = w_1 \\given \\ell + \\alpha \\tilde{\\ell})}{\\Pr(W = w_0 \\given \\ell + \\alpha \\tilde{\\ell})}  = \\frac{\\Pr(W = w_1 \\given \\ell )}{\\Pr(W = w_0 \\given \\ell)}.\n    \\end{cases}\n  \\end{equation}\n  Therefore, the vectors $\\bar\\ell_{w_0}, \\bar\\ell_{w_1}, \\bar\\ell_{w_2}$ form a 2-simplex.\n  \n  This argument extends immediately to higher $k$ by induction.\n  For each $i \\in \\{0, \\dots, k-1\\}$, there should exist a direction that is toward $\\bar\\ell_{w_i}$ and orthogonal to the opposite hyperplane ($(k-2)$-simplex) formed by the other $\\bar\\ell_{w_{i'}}$'s.\n  Then, the vectors $\\bar\\ell_{w_0}, \\dots, \\bar\\ell_{w_{k-1}}$ form a $(k-1)$-simplex.\n\\end{proof}",
        "metadata": {
            "Statement label": "Theorem 18",
            "Statement title": "Simplex Representation Theorem"
        }
    },
    {
        "text": "\\begin{lemma}\n\\label{lemma:prop_kro}\nGiven the matrices $\\Am \\in \\mathbb{R}^{n \\times m}$, $\\Bm \\in \\mathbb{R}^{p \\times q}$, $\\Cm \\in \\mathbb{R}^{m \\times r}$, $\\Dm \\in \\mathbb{R}^{q \\times s}$, then the following holds:\n\\begin{equation}\n    \\label{eq:trace_kro}\n    \\tr(\\Am \\kro \\Bm) = \\tr(\\Am)\\tr(\\Bm) ,\n\\end{equation}\nand \n\\begin{equation}\n    \\label{eq:prod_kro}\n    (\\Am \\kro \\Bm)(\\Cm \\kro \\Dm) = (\\Am \\Cm)\\kro(\\Bm \\Dm).\n\\end{equation}\n\\end{lemma}",
        "metadata": {
            "Statement label": "Lemma 10",
            "Statement title": "Properties of the Kronecker Product"
        }
    },
    {
        "text": "\\begin{lemma}[Gradients of Self Attention for parameter matrices]\n\\label{lemma:grads_SA}\nThe gradients of the self attention layer defined in Eq.~\\eqref{eq:self_att} have the following form:\n\\begin{align*}\n     & \\frac{\\partial \\Sm}{\\partial \\wV} =    \\soft\\left(\\frac{\\Xm\\wQ{\\wK}^\\top \\Xm^\\top}{\\sqrt{d_k}}\\right)\\Xm\\, \\kro \\Im_{d_v} \\, \\\\\n     & \\frac{\\partial \\Sm}{\\partial \\wQ} = \\left(\\Im_n \\kro {\\wV}^\\top\\Xm^\\top\\right) \\frac{\\partial \\Am}{\\partial \\Mm} \\left(\\frac{\\Xm\\kro\\Xm\\wK}{\\sqrt{d_k}}\\right),\n\\end{align*}\nwhere the gradients of the softmax with respect to its inputs are as follows:\n\\begin{equation}\n\\label{eq:grad_soft_complete}\n    \\frac{\\partial \\Am}{\\partial \\Mm} = \\blockdiag\\Bigg( \\dfrac{\\partial \\Am_{i}}{\\partial \\Mm_{i}^\\top}\\Bigg)\n\\end{equation}\nand where $ \\dfrac{\\partial \\Am_{i}}{\\partial \\Mm_{i}^\\top}=\\diag(\\Am_{i}) - \\Am_{i}\\Am_{i}^\\top$ with $\\Am_{i}$ being the $i$-th row of $\\Am$ in column vector format.\\\\\nFinally, note that under the uniform-attention assumption, Eq.~\\eqref{eq:grad_soft_complete} simplifies to:\n\\begin{equation}\n\\label{eq:grad_soft}\n    \\frac{\\partial \\Am}{\\partial \\Mm} = \\frac{1}{n}\\Im_n \\kro \\left(\\Im_n - \\frac{1}{n}\\bm{1}_{n\\times n} \\right).\n\\end{equation}\n\\end{lemma}\nProof:\n\\begin{proof}\nLet's start with the simple case of the values' weights $\\wV$. Using the rule in Eq.~\\eqref{eq:matrix-derivative}, it is immediate that:\n\\begin{equation*}\n \\frac{\\partial \\Sm}{\\partial \\wV} =    \\soft\\left(\\frac{\\Xm\\wQ{\\wK}^\\top \\Xm^\\top}{\\sqrt{d_k}}\\right)\\Xm\\, \\kro \\Im_{d_v} = \\Am\\Xm\\, \\kro \\Im_{d_v} \\,.\n\\end{equation*}\nFor the queries, a simple application of the chain rule and then again Eq.~\\eqref{eq:matrix-derivative} gives:\n\\begin{align*}\n    \\frac{\\partial \\Sm}{\\partial \\wQ} &= \\frac{\\partial \\Sm}{\\partial \\Am} \\frac{\\partial \\Am}{\\partial \\wQ}\n     = \\frac{\\partial \\Sm}{\\partial \\Am} \\frac{\\partial \\Am}{\\partial \\Mm} \\frac{\\partial \\Mm}{\\partial \\wQ} \\\\\n     &= \\left(\\Im_n \\kro {\\wV}^\\top\\Xm^\\top\\right) \\frac{\\partial \\Am}{\\partial \\Mm} \\left(\\frac{\\Xm\\kro\\Xm\\wK}{\\sqrt{d_k}}\\right) \\, ,\n\\end{align*}\nwhich is the desired results. Finally, for the gradients of the softmax note that:\n\\begin{equation*}\n    \\frac{\\partial{\\Am_{pq}}}{\\partial \\Mm_{ij}} =\n    \\frac{\\partial}{\\partial \\Mm_{ij}}\\frac{\\exp(\\Mm_{pq})}{\\sum_k \\exp(\\Mm_{pk})} =  \\delta_{ip}\\delta_{jq} \\Am_{ij} - \\delta_{ip} \\Am_{iq}\\Am_{ij} .\n\\end{equation*}\nBy writing the above expression in the matrix notation described above, we obtain the desired result. More specifically, the block diagonal structure is given from the term $\\delta_{ip}$ which stems from the fact that the softmax is applied row-wise. \n\\end{proof}",
        "metadata": {
            "Statement label": "Lemma 11",
            "Statement title": "Gradients of Self Attention"
        }
    },
    {
        "text": "\\begin{lemma}[Gradients of Self Attention with respect to the Embedding matrix]\n\\label{lemma:grads_SA_X}\nThe gradients of the self attention layer with respect to the embedding matrix $\\Xm$ defined in Eq.~\\eqref{eq:self_att} have the following form\n\\small{\n\\begin{align}\n\\label{eq:grad_inp}\n    \\frac{\\partial\\Sm}{\\partial\\Xm} \n    &= \\frac{1}{\\sqrt{d_k}}(\\Im_n\\kro \\wVT\\Xm^\\top)\\,\\frac{\\partial \\Am}{\\partial \\Mm}  \\,\\left(\\Im_n\\kro\\Xm\\wK\\wQT + \\Km_{nn}(\\Im_n\\kro\\Xm\\wQ\\wKT)\\right) \\,+ \\,\\Am\\kro{\\wV}^\\top,\n\\end{align}}\\normalsize\nwhere the gradients of the softmax with respect to its inputs are denoted by $\\frac{\\partial \\Am}{\\partial \\Mm}$ as before.\n\\end{lemma}\nProof:\n\\begin{proof}\nRemember that we defined $\\Sm = \\soft(\\extra{\\frac{1}{\\sqrt{d_k}}}\\Xm\\wQ\\wKT\\Xm^\\top)\\Xm\\wV$. Alongside with our previous shorthands $\\Am$, $\\Mm$, let us define the remaining $\\Xm\\wV$ as a matrix $\\Tm$, so that $\\Sm=\\Am\\,\\Tm$. Both $\\Am$ and $\\Tm$ are functions of $\\Xm$. So the matrix differential can be written as:\n\\begin{align}\n    \\frac{\\partial\\Sm}{\\partial\\Xm} &=  \\frac{\\partial\\Sm}{\\partial\\Am}\\frac{\\partial\\Am}{\\partial\\Xm} + \\frac{\\partial\\Sm}{\\partial\\Tm}\\frac{\\partial\\Tm}{\\partial\\Xm}\\\\\n    &=  \\frac{\\partial\\Sm}{\\partial\\Am}\\frac{\\partial\\Am}{\\partial\\Mm} \\frac{\\partial \\Mm}{\\partial\\Xm} + \\frac{\\partial\\Sm}{\\partial\\Tm}\\frac{\\partial\\Tm}{\\partial\\Xm}\\\\\n    &= (\\Im_n\\kro \\wVT\\Xm^\\top)\\,\\frac{\\partial \\Am}{\\partial \\Mm}  \\,\\frac{\\partial \\Mm}{\\partial \\Xm} + (\\Am\\kro\\Im_d)(\\Im_n\\kro\\wVT)\\\\\n    &= (\\Im_n\\kro \\wVT\\Xm^\\top)\\,\\frac{\\partial \\Am}{\\partial \\Mm}  \\,\\frac{\\partial \\Mm}{\\partial \\Xm} + (\\Am\\kro\\wVT)\\label{eq:grad-SA-X}\n\\end{align}\n\n\nNext, we use the matrix differential and then the identification theorem of matrix derivatives to compute the matrix gradient $\\frac{\\partial \\Am}{\\partial \\Xm}$\n\\begin{align*}\n    \\mathrm{d} \\Am &=  \\extra{\\frac{1}{\\sqrt{d_k}}}\\mathrm{d}(\\Xm)\\, \\wQ\\wKT\\Xm^\\top + \\extra{\\frac{1}{\\sqrt{d_k}}}\\Xm\\wQ\\wKT  \\,\\mathrm{d}(\\Xm^\\top).\n\\end{align*}\n\nVectorizing both sides:\n\\begin{align*}\n    \\mathrm{d} \\vect_r(\\Am) &=  \\extra{\\frac{1}{\\sqrt{d_k}}}(\\Im_n\\kro\\Xm\\wK\\wQT)\\mathrm{d}(\\vect_r(\\Xm))\\, + \\,\\extra{\\frac{1}{\\sqrt{d_k}}}(\\Xm\\wQ\\wKT \\kro\\Im_n) \\,\\mathrm{d}(\\vect_r(\\Xm^\\top)) \\\\\n    &=  \\extra{\\frac{1}{\\sqrt{d_k}}}(\\Im_n\\kro\\Xm\\wK\\wQT)\\mathrm{d}(\\vect_r(\\Xm))\\, + \\,\\extra{\\frac{1}{\\sqrt{d_k}}}(\\Xm\\wQ\\wKT \\kro\\Im_n) \\Km_{dn}\\,\\mathrm{d}(\\vect_r(\\Xm)).\n    \n\\end{align*}\n\nRecall, for an arbitrary matrix $\\Bm\\in\\mathbb{R}^{m\\times n}$, the commutation matrix $\\Km_{mn}$ transforms columnwise vectorization into rowwise vectorization. More precisely,\n\\begin{align*}\n    \\Km_{mn}\\vect_c(\\Bm) = \\vect_c(\\Bm^\\top)\n\\end{align*}\nand $\\vect_c(\\Bm) = \\vect_r(\\Bm^\\top)$. Therefore, for rowwise vectorization, we have a similar result:\n\\begin{align*}\n    \\Km_{mn}\\vect_r(\\Bm^\\top) &= \\vect_r(\\Bm)\\\\\n    \\vect_r(\\Bm^\\top) &= \\Km_{nm}\\vect_r(\\Bm),\n\\end{align*}\nwhere in the last line we used the fact the commutation is a permutation matrix, so $\\Km_{mn}^{-1}=\\Km_{mn}^\\top=\\Km_{nm}$. Thus, we get the required matrix derivative as follows:\n$$\\frac{\\partial \\Am}{\\partial \\Xm} = \\extra{\\frac{1}{\\sqrt{d_k}}}\\Im_n\\kro\\Xm\\wK\\wQT + \\extra{\\frac{1}{\\sqrt{d_k}}}(\\Xm\\wQ\\wKT\\kro\\Im_n)\\Km_{dn}\\,.$$\nNext, we will use a property of commutation matrix to make things simpler (Theorem 7.9, \\cite{magnus2019matrix}):\n$$\n\\frac{\\partial \\Am}{\\partial \\Xm} = \\extra{\\frac{1}{\\sqrt{d_k}}}\\Im_n\\kro\\Xm\\wK\\wQT + \\extra{\\frac{1}{\\sqrt{d_k}}}\\Km_{nn}(\\Im_n\\kro\\Xm\\wQ\\wKT).\n$$\nPlugging this into the above Eq.~\\eqref{eq:grad-SA-X}, we get:\n\\begin{align*}\n     \\frac{\\partial\\Sm}{\\partial\\Xm} \n    &= \\extra{\\frac{1}{\\sqrt{d_k}}}(\\Im_n\\kro \\wVT\\Xm^\\top)\\,\\frac{\\partial \\Am}{\\partial \\Mm}  \\,\\left(\\Im_n\\kro\\Xm\\wK\\wQT + \\Km_{nn}(\\Im_n\\kro\\Xm\\wQ\\wKT)\\right) \\,+ \\,\\Am\\kro{\\wV}^\\top.\n\\end{align*}\nAs a sanity check, we can calculate if the shapes of the matrices are consistent. LHS should be a $nd\\times nd$ matrix, while the constituent matrices of the first term on RHS: $\\Im_n\\kro {\\wV}^\\top\\Xm^\\top\\in\\mathbb{R}^{nd\\times n^2}$, $\\frac{\\partial \\Am}{\\partial \\Mm} \\in\\mathbb{R}^{n^2\\times n^2}$, the additive term next to it is a $n^2\\times nd$ matrix, and the second term on RHS is a Kronecker product of a $n\\times n$ and a $d\\times d$ matrix. \n\\end{proof}",
        "metadata": {
            "Statement label": "Lemma 12",
            "Statement title": "Gradients of Self Attention with respect to Embedding Matrix"
        }
    },
    {
        "text": "\\begin{lemma}[]\n\n\\label{lemma:gradients_queries}\nLet $\\Xm^{\\ell}$ be the representations of the input sequence at the $\\ell$-th layer. Under the uniform-attention assumption, we have\n    \\begin{align}\n    \\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\Wm^{V,\\ell}}\\right\\|^2_F &= d_v n \\mathbb{E}\\|\\bar{\\bm{x}}^{\\ell}\\|^2~\\label{eq:jacobian_values};\\\\ \\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\Wm^{Q,\\ell}} \\right\\|^2_F &= \\frac{\\sigma^2_v\\sigma^2_k d_v}{n^2}\\cdot \\Exp \\left[ \\|\\Xm^{\\ell}\\|^2_F \\cdot  \\|(\\Xm^{\\ell})^\\top\\Xm^{\\ell} - n\\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top\\|^2_F\\right]~\\label{eq:jacobian_queries};\\\\ \n    \\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\Xm^{\\ell}}\\right\\|^2_F &\\leq \\frac{8\\sigma^2_q\\sigma^2_k\\sigma^2_v d_kd_v}{n} \\, \\cdot \\mathbb{E} \\norm{(\\Xm^{\\ell})^\\top\\Xm^{\\ell} - n\\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top}^2_F + 2d_v^2\\sigma^2_v \\; .\n\\end{align}\n\n\\end{lemma}\nProof:\n\\begin{proof}\nBy using the chain rule and the fact that for two matrixes $\\Am, \\Bm$ we have that $\\norm{\\Am\\Bm}_F^2 \\leq \\norm{\\Am}_F^2\\norm{\\Bm}_F^2$, we can upper bound the gradient as:\n\\begin{align*}\n    \\norm{\\frac{\\partial \\mathcal{L}}{\\partial{\\Wm^{Q,\\ell}}}}_F^2 &\\leq \\prod_{i=\\ell+1}^{L-1} \\norm{\\frac{\\partial \\Xm^{i+1}}{\\partial \\Xm^i}}_{F}^2 \\norm{\\frac{\\partial \\mathcal{L}}{\\partial \\Xm^L}}_F^2 \\norm{\\frac{\\partial \\Xm^{\\ell+1}}{\\partial \\Wm^{Q,\\ell}}}_F^2 \\\\\n    &\\leq \\prod_{i=\\ell+1}^{L-1} \\norm{\\frac{\\partial \\Xm^{i+1}}{\\partial \\Xm^i}}_{F}^2 \\norm{\\frac{\\partial \\mathcal{L}}{\\partial \\Xm^L}}_F^2 \\norm{\\frac{\\partial \\Xm^{\\ell+1}}{\\partial \\Zm^{\\ell}}}_F^2 \\norm{\\frac{\\partial \\Zm^\\ell}{\\partial \\Wm^{Q, \\ell}}}_F^2 \\\\\n    &\\leq \\prod_{i=\\ell+1}^{L-1} \\norm{\\frac{\\partial \\Xm^{i+1}}{\\partial \\Xm^i}}_{F}^2 \\norm{\\frac{\\partial \\mathcal{L}}{\\partial \\Xm^L}}_F^2 \\norm{\\frac{\\partial \\Xm^{\\ell+1}}{\\partial \\Zm^{\\ell}}}_F^2 \\left(\\norm{\\frac{\\partial \\alpha_1 \\Sm^{\\ell}}{\\partial \\Wm^{Q, \\ell}}}_F^2 + \\underbrace{\\norm{\\frac{\\partial \\Xm^{\\ell}}{\\partial \\Wm^{Q, \\ell}}}_F^2}_{=0} \\right) ,\n\\end{align*}\nwhere we recall that $\\Zm^{\\ell} = \\alpha_1 \\Sm^{\\ell}+ \\Xm^{\\ell}$ and in the last step we have used that $\\Xm^\\ell$ does not depend on $\\Wm^{Q,\\ell}$, hence the gradient vanishes. By taking expectation and using the tower property, we have that:\n\\begin{equation*}\n    \\Exp \\norm{\\frac{\\partial \\mathcal{L}}{\\partial{\\Wm^{Q,\\ell}}}}_F^2 \\leq \\Exp\\left[\\underbrace{\\Exp\\left[\\prod_{i=\\ell+1}^{L-1} \\norm{\\frac{\\partial \\Xm^{i+1}}{\\partial \\Xm^i}}_{F}^2 \\norm{\\frac{\\partial \\mathcal{L}}{\\partial \\Xm^L}}_F^2 \\norm{\\frac{\\partial \\Xm^{\\ell+1}}{\\partial \\Zm^{\\ell}}}_F^2\\right]}_{=:G(\\Xm^\\ell)} \\norm{\\frac{\\partial \\alpha_1 \\Sm^{\\ell}}{\\partial \\Wm^{Q, \\ell}}}_F^2   \\right],\n\\end{equation*}\nwhere the expectations are taken with respect to $\\Xm^\\ell$ for the outer one and conditioning on $\\Xm^\\ell$ for inner one. Indeed, the first three terms only depend on the network values after $\\Xm^\\ell$. Now, a repeated application of the tower property in $G(\\Xm^\\ell)$, together with the results on the gradients of Lemma \\ref{lemma:gradients_queries}, easily shows that $G(\\Xm^\\ell)$ stays bounded under our hypothesis. To see this one can also simply note that, since the softmax and its derivatives are almost surely bounded, the boundedness of $G(\\Xm^\\ell)$ is implied by an analogous statement for a vanilla linear MLP~(i.e removing the softmax). In this setting, the random variable inside the expectation in $G(\\Xm^\\ell)$ is a finite linear combination of Gaussian products --- which has bounded expectation.\n\nAll in all, we have that\n\\begin{equation*}\n    \\Exp \\norm{\\frac{\\partial \\mathcal{L}}{\\partial{\\Wm^{Q,\\ell}}}}_F^2\\le \\Exp\\left[ B_{\\Xm^\\ell}\\norm{\\frac{\\partial \\alpha_1 \\Sm^{\\ell}}{\\partial \\Wm^{Q, \\ell}}}_F^2\\right],\n\\end{equation*}\nwhere $B_{\\Xm^\\ell}$ is an almost-surely-bounded function of $\\Xm^{\\ell}$. Hence, to show that $\\Exp \\norm{\\frac{\\partial \\mathcal{L}}{\\partial{\\Wm^{Q,\\ell}}}}_F^2=0$, we now just need to show that:\n\\begin{equation*}\n    \\Exp \\norm{\\frac{\\partial \\alpha_1 \\Sm^{\\ell}}{\\partial \\Wm^{Q, \\ell}}}_F^2 = 0\n\\end{equation*}\nunder the rank-1 hypothesis for $\\Xm^\\ell$.\nLet $\\Xm_{1}^\\ell, \\dots \\Xm_{n}^\\ell \\in \\mathbb{R}^{d_v}$  be the representations for the $n$ tokens. Under the rank-1 assumption, each token can be written as a multiple of a single vector $\\bm{x} \\in \\mathbb{R}^{d_v}$, and hence there exists $a_1, \\dots, a_n \\in \\mathbb{R}$ such that $\\Xm_1 = a_1 \\bm{x}, \\dots, \\Xm_n = a_n \\bm{x}$. From Lemma \\ref{lemma:gradients_queries}, we know that:\n\\begin{equation*}\n    \\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\wQ} \\right\\|^2_F = \\frac{\\sigma^2_v\\sigma^2_k d^2}{n^2}\\cdot \\Exp \\left[ \\|\\Xm^{\\ell}\\|^2_F \\cdot  \\|(\\Xm^{\\ell})^\\top\\Xm^{\\ell} - n\\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top\\|^2_F\\right] .\n\\end{equation*}\nThe mean token simplifies to $\\bar{\\bm{x}}^l = \\frac{\\bm{x}}{n}\\sum_k a_k$ and hence $\\left(\\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top\\right)_{ij} = \\frac{1}{n^2} (\\sum_{k}a_k)^2 x_ix_j$. Similarly, $\\left((\\Xm^{\\ell})^\\top\\Xm^{\\ell}\\right)_{ij} = \\sum_k a_k^2 x_i x_j$. If furthermore all the coefficients $a_i$ are the same (which corresponds to the rank collapse assumption $\\Xm^{\\ell}=\\bm{1}_{n}\\bm{x}^T$ analyzed here), then it is easy to see that $\\left((\\Xm^{\\ell})^\\top\\Xm^{\\ell}\\right)_{ij} - n \\left(\\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top\\right)_{ij} = 0 \\; \\forall i,j$ and hence $\\|(\\Xm^{\\ell})^\\top\\Xm^{\\ell} - n\\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top\\|^2_F = 0$.\n\\end{proof}",
        "metadata": {
            "Statement label": "Lemma 13",
            "Statement title": "Gradients of Uniform-Attention Representations"
        }
    },
    {
        "text": "\\begin{lemma}[Expectation of Linear Layers]\n\\label{lemma:exp_linear}\nLet $\\Dm = \\Xm \\Wm$, where $\\Wm\\in\\mathbb{R}^{d\\times d}$ is a random matrix with i.i.d random entries with variance $\\sigma^2 = \\frac{1}{d}$ and $\\Xm\\in\\mathbb{R}^{n\\times d}$ is a fixed matrix:\n\\begin{equation*}\n    \\Exp[\\Dm_{kj}\\Dm_{k'j}] = \\frac{1}{d}\\langle \\Xm_k, \\Xm_{k'} \\rangle\n\\end{equation*}\n\\end{lemma}\nProof:\n\\begin{proof}\n\\begin{equation*}\n     \\Exp [\\Dm_{kj} \\Dm_{k'j}] = \\sum_{zz'} \\Xm_{kz} \\Xm_{k'z'} \\Exp[\\Wm_{zj}\\Wm_{z'j}] = \\sigma^2 \\sum_{z}\\Xm_{kz}\\Xm_{k'z} = \\frac{1}{d}\\langle \\Xm_k, \\Xm_{k'} \\rangle.\n\\end{equation*}\n   \n\\end{proof}",
        "metadata": {
            "Statement label": "Lemma 14",
            "Statement title": "Expectation of Product of Linear Transformations"
        }
    },
    {
        "text": "\\begin{lemma}[Expectation of skip connection]\n\\label{lemma:exp_skip}\n    Let $\\Am, \\Bm \\in \\mathbb{R}^{p \\times q}$. Let $\\Dm := \\alpha\\Am + \\Bm$ with $\\Exp[\\Am | \\Bm] = \\bm{0}$ and $\\alpha \\in \\mathbb{R}$. Then:\n    \\begin{equation}\n        \\Exp\\left[\\Dm_{ij}\\Dm_{i'j}\\right] = \\alpha^2 \\mathbb{E}[\\Am_{ij}\\Am_{i'j}] + \\mathbb{E}[\\Bm_{ij}\\Bm_{ij'}] \n    \\end{equation}\n    holds for all $i,i' \\in [p], j \\in [q]$.\n\\end{lemma}\nProof:\n\\begin{proof}\n\\begin{align*}\n    \\mathbb{E}[\\Dm_{ij}\\Dm_{i'j}] &= \\mathbb{E}\\left[(\\alpha \\Am_{ij} + \\Bm_{ij})(\\alpha \\Am_{i'j} + \\Bm_{i'j})\\right]\\\\\n    &= \\mathbb{E}\\left[\\alpha^2 \\Am_{ij}\\Am_{i'j} + \\alpha \\Am_{ij}\\Bm_{i'j} + \\alpha \\Am_{i'j}\\Bm_{ij} + \\Bm_{ij}\\Bm_{i'j} \\right] \\\\\n    &= \\alpha^2\\mathbb{E}\\left[ \\Am_{kj}\\Am_{i'j}\\right] + \\mathbb{E}\\left[ \\Bm_{ij}\\Bm_{i',j} \\right],\n\\end{align*}\nwhere using iterated expectations $\\alpha \\Exp[\\Am_{i'j}\\Bm_{ij}] = \\alpha \\Exp[\\Exp [\\Am_{i'j} | \\Bm] \\Bm_{ij}]] = 0$ and identically $\\alpha \\Exp [\\Am_{ij}\\Bm_{i'j}] = 0$.\n\\end{proof}",
        "metadata": {
            "Statement label": "Lemma 15",
            "Statement title": "Linearity of Expectation with Skip Connection"
        }
    },
    {
        "text": "\\begin{lemma}[Expectation of Attention Layers]\n\\label{lemma:exp_softmax}\nUnder the uniform-attention assumption:\n\\begin{equation*}\n    \\Exp[\\Sm_{kj}\\Sm_{k'j}] = \\frac{1}{d_vn^2}\\Exp C(\\Xm).\n\\end{equation*}\n\\end{lemma}\nProof:\n\\begin{proof}\nNote that under the uniform-attention assumption:\n\\begin{equation*}\n    \\Sm_{kj} = \\frac{1}{n}\\left(\\bm{1}_{n\\times n}\\Xm \\Wm^V\\right)_{kj} = \\frac{1}{n}\\sum_{zi}\\Xm_{zi}\\Wm^V_{ij}.\n\\end{equation*}\nHence, using the fact that the weights are i.i.d with variance $\\sigma_v^2=\\frac{1}{d_v}$:\n\\begin{align*}\n    \\Exp[\\Sm_{kj}\\Sm_{k'j}] = \\frac{\\sigma_{v}^2}{n^2}\\sum_{z,z'}\\sum_i\\Exp[\\Xm_{zi} \\Xm_{z'i}] = \\frac{1}{d_vn^2}\\sum_{k,k'}\\langle\\Xm_z, \\Xm_{z'}\\rangle = \\frac{1}{d_vn^2}\\Exp C(\\Xm).\n\\end{align*}\n\\end{proof}",
        "metadata": {
            "Statement label": "Lemma 16",
            "Statement title": "Expectation of Softmax Products"
        }
    },
    {
        "text": "\\begin{lemma}[Propagation of inner products]\n\n\\label{lemma:propagation_of_inner_producets}\n Let $C(\\Xm^\\ell) = \\sum_{k,k'} \\langle \\Xm_{k}^\\ell, \\Xm_{k'}^\\ell \\rangle$ and $\\Xm$ the input sequence. Under the Assumption~\\ref{ass:uniform_softmax} and if $\\sigma$ is the linear activation function, we have that:\n \n \\begin{equation}\n     \\Exp \\left[C(\\Xm^{L})\\right] = (\\alpha_2^2 + 1)^{L}(\\alpha_1^2 + 1)^{L}C(\\Xm)  .\n \\end{equation}\n hence, under the depth scaling for the residual block parameters $\\alpha_1^2 = \\frac{\\tilde{\\alpha}_1}{L}, \\alpha_2^2 = \\frac{\\tilde{\\alpha}_2}{L}$ with $\\tilde{\\alpha}_1, \\tilde{\\alpha}_2 \\in \\mathbb{R}$ independent of $L$, we have that:\n \\begin{equation}\n      \\lim_{L\\to \\infty} \\Exp[C(\\Xm^L)] = \\text{e}^{\\tilde{\\alpha}_1 + \\tilde{\\alpha}_2}C(\\Xm).\n \\end{equation}\n\n\\end{lemma}\nProof:\n\\begin{proof}\nFirst, note that for the residual blocks we have that $\\Exp [Y^{\\ell}_{kj} | Z^{\\ell}_{k'j}] = 0$ due to the independence assumption on the feedforward weights, and similarly $\\Exp [S^{\\ell}_{kj} | X^{\\ell}_{k'j}] = 0$. Hence, we can use Lemma \\ref{lemma:exp_skip} in both the skip connections of the Transformer architecture.\nTherefore, using Lemma \\ref{lemma:exp_skip} (skip), Lemma \\ref{lemma:exp_linear} (linear) and Lemma \\ref{lemma:exp_softmax} (attention):\n\\begin{align*}\n        &\\mathbb{E}[C(\\Xm^{\\ell+1})] \\\\\n        \\overset{\\text{skip}}&{=} \\alpha_2^2\\mathbb{E}C(\\Ym^\\ell) + \\mathbb{E}C(\\Zm^{\\ell}) \\\\ \n        \\overset{\\text{linear}}&{=} \\alpha_2^2\\mathbb{E}C(\\Zm^{\\ell}) + \\mathbb{E}C(\\Zm^{\\ell}) \\\\ \n        &= (\\alpha_2^2 + 1)\\mathbb{E}C(\\Zm^{\\ell}) \\\\\n        \\overset{\\text{skip}}&{=} (\\alpha_2^2 + 1)\\left(\\alpha_1^2\\mathbb{E}C(\\Sm^{\\ell}) + \\mathbb{E}C(\\Xm^{\\ell})\\right) \\\\ \n        \\overset{\\text{attention}}&{=}  (\\alpha_2^2 + 1)(\\alpha_1^2 + 1) \\mathbb{E}[C(\\Xm^{\\ell})] \\\\ \n        \\overset{\\text{unroll recurs.}}&{=} (\\alpha_2^2 + 1)^{\\ell+1}(\\alpha_1^2 + 1)^{\\ell+1}C(\\Xm) ,\n    \\end{align*}\n    where in the last step we have unrolled the recursion until the input layer.\n    \n    For the limit as $L\\to \\infty$, simply note that:\n    $$\\lim_{L\\to \\infty}\\left(\\frac{\\tilde{\\alpha}_i}{L}+1\\right)^{L} = \\text{e}^{\\tilde{\\alpha}_i} ,$$ \n    with $i \\in \\{1, 2\\}$.\n\\end{proof}",
        "metadata": {
            "Statement label": "Lemma 17",
            "Statement title": "Exponential Growth of Inner Products"
        }
    },
    {
        "text": "\\begin{lemma}[Propagation of the norm]\n\n\\label{thm:forward_pass}\n  Let $\\Xm^{L}$ be the representations of the input sequence at the final layer. Under the assumptions of Lemma \\ref{lemma:propagation_of_inner_producets}, we have that:\n \\begin{equation}\n     \\Exp \\norm{\\Xm^{L}}_{F}^2 = n (\\alpha_2^2+1)^{L}\\alpha_1^2 \\sum_{k=0}^{L-1}(\\alpha_1^2+1)^k \\norm{\\bar{\\bm{x}}}^2 + (\\alpha_2^2+1)^{L} ||\\Xm||_F^2  ,\n \\end{equation}\n hence, under the depth scaling for the residual block parameters $\\alpha_1^2 = \\frac{\\tilde{\\alpha}_1}{L}, \\alpha_2^2 = \\frac{\\tilde{\\alpha}_2}{L}$ with $\\tilde{\\alpha_1}, \\tilde{\\alpha_2} \\in \\mathbb{R}$ independent of $L$, we have that:\n \\begin{equation}\n     \\lim_{L\\to \\infty} \\Exp \\norm{\\Xm^{L}}_{F}^2 = n \\text{e}^{\\tilde{\\alpha}_2}(\\text{e}^{\\tilde{\\alpha}_1} - 1)\\norm{\\bar{\\bm{x}}}^2 + \\text{e}^{\\tilde{\\alpha}_2} ||\\Xm||_F^2.\n \\end{equation}\n\n\\end{lemma}\nProof:\n\\begin{proof}\nDue to the rotational symmetries of the Gaussian random matrices, if the input  tokens have the same norm, then the expected norm at layer $\\ell \\in [L]$ is also the same across the token's representations. Hence, we can write $\\Exp\\norm{\\Xm^\\ell}_F^2 = n \\Exp\\norm{\\bm{x}^\\ell}^2$, where $\\norm{\\bm{x}^\\ell}^2$ is the norm of every token at layer $\\ell$. Furthermore, by definition of our correlation coefficient $\\rho^l_{kk'}$, we have that $\\Exp\\langle\\Xm^\\ell_k, \\Xm^\\ell_{k'}\\rangle = \\rho^\\ell_{kk'} \\Exp\\norm{\\bm{x}^\\ell}^2$. By summing over the indexes $k,k'$, we can expand the relation as:\n\\begin{equation*}\n    \\underbrace{\\sum_{k,k'}\\Exp\\langle\\Xm^\\ell_k, \\Xm^\\ell_{k'}\\rangle}_{\\Exp C(\\Xm)} = \\sum_{k,k'} \\rho^\\ell_{kk'} \\Exp\\norm{\\bm{x}^\\ell}^2 = (n + \\sum_{k\\neq k'}\\rho^\\ell_{k,k'})\\Exp\\norm{\\bm{x}^\\ell}^2 = \\underbrace{n\\Exp\\norm{\\bm{x}^\\ell}^2}_{\\Exp \\norm{\\Xm^\\ell}_F^2}(1 + (n-1) \\rho^\\ell).\n\\end{equation*}\nBy solving for $\\rho^\\ell$, we have that:\n\\begin{equation*}\n    \\rho^\\ell = \\frac{\\Exp C(\\Xm^\\ell)}{(n-1)\\Exp \\norm{\\Xm^\\ell}^2 } - \\frac{1}{n-1} .\n\\end{equation*}\nNow we plug in the expressions for $\\Exp C(\\Xm^\\ell)$ and $\\Exp \\norm{\\Xm^\\ell}^2 $ with the aid of Lemma \\ref{lemma:propagation_of_inner_producets} and Lemma \\ref{thm:forward_pass}, respectively. Finally, by taking the limits with respect to $L$, we get the desired result.\n\\end{proof}",
        "metadata": {
            "Statement label": "Lemma 18",
            "Statement title": "Exponential Norm Growth in Deep Networks"
        }
    },
    {
        "text": "\\begin{lemma}\n\\label{app:convergence_A}\nConsider initializing each entry of $\\wQ\\in\\mathbb{R}^{d_{v}\\times d_k}$ and $\\wK\\in\\mathbb{R}^{d_{v}\\times d_k}$ independently with variance $\\sigma^2_k = 2/(d_{v}+d_k)$ --- i.e. Glorot initialization~\\citep{glorot2010understanding}. Let $\\Mm = \\frac{1}{\\sqrt{d_k}}\\Xm^\\ell\\Wm^{Q,\\ell}{\\Wm^{K,\\ell}}^\\top{\\Xm^\\ell}^\\top$; for any $(i,j)\\in[n]\\times[n]$ we have\n\\begin{equation}\n    \\Exp[\\Mm_{i,j} \\ | \\ \\Xm] = 0,\\qquad \\Exp[\\Mm_{i,j}^2 \\ | \\ \\Xm] = \\sigma_k^4 \\cdot \\|\\Xm_{i,:}\\|^2 \\cdot \\|\\Xm_{j,:}\\|^2.\n\\end{equation}\nWhile keeping $d_v<\\infty$ fixed, taking $d_k$ to infinity yields\n\\begin{equation}\n    \\Exp[\\Mm_{i,j}^2 \\ | \\ \\Xm] = \\mathcal{O}\\left(\\frac{1}{d_k^2}\\right).\n\\end{equation}\nIn other words, $\\Mm$ converges to $\\bm{0}_{n\\times n}$ in $L^2$ as $d_k\\to\\infty$. \n\\end{lemma}\nProof:\n\\begin{proof}\nFirst, note that\n\\begin{align*}\n    \\Mm_{i,j} = \\frac{1}{\\sqrt{d_k}}\\sum_{a,c=1}^{d_{v}}\\sum_{b=1}^{d_k} \\Xm_{i,a}\\wQ_{a,b}\\wK_{c,b} \\Xm_{j,c}.\n\\end{align*}\nSince $\\wQ$ is independent from $\\wK$ at initialization, $\\Exp[\\Mm_{i,j} \\ | \\ \\Xm] = 0$. Next, we compute\n\\begin{align*}\n    \\Exp[\\Mm_{i,j}^2] &= \\frac{1}{d_k}\\sum_{a,c,a',c'=1}^{d_{v}}\\sum_{b,b'=1}^{d_{k}} \\Xm_{i,a} \\Xm_{i,a'} \\Xm_{j,c} \\Xm_{j,c'} \\Exp\\left[\\wQ_{a,b}\\wQ_{a',b'}\\wK_{c,b} \\wK_{c',b'}\\right] \\\\\n    &=\\frac{1}{d_k}\\sum_{a,c,a',c'=1}^{d_{v}}\\sum_{b,b'=1}^{d_{k}} \\Xm_{i,a} \\Xm_{i,a'} \\Xm_{j,c} \\Xm_{j,c'} \\Exp\\left[\\wQ_{a,b}\\wQ_{a',b'}\\right]\\Exp\\left[\\wK_{c,b} \\wK_{c',b'}\\right] \\\\\n    &= \\frac{\\sigma_k^4}{d_k}\\sum_{a,c=1}^{d_{v}}\\sum_{b=1}^{d_{k}}  \\Xm_{i,a}^2 \\Xm_{j,c}^2\\\\\n    &= \\sigma_k^4 \\|\\Xm_{i,:}\\|^2 \\|\\Xm_{j,:}\\|^2.\n\\end{align*}\nThis concludes the proof.\n\\end{proof}",
        "metadata": {
            "Statement label": "Lemma 19",
            "Statement title": "Vanishing Attention Lemma"
        }
    },
    {
        "text": "\\begin{lemma}[Borel-Cantelli]\nLet $(X_i)$ be a sequence of random variables. If for any $\\epsilon>0$\n\\begin{equation*}\n    \\sum_{i=0}^\\infty \\mathbb{P}[|X_i-X|>\\epsilon]<\\infty,\n\\end{equation*}\nthen $X_i$ converges to $X$ almost surely\\footnote{That is, $\\lim_{i\\to\\infty} X_i(\\omega) = X(\\omega)$ for almost every $\\omega\\in \\Omega$~(i.e. with probability one).}.\n\\end{lemma}",
        "metadata": {
            "Statement label": "Lemma 20",
            "Statement title": "Borel-Cantelli Lemma"
        }
    },
    {
        "text": "\\begin{theorem}[Isserlis]\n    Let $X_1, \\dots X_m$ be $m$ zero-mean Gaussian random variables. Then:\n    \\begin{equation}\n        \\Exp[X_1 \\cdots X_m] = \\begin{cases}\n    \\sum_{p\\in P_m^2} \\prod_{(i,j) \\in p} \\Exp[X_iX_j] & m \\text{ even} \\\\\n    0 & m \\text{ odd}\n    \\end{cases} \n    \\end{equation}\n    \n    where $P_m^2$ is the set of all the possible pairings of the indexes $1,\\dots, m$. \n    \\end{theorem}\nProof:\n",
        "metadata": {
            "Statement label": "Theorem 19",
            "Statement title": "Isserlis' theorem"
        }
    },
    {
        "text": "\\begin{theorem}[Almost-sure convergence]\n\\label{thm:soft_assumption_proof}\nConsider initializing each entry of $\\wQ\\in\\mathbb{R}^{d_v\\times d_k}$ and $\\wK\\in\\mathbb{R}^{d_v\\times d_k}$ independently with variance $\\sigma^2_k = 2/(d_v+d_k)$ --- i.e. Glorot initialization~\\citep{glorot2010understanding}. Let $d_v<\\infty$ be fixed, as $d_k\\to\\infty$ we have that, for any $\\Xm$,\n\\begin{equation*}\n    \\Am := \\soft\\left(\\frac{1}{\\sqrt{d_k}}\\Xm\\Wm^{Q}{\\Wm^{K}}^\\top{\\Xm}^\\top\\right)\\stackrel{a.s.}{\\to} \\frac{1}{n}\\bm{1}_{n\\times n}\n\\end{equation*}\nand\n\\begin{equation*}\n    \\frac{\\partial\\Am}{\\partial\\Mm} \\stackrel{a.s.}{\\to} \\frac{1}{n}\\Im_n \\otimes \\left(\\Im_n - \\frac{1}{n}\\bm{1}_{n\\times n} \\right).\n\\end{equation*}\n\\end{theorem}\nProof:\n\\begin{proof}\nThanks to Lemma~\\ref{app:convergence_A} and Markov Inequality, we have fast convergence in probability: for any fixed $\\Xm$,\n\\begin{equation*}\n    \\mathbb{P}[|\\Mm_{i,j}|>\\epsilon]\\le\\frac{\\Exp[\\Mm^2_{i,j}]}{\\epsilon^2} \\le \\frac{C_\\epsilon}{d_k^2}.\n\\end{equation*}\nBorel Cantelli then directly yields almost sure convergence of $\\Mm$ to $\\bm{0}_{n\\times n}$ as $d_k\\to\\infty$. Next, note that both $\\Am$ and $\\frac{\\partial\\Am}{\\partial\\Mm}$ are continuous functions of $\\Am$, hence we can apply standard continuity event-per-event. For almost every $\\omega\\in\\Omega$,\n\\begin{equation*}\n    \\lim_{d_k\\to\\infty} \\Am(\\Am(\\omega)) = \\Am\\left(\\lim_{d_k\\to\\infty}\\Am(\\omega)\\right) = \\Am( \\bm{0}_{n\\times n}) = \\frac{1}{n}\\bm{1}_{n\\times n}.\n\\end{equation*}\nHence $\\Am\\to \\frac{1}{n}\\bm{1}_{n\\times n}$ almost surely. This can also be seen as a simple application of the continuous mapping theorem. The same reasoning yields almost sure convergence of\n\\begin{equation*}\n    \\frac{\\partial \\Am}{\\partial \\Mm} = \\blockdiag\\Bigg(\\diag(\\Am_{i:}) - \\Am_{i:}\\Am_{i:}^\\top\\Bigg),\n\\end{equation*}\nto the corresponding limiting quantity.\n\\end{proof}",
        "metadata": {
            "Statement label": "Theorem 20",
            "Statement title": "Asymptotic Uniformity of Softmax Attention"
        }
    },
    {
        "text": "\\begin{definition}[\\NEIS and Locally-Optimal Direction]\\label{def loc opt} \nFix token indices $\\bal=(\\alpha_i)_{i=1}^n$. Solve \\eqref{eqn:sattnsvm} with $ (\\opt_i)_{i=1}^n$ replaced with $\\boldsymbol{\\alpha} = (\\alpha_i)_{i=1}^n$ to obtain $\\Wma$. Consider the set $\\Tc_i\\subset[T]$ such that $(\\x_{i\\alpha_i}-\\x_{it})^\\top \\Wma \\z_i=1$ for all $t\\in\\Tc_i$. We refer to $(\\Tc_i)_{i=1}^n$ as the \\neis of $\\bal$. Additionally, if for all $i\\in[n]$ and $t\\in\\Tc_i$ scores per Definition~\\ref{score def} obey $\\bgam_{i\\alpha_i}>\\bgam_{it}$, indices $\\bal=(\\alpha_i)_{i=1}^n$ are called \\emph{locally-optimal} and $\\Wma$ is called a \\emph{locally-optimal direction}.\n\\end{definition}",
        "metadata": {
            "Statement label": "Definition 12",
            "Statement title": "Locally-Optimal Indices and Direction"
        }
    },
    {
        "text": "\\begin{definition}[Token Score and Optimality]\\label{score def}\nGiven a prediction head $\\vb\\in\\R^d$, the score of a token $\\x_{it}$ of input $\\X_i$ is defined as $\\bgam_{it} = Y_i \\cdot \\vb^\\top\\x_{it}$. The optimal token for each input $\\X_i$ is given by the index $\\op_i \\in \\arg\\max_{t \\in [T]} \\bgam_{it}$ for all $i \\in [n]$.\n\\end{definition}",
        "metadata": {
            "Statement label": "Definition 13",
            "Statement title": "Token Scoring and Optimality Criterion"
        }
    },
    {
        "text": "\\begin{definition}[Low\\&High Score Tokens and Separating Cone]\\label{HL cone def main} Given $\\al\\in[T]$, input sequence $\\X$ with label $Y$, $h(\\cdot):\\R^d\\rightarrow\\R$, and score $\\bgam_t=Y\\cdot h(\\x_t)$ for all $t\\in[T]$, define the low and high score tokens as\n\\[\n\\low:=\\left\\{t\\in[T]\\bgl \\bgam_t<\\bgam_\\al\\right\\},\\quad \\high:=\\left\\{t\\in[T]-\\{\\alpha\\}\\bgl \\bgam_t\\geq \\bgam_\\al\\right\\}.\n\\]\nFor input $\\X_i$ and index $\\alpha_i$, we use the shorthand notations $\\texttt{low}^\\alpha_i$ and $\\texttt{high}^\\alpha_i$. Finally define $\\con{\\bal}$ as\n\\begin{align}\n\\con{\\bal}:=\\left\\{\\texttt{rank}(W)\\leq m\\bgl \\min_{i\\in[n]}\\max_{t\\in\\texttt{low}^\\alpha_i}\\min_{\\tau\\in\\texttt{high}^\\alpha_i} (\\x_\\itt-\\x_\\ittt)^\\top\\W\\z_i\\geq \\eps\\tf{\\W}\\right\\}.\\label{cone alpha eq1}\n\\end{align}\n\\end{definition}",
        "metadata": {
            "Statement label": "Definition 14",
            "Statement title": "Low and High Score Token Separation Criterion"
        }
    },
    {
        "text": "\\begin{definition} [Toy Distribution for Self-Attn]\\label{def data model} Data $(\\X,Y)$ is generated according to $\\Dc_{\\texttt{data}}$ as follows: Let $\\rho >1$ be the index of $\\x_1$'s \\emph{relevant token} (that is allowed to be random).\n\\begin{itemize}\n\\item \\textbf{Relevant token:} $(\\x_1,\\x_\\rho)$ has a uniform distribution over $r$ values $(\\ab_j,\\bb_j)_{j=1}^r$ with associated labels $(y_j)_{j=1}^r$. That is, whenever $(\\x_1,\\x_\\rho)=(\\ab_j,\\bb_j)$, the output label is $Y=y_j$. \n\\item \\textbf{Fixed score:} For some $\\gamma_1,\\gamma_\\rho$ and all $j\\in[r]$: $y_j \\vb^\\top\\ab_j=\\gamma_1$, $y_j \\vb^\\top\\bb_j=\\gamma_\\rho>0$.\n\\item \\textbf{Other tokens} $t\\not\\in\\{\\rho,1\\}$ are bounded, independent of the rest, and $\\E[\\x_t]=\\E[\\x_t\\x_t^\\top]\\vb=0$.\n\\end{itemize}\n\\end{definition}",
        "metadata": {
            "Statement label": "Definition 15",
            "Statement title": "Toy Distribution for Self-Attention"
        }
    },
    {
        "text": "\\begin{definition}[Low\\&High Score Tokens and Separating Cone]\\label{HL cone def} Given $\\al\\in[T]$, input sequence $\\X$ with label $Y$, $h(\\cdot):\\R^d\\rightarrow\\R$, and score $\\bgam_t=Y\\cdot h(\\x_t)$ for all $t\\in[T]$, define the low and high score tokens as\n\\[\n\\low=\\left\\{t\\in[T]\\bgl \\bgam_t<\\bgam_\\al\\},\\quad \\high=\\{t\\in[T]-\\{\\alpha\\}\\bgl \\bgam_t\\geq \\bgam_\\al \\right\\}.\n\\]\nFor input $\\X_\\ik$ and index $\\alpha_\\ik$, we use the shorthand notations $\\lowi,\\higi$. Finally define $\\con{\\bal}$ as\n\\begin{align}\n\\con{\\bal}=\\left\\{\\W\\in\\Rcm\\bgl \\min_{i\\in[n]}\\max_{t\\in\\lowi}\\min_{\\tau\\in\\higi} \\inn{\\F_\\ikt-\\F_\\iktt,\\W}\\geq \\eps\\tf{\\W} \\right\\}.\\label{cone alpha eq}\n\\end{align}\n\\end{definition}",
        "metadata": {
            "Statement label": "Definition 16",
            "Statement title": "Low and High Score Token Separation"
        }
    },
    {
        "text": "\\begin{definition}[\\Neis and Locally-Optimal Indices]\\label{seq loc opt} Fix token indices $\\bal=(\\alpha_\\ik)\\ikix$ for which \\eqref{seqattnsvm} is feasible to obtain $\\Wma:= \\Wm_{\\dm,\\bal}$. Define token scores as\n\\[\n\\bgam_\\ikt=Y_\\ik\\cdot h_k(\\x_\\itt),\\quad \\bga_\\ik:=\\bgam_{ik\\alpha_\\ik}=Y_\\ik\\cdot h_k(\\xa_\\ik).\n\\]\nConsider tokens $\\Tc_\\ik\\subset[T]$ such that $\\inn{\\Fa_\\ik-\\F_\\ikt,\\Wma}=1$ for all $t\\in\\Tc_\\ik$. $\\Tc_\\ik$ is allowed to be an empty set. We refer to $\\Tc_\\ik$ as \\neis of $\\Fa_\\ik=\\xa_\\ik\\z_\\ik^\\top$ and define its complement $\\Tcb_\\ik=[T]-\\Tc_\\ik-\\{\\al_\\ik\\}$.  Additionally, token indices $\\bal=(\\alpha_\\ik)\\ikix$ are called locally-optimal if for all $i\\in[n],k\\in[K]$ and  $t\\in\\Tc_\\ik$, token scores obey $\\bga_\\ik>\\bgam_\\ikt$. Associated $\\Wma$ is called a locally-optimal direction. Finally, let $\\op_\\ik\\in\\arg\\max_{t\\in[T]}\\bgam_\\ikt$ be the optimal indices and define the associated $\\Ws(\\op)$ to be a globally-optimal direction.\n\\end{definition}",
        "metadata": {
            "Statement label": "Definition 17",
            "Statement title": "Locally-Optimal Token Selection"
        }
    },
    {
        "text": "\\begin{lemma}[Optimal Tokens Minimize Training Loss]\\label{lem min risk} Suppose Assumption \\ref{assum:loss:prope} (i)-(ii) hold, and not all tokens are optimal per Definition~\\ref{score def}. Then, training risk obeys $\\Lc(\\W)>\\Lc_\\st:=\\frac{1}{n}\\sum_{i=1}^n \\ell(\\bgam_{i\\op_i})$. Additionally, suppose there are optimal indices $(\\op_i)_{i=1}^n$ for which \\eqref{eqn:sattnsvm} is feasible, i.e.~there exists a $\\W$ separating optimal tokens. This $\\W$ choice obeys $\\lim_{R\\rightarrow\\infty}\\Lc(R\\cdot\\W)=\\Lc_\\st$.\n\\end{lemma}",
        "metadata": {
            "Statement label": "Lemma 21",
            "Statement title": "Optimal Token Separation Lemma"
        }
    },
    {
        "text": "\\begin{lemma} \nLet $\\Wc^\\svm_\\star$ be the solution set of \\eqref{eqn:sattnsvmst} with nuclear norm achieving objective $C_\\st$. Further let $\\Wcs_{\\texttt{cvx}}$ be the solution set of \\eqref{eqn:sattnsvmst} with $m=d$ achieving objective $C_{\\texttt{cvx}}$. If $\\Wcs_\\st\\cap \\Wcs_{\\texttt{cvx}}\\neq\\emptyset$, then $C_\\st=C_{\\texttt{cvx}}$ and $\\Wcs_\\st\\subseteq \\Wcs_{\\texttt{cvx}}$. Also, if the elements of $\\Wcs_{\\texttt{cvx}}$ have rank at most $m$, then,  $\\Wcs_\\st=\\Wcs_{\\texttt{cvx}}$.  \n\\end{lemma}",
        "metadata": {
            "Statement label": "Lemma 22",
            "Statement title": "Equivalence of Solution Sets in Constrained Optimization"
        }
    },
    {
        "text": "\\begin{lemma}\\label{lem:lip}\nUnder Assumption~\\ref{assum:loss:prope}, $ \\nabla\\Lc(\\W)$,   $ \\nabla_{\\Kb} \\Lc(\\Kb,\\Qb)$, and  $\\nabla_{\\Qb} \\Lc(\\Kb,\\Qb)$ are $L_{\\W}$,  $L_{\\Kb}$, $L_{\\Qb}$--Lipschitz continuous, respectively, where $a_i=\\|\\vb\\|~\\|\\z_i\\|^2 \\|\\X_i \\|^3$,  $b_i= M_0\\|\\vb\\|~\\|\\X_i\\|+ 3  M_1 $ for all $i\\in[n]$,\n\\begin{align}\\label{eqn:lip:cons:erm}\nL_{\\W}:=\\frac{1}{n}\\sum_{i=1}^{n} a_i b_i, \\quad L_{\\Kb}:= \\|\\Qb\\|L_{\\W}, \\quad \\textnormal{and} \\quad L_{\\Qb}:= \\|\\Kb\\|L_{\\W}.\n\\end{align}\n\\end{lemma}",
        "metadata": {
            "Statement label": "Lemma 23",
            "Statement title": "Lipschitz Continuity of Gradients Lemma"
        }
    },
    {
        "text": "\\begin{lemma}\\label{lem:rank} Any optimal solution of \\eqref{eqn:sattnsvm} or \\eqref{eqn:sattnsvmst} is at most rank $n$. More precisely, the  row space of $\\Ws$ or $\\Ws_\\st$ lies within $\\texttt{span}(\\{\\z_i\\}_{i=1}^n)$.\n\\end{lemma}",
        "metadata": {
            "Statement label": "Lemma 24",
            "Statement title": "Rank Bound Lemma"
        }
    },
    {
        "text": "\\begin{lemma} \\label{lemma cone main}Suppose \\eqref{dmattnsvm} is feasible. If indices $\\bal$ are locally-optimal, $\\Wma\\in \\con{\\bal}$ for all sufficiently small $\\eps>0$. Otherwise, $\\Wma\\not\\in \\con{\\bal}$ for all $\\eps>0$. Additionally, suppose optimal indices $\\op_i\\in\\arg\\max_{t\\in[T]}\\bgam_\\itt$ are unique and set $\\bal\\gets\\op$. Then, $\\con{\\opt}$ is the set of all rank-$\\leq$$m$ matrices (i.e.~global set).\n\\end{lemma}",
        "metadata": {
            "Statement label": "Lemma 25",
            "Statement title": "Cone Membership Lemma"
        }
    },
    {
        "text": "\\begin{lemma}\\label{example dataset} Given $\\vb\\in\\R^d$, recall the score vector $\\bgam=\\X\\vb$. Without losing generality, assume $\\bgam$ is non-increasing. Define the vector of score gaps $\\bbg\\in\\R^{T-1}$ with entries $\\bbg_t=\\bgam_{t}-\\bgam_{t+1}$. Suppose all tokens within the input sequence are orthonormal and for some $\\tau\\geq 2$, we have that \n\\begin{align}\n\\tau\\bbg_\\tau/2>\\bbg_1.\\label{tau description}\n\\end{align}\nSet $h(\\x)=\\vb^\\top\\x-\\la\\tn{\\x}^2$ where $\\tau\\bbg_\\tau/2>\\la>\\bbg_1$, $\\ell(x)=-x$, and $Y=1$. Let $\\Bal_T$ denote the $T$-dimensional simplex. Define the unconstrained softmax optimization associated to the objective $h$ where we make $\\s:=\\sft{\\X\\W\\z}$ a free variable, namely,\n\\begin{align} \n\\min_{\\s\\in\\Bal_T}\\ell(h(\\X\\s))=\\min_{\\s\\in\\Bal_T}\\la \\tn{\\X^\\top \\s}^2-\\vb^\\top\\X^\\top \\s.\\label{direct opt}\n\\end{align}\nThen, the optimal solution $\\s^\\st$ contains at least $2$ and at most $\\tau$ nonzero entries.\n\\end{lemma}",
        "metadata": {
            "Statement label": "Lemma 26",
            "Statement title": "Sparse Softmax Optimization Lemma"
        }
    },
    {
        "text": "\\begin{lemma}\\label{lemma cone} Consider the cone definition of \\eqref{cone alpha eq} and suppose an SVM solution $\\Wma$ exists. If indices $\\bal$ are locally-optimal, $\\Wma\\in \\con{\\bal}$ for all sufficiently small $\\eps>0$. Otherwise, $\\Wma\\not\\in \\con{\\bal}$ for all $\\eps>0$. Additionally, suppose optimal indices $\\op_\\ik\\in\\arg\\max_{t\\in[T]}\\bgam_\\ikt$ are unique. Then, $\\con{\\opt}=\\Rcm$.\n\\end{lemma}\nProof:\n\\begin{proof} Suppose $\\bal$ is locally optimal. Observe that, thanks to local optimality, $\\Wma$ obeys\n\\[\n\\min_{t\\in\\Tc_\\ik}\\inn{\\F_\\ikt,\\Wma}>\\max_{\\tau\\not\\in\\Tc_\\ik\\cup\\{\\al_\\ik\\}}\\inn{\\F_\\iktt,\\Wma},\n\\]\nfor all $i\\in[n]$. Next, observe that $\\Tc_\\ik\\subseteq \\lowi$ and $\\higi\\subseteq\\Tcb_\\ik=[T]-\\Tc_\\ik-\\{\\al_\\ik\\}$. Thus, the inequality \\eqref{cone alpha eq} holds for small enough $\\eps>0$.\n\nConversely, suppose $\\bal$ is not locally-optimal. Fix \\nei $t\\in\\Tc_\\ik$ with $t\\in \\higi$. Since $t\\in\\Tc_\\ik$, observe that\n\\[\n\\inn{\\F_\\ikt,\\Wma}\\geq \\max_{\\tau\\neq \\al_\\ik}\\inn{\\F_\\iktt,\\Wma}.\n\\]\nIn other words, for this $i\\in[n]$, we found\n\\[\n\\max_{\\tau\\in\\lowi} \\inn{\\F_\\iktt-\\F_\\ikt,\\Wma}\\leq 0,\n\\]\nviolating \\eqref{cone alpha eq} definition for any $\\eps>0$. To show the final claim, observe that, setting $\\bal:=\\op$, we have that $\\higi=\\emptyset$ for all $i\\in[n]$ as $\\op_\\ik$ are unique optimal indices. Thus, there is no constraint enforced on the cone definition in \\eqref{cone alpha eq} making it equal to the rank-$m$ manifold $\\Rcm$.\n\\end{proof}",
        "metadata": {
            "Statement label": "Lemma 27",
            "Statement title": "Cone Membership Lemma"
        }
    },
    {
        "text": "\\begin{lemma}[Mapping regularization path of $(\\Kb,\\Qb)$ to $\\W$]\\label{kqw mapping} Let $\\Kb,\\Qb\\in\\R^{d\\times m}$ and consider regularization path solutions of \\eqref{serm-w} and \\eqref{serm-kq}\n\\begin{align}\n&\\Wb_R\\in\\underset{\\W\\in\\Rcm:\\tnuc{\\W}\\leq R}{\\arg\\min}\\Lc(\\W)\\label{Wpath}\\\\\n&\\Kbb_R,\\Qbb_R\\in\\underset{\\tf{\\Kb}^2+\\tf{\\Qb}^2\\leq 2R}{\\arg\\min}\\Lc(\\Kb,\\Qb).\\label{KQpath}\n\\end{align}\nFor all $R\\geq 0$, there is a one-to-one map between the set of solutions $\\Wb_R$ of \\eqref{Wpath} and $\\Kbb_R\\Qbb_R^\\top$ of \\eqref{KQpath}.\n\n\\end{lemma}\nProof:\n\\begin{proof} To prove the mapping, first fix a $\\Wb_R$ solution with rank $m$, set $\\Lc_F=\\Lc(\\Wb_R)$ and show the existence of $\\Kb,\\Qb$ with $\\Kb\\Qb^\\top=\\Wb_R$ feasible for \\eqref{KQpath} and $\\Lc(\\Kb,\\Qb)\\leq \\Lc_F$. Use the singular value decomposition $\\Wb_R=\\Ub\\bSi\\Vb^\\top$ with $\\bSi\\in\\R^{m\\times m}$ being diagonal matrix of singular values. Set $\\Kb=\\Ub\\sqrt{\\bSi}$ and $\\Qb=\\Vb\\sqrt{\\bSi}$. Observe that $\\Kb\\Qb^\\top=\\W$ and\n\\[\n\\tf{\\Kb}^2=\\tf{\\Qb}^2=\\sum_{i=1}^m\\sqrt{\\bSi_{ii}}^2=\\tnuc{\\Wb_R}\\leq R.\n\\]\nThus, $\\Kb,\\Qb$ achieves $\\Lc(\\Kb,\\Qb)=\\Lc_F$. Conversely, given $\\Kbb_R,\\Qbb_R$ with $\\Lc_\\st=\\Lc(\\Kbb_R,\\Qbb_R)$, $\\W=\\Kbb_R\\Qbb_R^\\top$ obeys $\\Lc(\\W)=\\Lc_\\st$ and, using the standard nuclear norm inequality, we have\n\\[\n\\tnuc{\\W}=\\tnuc{\\Kbb_R\\Qbb_R^\\top}\\leq \\frac{1}{2}(\\tf{\\Kbb_R}^2+\\tf{\\Qbb_R}^2)=R.\n\\]\nThis shows $\\W$ is feasible for \\eqref{Wpath}. Combining the two findings above, we find that optimal costs are equal ($\\Lc_\\st=\\Lc_F$) and for any $(\\Kbb_R,\\Qbb_R)$ solution there exists a $\\Wb_R$ solution and vice versa.\n\\end{proof}",
        "metadata": {
            "Statement label": "Lemma 28",
            "Statement title": "Equivalence of Regularization Paths"
        }
    },
    {
        "text": "\\begin{lemma}\\label{lem:q_reduce} For any $\\X \\in\\R^{T\\times d}$, $\\W,\\V \\in \\R^{d\\times d}$ and $\\z, \\vb \\in \\R^{d}$, let $\\ab= \\X\\V \\z$, $\\s=\\sft{\\X\\W\\z}$, and $\\bgam=\\X\\vb$. Set\n\\begin{equation*}\n\\Gamma=\\sup_{t,\\tau\\in[T]}|\\bgam_t-\\bgam_\\tau|~~~\\textnormal{and}~~~A=\\sup_{t\\in[T]}\\tn{\\ab_t}.\n\\end{equation*}\nWe have that\n  \\[\n    \\left|\\ab^\\top\\textnormal{diag}(\\s) \\bgam-\\ab^\\top\\s\\s^\\top\\bgam-\\sum_{t\\geq 2}^T (\\ab_1-\\ab_t)\\s_t(\\bgam_1-\\bgam_t)\\right|\\leq 2\\Gamma A(1-\\s_1)^2.\n  \\]\n\\end{lemma}\nProof:\n\\begin{proof}\nThe proof is similar to \\cite[Lemma~4]{tarzanagh2023margin}, but for the sake of completeness, we provide it here.  Set $\\gamb=\\sum_{t=1}^T \\bgam_t\\s_t$.  We have \n\\begin{align*}\n\\bgam_1-\\gamb=\\sum_{t\\geq 2}^T (\\bgam_1-\\bgam_t)\\s_t,~~\\textnormal{and}~~|\\bgam_1-\\gamb|\\leq \\Gamma (1-\\s_1).\n\\end{align*}    \nThen,\n  \\begin{align} \n  \\nonumber \n    \\ab^\\top\\diag{\\s}\\bgam-\\ab^\\top\\s\\s^\\top\\bgam&=\\sum_{t=1}^T \\ab_t\\bgam_t\\s_t-\\sum_{t=1}^T \\ab_t\\s_t\\sum_{t=1}^T \\bgam_t\\s_t\\\\\n    &=\\ab_1\\s_1(\\bgam_1-\\gamb)-\\sum_{t\\geq 2}^T\\ab_t\\s_t(\\gamb-\\bgam_t). \\label{grad def step3}\n  \\end{align}\nSince \n$$\n\\left|\\sum_{t\\geq 2}^T\\ab_t\\s_t(\\gamb-\\bgam_t)-\\sum_{t\\geq 2}^T\\ab_t\\s_t(\\bgam_1-\\bgam_t)\\right|\\leq A\\Gamma (1-\\s_1)^2,\n$$\nwe obtain\\footnote{For simplicity, we use $\\pm$ on the right hand side to denote the upper and lower bounds.}\n  \\begin{align*}  \n    \\ab^\\top\\diag{\\s}\\bgam-\\ab^\\top\\s\\s^\\top\\bgam&=\\ab_1\\s_1(\\bgam_1-\\gamb)-\\sum_{t\\geq 2}^T\\ab_t\\s_t(\\bgam_1-\\bgam_t)\\pm A\\Gamma (1-\\s_1)^2\\\\\n    &=\\ab_1\\s_1\\sum_{t\\geq 2}^T (\\bgam_1-\\bgam_t)\\s_t-\\sum_{t\\geq 2}^T\\ab_t\\s_t(\\bgam_1-\\bgam_t)\\pm A\\Gamma (1-\\s_1)^2\\\\\n    &=\\sum_{t\\geq 2}^T (\\ab_1\\s_1-\\ab_t)\\s_t(\\bgam_1-\\bgam_t)\\pm A\\Gamma (1-\\s_1)^2\\\\\n    &=\\sum_{t\\geq 2}^T (\\ab_1-\\ab_t)\\s_t(\\bgam_1-\\bgam_t)\\pm 2A\\Gamma (1-\\s_1)^2.\n    \n  \\end{align*}\nHere,  $\\pm$ on the right handside uses the fact that\n  \\[\n  \\left|\\sum_{t\\geq 2}^T (\\ab_1\\s_1-\\ab_1)\\s_t(\\bgam_1-\\bgam_t)\\right|\\leq (1-\\s_1)A\\Gamma\\sum_{t\\geq 2}^T\\s_t=(1-\\s_1)^2A\\Gamma.\n  \\]\n\\end{proof}",
        "metadata": {
            "Statement label": "Lemma 29",
            "Statement title": "'Quadratic Reduction Lemma'"
        }
    },
    {
        "text": "\\begin{lemma}[Descent Lemma]\\label{lem:grad:descent}\nUnder Assumption \\ref{assum:loss:prope}, if $\\eta \\leq 1/L_{\\W}$, then for any initialization $\\W(0)$, Algorithm~\\ref{GD-W} satisfies:\n\\begin{align}\\label{eq:descent:obj new}\n\\mathcal{L}(\\W(k+1))-\\mathcal{L}(\\W(k))\\leq-\\frac{\\eta}{2} \\tf{\\nabla \\mathcal{L}(\\W(k))}^2,\n\\end{align}\nfor all $k\\ge0$. Additionally, it holds that $\\sum_{k=0}^{\\infty} \\tf{\\nabla\\mathcal{L}\\left(\\W(k)\\right)}^{2}<\\infty$, and $\\lim_{k\\rightarrow \\infty}\n\\tf{\\nabla\\mathcal{L}\\left(\\W\\left(k\\right)\\right)}^{2}=0$.\n\\end{lemma}\nProof:\n\\begin{proof}\nThe proof is similar to \\cite[Lemma~6]{tarzanagh2023margin}.\n\n\\end{proof}",
        "metadata": {
            "Statement label": "Lemma 30",
            "Statement title": "Gradient Descent Convergence Lemma"
        }
    },
    {
        "text": "\\begin{lemma}\\label{global des lem} \nLet $ \\Wm$ be the SVM solution of \\eqref{eqn:sattnsvm}. Suppose Assumptions \\ref{assum:loss:prope} and \\ref{assum:token} hold.  Then,  for all $\\W\\in\\R^{d\\times d}$, the training loss \\eqref{eqn:erm:w} obeys $\\li\\nabla\\Lc(\\W),\\Wm\\ri<0$. \n\\end{lemma}\nProof:\n\\begin{proof}\nLet\n\\begin{equation}\n\\hbm_i=  \\X_{i} \\Wm \\z_i, ~~~\\bgam_i=Y_i\\cdot \\X_i\\vb,~~~\\textnormal{and}~~~\n \\hb_i=\\X_i\\W \\z_{i}.    \n\\end{equation}\nLet us recall the gradient evaluated at $\\W$ which is given by \n\\begin{align}\\label{grad def new}\n\\nabla\\Lc(\\W)=\\frac{1}{n}\\sum_{i=1}^n\\ell' \\left(\\bgam_i^\\top \\sft{\\hb_i}\\right) \\cdot \\X_i^\\top  \\sfp{\\hb_i}  \\bgam_i  \\z_{i}^\\top,\n\\end{align}\n which implies that \n\\begin{equation}\\label{eqn:grad:prod:p}\n    \\begin{split}\n\\li\\nabla\\Lc(\\W),\\Wm\\ri&= \\frac{1}{n}\\sum_{i=1}^n \\ell' \\left(\\bgam_i^\\top \\sft{\\hb_i}\\right)  \\cdot \\iprod{ \\X_i^\\top  \\sfp{\\hb_i}  \\bgam_i  \\z_{i}^\\top}{\\Wm}\\\\\n&= \\frac{1}{n}\\sum_{i=1}^n\n\\ell'_i  \\cdot \\tr\\left(  (\\Wm)^\\top  \\X_{i}^\\top \\sfp{\\hb_i} \\bgam_i  \\z_{i}^\\top\\right)\\\\\n\n\n\n\n\n\n&= \\frac{1}{n}\\sum_{i=1}^n\n\\ell'_i \\cdot \\hbm_{i}^\\top \\sfp{\\hb_i} \\bgam_i \\\\\n&=  \\frac{1}{n}\\sum_{i=1}^n\n\\ell'_i \\cdot  \\left(\\hbm^\\top_i\\diag{\\s_i}\\bgam_i-\\hbm^\\top_i\\s_i\\s^\\top_i\\bgam_i\\right).        \n    \\end{split}\n\\end{equation}\nHere, let $\\ell'_i:=\\ell'(\\bgam_i^\\top\\sft{\\hb_i})$, $\\s_i=\\sft{\\hb_i}$ and the third equality uses $\\tr\\left(\\bb\\ab^\\top\\right) = \\ab^\\top \\bb$.\n\nIn order to move forward, we will establish the following result, with a focus on the equal score condition (Assumption B.2 [in <a href=\"https://arxiv.org/pdf/2308.16898#Item.5\">original paper</a>]): Let $\\gamma=\\bgam_{t\\geq 2}$ be a constant, and let $\\bgam_1$ and $\\bar{\\hb}_1$ represent the largest indices of vectors $\\bgam$ and $\\hbm$ respectively. For any vector $\\s$ that satisfies $\\sum_{t\\in[T]}\\s_t=1$ and $\\s_t> 0$, we aim to prove that $\\hbm^\\top\\diag{\\s}\\bgam-\\hbm^\\top\\s\\s^\\top\\bgam>0$. To demonstrate this, we proceed by writing the following:\n\\begin{equation}\\label{grad def2}\n\\begin{split}\n\\hbm^\\top\\diag{\\s}\\bgam-\\hbm^\\top\\s\\s^\\top\\bgam&=\\sum_{t=1}^T \\bar{\\hb}_t\\bgam_t \\s_t-\\sum_{t=1}^T  \\bar{\\hb}_t \\s_t\\sum_{t=1}^T \\bgam_t \\s_t\\\\\n&=\\left(\\bar{\\hb}_1\\bgam_1\\s_1+\\gamma\\sum_{t\\geq 2}^T\\bar{\\hb}_t\\s_t\\right)-\\Big(\\bgam_1\\s_1+\\gamma(1-\\s_1)\\Big)\\left(\\bar{\\hb}_1\\s_1+\\sum_{t\\geq 2}^T \\bar{\\hb}_t\\s_t\\right)\\\\\n&=\\bar{\\hb}_1(\\bgam_1-\\gamma) \\s_1(1-\\s_1)-(\\bgam_1-\\gamma)\\s_1\\sum_{t\\geq 2}^T \\bar{\\hb}_t \\s_t\\\\\n&=(\\bgam_1-\\gamma)(1- \\s_1) \\s_1\\left[\\bar{\\hb}_1-\\frac{ \\sum_{t\\geq 2}^T \\bar{\\hb}_t \\s_t}{\\sum_{t\\geq 2}^T\\s_t}\\right]\\\\\n&\\geq(\\bgam_1-\\gamma)(1- \\s_1) \\s_1 (\\bar{\\hb}_1-\\max_{t\\geq 2}\\bar{\\hb}_t).\n\\end{split}\n\\end{equation}\nTo proceed, define\n\\begin{equation*}\n\\bgag^i=\\bgam_{i\\opt_i}-\\max_{t\\neq\\opt_i}\\bgam_{it}~~\\textnormal{and}~~\\bhbg^i=\\bar \\hb_{i\\opt_i}-\\max_{t\\neq\\opt_i}\\bar \\hb_{it}.\n\\end{equation*}\nWith these, we obtain \n\n\n\n\\begin{equation}\\label{eqn:al:lem}\n\n\\hbm^\\top_i\\diag{\\s_i}\\bgam_i-\\hbm^\\top_i\\s_i\\s^\\top_i\\bgam_i\\geq\\bgag^i\\bhbg^i(1-\\s_{i\\opt_i})\\s_{i\\opt_i}.\n\\end{equation}\nNote that \n\\begin{equation*}\n\\begin{split}\n& \\bhbg^i=\\min_{t\\neq \\opt_i}~(\\x_{i \\opt_i}-\\x_{it})^\\top\\Wm \\z_i \\geq1,  \\\\\n&\\bgag^i=\\min_{t\\neq \\opt_i}~\\bgam_{i\\opt_i}-\\bgam_{it} >0,\\\\\n&\\s_{i\\opt_i}(1-\\s_{i\\opt_i}) > 0.    \n\\end{split}\n\\end{equation*}\n\\begin{comment}\n    Hence,\n\\begin{equation}\\label{eqn:lower}\nc_0:=\\min_{i \\in [n]}\\left\\{ \\left(\\min_{t\\neq \\opt_i}~(\\x_{i \\opt_i}-\\x_{it})^\\top\\Wm \\z_i\\right) \\cdot \\left(\\min_{t\\neq \\opt_i}~\\bgam_{i\\opt_i}-\\bgam_{it}\\right) \\cdot \\s_{i\\opt_i}(1-\\s_{i\\opt_i}) \\right\\}>0.\n\\end{equation}\nIt follows from  \\eqref{eqn:al:lem} and \\eqref{eqn:lower} that \n\\begin{equation}\\label{eqn:al:lem:2}\n\n\\min_{i \\in [n]}\\left\\{\\hbm^\\top_i\\diag{\\s_i}\\bgam_i-\\hbm^\\top_i\\s_i\\s^\\top_i\\bgam_i \\right\\} \\geq c_0>0.\n\\end{equation}\nFurther, by our assumption $\\ell'_i<0$.  \nSince by Assumption \\ref{assum:loss:prope}, $\\ell'$ is continuous and the domain is bounded, the maximum is attained and negative, and thus  \n\\begin{equation}\\label{eqn:bound:lprim}\n-c_1= \\max_{x} \\ell'(x), \\qquad  \\textnormal{for some} \\quad c_1>0.     \n\\end{equation}\nHence, using \\eqref{eqn:al:lem:2} and \\eqref{eqn:bound:lprim} in  \\eqref{eqn:grad:prod:p}, we obtain \n\\begin{equation}\\label{eqn:grad:prod:p:fin}\n    \\begin{split}\n\\li\\nabla\\Lc(\\W),\\Wm\\ri \\leq  - c<0, \\quad \\textnormal{where} \\quad c=c_1 \\cdot c_0.\n    \\end{split}\n\\end{equation}\n\n\n\n\n\n\n\n\n\nIn the scenario that Assumption~\\ref{assum:token:supp} holds (all tokens are support), $\\hbm_t=\\x_{it}^\\top\\Wm \\z_i $ is constant for all $t\\geq 2$. Hence, following similar steps as in \\eqref{grad def2} completes the proof. \n\\end{comment}\nHence,\n\\begin{equation}\\label{eqn:lower}\n\\min_{i \\in [n]}\\left\\{ \\left(\\min_{t\\neq \\opt_i}~(\\x_{i \\opt_i}-\\x_{it})^\\top\\Wm \\z_i\\right) \\cdot \\left(\\min_{t\\neq \\opt_i}~\\bgam_{i\\opt_i}-\\bgam_{it}\\right) \\cdot \\s_{i\\opt_i}(1-\\s_{i\\opt_i}) \\right\\}>0.\n\\end{equation}\nIt follows from  \\eqref{eqn:al:lem} and \\eqref{eqn:lower} that \n\\begin{equation}\\label{eqn:al:lem:2}\n\n\\min_{i \\in [n]}\\left\\{\\hbm^\\top_i\\diag{\\s_i}\\bgam_i-\\hbm^\\top_i\\s_i\\s^\\top_i\\bgam_i \\right\\}>0.\n\\end{equation}\nFurther, by Assumption \\ref{assum:loss:prope}, $\\ell'_i<0$, $\\ell'$ is continuous and the domain is bounded, the maximum is attained and negative, and thus  \n\\begin{equation}\\label{eqn:bound:lprim}\n\\max_{x} \\ell'(x)<0.    \n\\end{equation}\nHence, using \\eqref{eqn:al:lem:2} and \\eqref{eqn:bound:lprim} in  \\eqref{eqn:grad:prod:p}, we obtain \n\\begin{equation}\\label{eqn:grad:prod:p:fin}\n    \\begin{split}\n\\li\\nabla\\Lc(\\W),\\Wm\\ri <0.\n    \\end{split}\n\\end{equation}\n\n\n\n\n\n\n\n\n\nIn the scenario that Assumption~\\ref{assum:token:supp} holds (all tokens are support), $\\hbm_t=\\x_{it}^\\top\\Wm \\z_i $ is constant for all $t\\geq 2$. Hence, following similar steps as in \\eqref{grad def2} completes the proof. \n\n\\end{proof}",
        "metadata": {
            "Statement label": "Lemma 31",
            "Statement title": "Global Descent Lemma"
        }
    },
    {
        "text": "\\begin{lemma}\n\\label{glocal cond} \nSuppose Assumption~\\ref{assum:loss:prope} holds and let $\\op=(\\op_i)_{i=1}^n$ be the unique globally-optimal indices with $\\Wm$ denoting the \\ref{eqn:sattnsvm} solution. Define the margin $\\Theta=1/\\tf{\\Ws}$. Let $\\s_{i}=\\sft{\\X_i\\W\\z_i}$. For any $\\mu>0$, there exists a sufficiently large $\\RR_\\mu=\\order{1/\\mu}$ (see \\eqref{R bound2}) such that:\n\\begin{enumerate}[label={\\textnormal{\\textbf{L\\arabic*.}}}, wide, labelwidth=!,itemindent=!, labelindent=5pt]\n\\item \\label{lem:gcond:l1} There is no stationary point within  $ \\conb_{\\mu,\\RR_\\mu}(\\Ws)$, where $\\conb_{\\mu,\\RR_\\mu} (\\Ws)$ is defined in \\eqref{eqn:con:nabla0}. \n\\item\\label{lem:gcond:l2} For all $\\V\\in \\Scc_{\\mu} (\\Ws)$ with $\\tf{\\V}=\\tf{\\Wm}$  and $\\W\\in\\conb_{\\mu,\\RR_\\mu}(\\Wm)$, there exist dataset dependent constants $C,c>0$ such that \n\\begin{subequations}\\label{zero:g:lbound}\n\\begin{align}\n&C\\cdot \\frac{1}{n}\\sum_{i=1}^n \\left(1-\\s_{i\\op_i}\\right) \\geq -\\Big\\langle\\nabla\\Lc(\\W),\\V \\Big\\rangle\\geq c\\cdot \\mu\\cdot  \\frac{1}{n} \\sum_{i=1}^n  \\left(1-\\s_{i\\op_i}\\right)>0, \\label{zero1:g:bound} \\\\\n& -\\li\\frac{\\V}{\\tf{\\V}},\\frac{\\nabla\\Lc(\\W)}{\\tf{\\nabla\\Lc(\\W)}}\\ri \\geq  \\frac{c}{C} \\cdot \\frac{\\Theta}{\\bar{A}}>0, \\label{zero2:g:bound}\\\\\n&\\tf{\\nabla\\Lc(\\W)}\\leq \\bar{A}C \\cdot \\frac{1}{n} \\sum_{i=1}^n  \\left(1-\\s_{i\\op_i}\\right). \\label{zero3:g:bound}\n\\end{align}\n\\end{subequations}\nHere,  $\\s_{i\\opt_i}=(\\sft{\\X_i\\W \\z_{i}})_{\\opt_i}$, $\\bar{A}=\\max_{i\\in[n],t,\\tau\\in[T]}\\tn{\\x_{it}- \\x_{i\\tau}}~\\tn{\\z_i}$, and $\\Theta=1/\\tf{\\Ws}$.\n\n\n\n\n\n \n\n\n\n\n\n\\end{enumerate}\n\\end{lemma}\nProof:\n\\begin{proof} For simplicity let $R=\\RR_\\mu$, $\\W\\in\\conb_{\\mu,R}(\\Ws)$ and \n\\begin{equation}\\label{mu choice2}\n\\begin{split}\n\n&A=\\max_{i\\in[n],t,\\tau\\in[T]} \\frac{(\\tn{\\x_{it}}\\vee\\tn{\\x_{it}-\\x_{i\\tau}})\\cdot\\tn{\\z_i}}{\\Theta}.\n\\end{split}\n\\end{equation}\n\nThe following inequalities hold for all $\\V\\in \\Scc_{\\mu},~\\tf{\\V}=\\tf{\\Wm}$ and all $i\\in[n], t\\neq \\op_i$:\n\\begin{equation}\\label{cone-A-eq}\n\\begin{split}\n\n\nA\\geq(\\x_{i\\op_i}-\\x_{it})^\\top \\V \\z_i &\\geq \\mu.\n\\end{split}\n\\end{equation}\n\n \n\nTo proceed, we write the gradient correlation following \\eqref{grad def} and \\eqref{eqn:grad:prod:p}\n\\begin{align}\\label{grad def32}\n\\li\\nabla\\Lc(\\W),\\V\\ri&=\\frac{1}{n}\\sum_{i=1}^n\\ell'_i\\cdot\\li\\hb_i,\\sfp{\\hp_i}\\bgam_i\\ri,\n\\end{align}\nwhere we denoted $\\ell'_i=\\ell'(Y_i\\cdot \\vb^\\top \\X_i^\\top\\sft{\\hp_i})$, $\\hb_i=\\X_i\\V \\z_{i}$, $\\hp_i= \\X_i\\W \\z_{i}$, $\\s_i=\\sft{\\hp_i}$. \n\nIt follows from \\eqref{mu choice2} that $A\\geq \\max_{i\\in[n],t\\in[T]}\\tn{\\hb_{it}}$. Using \\eqref{cone-A-eq}, we can bound the softmax probabilities $\\s_i=\\sft{\\hp_i}$ as follows, for all $i\\in[n]$:\n\\begin{align}\\label{soft prob bound2}\n&S_i:= \\sum_{\\tau\\neq \\op_i}\\s_{i\\tau}\\leq T e^{-R\\mu\\Theta}\\s_{i\\op_i}\\leq T e^{-R\\mu\\Theta}.\n\\end{align}\n\nRecall scores $\\bgam_{it}=Y_i\\cdot\\vb^\\top \\x_{it}$. Define the score gaps:\n\\begin{equation*}\n \\bgg_i=\\bgam_{i\\op_i}-\\max_{t\\neq\\op_i}\\bgam_{it},~~~ \\bgm_i=\\bgam_{i\\op_i}-\\min_{t\\neq\\op_i}\\bgam_{it},~~~\\textnormal{and}~~~\\Gamma=\\sup_{i\\in[n],t,\\tau\\in[T]}|\\bgam_{it}-\\bgam_{i\\tau}|. \n\\end{equation*}\nLet us focus on a fixed datapoint $i\\in[n]$, assume (without losing generality) $\\op_i=1$, and drop subscripts $i$.\n\nDirectly applying Lemma \\ref{lem:q_reduce}, we obtain\n\\[\n  \\big|\\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam-\\sum_{t\\geq 2}^T (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2\\Gamma A(1-\\s_1)^2.\n\\]\n\\noindent To proceed, let us upper/lower bound the gradient correlation. Since $A\\geq \\hb_1-\\hb_t\\geq \\mu>0$ from \\eqref{cone-A-eq}, setting $S:=\\sum_{t\\neq\\op_i}\\s_t=1-\\s_1$, we find\n\\begin{equation}\n A\\cdot S\\cdot \\bgm  \\geq\\sum_{t\\neq\\op} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\geq \\mu\\cdot S\\cdot \\bgg.\\label{aggregate2}\n\\end{equation}\nNext we show that $S=1-\\s_1$ dominates $(1-\\s_1)^2=S^2$ for large $R$. Specifically, we wish for \n\\begin{align}\\label{wishfor2}\n\\mu S \\bgg/2\\geq 2\\Gamma A(1-\\s_1)^2\\iff S\\geq \\frac{4}{\\mu}\\frac{\\Gamma A}{\\bgg}S^2\\iff S\\leq \\frac{\\mu\\bgg}{4\\Gamma A}.\n\\end{align}\nUsing \\eqref{soft prob bound2}, what we wish is ensured for all $i\\in[n]$, by guaranteeing $Te^{-R\\mu\\Theta}\\leq \\frac{\\mu\\bgg}{4\\Gamma A}$. That is, by choosing\n\\begin{align}\\label{R bound2}\nR\\geq \\frac{1}{\\mu\\Theta}\\log\\left(\\frac{4T\\Gamma A}{\\mu\\bggm}\\right),\n\\end{align}\nwhere $\\bggm=\\min_{i\\in[n]}\\bgg_i$ is the global scalar corresponding to the worst case score gap over all inputs. \n\nWith the above choice of $R$, we guaranteed\n\\[\n  2 A(1-\\s_1)\\cdot \\bgm\\geq 2A\\cdot S\\cdot \\bgm \\geq \\sum_{t\\neq\\op} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\geq\\frac{\\mu\\cdot S\\cdot \\bgg}{2}\\geq\\frac{\\mu(1-\\s_1) \\bgg}{2},\n\\]\nvia \\eqref{wishfor2} and \\eqref{aggregate2}. \n\n\nSince this holds over all inputs, going back to the gradient correlation \\eqref{grad def32} and averaging above over all inputs $i\\in[n]$ and plugging back the indices $i$, we obtain the advertised bound \n\\begin{align}\\label{pbb corr2}\n  \n  \\frac{2A}{n}\\sum_{i\\in [n]} -\\ell'_i\\cdot S_i\\cdot \\bgm_i\\geq -\\li\\nabla\\Lc(\\W),\\V\\ri\\geq \\frac{\\mu}{2n}\\sum_{i\\in [n]} -\\ell'_i\\cdot S_i\\cdot \\bgg_i.\n\\end{align}\nLet $-\\ell'_{\\min/\\max}$ be the min/max values negative loss derivative admits over the ball $[-A,A]$ and note that $\\max_{i\\in[n]}\\bgm_i>0$ and $\\min_{i\\in[n]}\\bgg_i>0$ are dataset dependent constants. Then, we declare the constants $C=-2A\\ell'_{\\max}\\cdot \\max_{i\\in[n]}\\bgm_i>0,c=-(1/2)\\ell'_{\\min}\\cdot \\min_{i\\in[n]}\\bgg_i>0$ to obtain the bound \\eqref{zero1:g:bound}. \n\nThe proof of  \\eqref{zero2:g:bound} and \\eqref{zero3:g:bound} follows similarly as the proof of Lemma \\ref{local cond}. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\end{proof}",
        "metadata": {
            "Statement label": "Lemma 32",
            "Statement title": "Global Optimality Conditions"
        }
    },
    {
        "text": "\\begin{lemma}\n\\label{lem:glocal:corr} \nSuppose Assumption~\\ref{assum:loss:prope} holds and let $\\op=(\\op_i)_{i=1}^n$ be the unique optimal tokens with $ \\Wm$ denoting the SVM solution. Fix any $\\mu>0$ (per Lemma \\ref{glocal cond}). For any choice of $\\pi>0$, there exists $R:=R_{\\pi} \\geq \\bar{R}_\\mu$ such that, for any $ \\W\\in \\conb_{\\mu,R}(\\Wm)$, we have\n\\[\n \\li \\nabla\\Lc(\\W), \\frac{\\W}{\\tf{\\W}} \\ri\\geq (1+\\pi)\\li \\nabla\\Lc(\\W), \\frac{\\Wm}{\\tf{\\Wm}}\\ri.\n\\]\nHere, $\\conb_{\\mu,R}(\\Wm)$ is the cone defined at  \\eqref{eqn:con:nabla0}.\n\\end{lemma}\nProof:\n\\begin{proof}\n \nLet  $\\Wb= \\tf{\\Wm} \\W/\\tf{\\W}$, $\\hb_i=\\X_i\\Wb \\z_{i}$, $\\hbm_i= \\X_i\\Ws \\z_{i}$, and $\\s_i=\\sft{\\X_i\\W \\z_{i}}$. To establish the result, we will prove that, for sufficiently large $R$ and for any $\\W\\in \\conb_{\\mu,R}(\\Wm)$:\n\\begin{align}\\label{main local cond2}\n\\nonumber \n\\li -\\nabla\\Lc(\\W),\\frac{\\W}{\\tf{\\W}}\\ri&= -\\frac{1}{n}\\sum_{i=1}^n\\ell'_i \\cdot  \\li \\hb_i, \\sfp{\\X_i\\W \\z_{i}}\\bgam_i\\ri\\\\\n&\\leq - \\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot  \\li\\hbm_i, \\sfp{\\X_i\\W \\z_{i}}\\bgam_i\\ri=(1+\\pi)\\li-\\nabla\\Lc(\\W), \\frac{\\ps}{\\tf{\\Ws}}\\ri.\n\\end{align}\nDirectly applying Lemma \\ref{lem:q_reduce}, for all $\\V\\in \\Scc_\\mu$ with $\\tf{\\V}=\\tf{\\Wm}$ and $\\hp_i=\\X_i\\V \\z_i$, we have found\n\\begin{align}\n  \\big|\\hp^\\top_i\\diag{\\s_i}\\bgam_i-\\hp^\\top_i\\s_i\\s^\\top_i\\bgam_i-\\sum_{t\\neq\\op_i} (\\hp_{i1}-\\hp_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\big|\\leq 2\\Gamma A(1-\\s_{i1})^2.\n\\end{align}\nRecalling $\\hbm_{i1}-\\hbm_{it}\\geq 1$, we note that $\\sum_{t\\neq\\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq \\sum_{t\\neq\\op_i} (\\hbm_{i1}-\\hbm_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})$. Now plugging in $\\hb,\\hbm$ in the bound above and assuming $\\pi\\leq 1$ (w.l.o.g.), \\eqref{main local cond2} is implied by the following stronger inequality\n\\begin{align*}\n-\\frac{1}{n}&\\sum_{i=1}^n\\ell'_i \\cdot \\left(6\\Gamma A(1-\\s_{i1})^2+ \\sum_{t\\neq \\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it}) \\right)\\\\\n&\\leq -\\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i  \\cdot \\sum_{t\\neq \\op_i} (\\hbm_{i1}-\\hbm_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n&\\leq-\\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\neq \\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align*}\nFirst, we claim that $0.5\\pi\\sum_{t\\in \\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\geq 6\\Gamma A(1-\\s_{i1})^2$ for all $i \\in [n]$.  \nThe proof of this claim directly follows the argument in Lemma~\\ref{glocal cond}, \n(namely following \\eqref{soft prob bound2}, \\eqref{wishfor2}, \\eqref{R bound2}) \nwe have that $1-\\s_{i1}\\leq Te^{-R\\mu\\Theta}$ and $\\bgam_{i1}-\\bgam_{it}\\geq \\bggm$ for all $i \\in [n]$. This leads to the choice (for $D_0\\geq 12$)\n\\begin{align}\n  R\\geq R_\\pi =\\frac{1}{\\mu\\Theta}\\log\\left(\\frac{D_{0}\\cdot T\\Gamma A}{\\pi\\bggm}\\right).\\label{Rpi choice2}\n\\end{align}\nWe shall choose $D_0$ sufficiently large such that $R_{\\pi}\\geq \\bar{R}_{\\mu}$, where $\\bar{R}_{\\mu}$ is defined in Lemma \\ref{glocal cond}.\n\nFollowing this control over the perturbation term $6\\Gamma A(1-\\s_{i1})^2$, to conclude with the result, what remains is proving the comparison\n\\begin{align}\\label{desired comp2}\n-\\frac{1}{n} \\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\neq\\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq - \\frac{1+0.5\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\neq\\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align}\n\\noindent\\textbf{Scenario 1:} $\\tf{\\Wb-\\Wm}\\leq \\eps=\\frac{\\pi}{4A\\Theta}$ for some $\\eps>0$.  In this scenario, for any $ t\\neq \\op_i$ and $i\\in [n ]$, we have\n\\[\n|\\hb_{it}-\\hbm_{it}|=|\\x_{it}^\\top (\\Wb-\\Wm)  \\z_{i}|\\leq A\\Theta\\eps=\\frac{\\pi}{4}.\n\\]\nConsequently, we obtain \n\\[\n\\hb_{i1}-\\hb_{it}\\leq \\hbm_{i1}-\\hbm_{it}+2A\\Theta\\eps= 1+0.5\\pi.\n\\] \nSimilarly, $\\hb_{i1}-\\hb_{it}\\geq 1-0.5\\pi\\geq 0.5$. Since all terms $\\hb_{i1}-\\hb_{it},\\s_{it},\\bgam_{i1}-\\bgam_{it}$ in \\eqref{desired comp2} are nonnegative, we  obtain \\eqref{desired comp2}. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\noindent\\textbf{Scenario 2:} $\\tf{\\Wb-\\Wm}\\geq \\eps=\\frac{\\pi}{4A\\Theta}$.  Since $\\Wb$ is not max-margin solution, in this scenario, for some $i \\in  [n]$, $\\nu=\\nu(\\eps)>0$, and $\\tau\\neq\\op_i$, we have that\n\\begin{align*}\n\\hb_{i1}-\\hb_{i\\tau}\\leq 1-2\\nu.\n\\end{align*}\nHere $\\tau=\\arg\\max_{\\tau\\neq\\op_i} \\x_{i\\tau}\\Wb \\z_i$ denotes the nearest point to $\\hb_{i1}$ (along the $\\Wb$ direction). Recall that $\\s=\\sft{  \\RR\\hb}$, where  $\\RR=R\\Theta=\\tf{\\W}/\\tf{\\Wm}$. To proceed, let $ \\underline{\\hb}_i:=\\min_{t \\neq\\op_i}\\hb_{i1}-\\hb_{it}$,\n\\begin{align*}\n\\mc{I}:=\\left\\{ i\\in[n]: \\underline{\\hb}_i \\leq 1-2\\nu \\right\\}, \\qquad [n]-\\mc{I}:=\\left\\{ i\\in[n]:  1-2\\nu  <  \\underline{\\hb}_i\\right\\}.\n\n\n\\end{align*}\nFor all $ i \\in [n]-\\mc{I}$,\n\\begin{equation}\\label{eqn:grad:difff0}\n\\begin{split}\n      \\sum_{t\\neq \\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it}) &- (1+0.5\\pi) \\sum_{t\\neq \\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n      & \\leq  \\left(2A - (1+0.5\\pi)\\right)\\Gamma\\sum_{t\\neq \\op_i,~\\hb_{i1}-\\hb_{it} \\geq 1+\\frac{\\pi}{2} } \\s_{it} \\\\\n      & \\leq  \\left(2A - (1+0.5\\pi)\\right)\\Gamma Te^{-\\RR(1+\\frac{\\pi}{2})} \\\\\n      &\\leq   2A\\Gamma  T e^{-\\RR(1+\\frac{\\pi}{2})}.\n      \n      \n      \n\\end{split}\n\\end{equation}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor all $ i \\in \\mc{I}$, split the tokens into two groups: Let $\\Nc_i$ be the group of tokens obeying $ \\hb_{i1}-\\hb_{it} \\leq 1-\\nu$ and $\\bar\\Nc_i:=[T]-\\{\\op_i\\}-\\Nc_i$ be the rest of the neighbors. Observe that\n\\[\n\\frac{\\sum_{t\\in\\bar{\\Nc}_i}\\s_{it}}{\\sum_{t\\neq\\op_i}\\s_{it}}\\leq  T\\frac{e^{\\nu \\RR}}{e^{2\\nu\\RR}}=Te^{-\\RR\\nu}.\n\\]\nUsing $|\\hb_{i1}-\\hb_{it}|\\leq 2A$ and  $\\bggm=\\min_{i\\in[n]}\\bgg_i =\\min_{i\\in[n]} (\\bgam_{i1}-\\max_{t\\neq\\op_i}\\bgam_{it})$, observe that \n\\[\n\\sum_{t\\in\\bar\\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq \\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm} \\sum_{t\\neq\\opt_i} \\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\]\nThus, \n\\begin{align*}\n  \\sum_{t\\neq \\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})&= \\sum_{t\\in \\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})+\\sum_{t\\in\\bar\\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\nonumber\\\\\n\n  &\\leq \\sum_{t\\in \\Nc_i} (1-\\nu)\\s_{it}(\\bgam_{i1}-\\bgam_{it})+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm} \\sum_{t\\neq \\op_i} \\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n  &\\leq \\left(1-\\nu+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}\\right)\\sum_{t\\neq \\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n &\\leq \\left(1+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}\\right)\\sum_{t\\neq \\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align*}\nHence, choosing \n\\begin{align}\nR\\geq\\frac{1}{\\nu\\Theta}\\log\\left(\\frac{8\\Gamma AT}{\\bggm\\pi}\\right)\\label{R bound pi}\n\\end{align}\nresults in that\n\\begin{equation}\\label{eqn:grad:difff1}\n    \\begin{split}\n     &\\sum_{t\\neq\\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})  - \\left(1+\\frac{\\pi}{2}\\right) \\sum_{t\\neq\\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}) \\\\\n   &\\leq\\left(\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}-\\frac{\\pi}{2}\\right)\\sum_{t\\neq\\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n   &\\leq -\\frac{\\pi}{4}\\sum_{t\\neq\\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n   &\\leq-\\frac{\\pi}{4T}\\bggm  e^{-\\bar{R} (1-2\\nu)}.      \n    \\end{split}\n\\end{equation}\nHere, the last inequality follows from the fact that $\\sum_{t\\neq\\op_i}\\s_{it}\\geq \\max_{t\\neq\\op_i}\\s_{it}\\geq\\frac{e^{-\\bar{R}(1-2\\nu)}}{\\sum_{t=1}^Te^{-\\bar{R}(\\hb_{i1}-\\hb_{it})}}\\geq e^{-\\bar{R}(1-2\\nu)}/T$.\n\n\nFrom Assumption~\\ref{assum:loss:prope}, we have $c_{\\min}\\leq-\\ell'\\leq c_{\\max}$ for some positive constants $c_{\\min}$ and $c_{\\max}$. It follows from  \\eqref{eqn:grad:difff0} and \\eqref{eqn:grad:difff1} that \n\\begin{align*}\n-\\frac{1}{n}\\sum_{i}^n \\ell_i' \\cdot&\\left(\n      \\sum_{t\\neq\\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})- \\sum_{t\\neq\\op_i} (1+0.5\\pi)\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\right)\\\\\n      & \\leq    c_{\\max}2A\\Gamma  T \\Gamma e^{-\\RR(1 +\\frac{\\pi}{2})}-\\frac{c_{\\min}}{nT}\\cdot\\frac{\\pi\\bggm}{4}e^{-\\bar{R} (1-2\\nu)}\\\\\n      \n      & \\leq 0.\n\\end{align*}\nCombing with \\eqref{R bound pi}, this is guaranteed by \nchoosing \n\n\n\n\\[\n  R\\geq \\max\\left\\{\\frac{1}{\\nu\\Theta}\\log\\left(\\frac{8\\Gamma AT}{\\bggm\\pi}\\right),\\frac{1}{(2\\nu+\\pi/2)\\Theta}\\log\\left(\\frac{8n\\Gamma AT^2 c_{\\max}}{c_{\\min}\\bggm\\pi}\\right)\\right\\},\n\\]\nwhere $\\nu=\\nu(\\frac{\\pi}{4A\\Theta})$ depends only on $\\pi$ and global problem variables. \n\nCombining this with the prior $R$ choice \\eqref{Rpi choice2} (by taking maximum), we conclude with the statement.\n\\end{proof}",
        "metadata": {
            "Statement label": "Lemma 33",
            "Statement title": "Cone Gradient Inequality Lemma"
        }
    },
    {
        "text": "\\begin{lemma}\n\\label{local cond} \nSuppose Assumption~\\ref{assum:loss:prope} on the loss function $\\ell$ holds, and let $\\bal=(\\alpha_i)_{i=1}^n$ be locally optimal tokens according to Definition \\ref{def loc opt}. Let $ \\Wm= \\Wm_\\bal$ denote the SVM solution obtained via \\eqref{eqn:sattnsvm} by applying the Frobenius norm and replacing $(\\opt_i)_{i=1}^n$ with $\\boldsymbol{\\alpha} = (\\alpha_i)_{i=1}^n$. \n\n\n\n\n\n\nThere exists a scalar $\\mu=\\mu(\\bal)>0$ such that for sufficiently large $\\RR_\\mu$:\n\\begin{enumerate}[label={\\textnormal{\\textbf{L\\arabic*.}}}, wide, labelwidth=!,itemindent=!, labelindent=5pt]\n\\item \\label{lem:cond:l1} There is no stationary point within  $ \\Cc_{\\mu,\\RR_\\mu} (\\Wm)$.\n\n\\item\\label{lem:cond:l2} For all $\\V\\in \\Sc_{\\mu}(\\Wm)$ with $\\tf{\\V}=\\tf{\\Wm}$  and $\\W\\in\\Cc_{\\mu,\\RR_\\mu}(\\Wm)$, there exist dataset dependent constants $C,c>0$ such that \n\\begin{subequations}\\label{local:g:lbound}\n\\begin{align}\n&C\\cdot \\frac{1}{n}\\sum_{i=1}^n \\left(1-\\s_{i\\alpha_i}\\right) \\geq -\\Big\\langle\\nabla\\Lc(\\W),\\V \\Big\\rangle\\geq c\\cdot  \\frac{1}{n} \\sum_{i=1}^n  \\left(1-\\s_{i\\alpha_i}\\right)>0, \\label{local1:g:bound} \\\\\n&\\tf{\\nabla\\Lc(\\W)}\\leq \\bar{A}C \\cdot \\frac{1}{n} \\sum_{i=1}^n  \\left(1-\\s_{i\\alpha_i}\\right), \\label{local2:g:bound}\\\\\n& -\\li\\frac{\\V}{\\tf{\\V}},\\frac{\\nabla\\Lc(\\W)}{\\tf{\\nabla\\Lc(\\W)}}\\ri \\geq  \\frac{c}{C} \\cdot \\frac{\\Theta}{\\bar{A}}>0. \\label{local3:g:bound}\n\\end{align}\n\\end{subequations}\nHere, $\\s_{i\\alpha_i}= (\\sft{\\X_i\\W \\z_{i}})_{\\alpha_i}$, $\\bar{A}=\\max_{i\\in[n],t,\\tau\\in[T]}\\tn{(\\x_{it}- \\x_{i\\tau})}~\\tn{\\z_i}$, and $\\Theta=1/\\tf{\\Ws}$.\n\n\n\n\n\n\\end{enumerate}\n\\end{lemma}\nProof:\n\\begin{proof}\nLet $R=\\RR_\\mu$, $(\\Tc_i)_{i=1}^n$ be the set of all \\neis per Definition \\ref{def loc opt}. Let $\\Tcb_i=[T]-\\Tc_i-\\{\\alpha_i\\}$ be the non-\\neis. Let\n\\begin{equation}\\label{mu choice}\n\\begin{split}\n&\\Theta=1/\\tf{\\Wm},\\\\\n&\\delta= \\frac{1}{2}\\min_{i\\in[n]}\\min_{t\\in\\Tc_i,\\tau\\in\\Tcb_i}(\\x_{it}-\\x_{i\\tau})^\\top \\Wm \\z_{i},\\\\\n&A=\\max_{i\\in[n],t\\in[T]} \\frac{\\tf{\\x_{it} \\z_i^\\top}}{\\Theta},\\\\\n& \\mu\\leq \\mu(\\delta)=\\frac{1}{8}\\left(\\frac{\\min(0.5,\\delta)}{A}\\right)^2.\n\\end{split}\n\\end{equation}\n\n\nSince $\\Wm$ is the max-margin model ensuring $(\\x_{i\\alpha_i}-\\x_{it})^\\top\\Wm \\z_i\\geq 1$, the following inequalities hold for all $\\W\\in \\cone_\\mu(\\Wm),~\\tf{\\W}=\\tf{\\Wm}$ and all $i\\in[n], t\\in\\Tc_i,\\tau\\in\\Tcb_i$:\n\\begin{equation}\\label{cone-non-nei}\n\\begin{split}\n(\\x_{it}-\\x_{i\\tau})^\\top \\W \\z_i&\\geq \\delta>0,\\\\\n(\\x_{i\\alpha_i}-\\x_{i\\tau})^\\top \\W \\z_i&\\geq 1+\\delta,\\\\\n\\frac{3}{2}\\geq(\\x_{i\\alpha_i}-\\x_{it})^\\top \\W \\z_i &\\geq \\frac{1}{2}.\n\\end{split}\n\\end{equation}\nHere, we used $\\tf{\\W-\\Wm}^2/\\tf{\\Wm}^2\\leq 2\\mu$ which implies $\\tf{\\W-\\Wm}\\leq \\sqrt{2\\mu}/\\Theta$.\n\n\n\n\n\nTo proceed, we write the gradient correlation following \\eqref{grad def} and \\eqref{grad def2}\n\\begin{align}\\label{grad def3}\n\\li\\nabla\\Lc(\\W),\\V\\ri&=\\frac{1}{n}\\sum_{i=1}^n\\ell'_i\\cdot\\hb_i^\\top\\sfp{\\hp_i}\\bgam_i,\n\\end{align}\nwhere we denoted $\\ell'_i=\\ell'(Y_i\\cdot \\vb^\\top \\X_i^\\top\\sft{\\hp_i})$, $\\hb_i=\\X_i\\V \\z_{i}$, $\\hp_i= \\X_i\\W \\z_{i}$, and $\\s_i=\\sft{\\hp_i}$.  \n\nUsing \\eqref{cone-non-nei}, for all $t\\in\\Tc_i,\\tau\\in \\Tcb_i$, for all $\\W\\in \\Cc_{\\mu,R}(\\Wm)$, we have that\n\\begin{align*}\n&\\hp_{it}-\\hp_{i\\tau}\\geq R\\Theta\\delta,\\\\\n&\\hp_{i\\alpha_i}-\\hp_{i\\tau}\\geq R\\Theta(1+\\delta),\\\\\n&\\hp_{i\\alpha_i}-\\hp_{it}\\geq R\\Theta/2.    \n\\end{align*}\nConsequently, we can bound the softmax probabilities $\\s_i=\\sft{\\hp_i}$ over non-\\neis as follows: For all $i\\in[n]$ and any $t_i\\in \\Tc_i$\n\\begin{subequations}\n\\begin{align}\\label{soft prob bound}\n&S_i:=\\sum_{\\tau\\in\\Tc_i}\\s_{i\\tau} \n\n\\leq T e^{-R\\Theta/2}\\s_{i\\alpha_i}\\leq T e^{-R\\Theta/2},\\\\\n&Q_i:=\\sum_{\\tau\\in\\Tcb_i}\\s_{i\\tau} \\leq T e^{-R\\Theta\\delta}\\s_{it_i}\\leq T e^{-R\\Theta\\delta}S_i.\n\\end{align}\n\\end{subequations}\n\nRecall scores $\\bgam_{it}=Y_i\\cdot\\vb^\\top \\x_{it}$. Define the score gaps over \\neis:\n\\begin{equation*}\n \\bgg_i=\\bgam_{i\\alpha_i}-\\max_{t\\in\\Tc_i}\\bgam_{it}~~~ \\textnormal{and}~~~ \\bgm_i=\\bgam_{i\\alpha_i}-\\min_{t\\in\\Tc_i}\\bgam_{it}. \n\\end{equation*}\nIt follows from \\eqref{mu choice} that \n\\begin{align*}\n&A=\\max_{i\\in[n],t\\in[T]} \\frac{\\tf{\\x_{it} \\z_i^\\top}}{\\Theta}\\geq \\max_{i\\in[n],t\\in[T]}\\tn{\\hb_{it}}.\n\\end{align*}\nDefine the $\\bal$-dependent global scalar $\\Gamma=\\sup_{i\\in[n],t,\\tau\\in[T]}|\\bgam_{it}-\\bgam_{i\\tau}|$.\n\n\nLet us focus on a fixed datapoint $i\\in[n]$, assume (without losing generality) $\\alpha_i=1$, and drop subscripts $i$.\n\nDirectly applying Lemma \\ref{lem:q_reduce}, we obtain\n\\[\n  \\big|\\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam-\\sum_{t\\geq 2}^T (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2\\Gamma A(1-\\s_1)^2.\n\\]\nTo proceed, let us decouple the non-\\neis within $\\sum_{t\\geq 2}^T (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)$ via\n\\[\n\\big|\\sum_{t\\in\\Tcb} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2Q\\Gamma A.\n\\]\nAggregating these, we found\n\\begin{align}\n  \\big|\\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam-\\sum_{t\\in \\Tc} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2\\Gamma A((1-\\s_1)^2+Q).\\label{aggregate}\n\\end{align}\n\nTo proceed, let us upper/lower bound the gradient correlation.  We use two bounds depending on $\\V\\in\\Sc_{\\mu}(\\Ws)$ (\\textbf{Case 1}) or general $\\V\\in\\R^{d\\times d}$ (\\textbf{Case 2}).\n\n\n\\noindent$\\bullet$ \\textbf{Case 1:  $\\V\\in\\Sc_{\\mu}(\\Ws)$.} Since $1.5\\geq \\hb_1-\\hb_t\\geq 0.5$ following \\eqref{cone-non-nei}, we find\n\\[\n 1.5\\cdot S\\cdot \\bgm  \\geq\\sum_{t\\in \\Tc} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\geq 0.5\\cdot S\\cdot \\bgg,\n\\]\nwhere recall the definition of $S$ (having dropped subscripts) in \\eqref{soft prob bound}. \n\n\\noindent$\\bullet$ \\textbf{Case 2: $\\Vb\\in\\R^{d\\times d}$ and $\\tf{\\V}=\\tf{\\Wm}$.}  Define $\\bar{A}=\\max_{i\\in[n],t,\\tau\\in[T]}\\tn{\\x_{it}-\\x_{i\\tau}}~\\tn{\\z_i}$. For any $\\tf{\\V}=\\tn{\\Ws}$, we use the fact that\n$$\\tn{\\hb_1-\\hb_t}\\leq \\tf{(\\x_{it}-\\x_{i\\tau}) \\z_i^\\top}\\cdot\\tf{\\V}\\leq \\frac{\\bar{A}}{\\Theta}.$$\nNote that by definition $ \\frac{\\bar{A}}{\\Theta} \\geq 1$. To proceed, we can upper bound\n\\begin{align}\n\\frac{\\bar{A}}{\\Theta}\\cdot S\\cdot \\bgm  \\geq\\sum_{t\\in \\Tc} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t).\\label{wishwish2}\n\\end{align}\n\nNext we claim that for both cases, $S$ dominates $((1-\\s_1)^2+Q)$ for large $R$. Specifically, we wish for \n\\begin{align}\\label{wishfor}\n\\frac{S\\cdot \\bgg}{4}\\geq 4\\Gamma A\\max((1-\\s_1)^2,Q)\\iff S\\geq 16\\frac{\\Gamma A}{\\bgg}\\max((1-\\s_1)^2,Q).\n\\end{align}\nNow choose $R\\geq \\delta^{-1}\\log(T)/\\Theta$  to ensure $Q\\leq S$ since $Q\\leq Te^{-R\\Theta\\delta}S$ from \\eqref{soft prob bound}. Consequently\n\\[\n(1-\\s_1)^2=(Q+S)^2\\leq 4S^2\\leq 4STe^{-R\\Theta/2}.\n\\]\nCombining these, what we wish is ensured by guaranteeing\n\\begin{align}\\label{s bound}\n  S\\geq 16\\frac{\\Gamma A}{\\bgg}\\max(4STe^{-R\\Theta/2},Te^{-R\\Theta\\delta}S).\n\\end{align}\nThis in turn is ensured for all inputs $i\\in[n]$ by choosing \n\\begin{align}\\label{R bound}\nR\\geq \\frac{\\max(2,\\delta^{-1})}{\\Theta}\\log\\left(\\frac{64T\\Gamma A}{\\bggm}\\right),\n\\end{align}\nwhere $\\bggm=\\min_{i\\in[n]}\\bgg_i$ is the global scalar which is the worst case score gap over all inputs. \n\\\\\n$\\bullet$ \\textbf{Case 1: $\\V\\in\\Sc_{\\mu}(\\Ws)$}. With the above choice of $R$, we guaranteed\n\\[\n  2 (1-\\s_1)\\cdot \\bgm\\geq 2\\cdot S\\cdot \\bgm \\geq \\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam\\geq\\frac{S\\cdot \\bgg}{4}\\geq\\frac{(1-\\s_1) \\bgg}{8}.\n\\]\nvia \\eqref{wishfor} and \\eqref{aggregate}. \n\nSince this holds over all inputs, going back to the gradient correlation \\eqref{grad def3} and averaging above over all inputs $i\\in[n]$ and plugging back the indices $i$, we obtain the advertised bound \n\\begin{align}\\label{pbb corr}\n\n\\frac{2}{n}\\sum_{i\\in [n]} -\\ell'_i\\cdot S_i\\cdot \\bgm_i\\geq -\\li\\nabla\\Lc(\\W),\\V\\ri\\geq \\frac{1}{8n}\\sum_{i\\in [n]} -\\ell'_i\\cdot S_i\\cdot \\bgg_i.\n\\end{align}\nLet $-\\ell'_{\\min/\\max}$ be the min/max values negative loss derivative admits over the ball $[-A,A]$ and note that $\\max_{i\\in[n]}\\bgm_i>0$ and $\\min_{i\\in[n]}\\bgg_i>0$ are dataset dependent constants. Then, we declare the constants $C=-2\\ell'_{\\max}\\cdot \\max_{i\\in[n]}\\bgm_i>0,c=-(1/8)\\ell'_{\\min}\\cdot \\min_{i\\in[n]}\\bgg_i>0$ to obtain the bound \\eqref{local1:g:bound}. \n\\vspace{.2cm}\n\\\\\n\\noindent$\\bullet$ \\textbf{Case 2: $\\Vb\\in\\R^{d\\times d}$ and $\\tf{\\V}=\\tf{\\Wm}$.} Next, we show \\eqref{local2:g:bound} and \\eqref{local3:g:bound}. For any $\\V \\in \\mathbb{R}^{d \\times d}$ satisfying $\\tf{\\V}=\\tf{\\Ws}$, using \\eqref{wishwish2} and the  choice of $R$ in \\eqref{R bound} similarly guarantees \n$$\n\\frac{2\\bar{A}}{\\Theta }(1-\\s_1) \\bgm\\geq \\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam,\n$$\nfor fixed input. Going back to the gradient correlation \\eqref{grad def3} and averaging above over all inputs $i\\in[n]$, with the same definition of $C>0$, we obtain\n\\begin{align}\n\\frac{ \\bar{A} C}{  \\Theta n}\\sum_{i\\in [n]} (1-\\s_{i\\alpha_i})\\geq -\\li\\nabla\\Lc(\\W),\\V\\ri.\\label{local lamma general upper}\n\\end{align}\nTo proceed, since \\eqref{local lamma general upper} holds for any $\\V\\in\\R^{d\\times d}$, we observe that when setting $\\V=\\frac{\\tf{\\Ws}}{\\tf{\\nabla\\Lc(\\W)}}\\cdot \\nabla\\Lc(\\W)$, this implies that\n\\[ \n\\li\\nabla\\Lc(\\W),\\V\\ri = \\tf{\\nabla\\Lc(\\W)}\\cdot \\tf{\\Ws}\\leq \\frac{\\bar{A} C}{\\Theta \n n}\\sum_{i\\in [n]} (1-\\s_{i\\alpha_i}).\n\\]\nSimplifying $\\Theta=1/\\tf{\\Ws}$ on both sides gives \\eqref{local2:g:bound}. \n\\\\\nCombining the above inequality with \\eqref{pbb corr}, we obtain that for all $\\V,\\W\\in\\Sc_{\\mu}(\\Ws)$\n\\[ \n-\\li\\frac{\\V}{\\tf{\\V}},\\frac{\\nabla\\Lc(\\W)}{\\tf{\\nabla\\Lc(\\W)}}\\ri\\geq \\frac{c \\Theta }{C\\bar{A}},\n\\]\nwhich gives \\eqref{local3:g:bound}.\n\n\n\n\n\\end{proof}",
        "metadata": {
            "Statement label": "Lemma 34",
            "Statement title": "Local Optimality Conditions for SVM Solutions"
        }
    },
    {
        "text": "\\begin{lemma}\n\\label{lem:local:corr} \nSuppose Assumption~\\ref{assum:loss:prope} on the loss function $\\ell$ holds, and let $\\bal=(\\alpha_i)_{i=1}^n$ be locally optimal tokens according to Definition \\ref{def loc opt}. Let $ \\Wm= \\Wm_\\bal$ denote the SVM solution obtained via \\eqref{eqn:sattnsvm} by replacing $(\\opt_i)_{i=1}^n$ with $\\boldsymbol{\\alpha} = (\\alpha_i)_{i=1}^n$. Let $\\mu=\\mu(\\bal)>0$ and $\\bar{R}_{\\mu}$ be defined as in Lemma~\\ref{local cond}. For any choice of $\\pi>0$, there exists $R_\\pi \\geq \\bar{R}_{\\mu}$ such that, for any $ \\W\\in \\Cc_{\\mu,R_\\pi}(\\Wm)$, we have\n\\[\n \\li \\nabla\\Lc(\\W), \\frac{\\W}{\\tf{\\W}} \\ri\\geq (1+\\pi)\\li \\nabla\\Lc(\\W), \\frac{\\Wm}{\\tf{\\Wm}}\\ri.\n\\]\n\\end{lemma}\nProof:\n\\begin{proof}\nLet  $R=R_{\\pi}$, $\\Wb=\\tf{\\Wm} \\W/\\tf{\\W} $, $\\hb_i=\\X_i\\Wb \\z_{i}$, and $\\hbm_i= \\X_i \\Wm \\z_{i}$.   To establish the result, we will prove that, for sufficiently large $R$ and for any $\\W\\in \\Cc_{\\mu,R}(\\Wm)$:\n\\begin{align}\\label{main local cond}\n\\nonumber \n\\li -\\nabla\\Lc(\\W),\\frac{\\W}{\\tf{\\W}}\\ri&= -\\frac{1}{n}\\sum_{i=1}^n\\ell'_i \\cdot  \\li \\hb_i, \\sfp{\\X_i\\W \\z_{i}}\\bgam_i\\ri\\\\\n&\\leq - \\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot  \\li\\hbm_i, \\sfp{\\X_i\\W \\z_{i}}\\bgam_i\\ri=(1+\\pi)\\li-\\nabla\\Lc(\\W), \\frac{\\ps}{\\tf{\\Ws}}\\ri.\n\\end{align}\n\nFollowing (67) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.D.67\">original paper</a>], for all $\\W\\in \\Sc_{\\mu}(\\Wm)$ with $\\tf{\\W}=\\tf{\\Wm}$, $\\hp=\\X\\W \\z$, and $\\s=\\sft{\\hp}$, we have found\n\\begin{align}\n  \\big|\\hp^\\top_i\\diag{\\s_i}\\bgam_i-\\hp^\\top_i\\s_i\\s^\\top_i\\bgam_i-\\sum_{t\\in \\Tc_i} (\\hp_{i1}-\\hp_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\big|\\leq 2\\Gamma A((1-\\s_{i1})^2+Q_i), \n\\end{align}\nwhere $\\Tc_i$ is the set of support indices. \n\n\nPlugging in $\\hb,\\hbm$ in the bound above and assuming $\\pi\\leq 1$ (w.l.o.g.), \\eqref{main local cond} is implied by the following stronger inequality\n\\begin{align*}\n-\\frac{1}{n}&\\sum_{i=1}^n\\ell'_i \\cdot \\left(6\\Gamma A((1-\\s_{i1})^2+Q_i)+ \\sum_{t\\in \\Tc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it}) \\right)\\\\\n&\\leq -\\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i  \\cdot \\sum_{t\\in \\Tc_i} (\\hbm_{i1}-\\hbm_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n&=-\\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align*}\nFirst, we claim that $0.5\\pi\\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\geq 6\\Gamma A((1-\\s_{i1})^2+Q_i)$ for all $i \\in [n]$.  The proof of this claim directly follows the earlier argument, namely, following \\eqref{wishfor}, \\eqref{s bound}, and \\eqref{R bound}  which leads to the choice \n\\begin{equation}\\label{R boundC0}\nR \\ge\\frac{\\max(2,\\delta^{-1})}{\\Theta}\\log\\left(\\frac{C_0\\cdot T\\Gamma A}{\\pi\\bggm}\\right),    \n\\end{equation}\nfor some constant $C_0>0$. Using \\eqref{R bound}, we choose $C_0 \\geq 64 \\pi$ to guarantee $R=R_\\pi \\geq \\bar{R}_{\\mu}$.\n\nFollowing this control over the perturbation term $6\\Gamma A((1-\\s_{i1})^2+Q_i)$, to conclude with the result, what remains is proving the comparison\n\\begin{align}\\label{desired comp}\n-\\frac{1}{n} \\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\in \\Tc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq - \\frac{1+0.5\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align}\nTo proceed, we split the problem into two scenarios. \n\n\\noindent\\textbf{Scenario 1:} $\\tf{\\Wb-\\Wm}\\leq \\eps=\\frac{\\pi}{4A\\Theta}$ for some $\\eps>0$.  In this scenario, for any $ t\\in \\Tc_i$ and $i\\in [n ]$, we have\n\\[\n|\\hb_{it}-\\hbm_{it}|=|\\x_{it}^\\top (\\Wb-\\Wm)  \\z_{it}|\\leq A\\Theta\\eps=\\frac{\\pi}{4}.\n\\]\nConsequently, we obtain \n\\[\n\\hb_{i1}-\\hb_{it}\\leq \\hbm_{i1}-\\hbm_{it}+2A\\Theta\\eps= 1+0.5\\pi.\n\\] \nSimilarly, $\\hb_{i1}-\\hb_{it}\\geq 1-0.5\\pi\\geq 0.5$. Since all terms $\\hb_{i1}-\\hb_{it},\\s_{it},\\bgam_{i1}-\\bgam_{it}$ in \\eqref{desired comp} are nonnegative and $(\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq (1+0.5\\pi)\\s_{it}(\\bgam_{i1}-\\bgam_{it})$, above implies the desired result in \\eqref{desired comp}.\n\n\\vspace{3pt}\n\\noindent\\textbf{Scenario 2:} $\\tf{\\Wb-\\Wm}\\geq \\eps=\\frac{\\pi}{4A\\Theta}$.  Since $\\Wb$ is not (locally) max-margin, in this scenario, for some $i \\in  [n]$, $\\nu=\\nu(\\eps)>0$, and $\\tau\\in\\Tc_i$, we have that\n\\begin{align*}\n\\hb_{i1}-\\hb_{i\\tau}\\leq 1-2\\nu.\n\\end{align*}\nHere $\\tau=\\arg\\max_{\\tau\\in\\Tc_i} \\x_{i\\tau}\\Wb \\z_i$ denotes the nearest point to $\\hb_{i1}$ (along the $\\Wb$ direction). Note that a non-neighbor $t\\in\\Tcb_i$ cannot be nearest because $\\Wb\\in \\cone_{\\mu}(\\ps)$ and \\eqref{cone-non-nei} holds. Recall that $\\s_i=\\sft{\\RR\\hb_i}$ where $\\RR=\\tf{\\W}\\Theta \\geq R\\Theta$. To proceed, let $ \\underline{\\hb}_i:=\\min_{t \\in\\mc{T}_i}\\hb_{i1}-\\hb_{it}$,\n\\begin{align*}\n\\mc{I}:=\\left\\{ i\\in[n]: \\underline{\\hb}_i \\leq 1-2\\nu \\right\\}, \\qquad [n]-\\mc{I}:=\\left\\{ i\\in[n]:  1-2\\nu  <  \\underline{\\hb}_i\\right\\}.\n\n\n\\end{align*}\nFor all $ i \\in [n]-\\mc{I}$,\n\\begin{equation}\\label{eqn:grad:difff2}\n\\begin{split}\n      \\sum_{t\\in \\Tc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it}) &- (1+0.5\\pi) \\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n      & \\leq  \\left(2A - (1+0.5\\pi)\\right)\\Gamma\\sum_{t\\in \\Tc_i,~\\hb_{i1}-\\hb_{it} \\geq 1+\\frac{\\pi}{2} } \\s_{it} \\\\\n      & \\leq  \\left(2A - (1+0.5\\pi)\\right)\\Gamma Te^{-\\RR(1+\\frac{\\pi}{2})} \\\\\n      &\\leq   2A\\Gamma  T e^{-\\RR(1+\\frac{\\pi}{2})}.\n      \n      \n      \n\\end{split}\n\\end{equation}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor all $ i \\in \\mc{I}$, split the tokens into two groups: Let $\\Nc_i$ be the group of tokens obeying $ \\hb_{i1}-\\hb_{it} \\leq 1-\\nu$ and $\\Tc_i-\\Nc_i$ be the rest of the neighbors. Observe that\n\\[\n\\frac{\\sum_{t\\in \\Tc_i-\\Nc_i}\\s_{it}}{\\sum_{t\\in\\Tc_i}\\s_{it}}\\leq  T\\frac{e^{\\nu \\RR}}{e^{2\\nu\\RR}}=Te^{-\\RR\\nu}.\n\\]\nUsing $|\\hb_{i1}-\\hb_{it}|\\leq 2A=2 \\max_{i\\in[n],t\\in[T]}\\tn{\\kb_{it}}/\\Theta$ and  $\\bggm=\\min_{i\\in[n]}\\bgg_i =\\min_{i\\in[n]} (\\bgam_{i1}-\\max_{t\\in\\Tc_i}\\bgam_{it})$, observe that \n\\[\n\\sum_{t\\in\\Tc_i-\\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq \\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm} \\sum_{t\\in \\Tc_i} \\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\]\nThus, \n\\begin{align*}\n  \\sum_{t\\in \\Tc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})&= \\sum_{t\\in \\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})+\\sum_{t\\in\\Tc_i-\\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\nonumber\\\\\n\n  &\\leq \\sum_{t\\in \\Nc_i} (1-\\nu)\\s_{it}(\\bgam_{i1}-\\bgam_{it})+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm} \\sum_{t\\in \\Tc_i} \\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n  &\\leq \\left(1-\\nu+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}\\right)\\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n &\\leq \\left(1+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}\\right)\\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align*}\nHence, choosing \n\\begin{align}\nR\\geq\\frac{1}{\\nu\\Theta}\\log\\left(\\frac{8\\Gamma AT}{\\bggm\\pi}\\right)\\label{R bound pi 1}\n\\end{align}\nresults in that\n\\begin{equation}\\label{eqn:grad:difff3}\n    \\begin{split}\n     &\\sum_{t\\in \\Tc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})  - (1+\\frac{\\pi}{2}) \\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}) \\\\\n   &\\leq\\left(\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}-\\frac{\\pi}{2}\\right)\\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n   &\\leq -\\frac{\\pi}{4}\\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n   &\\leq-\\frac{\\pi}{4T}\\bggm  e^{-\\bar{R} (1-2\\nu)}.      \n    \\end{split}\n\\end{equation}\nHere, the last inequality follows from the fact that $\\sum_{t\\in \\Tc_i}\\s_{it}\\geq \\max_{t\\in\\Tc_i}s_{it}\\geq\\frac{e^{-\\bar{R}(1-2\\nu)}}{\\sum_{t=1}^Te^{-\\bar{R}(\\hb_{i1}-\\hb_{it})}}\\geq e^{-\\bar{R}(1-2\\nu)}/T$.\n\n\nFrom Assumption~\\ref{assum:loss:prope}, we have $c_{\\min}\\leq-\\ell'\\leq c_{\\max}$ for some positive constants $c_{\\min}$ and $c_{\\max}$. It follows from  \\eqref{eqn:grad:difff2} and \\eqref{eqn:grad:difff3} that \n\\begin{align*}\n-\\frac{1}{n}\\sum_{i}^n \\ell_i' \\cdot&\\left(\n      \\sum_{t\\in \\Tc_i} (\\hb_{i1}-_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})- \\sum_{t\\in \\Tc_i} (1+0.5\\pi)\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\right)\\\\\n      & \\leq    c_{\\max}2A\\Gamma  T \\Gamma e^{-\\RR(1 +\\frac{\\pi}{2})}-\\frac{c_{\\min}}{nT}\\cdot\\frac{\\pi\\bggm}{4}e^{-\\bar{R} (1-2\\nu)}\\\\\n      \n      & \\leq 0.\n\\end{align*}\nCombing with \\eqref{R bound pi 1}, this is guaranteed by \nchoosing \n\n\n\n\\[\n  R\\geq \\max\\left\\{\\frac{1}{\\nu\\Theta}\\log\\left(\\frac{8\\Gamma AT}{\\bggm\\pi}\\right),\\frac{1}{(2\\nu+\\pi/2)\\Theta}\\log\\left(\\frac{8n\\Gamma AT^2 c_{\\max}}{c_{\\min}\\bggm\\pi}\\right)\\right\\},\n\\]\nwhere $\\nu=\\nu(\\frac{\\pi}{4A\\Theta})$ depends only on $\\pi$ and global problem variables. \n\nCombining this with the prior $R$ choice \\eqref{R boundC0} (by taking maximum), we conclude with the statement.\n\n\n\\end{proof}",
        "metadata": {
            "Statement label": "Lemma 35",
            "Statement title": "Local Correlation Inequality"
        }
    },
    {
        "text": "\\begin{lemma}[Gradient Condition for Optimal Tokens]\\label{glocal cond} \nSuppose Assumption~\\ref{assum:loss:prope} holds and let $\\op=(\\op_i)_{i=1}^n$ be the unique globally-optimal indices with $\\Wm$ denoting the SVM solution. Define the margin $\\Theta=1/\\tf{\\Ws}$. \n\n\n\nGiven $\\mu\\geq 0$, consider the following subset of the sphere and its associated cone\n\n\n\n\n\\begin{subequations}\\label{eqn:con:nabla0}\n\\begin{align}\n&\\Sc_{\\mu}=\\left\\{\\W~\\Big|~  \\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\frac{\\W}{\\tf{\\W}}\\ri\\geq \\mu\\cdot\\Theta\\quad \\textnormal{for all}\\quad t\\neq \\op_i, \\quad  i\\in[n]\\right\\},\\\\\n&\\conb_{\\mu,R}=\\left\\{  \\W\\in\\Sc_\\mu ~\\Big|~   \\tf{\\W}\\geq R\\right\\}.\n\\end{align}\n\\end{subequations}\n\nFor any $\\mu>0$, there exists sufficiently large $R=R_\\mu=\\order{1/\\mu}$ (see \\eqref{R bound2}) such that:\n\\begin{enumerate}[label={\\textnormal{\\textbf{L\\arabic*.}}}, wide, labelwidth=!,itemindent=!, labelindent=5pt]\n\\item \\label{lem:gcond:l1} There is no stationary point within  $ \\conb_{\\mu,R}$.\n\n\\item\\label{lem:gcond:l2}  Let $s_i=\\sft{\\X_i\\W\\z_i}$. For all $\\V\\in \\Sc_{\\mu},\\W\\in\\conb_{\\mu,R}$, there exist $C,c>0$ such that \n\\begin{align*}\nC\\cdot\\max_{i\\in[n]}  \\left(1-s_{i\\op_i}\\right) \\geq -\\li\\nabla\\Lc(\\W),\\V\\ri\\geq c\\cdot\\mu\\cdot\\min_{i\\in[n]}  \\left(1-s_{i\\op_i}\\right).\n\\end{align*}\n\\end{enumerate}\n\\end{lemma}\nProof:\n\\begin{proof} Let us introduce the norm upper bound\n\\begin{equation}\\label{mu choice2}\n\\begin{split}\n\n&A=\\max_{i\\in[n],t,\\tau\\in[T]} \\frac{(\\tn{\\x_{it}}\\vee\\tn{\\x_{it}-\\x_{i\\tau}})\\cdot\\tn{\\z_i}}{\\Theta}.\n\\end{split}\n\\end{equation}\n\nThe following inequalities hold for all $\\V\\in \\Sc_{\\mu},~\\tf{\\V}=\\tf{\\Wm}$ and all $i\\in[n], t\\neq \\op_i$:\n\\begin{equation}\\label{cone-A-eq}\n\\begin{split}\n\n\nA\\geq(\\x_{i\\op_i}-\\x_{it})^\\top \\V \\z_i &\\geq \\mu.\n\\end{split}\n\\end{equation}\n\n \n\nTo proceed, we write the gradient correlation following \\eqref{grad def} and \\eqref{grad def2}\n\\begin{align}\\label{grad def32}\n\\li\\nabla\\Lc(\\W),\\V\\ri&=\\frac{1}{n}\\sum_{i=1}^n\\ell'_i\\cdot\\li\\hb_i,\\sfp{\\hp_i}\\bgam_i\\ri,\n\\end{align}\nwhere we denoted $\\ell'_i=\\ell'(Y_i\\cdot \\vb^\\top \\X_i^\\top\\sft{\\hp_i})$, $\\hb_i=\\X_i\\V \\z_{i}$, $\\hp_i= \\X_i\\W \\z_{i}$, $\\s_i=\\sft{\\hp_i}$. It follows from \\eqref{mu choice2} that $A\\geq \\max_{i\\in[n],t\\in[T]}\\tn{\\hb_{it}}$. Using \\eqref{cone-A-eq}, we can bound the softmax probabilities $\\s_i=\\sft{\\hp_i}$ as follows, for all $i\\in[n]$:\n\\begin{align}\\label{soft prob bound2}\n&S_i:= \\sum_{\\tau\\neq \\op_i}\\s_{i\\tau}\\leq T e^{-R\\mu\\Theta}\\s_{i\\op_i}\\leq T e^{-R\\mu\\Theta}.\n\\end{align}\n\nRecall scores $\\bgam_{it}=Y_i\\cdot\\vb^\\top \\x_{it}$. Define the score gaps:\n\n\\begin{equation*}\n \\bgg_i=\\bgam_{i\\op_i}-\\max_{t\\neq\\op_i}\\bgam_{it},~~~ \\bgm_i=\\bgam_{i\\op_i}-\\min_{t\\neq\\op_i}\\bgam_{it},~~~\\textnormal{and}~~~\\Gamma=\\sup_{i\\in[n],t,\\tau\\in[T]}|\\bgam_{it}-\\bgam_{i\\tau}|. \n\\end{equation*}\nLet us focus on a fixed datapoint $i\\in[n]$, assume (without losing generality) $\\op_i=1$, and drop subscripts $i$.\n\nDirectly applying Lemma \\ref{lem:q_reduce}, we obtain\n\\[\n  \\big|\\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam-\\sum_{t\\geq 2}^T (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2\\Gamma A(1-\\s_1)^2.\n\\]\n\\noindent To proceed, let us upper/lower bound the gradient correlation. Since $A\\geq \\hb_1-\\hb_t\\geq \\mu>0$ from \\eqref{cone-A-eq}, setting $S:=\\sum_{t\\neq\\op_i}\\s_t=1-\\s_1$, we find\n\\begin{equation}\n A\\cdot S\\cdot \\bgm  \\geq\\sum_{t\\neq\\op} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\geq \\mu\\cdot S\\cdot \\bgg.\\label{aggregate2}\n\\end{equation}\nNext we show that $S=1-\\s_1$ dominates $(1-\\s_1)^2=S^2$ for large $R$. Specifically, we wish for \n\\begin{align}\\label{wishfor2}\n\\mu S \\bgg/2\\geq 2\\Gamma A(1-\\s_1)^2\\iff S\\geq \\frac{4}{\\mu}\\frac{\\Gamma A}{\\bgg}S^2\\iff S\\leq \\frac{\\mu\\bgg}{4\\Gamma A}.\n\\end{align}\nUsing \\eqref{soft prob bound2}, what we wish is ensured for all $i\\in[n]$, by guaranteeing $Te^{-R\\mu\\Theta}\\leq \\frac{\\mu\\bgg}{4\\Gamma A}$. That is, by choosing\n\\begin{align}\\label{R bound2}\nR:=R_\\mu= \\frac{1}{\\mu\\Theta}\\log\\big(\\frac{4T\\Gamma A}{\\mu\\bggm}\\big),\n\\end{align}\nwhere $\\bggm=\\sup_{i\\in[n]}\\bgg_i$ is the global scalar corresponding to the worst case score gap over all inputs. \n\nWith the above choice of $R$, we guaranteed\n\\[\n  2 A(1-\\s_1)\\cdot \\bgm\\geq 2A\\cdot S\\cdot \\bgm \\geq \\sum_{t\\neq\\op} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\geq\\frac{\\mu\\cdot S\\cdot \\bgg}{2}\\geq\\frac{\\mu(1-\\s_1) \\bgg}{2}.\n\\]\nvia \\eqref{wishfor2} and \\eqref{aggregate2}. Since this holds over all inputs, going back to the gradient correlation \\eqref{grad def32} and averaging above over all inputs $i\\in[n]$ and plugging back the indices $i$, we obtain the advertised bound by setting $q_i=1-\\s_{i\\op_i}$ (where we have used $\\op_i=1$ above, without losing generality)\n\\begin{align}\\label{pbb corr2}\n  \\frac{2A}{n}\\sum_{i\\in [n]} \\ell'_i\\cdot q_i\\cdot \\bgm_i\\geq \\li\\nabla\\Lc(\\W),\\V\\ri\\geq \\frac{\\mu}{2n}\\sum_{i\\in [n]} \\ell'_i\\cdot q_i\\cdot \\bgg_i.\n\\end{align}\n\\end{proof}",
        "metadata": {
            "Statement label": "Lemma 36",
            "Statement title": "Gradient Cone Lemma"
        }
    },
    {
        "text": "\\begin{lemma}\\label{lem add up}Let $\\A\\in\\R^{n\\times p},\\B\\in\\R^{m\\times p}$. Suppose $n+m\\leq p$ and $\\A$ is full row-rank. Denote the null space of $\\A$ by $S_\\A^\\perp$. Let $P$ be a subspace that is its subset i.e. $P\\subseteq S_\\A^\\perp$. Let $\\B'$ be the matrix obtained by projecting each of row of $\\B$ on $P$ and suppose $\\B'$ is full rank. Then, the concatenation $\\Cb= [\\A; \\B ]$\n\nis full row-rank.\n\\end{lemma}\nProof:\n\\begin{proof} Let $(\\ab_i)_{i=1}^n$, $(\\bb_i)_{i=1}^m$, $(\\bb'_i)_{i=1}^m$ be the rows of $\\A,\\B,\\B'$, respectively. Suppose the set of rows of $\\A$ and $\\B$ are linearly dependent. Then, for some $(c_i)_{i=1}^{n},(c'_i)_{i=1}^{m}$ (which are not all-zeros), we have that\n\\begin{align}\n\\sum_{i=1}^n c_i\\ab_i+\\sum_{i=1}^m c'_i\\bb_i=0.\\label{cs are nonzero}\n\\end{align}\nWe now rewrite this as follows to decouple $P$ and $P^\\perp$:\n\\[\n\\sum_{i=1}^n c_i\\ab_i+\\sum_{i=1}^m c'_i\\bb'_i+\\sum_{i=1}^m c'_i(\\bb'_i-\\bb_i)=0.\n\\]\n%\\red{The two terms on the left side lie in $P^\\perp$.}\nProjecting above inequality to $P$, we find that $\\sum_{i=1}^m c'_i\\bb'_i=0$. Since $(\\bb'_i)_{i=1}^m$ are linearly independent, we find $c'_i=0$ for all $i\\in[m]$. This implies $\\sum_{i=1}^n c_i\\ab_i=0$. Since $(\\ab_i)_{i=1}^n$ are linearly independent, this implies $c_i=0$ for all $i\\in[n]$. Thus, \\eqref{cs are nonzero} can only hold if all coefficients are zero which is a contradiction.\n\\end{proof}",
        "metadata": {
            "Statement label": "Lemma 37",
            "Statement title": "Full Row-Rank Concatenation Lemma"
        }
    },
    {
        "text": "\\begin{theorem}\\label{thm global reg path}\nSuppose Assumptions \\ref{assum:loss:prope} holds, optimal indices $(\\op_i)_{i=1}^n$ are unique, and \\eqref{eqn:sattnsvm} is feasible. Let $\\Wm$ be the unique solution of \\eqref{eqn:sattnsvm}, and let $\\Wc^\\svm_\\star$ be the solution set of \\eqref{eqn:sattnsvmst} with nuclear norm achieving objective $C_\\st$. Then, Algorithms~\\ref{RP-W} and \\ref{RP-QK}, respectively, satisfy:\n\\begin{itemize}\n\\item $\\W$-parameterization has Frobenius norm bias: $\\underset{R\\rightarrow\\infty}{\\lim} \\frac{\\Wb_R}{R}=\\frac{\\Wm}{\\tf{\\Wm}}$.\n\\item $(\\Kb,\\Qb)$-parameterization has nuclear norm bias: $\\underset{R\\rightarrow\\infty}{\\lim} \\dist{\\frac{\\Kbb_R\\Qbb_R^\\top}{R},\\frac{\\Wc^\\svm_\\star}{C_\\st}}=0$.\n\\begin{itemize}\n\\item Setting $m=d$: \\eqref{eqn:sattnsvmst} is a convex problem without rank constraints.\n\\end{itemize} \n\\end{itemize}\n\\end{theorem}\nProof:\nCorollary \\ref{cor global reg path} already proves Theorem \\ref{thm global reg path} through the more general Theorem \\ref{local RP thm}. Below, we provide a self contained proof of Theorem \\ref{thm global reg path} for clarity.\n\n\\begin{proof} Throughout $\\dm$ denotes either Frobenius norm or nuclear norm. We will prove that $\\wrb{R}$ asymptotically aligns with the set of globally-optimal directions and also $\\td{\\wrb{R}}\\rightarrow \\infty$. $\\Rcm\\subseteq\\R^{d\\times d}$ denote the manifold of rank $\\leq$$m$ matrices.\n\\\\\n\\noindent\\textbf{Step 1:} Let us first prove that $\\wrb{R}$ achieves the optimal risk as $R\\rightarrow\\infty$ -- rather than problem having finite optima. Define $\\xdm=1/\\td{\\Wm}$ and norm-normalized $\\Wsb=\\xdm\\Wm$. Note that $\\Wm$ separates tokens $\\op$ from rest of the tokens for each $i \\in[n]$. Thus, we have that\n\\begin{align}\n\\lim_{R\\rightarrow\\infty}\\Lc(\\wrb{R})\\leq\\lim_{R\\rightarrow\\infty}\\Lc(R\\cdot\\Wsb):=\\Lc_\\star= \\frac{1}{n}\\sum_{i=1}^n \\ell(\\bgam^{\\op}_i).\\label{glob:asymp loss}\n\\end{align}\nOn the other hand, for any $\\W\\in \\Rcm$, define the softmax probabilities $\\s^{(i)}=\\sft{\\X_i\\W\\z_i}$ and attention features $\\x^{\\W}_i=\\sum_{t=1}^T \\s^{(i)}_t\\x_t$. Decompose $\\x^{\\W}_i$ as \n$\n\\x^{\\W}_i=\\s^{(i)}_{\\op_i}\\x_{i\\opt_i}+\\sum_{t\\neq \\op_i}\\s^{(i)}_t\\x_\\itt.\n$ Set $\\bgg_\\itt=\\bgam^{\\op}_i-\\bgam_\\itt=Y_i\\cdot \\vb^\\top(\\x_{i\\opt_i}-\\x_\\itt)>0$, and define\n\\begin{align}\n&B:=\\max_{i\\in[n]}\\max_{t,\\tau\\in[T]}\\tn{\\vb}\\cdot \\tn{\\x_\\itt-\\x_\\ittt}\\geq \\bgg_\\itt.\\label{glob:BB eq}\n\\end{align}\n\n\n\n\n\n\n\nDefine $c_\\op=\\min_{i\\in[n],t\\neq\\op_i}\\bgg_\\itt>0$ and $\\bgam^{\\W}_i=Y_i\\cdot \\vb^\\top\\x^{\\W}_i$. We obtain the following score inequalities\n\\begin{align}\\label{glob:score decomp}\n&\\bgam^{\\W}_i\\leq \\bgam^{\\op}_i-c_\\op (1-\\s^{(i)}_{\\op_i})<\\bgam^{\\op}_i,\\\\\n\n&|\\bgam^{\\W}_i-\\bgam^{\\op}_i|\\leq \\tn{\\vb}\\cdot\\tn{\\x^{\\W}_i-\\xa_i}\\leq \\tn{\\vb} \\sum_{t\\neq \\op_i}\\s^{(i)}_t\\tn{\\x_\\itt-\\xa_i}\\leq B (1-\\s^{(i)}_{\\op_i}).\\nonumber\n\n\n\\end{align}\nWe will use the $\\bgam^{\\W}_i-\\bgam^{\\op}_i$ term in \\eqref{glob:score decomp} to evaluate $\\W$ against the reference loss $\\Lc_\\star$ of \\eqref{glob:asymp loss}. \n\n\n\n\n\nUsing the strictly-decreasing nature of $\\ell$, we conclude with the fact that for all (finite) $\\W\\in \\Rcm$, \n\\[\n\\Lc(\\W)=\\frac{1}{n}\\sum_{i=1}^n \\ell(\\bgam^{\\W}_i)> \\Lc_\\st=\\frac{1}{n}\\sum_{i=1}^n \\ell(\\bgam^{\\op}_i),\n\\]\nwhich implies $\\td{\\wrb{R}}\\rightarrow\\infty$ together with \\eqref{glob:asymp loss}.\n\n\\noindent\\textbf{Step 2:} To proceed, we show that $\\wrb{R}$ converges in direction to $\\Wcs$, which denotes the set of SVM minima. Suppose this is not the case and~convergence fails. We will obtain a contradiction by showing that $\\Wsb_R=R\\cdot\\Wsb$ achieves a strictly superior loss compared to $\\wrb{R}$. Let us introduce the normalized parameters $\\wrt{R}=\\frac{\\wrb{R}}{R\\xdm}$ and $\\W'=\\frac{\\wrb{R}}{\\td{\\wrb{R}}\\xdm}$. Note that $\\wrt{R}$ is obtained by scaling down $\\W'$ since $\\td{\\wrb{R}}\\leq R$ and $\\W'$ obeys $\\td{\\W'}=\\td{\\Wm}$.\nSince $\\wrt{R}$ fails to converge to $\\Wcs$, for some $\\delta>0$, there exists arbitrarily large $R>0$ such that $\\dist{\\wrt{R},\\Wcs}\\geq \\delta$. This translates to the suboptimality in terms of the margin constraints as follows: First, since nuclear norm dominates Frobenius, distance with respect to the $\\dm$-norm obeys $\\distd{\\wrt{R},\\Wcs}\\geq \\delta$. Secondly, using triangle inequality,\n\\[ \n\\text{this implies that either~~~}\\td{\\wrt{R}}\\leq \\td{\\Wm}-\\delta/2\\text{~~~or~~~}\\distd{\\W',\\Wcs}\\geq \\delta/2.\n\\]\nIn either scenario, $\\wrt{R}$ strictly violates one of the margin constraints of \\eqref{seqattnsvm}: If $\\td{\\wrt{R}}\\leq \\td{\\Wm}-\\delta/2$, then, since the optimal SVM objective is $\\td{\\Wm}$, there exists a constraint $i,t\\neq\\op_i$ for which $\\inn{(\\x^\\op_i-\\x_\\itt)\\z_i^\\top,\\wrt{R}}\\leq 1-\\frac{\\delta}{2\\td{\\Wm}}$. If $\\distd{\\W',\\Wcs}\\geq \\delta/2$, then, $\\W'$ has the same SVM objective but it is strictly bounded away from the solution set. Thus, for some $\\eps:=\\eps(\\delta)>0$, $\\W'$ and its scaled down version $\\wrt{R}$ strictly violate an SVM constraint achieving margin $\\leq 1-\\eps$. Without losing generality, suppose $\\wrt{R}$ violates the first constraint $i=1$. Thus, for a properly updated $\\delta>0$ (that is function of the initial $\\delta>0$) and for $i=1$ and some \\nei $\\tau\\in \\Tc_1$,\n\\begin{align}\n\\inn{ (\\x^\\op_1-\\x_{1t})\\z_1^\\top,\\wrt{R}}\\leq 1-\\delta.\\label{margin violate:glob}\n\n\\end{align}\nNow, we will argue that this leads to a contradiction by proving $\\Lc(\\Wsb_R)<\\Lc(\\wrb{R})$ for sufficiently large $R$.\n\nTo obtain the result, we establish a refined softmax probability control as in Step 1 by studying distance to $\\Lc_\\star$. Following \\eqref{glob:score decomp}, denote the score function at $\\wrb{R}$ via $\\bgam^R_i:=\\bgam_i^{\\wrb{R}}$. Similarly, let $\\sir_i=\\sft{\\abr_i}$ with $\\abr_i=\\X_i\\wrb{R}\\z_i$. Set the corresponding notation for the reference parameter $\\Wsb_R$ as $\\bgam^\\st_i,\\s^\\st_i,\\ab^\\st_i$.  Recall that $R\\geq \\td{\\wrb{R}}$ and $\\xdm:=1/\\td{\\Wm}$. We note the following softmax inequalities \n\\begin{align}\n&\\s^\\st_{i\\op_i}\\geq \\frac{1}{1+Te^{-R\\xdm}}\\geq 1-Te^{-R\\xdm}\\quad \\text{for all}\\quad i\\in[n], \\label{glob:salpha bounds}\\\\\n&s^R_{i\\op_i}\\leq \\frac{1}{1+e^{-(1-\\delta)\\td{\\wrb{R}}\\xdm}}\\leq \\frac{1}{1+e^{-(1-\\delta)R\\xdm}}\\quad\\text{for}\\quad i=1.\\nn\n\\end{align}\nThe former inequality is thanks to $\\Wm$ achieving $\\geq$$1$ margins on all tokens $[T]-\\op_i$ and the latter arises from the $\\delta$-margin violation of $\\wrb{R}$ at $i=1$ i.e.~Eq.~\\eqref{margin violate:glob}. Since $\\ell$ is strictly decreasing with Lipschitz derivative and the scores are upper/lower bounded by an absolute constant (as tokens are bounded and fixed), we have that $\\cop\\geq -\\ell'(\\bgam_i^{\\W})\\geq \\cdn$ for some constants $\\cop>\\cdn>0$. Thus, following Eq.~\\eqref{glob:BB eq}, the score decomposition \\eqref{glob:score decomp}, and \\eqref{glob:salpha bounds} we can write\n\\begin{align}\\label{glob:ineq prl}\n\\Lc(\\wrb{R})-\\Lc_\\star&\\geq \\frac{1}{n}[\\ell(\\bgam_1^{\\wrb{R}})-\\ell(\\bgam^{\\op}_1)]\\geq \\frac{\\cdn}{n}(\\bgam^{\\op}_{1}-\\bgam_1^{\\wrb{R}})\\\\\n\\nonumber\n&\\geq \\frac{\\cdn}{n}c_\\op (1-\\s^R_{1\\op_1}).\n\\\\\n\\nonumber \n&\\geq \\frac{\\cdn c_\\op}{n}\\frac{1}{1+e^{(1-\\delta)R\\xdm}}.\n\\end{align}\nConversely, we upper bound the difference between $\\Lc(\\Wsb_R)$ and $\\Lc_\\star$ as follows. Define the worst-case loss difference for $\\wrb{R}$ as $j=\\arg\\max_{i\\in[n]}[\\ell(\\bgam_i^\\st)-\\ell(\\bgam^{\\op}_i)]$. Using \\eqref{glob:score decomp}\\&\\eqref{glob:salpha bounds}, we write\n\n\\begin{equation*}\n\\begin{aligned}\n\\Lc(\\Wsb_R)-\\Lc_\\star&\\leq \\max_{i\\in[n]}[\\ell(\\bgam_i^\\st)-\\ell(\\bgam^{\\op}_i)]\\leq \\cop\\cdot(\\bgam^{\\op}_{j}-\\bgam^\\st_{j})\\\\\n&\\leq \\cop\\cdot(1-\\s^\\st_{j\\op_j})B\\\\\n&\\leq \\cop\\cdot Te^{-R\\xdm}B.\n\\end{aligned}\n\\end{equation*}\nCombining the last inequality and \\eqref{glob:ineq prl}, we conclude that $\\Lc(\\Wsb_R)<\\Lc(\\wrb{R})$ whenever\n\\[\n\\cop T\\cdot e^{-R\\xdm}B<\\frac{\\cdn\\cdot c_\\op }{n}\\frac{1}{1+e^{(1-\\delta)R\\xdm}}\\iff \\frac{e^{R\\xdm}}{1+e^{(1-\\delta)R\\xdm}}> \\frac{\\cop Tn B}{\\cdn c_\\op }.\n\\]\nThe left hand-side inequality holds for all sufficiently large $R$: Specifically, as soon as $R$ obeys $R>\\frac{1}{\\delta\\xdm}\\log(\\frac{2\\cop Tn B}{\\cdn c_\\op})$. This completes the proof of the theorem by contradiction since we obtained $\\Lc(\\wrb{R})>\\Lc(\\Wsb_R)$.\n\\end{proof}",
        "metadata": {
            "Statement label": "Theorem 21",
            "Statement title": "Regularization Path Bias Theorem"
        }
    },
    {
        "text": "\\begin{theorem}\n\\label{diverg:norm:w}\nSuppose Assumption~\\ref{assum:loss:prope} on the loss function $\\ell$ and Assumption \\ref{assum:token} on the tokens hold.  \n\n\\begin{itemize}\n\\item \nThere is no $\\W\\in\\R^{d\\times d}$ satisfying $\\nabla \\Lc(\\W)=0$.\n    \\item  Algorithm~\\ref{GD-W} with the step size $\\eta \\leq 1 /L_{\\W}$ and any starting point $\\W(0)$ satisfies \n\n$\\lim_{k\\rightarrow\\infty} \\tf{\\W(k)}=\\infty$.\n\n\n\n\n\\end{itemize}\n\\end{theorem}\nProof:\nIt follows from Lemma~\\ref{lem:grad:descent} that under Assumption \\ref{assum:loss:prope}, $\\eta \\leq 1/L_{\\W}$, and for any initialization $\\W(0)$, the gradient descent sequence $\\W(k+1)=\\W(k)-\\eta\\nabla \\mathcal{L}(\\W(k))$ satisfies $\\lim_{k\\rightarrow \\infty}\n\\tf{\\nabla\\mathcal{L}\\left(\\W\\left(k\\right)\\right)}^{2}=0$.  \n\nFurther,  it follows from Lemma~\\ref{global des lem} that $\\li\\nabla\\Lc(\\W), \\Wm\\ri <0$  for all $\\W\\in\\R^{d\\times d}$. Hence, for any finite $\\W$, $\\li\\nabla\\Lc(\\W), \\Wm\\ri$ cannot be equal to zero.  Therefore, there are no finite critical points $\\W$, for which $\\nabla \\mc{L} (\\W)=0$ which contradicts Lemma~\\ref{lem:grad:descent}. This\nimplies that $\\left\\Vert \\W\\left(k\\right)\\right\\Vert \\rightarrow\\infty$.",
        "metadata": {
            "Statement label": "Theorem 22",
            "Statement title": "Divergence of Weight Norms in Gradient Descent"
        }
    },
    {
        "text": "\\begin{theorem}\\label{conv:gd:w:global:nabla0}\nSuppose Assumption~\\ref{assum:loss:prope} on the loss function $\\ell$ and Assumption \\ref{assum:nabla0} on the initial gradient hold. \n\n\\begin{itemize}\n\n\n\\item  For any $\\mu>0$, there exists  $R>0$ such  that   $\\conb_{\\mu,R}$ does not contain any  stationary points. \n\\item Fix any $\\mu \\in  (0, \\iota/\\tf{\\nabla \\Lc(0)})$. Consider GD iterations with $\\W(0)=0$, $\\W(1)=-R\\nabla\\,\\Lc(0)/\\tf{\\nabla\\Lc(0)}$, and $\\W(k+1)=\\W(k)-\\eta\\nabla\\Lc(\\W(k))$ for $k\\ge 1$, where $\\eta\\le 1/L_{\\W}$ and $R$ sufficiently large. If all iterates remain within $\\conb_{\\mu,R}$, then $\\lim_{k\\rightarrow\\infty} \\tf{\\W(k)}=\\infty$ and $\\lim_{k\\rightarrow\\infty}\\frac{\\W(k)}{\\tf{\\W(k)}}=\\frac{\\Wm}{\\tf{\\Wm}}$.\n\\end{itemize}\n\\end{theorem}",
        "metadata": {
            "Statement label": "Theorem 23",
            "Statement title": "Global Convergence of Gradient Descent with Initial Gradient Condition"
        }
    },
    {
        "text": "\\begin{theorem}\n\\label{thm:local:gd} \nSuppose Assumption~\\ref{assum:loss:prope} on the loss $\\ell$ holds, and let $\\bal=(\\alpha_i)_{i=1}^n$ be locally optimal tokens according to Definition \\ref{def loc opt}. Let $ \\Wma$ denote the SVM solution obtained via \\eqref{eqn:sattnsvm} by  replacing $(\\opt_i)_{i=1}^n$ with $\\boldsymbol{\\alpha} = (\\alpha_i)_{i=1}^n$. \n\n\\begin{itemize}\n    \\item \\label{lem:cond:t1}  There exist parameters $\\mu=\\mu(\\bal) \\in (0,1)$ and  $R>0$ such  that   $ \\Cc_{\\mu,R} (\\Wma)$ does not contain any  stationary points.\n    \\item  Algorithm~\\ref{GD-W} with $\\eta \\leq 1 /L_{\\W}$ and any $\\W(0) \\in \\Cc_{\\mu,R}(\\Wma)$ satisfies $\\lim_{k\\rightarrow\\infty} \\tf{\\W(k)} = \\infty$  and $\\lim_{k\\rightarrow\\infty} \\frac{\\W(k)}{\\tf{\\W(k)}} = \\frac{\\Wma}{\\tf{\\Wma}}$.\n\\end{itemize}\n\\end{theorem}",
        "metadata": {
            "Statement label": "Theorem 24",
            "Statement title": "Local Gradient Descent Convergence Theorem"
        }
    },
    {
        "text": "\\begin{theorem}\\label{thm:separation} Suppose $d\\geq \\max(T-1,n)$. Then, almost all datasets $(Y_i,\\X_i,\\z_i)_{i=1}^n$ -- including the self-attention setting with $\\z_i\\gets\\x_{i1}$ -- obey the following: \n\\eqref{eqn:sattnsvm} is feasible i.e.,~$\\Wm$ separates the desired tokens $\\opt=(\\opt_i)_{i=1}^n$.\n\\end{theorem}",
        "metadata": {
            "Statement label": "Theorem 25",
            "Statement title": "Separation Theorem for Self-Attention"
        }
    },
    {
        "text": "\\begin{theorem}\\label{separation thm} Suppose $d\\geq \\max(T-1,n)$ and $m=d$. Then, almost all datasets\\footnote{Here, \\emph{``almost all datasets''} means that adding i.i.d.~gaussian noise, with arbitrary nonzero variance, to the input features will almost surely result in SVM's feasibility.} $(Y_i,\\X_i,\\z_i)_{i=1}^n$ -- including the self-attention setting with $\\z_i\\gets\\x_{i1}$ -- obey the following: For any choice of indices $\\bal=(\\alpha_i)_{i=1}^n\\subset[T]$, \\eqref{dmattnsvm} is feasible,  i.e.~the attention layer can separate and select indices $\\bal$.\n\\end{theorem}",
        "metadata": {
            "Statement label": "Theorem 26",
            "Statement title": "Separation Feasibility Theorem"
        }
    },
    {
        "text": "\\begin{theorem} [Convergence of Local Regularization Path]\\label{local RP thm1} Suppose Assumption \\ref{assum:loss:prope} holds. Fix locally-optimal token indices $\\bal=(\\al_i)_{i=1}^n$ and $R_0,\\eps>0$. Consider the norm-constrained variation of \\eqref{cone alpha eq1} defined as \n\\[\n\\Ccd:=\\con{\\bal}\\bigcap \\left\\{\\W\\bgl \\td{\\W}\\geq R_0\\right\\}.\n\\]\nDefine local RP as $\\Wb_R=\\min_{\\Ccd,\\td{\\W}\\leq R}\\Lc(\\W)$ where $\\Lc(\\W)$ is given by \\eqref{eqn:erm:w}. Let $\\Wcs$ be the set of minima for \\eqref{dmattnsvm} and $\\xdm>0$ be the associated margin i.e.~$\\xdm=1/\\td{\\Wma}$. For any sufficiently small $\\eps>0$ and sufficiently large $R_0= \\order{1/\\eps}>0$, $\\lim_{R\\rightarrow\\infty} \\dist{\\frac{\\Wb_R}{R\\xdm},\\Wcs}=0$. Additionally, suppose optimal indices $\\op=(\\op_i)_{i=1}^n$ are unique and set $\\bal\\gets\\op$. Then, the same convergence guarantee on regularization path holds by setting $\\Ccd$ as the set of rank-$\\leq$$m$ matrices.\n\\end{theorem}",
        "metadata": {
            "Statement label": "Theorem 27",
            "Statement title": "Convergence of Local Regularization Paths"
        }
    },
    {
        "text": "\\begin{theorem}\\label{toy data thm} Consider the dataset model $\\Dc_{\\texttt{data}}$ of Def.~\\ref{def data model}. Denote the initial population gradient $\\nabla\\Lc(0):=\\E_{\\Dc_{\\texttt{data}}}[\\nabla\\Lc(0)]$. Let $\\W_1=\\frac{1}{r}\\sum_{j=1}^r\\ab_j\\ab_j^\\top$ and $\\W_\\rho=\\frac{1}{r}\\sum_{j=1}^r\\bb_j\\ab_j^\\top$. We have that\n\\begin{align}\n&\\nabla\\Lc(0)=\\frac{\\ell'(0)}{T}(\\gamma_1\\W_1+\\gamma_\\rho\\W_\\rho)-\\frac{\\ell'(0)}{T^2}(\\gamma_1+\\gamma_\\rho)(\\W_1+\\W_\\rho)\\label{eq nablaL0}\n\\end{align}\nAdditionally, suppose Assumption \\ref{assume sep} holds. Then, $\\x_\\rho$ is the optimal token and Assumption \\ref{assum:nabla0} holds almost surely i.e.\n\n\\[\n\\underset{t\\in[T]}{\\min}\\li(\\x_t-\\x_\\rho)^\\top\\nabla\\Lc(0)\\x_1\\ri>0.\n\\]\n\\end{theorem}\nProof:\n\\begin{proof} We will make use of the simple structure of softmax at $\\W=0$. Let $\\bgam=Y\\cdot\\X\\vb$ and $\\Fb_t=\\x_t\\x_1^\\top$. Using the fact that softmax derivative at $0$ is simply all $1/T$ vector, we have that\n\\begin{align}\n\\nabla\\Lc(0)=\\frac{\\ell'(0)}{T}\\E[\\sum_{t=1}^T \\bgam_t\\Fb_t]-\\frac{\\ell'(0)}{T^2}\\E[\\sum_{t=1}^T \\bgam_t\\sum_{t=1}^T \\Fb_t].\n\\end{align}\n\nLet $\\bSi=\\E[\\x_t\\x_t^\\top]$ for $t\\notin\\{1,\\rho\\}$. To proceed, observe that, for $t\\not\\in\\{1,\\rho\\}$\n\\[\n\\E[\\bgam_t\\Fb_t]=\\E[Y\\vb^\\top\\x_t\\x_t\\x_1^\\top]=\\E[\\vb^\\top\\x_t\\x_t]\\E[Y\\x_1]^\\top=\\bSi\\vb\\E[Y\\x_1]^\\top=0.\n\\]\nSimilarly for $t\\not\\in\\{1,\\rho\\}$, we have $\\E[\\sum_{i=1}^T \\bgam_i\\Fb_t]=0$ by additionally using $\\E[\\x_t]=0$.\n\nWhat remains is $t=1$ and $t=\\rho$. Using fixed score assumption, we find\n\\[\n\\E[\\bgam_1\\Fb_1]=\\E[Y\\vb^\\top\\x_1\\x_1\\x_1^\\top]=\\gamma_1\\W_1,\\quad\\E[\\bgam_\\rho\\Fb_\\rho]=\\E[Y\\vb^\\top\\x_\\rho\\x_\\rho\\x_1^\\top]=\\gamma_\\rho\\W_\\rho.\n\\]\nSimilarly, we obtain\n\\[\n\\E[\\sum_{t=1}^T \\bgam_t\\sum_{t=1}^T \\Fb_t]=\\frac{1}{T^2}(\\gamma_1+\\gamma_\\rho)(\\W_1+\\W_\\rho),\n\\]\nto conclude with \\eqref{eq nablaL0}. To conclude with Assumption \\ref{assum:nabla0}, we use Assumption \\ref{assume sep} and observe that $T\\nabla\\Lc(0)$ is arbitrarily close to $T\\hat{\\nabla}\\Lc(0)$ where $\\hat{\\nabla}\\Lc(0)=\\frac{\\ell'(0)\\gamma_\\rho}{T}\\W_\\rho$. Now, recalling $\\ell'(0)<0$, applying the right hand-side of \\eqref{sep condition} with $\\hat{\\nabla}\\Lc(0)\\propto-\\W_\\rho$, and using the boundedness of tokens and $\\eps>0$ as perturbation buffer, we obtain the desired statement.\n\\end{proof}",
        "metadata": {
            "Statement label": "Theorem 28",
            "Statement title": "Gradient Initialization in Dataset Models"
        }
    },
    {
        "text": "\\begin{theorem} [Convergence of Local Regularization Path]\\label{local RP thm} Suppose \\eqref{seqattnsvm} is feasible and $\\bal=(\\al_\\ik)\\ikix$ are locally-optimal token indices. Suppose Assumptions \\ref{assum:loss:prope}\\&\\ref{ass cvx seq} hold. Recall $\\con{\\bal}$ of \\eqref{cone alpha eq} and consider the norm-constrained cone \\[\n\\Ccd:=\\con{\\bal}\\bigcap\\{\\W\\bgl \\td{\\W}\\geq R_0\\}.\n\\]\nDefine the conic regularization path $\\wrb{R}=\\min_{\\Ccd,\\td{\\W}\\leq R}\\Lc(\\W)$. Let $\\Wcs_{\\bal}$ be its set of minima and $\\xdm>0$ be the associated margin i.e.~$\\xdm=1/\\td{\\Wcs_{\\bal}}$. For any sufficiently small $\\eps>0$ and sufficiently large $R_0= \\order{1/\\eps}>0$, $\\lim_{R\\rightarrow\\infty} \\dist{\\frac{\\wrb{R}}{R\\xdm},\\Wcs_{\\bal}}=0$. Additionally, suppose optimal indices $\\op=(\\op_\\ik)\\ikix$ are unique and set $\\bal\\gets\\op$. Then, the same RP convergence guarantee holds with $\\Ccd=\\Rcm$.\n\\end{theorem}\nProof:\n\\begin{proof} We will prove that $\\wrb{R}$ is the optimal direction and also $\\td{\\wrb{R}}\\rightarrow \\infty$. Define the absolute constant \n\\[\n\\cdm=\\min_{\\td{\\W}=1}\\tf{\\W}.\n\\]\nThis guarantees that for any $\\W$ we have $\\tf{\\W}\\geq \\cdm\\td{\\W}$. Also denote $\\epsd=\\cdm\\eps$. Let us first determine the $\\eps$ parameter: Fix $\\Wma\\in\\Wcs_{\\bal}$. For general $\\bal$, we can choose any $\\eps>0$ that is sufficiently small to guarantee $\\Wma\\in\\con{\\bal}$ based on Lemma \\ref{lemma cone}. For $\\bal=\\op$, our analysis will entirely avoid using $\\eps$, specifically, observe that $\\con{\\bal}=\\Rcm$ based on Lemma \\ref{lemma cone}.\n    \n\\noindent\\textbf{Step 1:} Let us first prove that $\\wrb{R}$ achieves the optimal risk as $R\\rightarrow\\infty$ -- rather than problem having finite optima. Define norm-normalized $\\Wsb=\\xdm\\Wma$. Note that $\\Wma$ separates tokens $\\bal$ from rest of the tokens for each $i,k\\in[n]\\times [K]$. Thus, we have that\n\\begin{align}\n\\lim_{R\\rightarrow\\infty}\\Lc(\\wrb{R})\\leq\\lim_{R\\rightarrow\\infty}\\Lc(R\\cdot\\Wsb):=\\Lc_\\star= \\frac{1}{n}\\sum_{i=1}^n\\sum_{k=1}^K\\ell(\\bga_\\ik).\\label{asymp loss}\n\\end{align}\nOn the other hand, for any choice of $\\W\\in \\con{\\bal}$, set $\\x^{\\W}_\\ik=\\sum_{t=1}^T \\sft{\\X_i\\W\\z_\\ik}_t\\x_t$. Set softmax probabilities $\\sik=\\sft{\\X_i\\W\\z_\\ik}$. Recalling $\\lowi,\\higi$ definitions, we can decompose the attention features as\n\\begin{align}\n\\x^{\\W}_\\ik=\\sik_{\\al_\\ik}\\xa_\\ik+\\sum_{t\\in\\lowi}\\sik_t\\x_\\itt+\\sum_{\\tau\\in\\higi}\\sik_\\tau\\x_{\\ittt}.\n\\end{align}\nWhen $\\bal=\\op$, note that we simply have $\\higi=\\emptyset$. This will be important for setting $R_0=0$ and $\\Ccd=\\Rcm$ in the proof for $\\op$ indices.\n\nSet $\\bgg_\\ikt=\\bgam_\\ikt-\\bga_\\ik=Y_\\ik\\cdot (h_k(\\x_\\itt)-h_k(\\xa_\\ik))$. Building on $L_h$-Lipschitzness of the prediction head $h_k(\\cdot)$, we define\n\\begin{align}\n&B:=\\max_{i\\in[n],k\\in[K]}\\max_{t,\\tau\\in[T]}L_h\\cdot \\tn{\\x_\\itt-\\x_\\ittt}\\geq |\\bgg_\\ikt|.\\label{BB eq}\n\\end{align}\n\n\n\n\n\n\n\nDefine $P^\\ik:=\\sum_{t\\in\\lowi}\\sik_t$, $Q^\\ik:=\\sum_{t\\in\\higi}\\sik_t$, and $\\bgam^{\\W}_\\ik=Y_\\ik\\cdot h_k(\\x^{\\W}_\\ik)$. Also set temporary variables $\\x'=(\\sik_{\\al_\\ik}+Q^\\ik)\\xa_\\ik+\\sum_{t\\in\\lowi}\\sik_t\\x_\\itt$ and $\\bgam'=Y_\\ik\\cdot h_k(\\x^{\\W}_\\ik)$. \nUsing Assumption \\ref{ass cvx seq} on $\\x'$ and noticing $P^\\ik=1-\\sik_{\\al_\\ik}-Q^\\ik$, observe that\n\\[ \n|\\bgam^{\\W}_\\ik-\\bgam'|\\leq BQ^\\ik\\quad\\text{and}\\quad \\bgam'\\leq \\bga_\\ik-c_\\bal P^\\ik. \n\\] \nRecall from \\eqref{c choice} that, when $h_k$ are linear functions, $c_\\bal$ can be chosen as \n\n\\begin{align*}\nc_\\bal:=\\min_{i\\in[n],k\\in[K]}\\min_{t\\in\\lowi}-\\bgg_\\ikt>0.\n\\end{align*}\n\n\nTo summarize, applying Assumption \\ref{ass cvx seq}, we obtain the following score inequalities\n\\begin{align}\\label{score decomp}\n&\\bgam^{\\W}_\\ik\\leq \\bga_\\ik-c_\\bal P^\\ik+BQ^\\ik,\\\\\n\n&|\\bgam^{\\W}_\\ik-\\bga_\\ik|\\leq L_h\\tn{\\x^{\\W}_\\ik-\\xa_\\ik}\\leq L_h \\sum_{t\\neq \\al_\\ik}\\sik_t\\tn{\\x_\\ikt-\\xa_\\ik}\\leq B(1-\\sik_{\\al_\\ik}).\\label{lip score gap}\n\n\n\\end{align}\nWe will use the $\\bgam^{\\W}_\\ik-\\bga_\\ik$ term in \\eqref{score decomp} to evaluate $\\W$ against the reference loss \\eqref{asymp loss}. Let $\\abik=\\X_i\\W\\z_\\ik$. Now since $\\W\\in \\con{\\bal}$, there exists $t\\in \\lowi$ obeying $\\abik_t-\\max_{\\tau\\in\\higi} \\abik_\\tau\\geq \\eps \\tf{\\W}\\geq \\epsd\\td{\\W}$. Denote $D^\\ik:=(\\sum_{t\\in [T]}e^{\\abik_t})^{-1}$ to be the softmax denominator i.e.~sum of exponentials. We find that,\n\\begin{align}\nQ^\\ik=\\sum_{\\tau\\in\\higi}\\sik_\\tau=D^\\ik\\sum_{\\tau\\in\\higi}e^{\\abik_\\tau}\\leq D^\\ik Te^{\\abik_t-\\eps\\tf{\\W}}\\leq Te^{-\\epsd\\td{\\W}}P^\\ik.\\label{qikeq}\n\\end{align}\nConsequently, the score difference obeys\n\n\\[\n\\bgam^{\\W}_\\ik-\\bga_\\ik\\leq BQ^\\ik-c_\\bal P^\\ik\\leq (BTe^{-\\epsd\\td{\\W}}-c_\\bal)P^\\ik.\n\\]\nAbove, the right hand side is strictly negative as soon as $\\td{\\W}\\geq R_0:=\\frac{1}{\\epsd}\\log\\frac{BT}{c_\\bal}$. Note that, this condition applies to all $(i,k)\\in[n]\\times [K]$ pairs uniformly for the same $R_0$. Consequently, for any $\\td{\\W}\\geq R_0$, for all $i,k$ and $\\W\\in \\con{\\bal}$, we have that $\\bgam^{\\W}_\\ik<\\bga_\\ik$. Additionally, when $\\bal=\\op$, note that $Q^\\ik=0$ since $\\higi=\\emptyset$. Thus, $R_0=0$ suffices to ensure $\\bgam^{\\W}_\\ik<\\bga_\\ik$. Using the strictly-decreasing nature of $\\ell$, we conclude with the fact that for all (finite) $\\W\\in \\con{\\bal}$, \n\\[\n\\Lc(\\W)=\\frac{1}{n}\\sum_{i=1}^n\\sum_{k=1}^K\\ell(\\bgam^{\\W}_\\ik)> \\Lc_\\st=\\frac{1}{n}\\sum_{i=1}^n\\sum_{k=1}^K\\ell(\\bga_\\ik),\n\\]\nwhich implies $\\td{\\wrb{R}}\\rightarrow\\infty$.\n\n\\noindent\\textbf{Step 2:} To proceed, we show that $\\wrb{R}$ converges in direction to $\\Wcs_{\\bal}$. Suppose this is not the case i.e.~convergence fails. We will obtain a contradiction by showing that $\\Wsb_R=R\\cdot\\Wsb$ achieves a strictly superior loss compared to $\\wrb{R}$. Also define the normalized parameter $\\wrt{R}=\\frac{\\wrb{R}}{R\\xdm}$ and $\\W'=\\frac{\\wrb{R}}{\\td{\\wrb{R}}\\xdm}$. Note that $\\wrt{R}$ is obtained by scaling down $\\W'$ since $\\td{\\wrb{R}}\\leq R$ and $\\W'$ obeys $\\td{\\W'}=\\td{\\Wma}$.\n\nSince $\\wrt{R}$ fails to converge to $\\Wcs_{\\bal}$, for some $\\delta>0$, there exists arbitrarily large $R>0$ such that $\\dist{\\wrt{R},\\Wcs_{\\bal}}\\geq \\delta$. This translates to the suboptimality in terms of margin constraints as follows: First, distance with respect to the $\\dm$-norm obeys $\\distd{\\wrt{R},\\Wcs_{\\bal}}\\geq \\delta$ for some updated $\\delta\\gets \\cdm\\delta$. Secondly, using triangle inequality,\n\\[ \n\\text{This implies that either~~~}\\td{\\wrt{R}}\\leq \\td{\\Wma}-\\delta/2\\text{~~~or~~~}\\distd{\\W',\\Wcs_{\\bal}}\\geq \\delta/2.\n\\]\nIn either scenario, $\\wrt{R}$ strictly violates one of the margin constraints of \\eqref{seqattnsvm}: If $\\td{\\wrt{R}}\\leq \\td{\\Wma}-\\delta/2$, then, since the optimal SVM objective is $\\td{\\Wma}$, there exists a constraint $(i,k)$ for which $\\inn{\\Fa_{\\ik}-\\F_{\\ikt},\\wrt{R}}\\leq 1-\\frac{\\delta}{2\\td{\\Wma}}$. If $\\distd{\\W',\\Wcs_{\\bal}}\\geq \\delta/2$, then, $\\W'$ has same SVM objective but it is strictly bounded away from the solution set. Thus, for some $\\eps:=\\eps(\\delta)>0$, $\\W'$ and its scaled down version $\\wrt{R}$ strictly violate an SVM constraint achieving margin $\\leq 1-\\eps$. Without losing generality, suppose $\\wrt{R}$ violates the first constraint. Thus, for a properly updated $\\delta>0$ (that is function of the initial $\\delta>0$) and for $(i,k)=(1,1)$ and some \\nei $\\tau\\in \\Tc_\\oo$,\n\\begin{align}\n\\inn{\\Fa_{\\oo}-\\F_{\\oo t},\\wrt{R}}\\leq 1-\\delta.\\label{margin violate}\n\n\\end{align}\nNow, we will argue that this will lead to a contradiction by proving $\\Lc(\\Wsb_R)<\\Lc(\\wrb{R})$ for sufficiently large $R$.\n\nTo obtain the result, we establish a refined softmax probability control as in Step 1 by studying distance to $\\Lc_\\star$. Following \\eqref{score decomp}, denote the score function at $\\wrb{R}$ via $\\bgam^R_\\ik=\\bgam_\\ik^{\\wrb{R}}$ as shorthand notation. Similarly, let $\\sir_\\ik=\\sft{\\abr_\\ik}$ with $\\abr_\\ik=\\X_i\\wrb{R}\\z_\\ik$. Set the corresponding notation for the reference parameter $\\Wsb_R$ as $\\bgam^\\st_\\ik,\\s^\\st_\\ik,\\ab^\\st_\\ik$. \n\nCritically, recall the above inequalities \\eqref{qikeq} that applies to both $\\W\\in\\{\\wrb{R},\\Wsb_R\\}\\subset\\con{\\bal}$ for an index $(i,k)$ and \\nei $t\\in\\Tc_\\ik$\n\\begin{align}\n\\nonumber \nQ^\\ik&=\\sum_{\\tau\\in\\higi}\\s_\\iktt=D^\\ik\\sum_{\\tau\\in\\higi}e^{\\ab_{\\iktt}} \\\\\n&\\leq D^\\ik Te^{\\ab_\\ikt-\\epsd\\td{\\W}}\\leq Te^{-\\epsd\\td{\\W}}P^\\ik\\leq Te^{-\\epsd\\td{\\W}}(1-\\s_{\\ik\\al_\\ik}), \\label{qik bound}\n\\end{align}\nwhere $P^\\ik=\\sum_{\\tau\\in\\lowi}\\s_{\\iktt}$ and $P^\\ik+Q^\\ik= 1-\\s_{\\ik\\al_\\ik}$. \n\n\nNote that, setting $R_0\\geq \\order{1/\\epsd}=\\order{1/\\eps}$, we guarantee that, for any $(i,k)\\in[n]\\times [K]$\n\\begin{align}\nP^\\ik\\geq Q^\\ik\\implies P^\\ik \\geq 0.5(1-\\s_{\\ik\\al_\\ik}). \\label{pik bound}\n\\end{align}\nAdditionally, when $\\bal=\\op$, note that $Q^\\ik=0$ since $\\higi=\\emptyset$. Thus, $R_0=0$ suffices to ensure \\eqref{pik bound}.\n\nTo proceed, recall that $R\\geq \\td{\\wrb{R}}\\geq R_0$ by definition since $\\wrb{R}\\in \\Ccd$ and recall $\\xdm:=1/\\td{\\Wma}$. Equipped with these, we note the following softmax inequalities on the selected tokens $\\al_\\ik$\n\\begin{align}\n&\\s^\\st_{\\ik\\al_\\ik}\\geq \\frac{1}{1+Te^{-R\\xdm}}\\geq 1-Te^{-R\\xdm}\\quad \\text{for all}\\quad (i,k)\\in[n]\\times [K], \\label{salpha bounds}\\\\\n&s^R_{\\ik\\al_\\ik}\\leq \\frac{1}{1+e^{-(1-\\delta)\\td{\\wrb{R}}\\xdm}}\\leq \\frac{1}{1+e^{-(1-\\delta)R\\xdm}}\\quad\\text{for}\\quad (i,k)=(1,1).\\nn\n\\end{align}\nThe former inequality is thanks to $\\Wma$ achieving $\\geq 1$ margins on all tokens $[T]-\\al_\\ik$ and the latter arises from the $\\delta$-margin violation of $\\wrb{R}$ at $(i,k)=(1,1)$ i.e.~Eq.~\\eqref{margin violate}. Since $\\ell$ is strictly decreasing with Lipschitz gradient and the scores are upper/lower bounded by an absolute constant (as tokens are bounded, $(h_k)_{k=1}^K$ are Lipschitz, and both are fixed), we know that $\\cop\\geq -\\ell'(\\bgam_\\ik^{\\W})\\geq \\cdn$ for some constants $\\cop>\\cdn>0$. Thus, following Eq.~\\eqref{BB eq} and the score decomposition \\eqref{score decomp}, and using \\eqref{qik bound},\\eqref{pik bound},\\eqref{salpha bounds} we can write\n\\begin{align}\n\\nonumber\n\\Lc(\\wrb{R})-\\Lc_\\star&\\geq \\frac{1}{n}[\\ell(\\bgam_\\oo^{\\wrb{R}})-\\ell(\\bga_\\oo)]\\geq \\frac{\\cdn}{n}(\\bga_{\\oo}-\\bgam_\\oo^{\\wrb{R}})\\\\\n&\\geq \\frac{\\cdn}{n}(c_\\bal P^\\oo_{\\wrb{R}}-BQ^\\oo_{\\wrb{R}})\\label{q11 eq}\\\\\n&\\geq \\frac{\\cdn}{n}(1-\\s_{\\oo\\al_\\oo}^R)(0.5c_\\bal -BTe^{-\\epsd \\td{\\wrb{R}}}) \\nonumber\\\\\n\\nonumber\n&\\geq \\frac{\\cdn}{n}\\frac{1}{1+e^{(1-\\delta)R\\xdm}}(0.5c_\\bal-BTe^{-\\epsd R_0}).\n\\end{align}\nAbove, recalling the choice $R_0\\geq \\order{1/\\epsd}=\\order{1/\\eps}$, $R\\geq R_0$ implies $BTe^{-\\epsd R_0}\\leq c_\\bal/4$ to obtain\n\\begin{align}\n\\Lc(\\wrb{R})-\\Lc_\\star\\geq \\frac{\\cdn\\cdot c_\\bal}{4n}\\frac{1}{1+e^{(1-\\delta)R\\xdm}}.\\label{ineq prl}\n\\end{align}\nAdditionally when $\\bal=\\op$, since $Q^\\oo_{\\wrb{R}}=0$ in \\eqref{q11 eq}, the bound above holds with $R_0=0$ by directly using \\eqref{q11 eq}.\n\nConversely, we upper bound the difference between $\\Lc(\\Wsb_R)$ and $\\Lc_\\star$ as follows. Define the worst-case loss difference for $\\wrb{R}$ as $(i',k')=\\arg\\max_{i\\in[n],k\\in[K]}[\\ell(\\bgam_\\ik^\\st)-\\ell(\\bga_\\ik)]$. Using \\eqref{lip score gap}\\&\\eqref{salpha bounds}, we write\n\n\\begin{equation}\n\\begin{aligned}\n\\Lc(\\Wsb_R)-\\Lc_\\star&\\leq \\max_{i\\in[n],k\\in[K]}[\\ell(\\bgam_\\ik^\\st)-\\ell(\\bga_\\ik)]\\leq \\cop\\cdot(\\bga_{i'k'}-\\bgam^\\st_{i'k'})\\\\\n&\\leq \\cop\\cdot(1-\\s_{i'k'\\al_{i'k'}}^\\st)B\\\\\n&\\leq \\cop\\cdot Te^{-R\\xdm}B.\\label{desired Wmm bound}\n\\end{aligned}\n\\end{equation}\nCombining the last inequality and \\eqref{ineq prl}, we conclude that $\\Lc(\\Wsb_R)<\\Lc(\\wrb{R})$ whenever\n\\[\n\\cop T\\cdot e^{-R\\xdm}B<\\frac{\\cdn\\cdot c_\\bal }{4n}\\frac{1}{1+e^{(1-\\delta)R\\xdm}}\\iff \\frac{e^{R\\xdm}}{1+e^{(1-\\delta)R\\xdm}}> \\frac{4\\cop Tn B}{\\cdn c_\\bal }.\n\\]\nThe left hand-side inequality holds for all sufficiently large $R$: Specifically, as soon as $R$ obeys $R>\\frac{1}{\\delta\\xdm}\\log(\\frac{8\\cop Tn B}{\\cdn c_\\bal})$. This completes the proof of the theorem via contradiction as we obtained $\\Lc(\\wrb{R})>\\Lc(\\Wsb_R)$.\n\\end{proof}",
        "metadata": {
            "Statement label": "Theorem 29",
            "Statement title": "Convergence of Conic Regularization Paths"
        }
    },
    {
        "text": "\\begin{theorem}\\label{conv:gd:w:global:nabla0:app}\nSuppose Assumption~\\ref{assum:loss:prope} on the loss function $\\ell$ and Assumption \\ref{assum:nabla0} on the initial gradient hold.  \n\\begin{enumerate}[label={\\textnormal{\\textbf{L\\arabic*.}}}, wide, labelwidth=!,itemindent=!, labelindent=5pt]\n  \\item   \\label{lem:zglob:l1}For any $\\mu>0$, there exists  $R>0$ such  that   $\\conb_{\\mu,R}(\\Ws)$ defined in \\eqref{eqn:con:nabla0} does not contain any  stationary points. \n\\item \\label{lem:zglob:l2} Fix any  $\\mu \\in  (0,\\min (1,\\iota \\tf{\\Ws}/\\tf{\\nabla \\Lc(0)})$. Consider GD iterations with $\\W(0)=0$, $\\W(1)=-R\\nabla\\Lc(0)/\\tf{\\nabla\\Lc(0)}$, and $\\W(k+1)=\\W(k)-\\eta\\nabla\\Lc(\\W(k))$ for $\\eta\\le 1/L_{\\W}$, $k\\ge 1$, and $R$ sufficiently large. If all iterates remain within $\\conb_{\\mu,R}$, then $\\lim_{k\\rightarrow\\infty} \\tf{\\W(k)}=\\infty$ and $\\lim_{k\\rightarrow\\infty}\\frac{\\W(k)}{\\tf{\\W(k)}}=\\frac{\\Wm}{\\tf{\\Wm}}$.\n\\item  \\label{lem:zglob:l3} Assume $\\eta\\leq 1/L_{\\W}$ and for all $\\W \\in \\conb_{\\mu,R}(\\Ws)$ with sufficiently large $R$?\n\\begin{align}\\label{assum:extra}\n   \\min_{i \\in [n]}\\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\W-\\eta\\nabla\\Lc(\\W) \\ri \\geq     \\min_{i \\in [n]}\\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\W\\ri - \\frac{2\\eta\\mu}{\\tf{\\Wm}^2}\\iprod{\\nabla\\mc{L}(\\W)}{\\Wm},\n\\end{align}\nthen all GD iterations remain within $\\conb_{\\mu,R}(\\Wm)$.\n\\end{enumerate}\n\\end{theorem}\nProof:\n\\begin{proof}\nNote that \\ref{lem:zglob:l1} is a direct corollary of Lemma~\\ref{glocal cond}. We proceed with the proof of \\ref{lem:zglob:l2} and \\ref{lem:zglob:l3}.  \n\nWe provide the proof in four steps:\\\\\n\\textbf{Step~1: $\\conb_{\\mu,R^0_\\mu}(\\Wm)$ construction}.  Let us denote the initialization lower bound as $R^0_\\mu:=R$, where $R$ is given in the Theorem~\\ref{conv:gd:w:global:nabla0:app}'s statement. Consider an arbitrary value of  $\\epsilon \\in (0, \\mu/2)$ and let $1/(1+\\pi)=1-\\epsilon$. We additionally denote $R_\\eps\\gets R_\\pi\\vee 1/2$ where $R_\\pi$ was defined in Lemma~\\ref{lem:glocal:corr}. At initialization $\\W(0)$, we set $\\eps=\\mu/2$ to obtain $R^0_\\mu= R_{\\mu/2}$. \n\nWe proceed to show  $\\mu \\in  (0,\\min (1,\\iota \\tf{\\Ws}/\\tf{\\nabla \\Lc(0)})$.  It follows from  Assumption~\\ref{assum:nabla0}  and under zero initialization for GD ($\\W(0)=0$) that\n$$\n\\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top,   - \\nabla \\mc{L}(\\W(0)) \\right\\rangle =\\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top,   - \\nabla \\mc{L}(0) \\right\\rangle\\ge \\iota  > 0, \n$$\nfor some positive constant $\\iota $. Hence, for any initial step size $\\eta (0)>0$ and $\\W(1)=-\\eta(0) \\nabla\\Lc(0)$, \n\\begin{equation}\\label{eqn:decpath:zinit}\n\\begin{split}\n    \\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\frac{\\W (1)}{\\tf{\\W(1)}}\\ri &=  \\frac{\\eta (0) }{\\tf{\\W(1)}}  \\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top,   - \\nabla \\mc{L}(0) \\right\\rangle \\\\\n     & \\geq  \\frac{\\iota  \\eta (0) }{\\tf{\\W(1)}}=  \\frac{\\iota }{ \\tf{\\nabla \\mc{L}(0)}}\\\\\n     &\\geq  \\frac{\\mu}{\\tf{\\Wm}}.\n\\end{split}\n\\end{equation}\nHere, the last inequality follows from our choice of $\\mu$ in the theorem statement, i.e.\n\\begin{align}\\label{eqn:mu:zero}\n \\mu \\in \\left(0, \\min\\left(1, \\frac{\\iota \\tf{\\Wm}}{\\tf{\\nabla \\mc{L}(0)}}\\right)\\right).\n\\end{align}\n\nThis $\\mu$ choice induces the conic set $\\conb_{\\mu,R^0_\\mu}(\\Wm)$ with  $R^0_\\mu= R_{\\mu/2}$, where $R_{\\mu/2}$ was defined in Lemma~\\ref{lem:glocal:corr}. \nNow, given the parameter  $ \\mu $ satisfying \\eqref{eqn:mu:zero}, we can choose $\\eta (0)$ such that $\\tf{\\W (1)} \\geq R^0_\\mu$ and $\\W(1)\\in\\conb_{\\mu, R^0_\\mu}(\\Wm)$. To achieve this, since $\\W(0)=0$, we obtain  \n\\begin{equation}\\label{eqn:stepeta0}\n    \\eta (0) =\\frac{R^0_\\mu}{\\tf{\\nabla \\mc{L}(0) }}. \n    \n\\end{equation}\nSince by our definition, $R^0_\\mu \\leftarrow R$,  \\eqref{eqn:stepeta0} gives $\\W(1)$ in the theorem's statement.\n\n\n\n\\noindent \\textbf{Step~2: There are no stationary points within $\\conb_{\\mu,R_\\mu^0}(\\Wm)$.} \nThis step follows from \\ref{lem:zglob:l1}. Specifically, \n\n\n\nwe can apply Lemma~\\ref{glocal cond} to find that: For all $\\V,\\W\\in  \\bar{\\Sc}_{\\mu}(\\Ws)$ with $\\tf{\\W} \\neq 0$ and $\\tf{\\W} \\geq R^0_\\mu$,  we have that $-\\li\\V, \\nabla \\Lc(\\W)\\ri$ is strictly positive.\n\n\\\\\n\\emph{Gradient correlation holds for large parameter norm.}  \n\n\n\nIt follows from  Lemma~\\ref{lem:glocal:corr} that, there exists $ R_\\epsilon\\geq \\bar{R}_\\mu\\vee 1/2$ such that all  $ \\W \\in \\conb_{\\mu,R_\\epsilon}(\\Wm)$ satisfy\n\\begin{align}\\label{eqn:neg:corr:0}\n\\iprod{-\\nabla\\mc{L}(\\W)}      {\\frac{\\Wm}{\\tf{\\Wm}}} \\geq (1-\\epsilon)    \\iprod{-\\nabla \\mc{L}(\\W)}{\\frac{\\W}{\\tf{\\W}}}.\n\\end{align}\nThe following argument applies to a general $\\eps\\in(0,\\mu/2)$. However, at initialization $\\W(0)=0$, we have set $\\eps=\\mu/2$ and defined the initialization radius as $R^0_\\mu= R_{\\mu/2}$. To proceed, we will prove the main statements \\eqref{lem:zglob:l2} and \\eqref{lem:zglob:l3} as follows.\n\\begin{itemize}\n\\item Proving \\ref{lem:zglob:l3}: In \\textbf{Step 3}, we will assume Condition \\eqref{assum:extra} to prove that gradient iterates remain within $\\conb_{\\mu,R_\\eps}(\\Ws)$. Concretely, for any $\\epsilon \\in (0, \\mu/2)$, we will show that after gradient descent enters the conic set $\\conb_{\\mu,R_\\eps}(\\Ws)$ for the first time, it will never leave the set under Condition \\eqref{assum:extra} of the theorem statement and \\eqref{eqn:neg:corr:0}. In what follows, let us denote $k_\\eps$ to be the first time gradient descent enters $\\conb_{\\mu,R_\\eps}(\\Ws)$. Note that for $\\eps\\gets\\mu/2$, $k_\\eps=0$ i.e.~the point of initialization.\n\n\\item Proving \\ref{lem:zglob:l2}: In \\textbf{Step 4}, assuming iterates within $\\conb_{\\mu,R_\\eps}(\\Ws)$, we will prove that the norm diverges (as a result such $k_\\eps$ is guaranteed to exist) and, additionally, the gradient updates asymptotically aligns with $\\Ws$. \n\\end{itemize}\n\n\n\n\n\n\n\n\\textbf{Step~3 (Proof of \\ref{lem:zglob:l3}): Updates remain inside the cone $\\conb_{\\mu,R_\\eps}(\\Ws)$.}   Note that if $\\W(k) \\in \\conb_{\\mu,R_\\eps}(\\Ws)$ for all $k \\geq 1$, the required condition in \\ref{lem:zglob:l2} holds, and we proceed to \\textbf{Step 4}. In this step, we show \\ref{lem:zglob:l3}. Specifically, we show that under Condition \\eqref{assum:extra} and using  \\eqref{eqn:neg:corr:0}, all iterates $\\W(k) \\in \\conb_{\\mu,R_\\eps}(\\Ws)$ remain within $\\conb_{\\mu,R_\\eps}(\\Ws)$.\n\nTo proceed, by leveraging the results from \\textbf{Step 1} and \\textbf{Step 2}, we demonstrate that the gradient iterates, with an appropriate constant step size, starting from $\\W(k_\\eps) \\in \\conb_{\\mu,R_\\eps}(\\Ws)$, remain within this set. We proceed by induction.  Suppose that the claim holds up to iteration $k \\geq k_\\eps$. This implies that $ \\W(k) \\in \\conb_{\\mu,R_\\eps}(\\Ws)$. Hence, recalling $\\conb_{\\mu,R_\\eps}(\\Ws)$ defined in \\eqref{eqn:con:nabla0}, there exists scalar $\\mu=\\mu(\\bal) \\in (0,1)$  and $R_\\eps$ such that  $\\tf{\\W(k)}\\geq R_\\eps$, and\n\n\n\n\n\n\\begin{equation*}\n\\begin{split}\n\\left\\langle (\\x_{i\\op_i}-\\x_{it})\\z_i^\\top,\\frac{\\W(k)}{\\tf{\\W(k)}} \\right\\rangle  \\geq \\mu\\Theta,\n\\end{split}\n\\end{equation*}\n\n\n\nwhere $\\Theta=1/\\tf{\\Wm}$. \n\nLet \n\\begin{subequations}\\label{eqn:rho:def:nabla0}\n\\begin{align}\n\\frac{1}{1-\\epsilon} \\iprod{ {\\frac{\\Wm}{\\tf{\\Wm}}}}{-\\nabla\\mc{L}(\\W(k))} =:\\rho(k)>0.\n\\end{align}\n\\end{subequations}\nUsing \\eqref{assum:extra}, we have \n\\begin{equation}\\label{eqn:localgd:1:nabla0}\n    \\begin{split}\n   \\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top,    \\frac{\\W(k+1)}{\\tf{\\W(k)}} \\right\\rangle &=   \\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top,    \\frac{\\W(k)}{\\tf{\\W(k)}} -\\frac{\\eta}{\\tf{\\W(k)}}\\nabla \\mc{L}(\\W(k)) \\right\\rangle\\\\   \n\n      & \\geq \\mu \\Theta +\\frac{ 2\\eta  (1-\\epsilon)\\mu \\Theta \\rho(k) }{\\tf{\\W(k)}}.\n    \\end{split}\n\\end{equation}\n\n\nFrom Lemma~\\ref{glocal cond},   we have $\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle<0$~ which implies that $\\tf{\\W(k+1)} \\geq \\tf{\\W(k)}$.  This together with  $R_\\eps$ definition and $\\tf{\\W(k)}\\geq 1/2$ implies that  \n\\begin{align*}\n\\tf{\\W(k+1)}&\\leq\\frac{1}{{2\\tf{\\W(k)}}} \\left(\\tf{\\W(k+1)}^2+\\tf{\\W(k)}^2\\right)\\\\\n& = \\frac{1}{2\\tf{\\W(k)}} \\left(2\\tf{\\W(k)}^2-2\\eta\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle+\\eta^2\\tf{\\nabla \\Lc(\\W(k))}^2\\right)\\\\\n       &\\leq  \\tf{\\W(k)}- \\frac{\\eta}{\\tf{\\W(k)}}\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle + \\eta^2 \\|\\nabla \\Lc(\\W(k))\\|_F^2.\n\\end{align*}\n\nThus,\n\\begin{equation}\\label{eqn:localgd:2:nabla0}\n\\begin{split}\n  \\frac{\\tf{\\W(k+1)}}{\\tf{\\W(k)}}& \\leq  1- \\frac{\\eta}{\\tf{\\W(k)}}\n       \\left\\langle \\nabla \\Lc(\\W(k)),\\frac{\\W(k)}{\\tf{\\W(k)}} \\right\\rangle + \\eta^2 \\frac{\\|\\nabla \\mc{L}(\\W(k))\\|_F^2}{\\tf{\\W(k)}}\\\\\n& \\leq 1- \\frac{\\eta}{(1-\\epsilon)\\tf{\\W(k)}}  \\iprod{\\nabla\\mc{L}(\\W(k))}\n     {\\frac{\\Wm}{\\tf{\\Wm}}}+ \\eta^2 \\frac{\\|\\nabla \\mc{L}(\\W(k))\\|_F^2}{\\tf{\\W(k)}}\\\\\n      & \\leq  1 + \\frac{\\eta \\rho(k)}{\\tf{\\W(k)}} + \\frac{\\eta^2\\|\\nabla \\mc{L}(\\W(k))\\|_F^2}{\\tf{\\W(k)}}=:C_1(\\rho(k),\\eta).\n\\end{split}\n\\end{equation}\nHere, the second inequality uses \\eqref{eqn:neg:corr:0}. \n\nNow, it follows from \\eqref{eqn:localgd:1:nabla0} and \\eqref{eqn:localgd:2:nabla0} that \n\\begin{equation}\\label{eqn:localgd:3:nabla0}\n\\begin{split}\n\\min_{t\\neq \\op_i,~i\\in[n]} ~~  \\left\\langle  (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top, \\frac{\\W(k+1)}{\\tf{\\W(k+1)}}\\right\\rangle   &\\geq \\frac{1}{C_1({\\rho}(k),\\eta)} \\left(\\mu \\Theta+\\frac{2\\eta (1-\\epsilon)\\mu \\Theta  {\\rho}(k)}{\\tf{\\W(k)}}\\right)\\\\\n& = \\mu \\Theta+\\frac{\\eta\\mu \\Theta}{C_1({\\rho}(k),\\eta)} \\left(\\frac{ \\big(2(1-\\epsilon) -1 \\big)  \\rho(k)}{\\tf{\\W(k)}}\n-   \\eta \\frac{\\tf{\\nabla \\mc{L}(\\W(k))}^2 }{\\tf{\\W(k)}}\\right)\\\\\n& = \\mu \\Theta+\\frac{\\eta\\mu \\Theta}{C_1({\\rho}(k),\\eta)} \\left(\\frac{ (1-2\\epsilon)   \\rho(k)}{\\tf{\\W(k)}}\n-   \\eta  \\frac{\\tf{\\nabla \\mc{L}(\\W(k))}^2 }{\\tf{\\W(k)}}\\right)\\\\\n\n\n& \\geq \\mu \\Theta,\n\\end{split}\n\\end{equation}\n where the last inequality uses our choice of stepsize $\\eta\\leq 1/L_W$ in Theorem~\\ref{conv:gd:w:global:nabla0}'s statement. Specifically, we need $\\eta$ to be small to ensure the last inequality. We will guarantee this by choosing a proper $R_\\eps$ in Lemma \\ref{lem:glocal:corr}. Specifically, Lemma \\ref{lem:glocal:corr} leaves the choice of $D_0$ in $R_\\eps$ lower bound of \\eqref{Rpi choice2} open (it can always be chosen larger). Here, by choosing $D_0\\gtrsim 1/L_{\\W}$ will ensure $\\eta\\leq 1/L_{\\W}$ works well.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{equation}\\label{eqn:zeta:mu:0}\n\\begin{split}\n    \\eta &\\leq    \\big( 1-\\mu\\big)\\mu\n \\frac{c}{C}  \\frac{\\Theta}{\\bar{A}}   \\frac{1}{\\bar{A}C T}    e^{R_\\mu^0\\Theta/2}\\\\\n \n \n &\\leq      \\frac{1-2\\epsilon  }{1-\\epsilon}   \\frac{c \\mu}{C}   \\frac{\\Theta}{\\bar{A}}    \\frac{1}{\\bar{A}C T}  e^{R_\\mu^0\\Theta/2} \\\\\n &\\leq   \\big(1-2\\epsilon \\big)   \\frac{\\rho(k) } { \\|\\nabla \\mc{L}(\\W(k))\\|^2_F}.\n\\end{split}    \n\\end{equation}\n\n\n\n\n\n\n\n\n\nHere, the first inequality follows since $\\epsilon \\in (0, \\mu/2)$ (as seen in \\textbf{Step 2}). Also,  $\\mu < 1$ implies that $1-\\mu > 0$, we obtain $\\eta > 0$. The last inequality is obtained from Lemma~\\ref{glocal cond}:\n\\begin{align*}\n    \\frac{\\rho(k) } { \\tf{\\nabla \\mc{L}(\\W(k))}} &= - \\frac{1}{1-\\epsilon} \\iprod{ \\frac{\\nabla\\mc{L}(\\W(k))}{\\tf{\\nabla \\mc{L}(\\W(k))}}}\n     {\\frac{\\Wm}{\\tf{\\Wm}}}  \\geq \\frac{1}{1-\\epsilon} \\cdot \\frac{c \\mu}{C} \\cdot \\frac{\\Theta}{\\bar{A}},\\\\\n         \\frac{1} { \\tf{\\nabla \\mc{L}(\\W(k))}} &{\\geq \\frac{1}{\\bar{A}C \\cdot \\frac{1}{n} \\sum_{i=1}^n  \\left(1-\\s_{i\\op_i}\\right)} \\geq     \\frac{1}{ \\bar{A} C T e^{-R_\\mu^{0}\\Theta/2}} }\n\\end{align*}\nfor some data dependent constrants $c$, $C$, $\\bar{A}=\\max_{i\\in[n],t,\\tau\\in[T]}\\tn{(\\x_{it}- \\x_{i\\tau})}~\\tn{\\z_i}$, and $\\Theta=1/\\tf{\\Ws}$.\n\nThe remainder of the proof of this step is identical to \\eqref{eqn:pitoC0}--\\eqref{eqn:pitoC02}, with the replacement of $C_0$ by $D_0$ and the tracking of changes. Specifically, Lemma \\ref{lem:glocal:corr} leaves the choice of $D_0$ in $R_\\eps$ lower bound of \\eqref{Rpi choice2} open (it can always be chosen larger). Hence,  for sufficiently large $D_0$, we have\n\\begin{align}\n\\eta \\leq \\frac{1}{L_{\\W}}\\leq   \\big( 1-\\mu\\big)\\mu\n \\frac{c}{C}  \\frac{\\Theta}{\\bar{A}}   \\frac{1}{\\bar{A}C T}    e^{R_\\mu^0\\Theta/2}.\n\\end{align}\n\n\nThis implies \\eqref{eqn:localgd:3:nabla0} and  $\\W(k+1) \\in\\conb_{\\mu,R_\\eps}(\\Ws)$. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\noindent\\textbf{Step 4 (Proof of \\ref{lem:zglob:l2}): $\\W(k)$ and $\\Wm$ perfectly align over time.} \nBy theorem statement (alternatively via \\textbf{Step 3}), we have that all iterates remain within the initial conic set i.e.~$\\W(k)\\in\\conb_{\\mu,R^0_\\mu}(\\Ws)$ for all $k\\geq 0$. Note that it follows from Lemma~\\ref{glocal cond}  that  $\\li\\nabla\\Lc(\\W), \\Ws/\\tf{\\Ws}\\ri<0$, for any finite $\\W \\in \\conb_{\\mu,R^0_\\mu}(\\Ws)$. Hence, there are no finite critical points $\\W \\in \\conb_{\\mu,R^0_\\mu}(\\Ws)$, for which $\\nabla \\mc{L} (\\W)=0$. Now, based on Lemma~\\ref{lem:grad:descent}, which guarantees that $\\nabla\\Lc(\\W(k))\\rightarrow 0$, this\nimplies that $\\left\\Vert \\W\\left(k\\right)\\right\\Vert \\rightarrow\\infty$. Consequently, for any choice of $\\eps\\in (0,\\mu/2)$ there is an iteration $k_\\eps$ such that, for all $k\\geq k_\\eps$, $\\W(k)\\in\\conb_{\\mu,R_\\eps}(\\Ws)$. Once within $\\conb_{\\mu,R_\\eps}(\\Ws)$,  multiplying both sides of \\eqref{eqn:neg:corr:0} by the stepsize $\\eta$ and using the gradient descent update, we get\n\\begin{equation*}\n\\begin{split}\n     \\left\\langle \\W(k+1)-\\W(k),\\frac{ \\Wm}{\\tf{\\Wm}} \\right\\rangle &\\geq  (1-\\epsilon) \\left\\langle \\W(k+1)-\\W(k), \\frac{\\W(k)}{\\tf{\\W(k)}}\\right\\rangle\\\\\n     &= \\frac{(1-\\epsilon)}{2\\tf{\\W(k)}}\\left(\\tf{\\W(k+1)}^2- \\tf{\\W(k)}^2-\\tf{\\W(k+1)-\\W(k)}^2\\right) \\\\\n     & \\geq (1-\\epsilon)\\left( \\frac{1}{2\\tf{\\W(k)}} \\left(\\tf{\\W(k+1)}^2- \\tf{\\W(k)}^2\\right)-\\tf{\\W(k+1)-\\W(k)}^2\\right) \\\\\n     & \\geq (1-\\epsilon)\\left(\\tf{\\W(k+1)}- \\tf{\\W(k)}-\\tf{\\W(k+1)-\\W(k)}^2\\right) \\\\\n          & \\geq (1-\\epsilon)\\Big(\\tf{\\W(k+1)}- \\tf{\\W(k)}- 2\\eta  \\left(\\mc{L}(\\W(k))-\\mc{L}(\\W(k+1))\\right) \\Big).\n\\end{split}\n\\end{equation*}\n\nHere, the second inequality is obtained from  $\\tf{\\W(k)}\\geq 1/2$; the third inequality follows since  for any $a, b >0$, we have $  (a^2-b^2)/(2b) -  (a-b) \\geq 0$; and the last inequality  uses Lemma~\\ref{lem:grad:descent}.\n\n\n\nSumming the above inequality over $k\\geq k_\\eps$ gives \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{align*}\n      \\left\\langle\\frac{\\W(k)}{\\tf{\\W(k)}}, \\frac{\\Wm}{\\tf{\\Wm}} \\right\\rangle \\ge1-\\epsilon+ \\frac{C(\\epsilon,\\eta)}{\\tf{\\W(k)}}, \\qquad \\W(k)\\in\\conb_{\\mu,R_\\eps}(\\Ws),   \n\\end{align*}\nwhere $\\mathcal{L}_{\\star}\\leq\\mathcal{L}\\left(\\W\\left(k\\right)\\right)$ for all $k\\geq k_\\eps$, and \n\\begin{equation*}\nC(\\epsilon,\\eta)= \\left\\langle \\W(k_\\eps), \\frac{ \\Wm}{\\tf{\\Wm}}\\right\\rangle-(1-\\epsilon)\\tf{\\W(k_\\eps)} -2\\eta (1-\\epsilon) (\\mc{L}(\\W(k_\\eps))-\\mathcal{L}_{\\star}).\n\\end{equation*}\nConsequently,\n    \\begin{align*}\n      \\liminf_{k\\to\\infty}\\iprod{\\frac{\\W(k)}{\\tf{\\W(k)}}}{\\frac{\\Wm}{\\tf{\\Wm}}}\\ge1-\\epsilon, \\qquad \\W(k)\\in\\conb_{\\mu,R_\\eps}(\\Ws).  \n    \\end{align*}\nSince  $\\epsilon \\in (0, \\mu/2)$  is arbitrary, this implies $\\W(k)/\\tf{\\W(k)}\\to  \\Wm/\\tf{\\Wm}$.\n\\end{proof}",
        "metadata": {
            "Statement label": "Theorem 30",
            "Statement title": "Global Convergence of Gradient Descent with Initial Gradient Assumption"
        }
    },
    {
        "text": "\\begin{theorem}\\label{diverg:norm:qk}\nSuppose Assumption~\\ref{assum:loss:prope} on the loss function $\\ell$ and Assumption \\ref{assum:token} on the tokens hold. Assume the initialization $(\\Kb(0), \\Qb(0))$ satisfies $\\nabla \\Lc (\\Kb(0),\\Qb(0)) \\neq 0$. Let $\\eta_k=\\min\\{1/L(R_k),1\\}$, where $R_k$ is chosen such that $(\\Kb(k),\\Qb(k))\\in \\mc{S}(R_k-1)$, and if $(\\Kb(k+1),\\Qb(k+1))\\in \\mc{S} (R_k-1)$, then $R_{k+1}=R_k$.  Then, the following statements hold:\n\\begin{itemize}\n\\item \nThere is no $\\Kb,\\Qb\\in\\R^{d\\times m}$ satisfying $\\nabla \\Lc(\\Kb,\\Qb)=0$.\n\\item Algorithm~\\ref{GD-QK} with the step size $\\eta_k$  satisfies  $\\lim_{k \\rightarrow \\infty} \\tf{\\nabla \\Lc_\\Kb(\\Kb(k), \\Qb(k))}\\vee\\tf{\\nabla\\Lc_\\Qb(\\Kb(k),\\Qb(k))}=0$, and  $\\lim_{k\\rightarrow\\infty} \\tf{\\Kb(k)}\\wedge\\tf{\\Qb(k)}=\\infty$.\n\\end{itemize}\n\\end{theorem}\nProof:\n\\begin{proof}\nSince $(\\Kb(k),\\Qb(k))\\in \\mc{S}(R_k)$, \nwe have \n    \\begin{align}\n        \\|\\Qb(k+1)\\|_F \\le\\|\\Qb(k)\\|_F+\\eta_k\\tf{ \\nabla_{\\Qb} \\Lc(\\Kb(k),\\Qb(k))} \\nonumber    & \\le\\|\\Qb(k)\\|_F+\\frac{1}{L(R_k)}\\tf{\\nabla_{\\Qb} \\Lc(\\Kb(k),\\Qb(k))} \\nonumber \\\\\n\n         & \\le\\|\\Qb(k)\\|_F+1. \\label{eq:gd_inc}\n    \\end{align}\n\n    \nSince $R_k\\to\\infty$ by Lemma~\\ref{lem:out:S} and $R_{k+1}=R_k$ as long as $ (\\Kb(k+1),\\Qb(k+1))\\in \\mc{S}(R_k-1)$, we obtain\n\\begin{equation}\\label{eqn:c1}\n \\max\\{ \\|\\Qb(k)\\|_F,  \\|\\Kb(k)\\|_F \\} \\quad    \\textnormal{is unbounded}. \\tag{C1}\n\\end{equation}\n\n    \nIt then follows that for any $k$, by Cauchy-Schwarz,\n\\begin{align*}\n\\left(\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}\\right) \\left(\\sum_{\\tau=0}^{k-1}\\eta_{\\tau} \\|\\nabla \\Lc(\\Kb(\\tau),\\Qb(\\tau))\\|^2_F\\right) & \n\\ge \\left(\\sum_{\\tau=0}^{k-1}\\eta_{\\tau} \\|\\nabla \\Lc(\\Kb(\\tau),\\Qb(\\tau))\\|^2_F\\right)^2\\to\\infty.\n\\end{align*}\nSince by  \\eqref{eq:descent:obj},\n\\begin{align*}\n        \\sum_{\\tau=0}^{k-1}\\eta_{\\tau} \\left\\|\\nabla \\Lc(\\Kb(\\tau),\\Qb(\\tau))\\right\\|^2_F\\le 2\\Lc(\\Kb(0),\\Qb(0))-2\\Lc(\\Kb(k),\\Qb(k) )\\le2 \\Lc(\\Kb(0),\\Qb(0)),\n\\end{align*}\nwe have $\\sum_{t=0}^{\\infty}\\eta_k=\\infty$.\n\nSince gradient descent never increases the risk,  for $(\\Kb(k),\\Qb(k))\\in \\mc{S}(R)$, $\\|\\partial\\Lc/\\partial \\Qb(k)\\|_F\\ge\\epsilon(R)$ for some constant $\\epsilon(R)>0$.  Following similar steps  as the proof of \\eqref{eqn:grad:low:eps},   we get that $\\sum_{k:(\\Kb(k),\\Qb(k))\\in \\mc{S}(R)}^{}\\eta_k<\\infty$.  \n\n\nFor simplicity, we  replace $\\Qb^\\top$ with $\\Qb$ in \\eqref{eqn:erm:kq}. \n\n\n\n For gradient descent iterates \\ref{GD-QK}, summing from $0$ to $k-1$, we get\n\\begin{align}\\label{eqn:qk:recur}\n& \\quad\\Qb(k)^\\top\\Qb(k)-\\Qb(0)^\\top\\Qb(0)+\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\nabla_{\\Qb} \\Lc(\\Kb(\\tau),\\Qb(\\tau))^\\top \\nabla_{\\Qb} \\Lc(\\Kb(\\tau),\\Qb(\\tau)) \\nonumber \\\\\n= & \\quad \\Kb(k)\\Kb^{\\top}(k)-\\Kb(0)\\Kb^{\\top}(0)+\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2  \\nabla_{\\Kb} \\Lc(\\Kb(\\tau),\\Qb(\\tau)) \\nabla_{\\Kb} \\Lc(\\Kb(\\tau),\\Qb(\\tau))^\\top.\n\\end{align}\nLet\n\\begin{subequations}\n    \\begin{align*}\n       P_{\\Qb}&=\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\nabla_{\\Qb} \\Lc(\\Kb,\\Qb) \\nabla_{\\Qb} \\Lc(\\Kb(\\tau), \\Qb (\\tau)))^\\top,\\\\\n   P_{\\Kb}&:=\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\nabla_{\\Kb} \\Lc(\\Kb,\\Qb)  \\nabla_{\\Qb} \\Lc(\\Kb(\\tau), \\Kb (\\tau)))^\\top,\n    \\end{align*}\n    and\n\\begin{align*}\nS_{\\Qb}(k)=\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\nabla_{\\Qb} \\Lc(\\Kb(\\tau), \\Qb (\\tau)))^\\top \\nabla_{\\Qb} \\Lc(\\Kb(\\tau), \\Qb (\\tau))),\\\\\nS_{\\Kb}(k)=\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\nabla_{\\Kb} \\Lc(\\Kb(\\tau), \\Qb (\\tau)))^\\top \\nabla_{\\Kb} \\Lc(\\Kb(\\tau), \\Qb (\\tau))) .\n    \\end{align*}\n\\end{subequations}\nWe obtain\n\\begin{subequations}\n    \\begin{align}\\label{eq:qk:tr:b1}\n           \\tr(P_{\\Kb}(k))+ \\tr(P_{\\Qb}(k))  & =\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\|\\nabla \\Lc(\\Kb(\\tau), \\Qb (\\tau))\\|^2_F \\nonumber \\\\\n         & \\le \\sum_{\\tau=0}^{k-1}\\eta_{\\tau} \\|\\nabla \\Lc(\\Kb(\\tau), \\Qb (\\tau))\\|^2_F  \\nonumber \\\\\n         &  \\leq 2 \\Lc(\\W(0)) -2 \\Lc(\\W(k))  \\nonumber\\\\\n            &  \\leq 2 \\Lc(\\W(0)) -2 \\Lc_\\star.\n    \\end{align}\nIt follows from \\eqref{eqn:qk:recur} that\n\\begin{align}\\label{eq:qk:tr:b2}\n\\|\\Kb(k)\\|_F^2=\\|\\Qb(k)\\|_F^2+\\|\\Kb(0)\\|_F^2-\\|\\Qb(0)\\|_F^2-\\tr(P_{\\Kb}(k))+\\tr(S_{\\Qb}(k)).\n\\end{align}    \n\\end{subequations}\nIn other words, the difference between the squares of Frobenius norms of $(\\Kb,\\Qb)$ is still bounded.\n\n\nNow, combining \\eqref{eq:qk:tr:b1} and \\eqref{eq:qk:tr:b2}, we get\n \\begin{align*}\n   2 \\Lc(\\W(0))\n           \\ge\\sum_{t=0}^{\\infty} \\eta_k \\left\\|\\nabla_{\\Qb} \\Lc(\\Kb (k),\\Qb(k))\\right\\|_F^2 \n          \n          \n           \\geq \\infty,\n     \\end{align*}\n     which is a contradiction. This implies $\\tf{\\Kb(k)}\\to\\infty$, since $\\Lc(\\Kb,\\Qb)$ has no finite optimum.\n\\end{proof}",
        "metadata": {
            "Statement label": "Theorem 31",
            "Statement title": "Divergence of Norms in Gradient Descent"
        }
    },
    {
        "text": "\\begin{corollary}[Global Convergence of Regularization Path]\\label{cor gm} Suppose Assumptions \\ref{assum:loss:prope}\\&\\ref{ass cvx seq} hold and the optimal indices $\\op_\\ik=\\arg\\max_{t\\in[T]}\\bgam_\\ikt$ are unique. Consider the global regularization path $\\Wb_{\\dm,R}=\\min_{\\W\\in\\Rcm,\\td{\\W}\\leq R}\\Lc(\\W)$. Let $\\Wcb^\\svm_\\dm$ be the non-empty solution set of \\eqref{seqattnsvm} with $\\bal\\gets\\op$ normalized to have unit $\\dm$-norm. Then\n\\[\n\\lim_{R\\rightarrow\\infty}\\dist{\\frac{\\Wb_{\\dm,R}}{R},\\Wcb^\\svm_\\dm}\n\\]\n\\end{corollary}",
        "metadata": {
            "Statement label": "Corollary 2",
            "Statement title": "Global Convergence of Regularization Path"
        }
    },
    {
        "text": "\\begin{corollary}\\label{cor global reg path} Suppose Assumptions \\ref{assum:loss:prope}\\&\\ref{ass cvx seq} hold and the optimal indices $\\op_\\ik=\\arg\\max_{t\\in[T]}\\bgam_\\ikt$ are unique. Consider the regularization paths associated to \\eqref{serm-w} and \\eqref{serm-kq}:\n\\begin{align}\n&\\Wb_R=\\underset{\\W\\in\\Rcm,\\tf{\\W}\\leq R}{\\arg\\min}\\Lc(\\W)\\quad\\text{and}\\quad \\Kbb_R,\\Qbb_R=\\underset{\\tf{\\Kb}^2+\\tf{\\Qb}^2\\leq 2R}{\\arg\\min}\\Lc(\\Kb,\\Qb)\n\\end{align}\nSuppose \\eqref{seqattnsvm} is feasible for $\\bal\\gets\\op$. Let $\\Ws$ be the unique solution of \\eqref{seqattnsvm} with Frobenius norm and $\\Wc^{\\svm}_\\star$ be the solution set of \\eqref{seqattnsvm} with nuclear norm and cost function $\\tnuc{\\Wc^{\\svm}_\\star}$. We have that\n\n\\[\n\\lim_{R\\rightarrow\\infty} \\frac{\\Wb_R}{R}=\\frac{\\Ws}{\\tf{\\Ws}},\\quad\\lim_{R\\rightarrow\\infty} \\dist{\\frac{\\Qbb_R\\Kbb_R^\\top}{R},\\frac{\\Wcs_\\star}{\\tnuc{\\Wc^{\\svm}_\\star}}}=0.\n\\]\n\\end{corollary}\nProof:\n\\begin{proof} We directly apply Corollary \\ref{cor gm} with $\\dm=F$ and $\\dm=\\star$ respectively. To obtain the result on $\\Wb_R$, we note that $\\Ws$ is unique because Frobenius norm-squared is strongly convex. To obtain the result on $(\\Qbb_R,\\Kbb_R)$, we use Lemma \\ref{kqw mapping} and observe that\n\\[\n\\Wb_{\\st,R}:=\\Qbb_R\\Kbb_R^\\top\\in\\underset{\\W\\in\\Rcm,\\tnuc{\\W}\\leq R}{\\arg\\min}\\Lc(\\W).\n\\]\nWe then apply Corollary \\ref{cor gm} with $\\dm=\\star$ to conclude with the convergence of the path $ \\Wb_{\\st,R}$.\n\\end{proof}",
        "metadata": {
            "Statement label": "Corollary 3",
            "Statement title": "Asymptotic Regularization Path Convergence"
        }
    }
]
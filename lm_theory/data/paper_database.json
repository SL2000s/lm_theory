{
    "db_path": "/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory/lm_theory/data/paper_database.json",
    "papers": [
        {
            "paper_id": "895bbef7-305d-4454-850a-7279bb832d9d",
            "title": "",
            "authors": [],
            "year": null,
            "source_url": "https://arxiv.org/abs/2006.04710",
            "html_url": "library/papers/895bbef7-305d-4454-850a-7279bb832d9d/index.html",
            "bibtex": "",
            "original_tex": "==== BEGINNING OF /2006.04710/main.tex ====\n\\documentclass{article}\n\\usepackage{times}\n\\usepackage[utf8]{inputenc} % allow utf-8 input\n\\usepackage[T1]{fontenc}    % use 8-bit T1 fonts\n\\usepackage{hyperref}       % hyperlinks\n\\usepackage{url}            % simple URL typesetting\n\\usepackage{booktabs}       % professional-quality tables\n\\usepackage{amsfonts}       % blackboard math symbols\n\\usepackage{nicefrac}       % compact symbols for 1/2, etc.\n\\usepackage{microtype}      % microtypography\n\\usepackage{graphicx}\n\\usepackage{amsmath}\n\\usepackage{mathtools} % for \\coloneqq\n\\usepackage{amsthm}\n\\usepackage{verbatim}\n\\usepackage{dsfont}  % for \\mathds\n\\usepackage{wrapfig}  % for \\begin{wrapfigure}\n\\usepackage{caption}\n\\usepackage{subcaption}\n\\usepackage{xcolor}\n\\usepackage{amsbsy}\n\\usepackage{paralist}\n\\usepackage{enumitem}\n\n\\newtheorem{theorem}{Theorem}[section]\n\\newtheorem{lemma}{Lemma}[section]\n\\theoremstyle{definition}\n\\newtheorem{definition}{Definition}[section]\n\\newtheorem{corollary}{Corollary}[section]\n\n\\DeclareMathOperator{\\Tr}{Tr}\n\\newcommand{\\spaces}{\\hspace{2mm}}\n\\DeclareMathOperator{\\softmaxOp}{softmax}\n\\newcommand{\\softmax}[1]{\\softmaxOp\\left(#1\\right)}\n\\DeclareMathOperator{\\lip}{Lip}\n\\DeclareMathAlphabet{\\pazocal}{OMS}{zplm}{m}{n}\n\\newcommand{\\unif}{\\pazocal{U}}\n\\DeclareMathOperator{\\diag}{diag}\n\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n\n\\newcommand{\\andriy}[1]{\\textcolor{blue}{[AM: #1]}}\n\\newcommand{\\hyunjik}[1]{\\textcolor{red}{[HK: #1]}}\n\\newcommand{\\george}[1]{\\textcolor{magenta}{[George: #1]}}\n\n% Use the following line for the initial blind version submitted for review:\n\\usepackage[accepted]{icml2021}\n\n% If accepted, instead use the following line for the camera-ready submission:\n%\\usepackage[accepted]{icml2021}\n\n% The \\icmltitle you define below is probably too long as a header.\n% Therefore, a short form for the running title is supplied here:\n\\icmltitlerunning{The Lipschitz Constant of Self-Attention}\n\n\\begin{document}\n\n\\twocolumn[\n\\icmltitle{The Lipschitz Constant of Self-Attention}\n\n% It is OKAY to include author information, even for blind\n% submissions: the style file will automatically remove it for you\n% unless you've provided the [accepted] option to the icml2021\n% package.\n\n% List of affiliations: The first argument should be a (short)\n% identifier you will use later to specify author affiliations\n% Academic affiliations should list Department, University, City, Region, Country\n% Industry affiliations should list Company, City, Region, Country\n\n% You can specify symbols, otherwise they are numbered in order.\n% Ideally, you should not use this facility. Affiliations will be numbered\n% in order of appearance and this is the preferred way.\n\\icmlsetsymbol{equal}{*}\n\n\\begin{icmlauthorlist}\n\\icmlauthor{Hyunjik Kim}{dm}\n\\icmlauthor{George Papamakarios}{dm}\n\\icmlauthor{Andriy Mnih}{dm}\n\\end{icmlauthorlist}\n\n\\icmlaffiliation{dm}{DeepMind, UK}\n\n\\icmlcorrespondingauthor{Hyunjik Kim}{hyunjikk@google.com}\n\n% You may provide any keywords that you\n% find helpful for describing your paper; these are used to populate\n% the \"keywords\" metadata in the PDF but will not be shown in the document\n\\icmlkeywords{Machine Learning, ICML}\n\n\\vskip 0.3in\n]\n\n% this must go after the closing bracket ] following \\twocolumn[ ...\n\n% This command actually creates the footnote in the first column\n% listing the affiliations and the copyright notice.\n% The command takes one argument, which is text to display at the start of the footnote.\n% The \\icmlEqualContribution command is standard text for equal contribution.\n% Remove it (just {}) if you do not need this facility.\n\n%\\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution\n\\printAffiliationsAndNotice{} % otherwise use the standard text.\n\n\n\\begin{abstract}\nLipschitz constants of neural networks have been explored in various contexts in deep learning, such as provable adversarial robustness, estimating Wasserstein distance, stabilising training of GANs, and formulating invertible neural networks.\nSuch works have focused on bounding the Lipschitz constant of fully connected\nor convolutional networks, composed of linear maps and pointwise non-linearities. \nIn this paper, we investigate the Lipschitz constant of\nself-attention, a non-linear neural network module widely used in sequence modelling.\nWe prove that the standard dot-product self-attention is \\emph{not} Lipschitz for unbounded input domain, and propose an alternative L2 self-attention that \\emph{is} Lipschitz. \nWe derive an upper bound on the Lipschitz constant of L2 self-attention and provide empirical evidence for its asymptotic tightness. \nTo demonstrate the practical relevance of our theoretical work, we formulate invertible self-attention and use it in a Transformer-based architecture for a character-level language modelling task.\n\\end{abstract}\n\n\\section{Introduction}\n\\label{sec:intro}\nLipschitz continuity is a strong form of continuity for functions. \nLoosely speaking, a function is \\textit{Lipschitz continuous} if changing its input by a certain amount cannot change its output by more than $K$ times that amount. \nThe constant $K$ is a hard constraint on how rapidly the function's output can vary, and the smallest such $K$ is known as the function's \\textit{Lipschitz constant}.\nFor example, \\smash{$f_1(x) = \\sqrt{|x|}$} and $f_2(x) = \\exp(x)$ for $x\\in\\mathbb{R}$ are not Lipschitz continuous, because their output can change arbitrarily fast as $x$ approaches $0$ and $+\\infty$ respectively. \nOn the other hand, $g_1(x) = \\tanh(x)$ and $g_2(x) = \\alpha x$ are Lipschitz continuous, because their rate of change (derivative) is bounded.\n\nIn deep learning, we often use Lipschitz continuity as a constraint for neural networks, to control how much a network's output can change relative to its input. \nSuch Lipschitz constraints are useful in several contexts. \nFor example, Lipschitz constraints can endow models with provable robustness against adversarial pertubations \\citep{cisse2017parseval, tsuzuku2018lipschitz, anil2019sorting}, and guaranteed generalisation bounds \\citep{sokolic2017robust}.\nMoreover, the dual form of the Wasserstein distance is defined as a supremum over Lipschitz functions with a given Lipschitz constant, hence Lipschitz-constrained networks are used for estimating Wasserstein distances \\citep{peyre2019computational}. \nFurther, Lipschitz-constrained networks can stabilise training for GANs, an example being spectral normalisation \\citep{miyato2018spectral}.\nFinally, Lipschitz-constrained networks are also used to construct invertible models and normalising flows. \nFor example, Lipschitz-constrained networks can be used as a building block for invertible residual networks and hence flow-based generative models \\citep{behrmann2018invertible, chen2019residual}.\nAdditionally, Neural ODEs \\citep{chen2018neural, grathwohl2018ffjord} are typically defined using vector fields parameterized via Lipschitz networks, so that the flow generated by the vector field is guaranteed to exist for all times.\n\nNonetheless, designing Lipschitz-continuous neural networks and computing (or even upper-bounding) their Lipschitz constant is a hard problem. \nPrevious work mostly focused on fully-connected and convolutional networks, not only because they are common in deep learning, but also because they are relatively simple to analyze, as compositions of linear maps and pointwise non-linearities. \nEven in this case however, exact evaluation of the Lipschitz constant\nof fully-connected and convolutional networks is NP-hard \\citep{virmaux2018lipschitz} and obtaining a tight upper bound remains a challenging task \\citep{virmaux2018lipschitz, fazlyab2019efficient, Latorre2020Lipschitz}.\n\nFully-connected and convolutional networks are not the only neural networks worthy of interest.\nRecently, \\textit{self-attention} \\citep{vaswani2017attention} has become a popular alternative to recurrent neural networks. Self-attention is a key component of the Transformer \\citep{vaswani2017attention}, that has found success as a building block in models of various data modalities, starting with natural-language processing \\citep{vaswani2017attention, devlin2018bert, brown2020language} and extending to computer vision \\mbox{\\citep{zhang2018self, ramachandran2019stand}}, audio generation \\citep{huang2018music}, and reinforcement learning \\citep{parisotto2019stabilizing}. However, so far no previous work has analysed the Lipschitz properties of self-attention, and thus it has been unclear whether self-attention is a viable option in applications that require Lipschitz constraints.\nIn this work, we address this gap in the theory of self-attention by providing a thorough analysis of its Lipschitz properties. In particular, we make the following contributions: \n\\begin{itemize}[leftmargin=*]\n    \\item We prove that the widely used \\textit{dot-product self-attention} is \\emph{not} Lipschitz, and therefore not suitable to use in applications requiring Lipschitz constraints.\n    \\item We formulate \\textit{L2 self-attention} as an alternative, and show that it \\emph{is} Lipschitz.\n    \\item We derive a theoretical upper bound on the Lipschitz constant of L2 self-attention, and provide empirical evidence of the asymptotic tightness of the bound.\n    \\item As a practical demonstration of the theory, we use this bound to formulate invertible self-attention, and explore its use in a Transformer architecture for character-level language modelling. We compare its test log-likelihood and stability to dot-product self-attention.\n\\end{itemize}\n\n\\section{Lipschitz Constant of Fully-Connected/Convolutional Layers}\nWe first define the notion of Lipschitz continuity, and proceed to define the Lipschitz constant.\n\\begin{definition}\\label{Lipschitz_definition}\nGiven two metric spaces $(\\mathcal{X}, d_{\\mathcal{X}})$ and $(\\mathcal{Y}, d_{\\mathcal{Y}})$, a function $f:\\mathcal{X} \\rightarrow \\mathcal{Y}$ is called \\textit{Lipschitz continuous} (or $K$-\\textit{Lipschitz}) if there exists a constant $K\\geq 0$ such that \n\\begin{equation}\nd_{\\mathcal{Y}}(f(\\mathbf{x}),f(\\mathbf{x'})) \\leq K d_{\\mathcal{X}}(\\mathbf{x},\\mathbf{x'}) \\spaces\\spaces \\text{for all } \\mathbf{x},\\mathbf{x'} \\in \\mathcal{X}.\n\\end{equation}\nThe smallest such $K$ is the \\textit{Lipschitz constant} of $f$, denoted $\\lip(f)$.\n\\end{definition}\nIn this paper, we focus on the common case where $\\mathcal{X} = \\mathbb{R}^n$, $\\mathcal{Y} = \\mathbb{R}^m$, and $d_{\\mathcal{X}}, d_{\\mathcal{Y}}$ are induced by a $p$-norm \\smash{$\\|\\mathbf{x}\\|_p \\coloneqq (\\sum_{i} |x_i|^p)^{{1}/{p}}$}. \nWe will primarily consider the cases $p=2$ and $p=\\infty$, where $\\|\\mathbf{x}\\|_{\\infty} \\coloneqq \\max_i |x_i|$.\nTo emphasise the dependence of the Lipschitz constant on the choice of $p$-norm, we will often denote it by $\\lip_p(f)$. \nIn this case, it follows directly from Definition \\ref{Lipschitz_definition} that the Lipschitz constant is given by\n\\begin{equation}\\label{eq:lipschitz_supremum_definition}\n    \\lip_p(f) = \\sup_{\\mathbf{x}\\neq \\mathbf{x'} \\in \\mathbb{R}^n} \\frac{\\|f(\\mathbf{x})-f(\\mathbf{x'})\\|_p}{\\|\\mathbf{x}-\\mathbf{x'}\\|_p}.\n\\end{equation}\nNext, we outline some basic results that are useful for estimating Lipschitz constants, also covered in related works \\citep{virmaux2018lipschitz, behrmann2018invertible}. \nWe describe how these results are used to provide bounds on the Lipschitz constant of fully-connected networks (\\verb!FCN!) and convolutional neural networks (\\verb!CNN!), using the fact that both are compositions of linear maps and pointwise non-linearities.\nTo begin with, the following theorem suggests a way to bound $\\lip_p(f)$ for a differentiable Lipschitz function $f$:\n\\begin{theorem}[\\citealp{federer1969geometric}] \\label{thm:jacobian} Let $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ be differentiable and Lipschitz continuous under a choice of $p$-norm $\\|\\cdot\\|_p$. \nLet $J_f(x)$ denote its total derivative (Jacobian) at $x$. Then $\\lip_p(f) = \\sup_{\\mathbf{x}\\in \\mathbb{R}^n} \\|J_f(\\mathbf{x})\\|_p$ where $\\|J_f(\\mathbf{x})\\|_p$ is the induced operator norm on $J_f(\\mathbf{x})$. \n\\end{theorem}\nHence if $f$ is a linear map represented by a matrix $W$ then\n\\begin{align*}\n    \\lip_p(f)&= \\|W\\|_p \\coloneqq \\sup_{\\|\\mathbf{x}\\|_p=1} \\|W\\mathbf{x}\\|_p \\\\\n    &=\n    \\begin{cases}\n        \\sigma_{\\max}(W), & \\text{if } p=2\\\\\n        \\max_i \\sum_j |W_{ij}|      & \\text{if } p = \\infty\n    \\end{cases}\n\\end{align*}\nwhere $\\|W\\|_p$ is the operator norm on matrices induced by the vector $p$-norm, and $\\sigma_{\\max}(W)$ is the largest singular value of $W$. \nUnder this choice of norm, many common non-linearities (including \\verb!relu!, \\verb!sigmoid!, \\verb!tanh!, \\verb!elu!) are $1$-Lipschitz. \n$\\|W\\|_2= \\sigma_{\\text{max}}(W)$ is usually estimated via \\textit{power iteration}; we provide details on how this is done in Appendix \\ref{apd:power_iteration}.\n\nSince we now know the Lipschitz constants of the components of both \\verb!FCN! and \\verb!CNN!, we can bound their Lipschitz constants by applying the following lemma:\n\\begin{lemma}[\\citealp{federer1969geometric}]\nLet $g,h$ be two composable Lipschitz functions. Then $g \\circ h$ is also Lipschitz with $\\lip(g \\circ h) \\leq \\lip(g) \\lip(h)$.\n\\end{lemma}\n\\begin{corollary} \\label{cor:lip_conv}\nFor a fully-connected network (\\verb!FCN!) or a convolutional neural network (\\verb!CNN!) $f=W_K \\circ \\rho_{K-1} \\circ W_{K-1} \\circ \\ldots \\circ \\rho_1 \\circ W_1$, we have $\\lip_p(f) \\leq \\prod_k \\|W_k\\|_p$ under a choice of $p$-norm with $1$-Lipschitz non-linearities $\\rho_k$.\n\\end{corollary}\nThe above bound is not necessarily tight; there are various works that compute tighter bounds for \\verb!FCN! and \\verb!CNN! \\citep[e.g.][]{virmaux2018lipschitz, fazlyab2019efficient, Latorre2020Lipschitz}.\n\n\\section{Lipschitz Constant of Self-Attention}\n\n\\subsection{Dot-product self-attention is \\emph{not} Lipschitz}\n\nMoving on, we investigate whether self-attention is Lipschitz. We first consider the widely used \\textit{(scaled) dot-product multihead self-attention}  as formulated by \\citet{vaswani2017attention}.\nLet $\\mathbf{x}_1, \\ldots, \\mathbf{x}_N$ be a sequence of $N$ elements, where $\\mathbf{x}_i \\in \\mathbb{R}^D$ for $i=1,\\ldots,N$.\nWe represent this sequence as a matrix $X$:\n\\begin{equation}\n    X = \\begin{bmatrix}\n    \\text{---} & \\mathbf{x}_1^\\top & \\text{---}\\\\\n    & \\vdots & \\\\\n    \\text{---} & \\mathbf{x}_N^\\top & \\text{---} \\\\\n    \\end{bmatrix}\\in \\mathbb{R}^{N \\times D},\n\\end{equation}\n\nDot-product multihead self-attention (\\verb!DP-MHA!) is a map from $\\mathbb{R}^{N \\times D}$ to $\\mathbb{R}^{N \\times D}$ consisting of $H$ `heads', where $H$ is chosen to divide $D$. Each head is a map from $\\mathbb{R}^{N \\times D}$ to $\\mathbb{R}^{N \\times D/H}$ defined by\n\\begin{align*} \\label{eq:dot_product_self_attention_definition}\n    \\mathit{DP}(X) & \\coloneqq \\softmax{\\frac{X W^Q (X W^K)^\\top}{\\sqrt{D/H}}} X W^V \\\\\n    &= P X W^V, \n\\end{align*}\nwhere $W^Q, W^K, W^V \\in \\mathbb{R}^{D \\times D/H}$ are learnable parameters specific to each head, and $P \\in \\mathbb{R}^{N \\times N}$ is the output of the softmax (we suppress the dependence of $P$ on $X$ to reduce clutter below). The input to the softmax is an $N\\times N$ matrix of pairwise dot products (hence \\textit{dot-product} self-attention), and the softmax is applied to each row of this matrix. Finally, the outputs of all heads are concatenated into an $N\\times D$ matrix and are right multiplied by $W^O\\in \\mathbb{R}^{D \\times D}$, thus \\verb!DP-MHA! is defined by\n\\begin{equation}\n    \\mathit{MHA}_{DP}(X) \\coloneqq \\left[\\mathit{DP}^1(X), \\ldots, \\mathit{DP}^H(X)\\right] W^O.\n\\end{equation}\nIn what follows, we will prove that $\\mathit{MHA}$ as defined above is  \\emph{not} Lipschitz, assuming that the $\\mathit{MHA}$ map is non-trivial, i.e.~$W^Q, W^K, W^V, W^O \\neq 0$. It is sufficient to show that a single head $\\mathit{DP}$ is not Lipschitz, since $\\mathit{MHA}$ is a linear combination of the outputs of each head. Also note that $P$ is a stochastic matrix, i.e.~its entries are non-negative and its rows sum to $1$.\nSince the rows of $X$ are the $\\mathbf{x}_i$'s, a linear transformation of each $\\mathbf{x}_i$ by some matrix $A$ is equivalent to right multiplication of $X$ by $A^\\top$. \nSo right multiplication of $X$ by $W^V$ is a linear map and thus Lipschitz.\nTherefore, we are interested in the mapping $f(X) = PX$; this is \\emph{not} a linear mapping because $P$ itself is a non-linear function of $X$. \nIn fact, we show that $f$ \nis \\emph{not} Lipschitz, thus proving the first main result of the paper:\n\\begin{theorem} \\label{thm:dp_not_lipschitz}\n\\verb!DP-MHA! is not Lipschitz for any vector $p$-norm $\\|\\cdot\\|_p$ with $p \\in [1, \\infty]$.\n\\end{theorem}\n\\textit{Summary of Proof}. We use Theorem \\ref{thm:jacobian}, noting that if the supremum of the norm of the Jacobian is infinite, then the mapping is not Lipschitz.\nIn particular, we show that when $\\mathbf{x}_i=\\mathbf{0}$ for some $i$, some elements of the Jacobian of $f$ grow proportionally to the sample variance of $\\mathbf{x}_{\\neq i}$, which is unbounded.\n\\begin{proof}\nWe show the proof for the case $D=H=1$ (i.e.~$X \\in \\mathbb{R}^{N \\times 1}$, a column vector, and $x_i \\in \\mathbb{R}$) for readability. See Appendix \\ref{apd:general_d} for the general case, which follows the same logic. \n\nThe mapping $f$ can be written as\n\\begin{align*}\nf(X) = PX = & \\softmax{a X X^\\top} X = \\begin{bmatrix}\n    f_1(X) \\\\\n    \\vdots \\\\\n    f_N(X)\n\\end{bmatrix} \\in \\mathbb{R}^{N \\times 1}, \\\\\n\\text{where} \\quad & f_i(X) = \\sum_{j=1}^N P_{ij}x_j \\in \\mathbb{R}\n\\end{align*}\nand $a = W^K W^Q \\in \\mathbb{R}$ (we assume $a \\neq 0$ such that self-attention is non-trivial).\nHence $f$ can be interpreted as a map of each $x_i$ to a point in the convex hull of ${x_1,...,x_N}$.\nSince $f$ is a map from $\\mathbb{R}^{N \\times 1}$ to $\\mathbb{R}^{N \\times 1}$, its Jacobian is\n\\begin{equation}\n    J_f = \\begin{bmatrix}\n    J_{11} & \\dots & J_{1N} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    J_{N1} & \\dots & J_{NN} \\\\\n    \\end{bmatrix}\\in \\mathbb{R}^{N \\times N},\n\\end{equation}\nwhere $\\smash{J_{ij} = \\frac{\\partial f_i(X)}{\\partial x_j} \\in \\mathbb{R}}$. \nBy taking partial derivatives we can show that\n\\begin{equation*}\n    J_{ij} = a X^\\top P^{(i)} \\left[E_{ji}X + \\delta_{ij}X \\right] + P_{ij}I\n\\end{equation*}\nwhere \n\\begin{itemize}\n    \\item $E_{ij} \\in \\mathbb{R}^{N \\times N}$ is a binary matrix with zeros everywhere except the $(i,j)$th entry\n    \\item $\\delta_{ij} \\in \\{0,1\\}$ is the Kronecker delta\n    \\item $P^{(i)} \\coloneqq \\diag(P_{i:}) - P_{i:}^\\top P_{i:} \\in \\mathbb{R}^{N \\times N}$.\n\\end{itemize}\nSee Appendix \\ref{apd:identities} for useful identities in deriving the above Jacobian.\n\nSo for $i=j$:\n\\begin{align}\nJ_{ii} =\n a X^\\top P^{(i)} e_{ii} X + a X^\\top P^{(i)} X + P_{ii} \\label{eq:jac_dot}\n\\end{align}\n\nLet us investigate the scalar $X^\\top P^{(i)}X$. We observe that it is in fact a variance of a discrete distribution. Specifically:\n\\begin{equation} \\label{eq:cov}\n    X^\\top P^{(i)}X  = \\textstyle\\sum_k P_{ik} x_k^2 - \\left(\\textstyle\\sum_k P_{ik}  x_k\\right)^2 = \\mathrm{Var}(\\mathbb{X}),\n\\end{equation}\nwhere $\\mathbb{X}$ is a discrete distribution with support at the inputs $\\{x_1,\\ldots,x_N \\}$ and probability mass function given by their softmax probabilities $\\mathbb{P}(\\mathbb{X}=x_j)=P_{ij}$. \nA consequence of this interpretation is that $P^{(i)}$ is \\textit{positive semi-definite} (PSD) since $X^\\top P^{(i)} X = \\mathrm{Var}(\\mathbb{X}) \\geq 0$, with equality if and only if the $x_j$ are all equal.\n\nWe use this observation to show that $J_{ii}$ is unbounded, and so $\\|J_f\\|_p$ is unbounded, hence \\verb!DP-MHA! is \\emph{not} Lipschitz.\nConsider the case $x_i=0$. Then \n\\begin{equation*}\n    P_{i:}^\\top = \\softmax{XAx_i} = \\frac{1}{N} \\mathds{1},\n\\end{equation*}\ni.e.\\ we have uniform attention regardless of $x_{ \\neq i}$. \nThe first term of $J_{ii}$ in Equation \\eqref{eq:jac_dot} disappears since $e_{ii} X = [0, \\ldots, x_i, \\ldots, 0] = \\mathbf{0}$, and the last term becomes $\\frac{1}{N} I$. Now consider the second term $a X^\\top P^{(i)}X = a \\mathrm{Var}(\\mathbb{X}_l)$. Note $\\mathbb{X}$ is uniformly distributed, since $\\mathbb{P}(\\mathbb{X}=x_j)=P_{ij}= 1/N$. Hence the second term is equal to $a$ times the sample variance of ${x_1,\\ldots,x_N}$, which can be arbitrarily large. Hence $J_{ii}$ can become arbitrarily large, so the full Jacobian $J_f$ is unbounded.\n\\end{proof}\n\\textit{High-level intuition for proof.}\nAt $x_i=0$, $f_i(X) = \\frac{1}{N} \\sum_{k} x_k$, the mean of the inputs. \nThe rate of change of $f_i$ is governed by how fast the softmax saturates when $x_i$ is perturbed, which is determined by how spread out the $x_{\\neq i}$ are. \nThe more spread out they are (the higher the sample variance), the greater the rate of saturation of the softmax, and the faster the rate of change of $f_i$.\nSince the sample variance of $x_{\\neq i}$ can be arbitrarily large, the rate of change of $f_i$ can also be arbitrarily large, i.e.~the entries of the Jacobian (and hence its $p$-norm) can become arbitrarily large. In Appendix \\ref{apd:bias}, we show that adding bias terms to $\\mathbf{x}_i^\\top W^Q$ and $\\mathbf{x}_j^\\top W^K$ does \\emph{not} resolve the issue.\n\nThe implications of this result are the following.\n\\begin{inparaenum}[(1)]\n\\item There can be undesirable behaviour (e.g.~training instabilities) for the Transformer when some inputs are close to zero and others have large magnitude.\n\\item Dot-product self-attention (and hence the standard Transformer) is not a suitable choice when we require a Lipschitz neural network, such as for formulating invertible residual networks \\citep{behrmann2018invertible}.\n\\end{inparaenum}\nTherefore, to use self-attention and Transformers in such applications, a Lipschitz formulation of self-attention is required, together with an explicit (ideally tight) upper bound to its Lipschitz constant, to quantify how much the output can change with respect to changes in the input.\n\nOne method to make dot-product self-attention Lipschitz is by ensuring its inputs are bounded. Indeed, if the input space is compact, e.g.\\ $[0,1]^{N \\times D}$, any continuously differentiable function is Lipschitz, including dot-product self-attention.\nHowever, as we further discuss in Section \\ref{sec:conclusion}, such an approach has its own challenges, since it makes the Lipschitz constant depend on the input range. Instead, in the next section we formulate a version of self-attention that is provably Lipschitz on all of $\\mathbb{R}^{N\\times D}$, allowing us to derive an upper bound that holds for any subset of $\\mathbb{R}^{N\\times D}$.\n\n\\subsection{L2 self-attention: a Lipschitz formulation of self-attention}\nThe pathology in dot-product self-attention arises because the softmax probabilities $P_{i:}$ are constant with respect to $\\mathbf{x}_{\\neq i}$ when $\\mathbf{x}_i=0$. \nThis behaviour can be undesirable as we want $P_{ij}$ to vary according to $\\mathbf{x}_j$, regardless of whether $\\mathbf{x}_i$ is zero or not.\nHence we propose an alternative form of self-attention based on L2 distance:\n\\begin{align}\\label{eq:L2_self_attention_definition}\nP_{ij} \\propto \\exp(L_{ij}) \\coloneqq \\exp\\left(-\\frac{\\norm{ \\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K}_2^2}{\\sqrt{D/H}}\\right),\n\\end{align}\nwith the normalisation constant ensuring that $\\sum_j P_{ij} = 1$. \nWe will refer to it as \\textit{L2 self-attention}. \nIt is reminiscent of the standard squared-exponential kernel, but with softmax normalisation that ensures that each row of the kernel matrix sums to $1$. \nNormalisation is usually necessary to deal with inputs of varying length $N$ \\citep{wang2018non}, hence we keep the softmax for L2 self-attention. Similarly to dot-product self-attention, L2 self-attention can be computed efficiently with matrix operations; see Appendix \\ref{apd:l2_att_computation} for details, with a comparison of wall-clock runtimes between different choices of attention. \n\nWe first state the mathematical formulation of L2 multihead self-attention (\\verb!L2-MHA!) before proving the main result --- the upper bound of its Lipschitz constant with respect to $\\|\\cdot\\|_p$ for $p=2, \\infty$. The full \\verb!L2-MHA! map $F: \\mathbb{R}^{N \\times D} \\rightarrow \\mathbb{R}^{N \\times D}$ is defined as\n\\begin{align*}\n    F(X) &\\coloneqq \\left[f^1(X)W^{V,1}, \\ldots, f^H(X)W^{V,H}\\right] W^O \\\\\n    & \\quad\\text{where}\\quad\n    f^h(X) \\coloneqq P^h X A_h.\n\\end{align*}\nIn the above, $W^{V,h} \\in \\mathbb{R}^{D \\times D/H}$, $W^O \\in \\mathbb{R}^{D \\times D}$, $P^h$ is defined as in Equation \\eqref{eq:L2_self_attention_definition} with $W^{Q,h}=W^{K,h} \\in \\mathbb{R}^{D \\times D/H}$, and $A_h \\coloneqq W^{Q,h} W^{{Q,h}^\\top} / \\sqrt{D/H} \\in \\mathbb{R}^{D \\times D}$. \nThere are two changes from the usual form of multihead self-attention:\n\\begin{enumerate}[label=(\\arabic*), leftmargin=*]\n    \\item We require $W^{Q,h} = W^{K,h}$ for each head $f^h(X)$ to be Lipschitz. In Lemma \\ref{lemma:tie_weights} of Appendix \\ref{apd:proof} we show that \\verb!L2-MHA! is \\emph{not} Lipschitz for arbitrary $W^{Q,h}$, $W^{K,h}$, and that tying $W^{Q,h} = W^{K,h}$ is sufficient for \\verb!L2-MHA! to be Lipschitz, with intuition for why tying is sufficient.\n    \\item In each head of the self-attention $f^h(X)$, right multiplication by $A_h$ has been included for the theorem below to hold (details are in the proof). In practice, there is little harm done by this extra linear transformation, since when the heads are combined together in $F$, each $f^h(X)$ is additionally transformed by $W^{V,h}$, a free parameter.\n\\end{enumerate}\n\nThe second main result of the paper is the following:\n\\begin{theorem} \\label{thm:main}\n\\verb!L2-MHA! is Lipschitz, with the following bound on $\\lip_{\\infty}(F)$:\n\\begin{align*}\n    \\lip_{\\infty}(F)  \\leq &\\left(4 \\phi^{-1}(N-1) + \\frac{1}{\\sqrt{D/H}}\\right) \\|W^{O^\\top}\\|_{\\infty} \\\\\n    &\\max_h \\|W^{Q,h}\\|_{\\infty} \\|W^{{Q,h}^\\top}\\|_{\\infty} \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty} \n\\end{align*}\nand the following bound on $\\lip_{2}(F)$:\n\\begin{align*}\n    \\lip_2(F) \\leq & \\frac{\\sqrt{N}}{\\sqrt{D/H}}\n    \\left(4 \\phi^{-1}(N-1) + 1 \\right) \\\\ \n    & \\left(\\sqrt{\\textstyle\\sum_h \\|W^{Q,h}\\|_2^2\\, \\|W^{V,h}\\|_2^2}\\right) \\|W^O\\|_2 \n\\end{align*}\nwhere $\\phi(x) \\coloneqq x\\exp(x+1)$ is an invertible univariate function on $x > 0$, and $N$ is the input sequence length.\n\nSpecifically, $\\phi^{-1}(N-1) = W_0(\\frac{N}{e})$ where $W_0$ is the Lambert $W$-function, which grows sub-logarithmically as $O(\\log N - \\log \\log N)$ \\citep{corless1996lambertw}. Hence the above bounds can be simplified to $O(\\log N)$ for $p=\\infty$ and $O(\\sqrt{N} \\log N)$ for $p=2$.\n\\end{theorem}\n\\begin{proof}\nSee Appendix \\ref{apd:proof}, which uses the key observation that $X^\\top P^{(i)}X$ is a covariance matrix (c.f.\\ Equation \\eqref{eq:cov}) to bound $\\|J_F\\|_p$, the norm of the Jacobian of $F$. Appendix \\ref{apd:masking} shows how the argument can be modified to prove the analogous result for the case with masking in the self-attention.\n\\end{proof}\n\nThese bounds are complemented by the concurrent work of \\citet{vuckovic2020attention}, which provides a $O(\\sqrt{D\\log N})$ bound on $\\lip_1(F)$ using measure-theoretic tools.\n\\section{Application: Invertible Self-Attention}\n\\subsection{Invertible residual network} \\label{sec:invertible_resnet}\n\n\nConsider the residual function $g(x) \\coloneqq \\mathbf{x} + f(\\mathbf{x})$. \\citet{behrmann2018invertible} give the following sufficient condition for its invertibility: \nif $f$ is a \\textit{contraction} with respect to some\nmetric, i.e.~if $\\lip(f) < 1$, and the metric space on which $f$ is defined is complete,\nthen $g$ is invertible. \n(A Euclidean space with a metric induced by a $p$-norm $\\|\\cdot\\|_p$ for $p \\in [1, \\infty]$ is always complete.)\nSpecifically, the inverse $g^{-1}(\\mathbf{y})$ is the unique fixed point of the recursion $\\mathbf{x}^{i+1} \\coloneqq \\mathbf{y} - f(\\mathbf{x}^i)$, since by the definition of the inverse we have $\\mathbf{y} = g^{-1}(\\mathbf{y}) + f(g^{-1}(\\mathbf{y}))$. \nBecause $f$ is a contraction, \\textit{Banach's Fixed Point Theorem} guarantees that this fixed point exists and is unique for all $\\mathbf{y}$, and that the recursion converges for all initial values $\\mathbf{x}^0$ (often set to $\\mathbf{y}$ in practice) exponentially fast. \nHence the inverse can be computed to arbitrary accuracy (up to numerical precision in practice) by the above fixed-point iteration.\n\nNote that a composition of such invertible residual blocks is also invertible. \n\\citet{behrmann2018invertible} use this observation to design invertible ResNets: they take $f$ to be a \\verb!CNN! normalised by an upper bound on $\\lip(f)$ given by Corollary \\ref{cor:lip_conv}, making the resulting function \\textit{contractive}. \nFor the $2$-norm $\\|\\cdot\\|_2$, a hyperparameter $c < 1$ is chosen and each linear map (convolution) $W$ in the \\verb!CNN! is multiplied by $c/\\|W\\|_2$ if $c < \\|W\\|_2$ where $\\|W\\|_2$ is estimated by power iteration (c.f.\\ Appendix \\ref{apd:power_iteration}).\nThis multiplicative factor determines the scale of the Lipschitz constant of the normalised function.\n\n\n\\subsection{Invertible self-attention}\n\n\\begin{wrapfigure}{r}{0.2\\textwidth}\n    \\centering\n    \\includegraphics[width=0.2\\textwidth]{transformer_block_single.pdf}\n    \\caption{Transformer block.}\n    \\label{fig:transformer_block}\n\\end{wrapfigure}\n\nThe standard use case of self-attention is with a skip connection inside the Transformer. A Transformer block is composed of residual blocks of multihead self-attention (\\verb!MHA!) and fully-connected (\\verb!FCN!) layers (Figure \\ref{fig:transformer_block}).\nHence similarly to invertible ResNets, we can normalise \\verb!L2-MHA! by the upper bounds given in Theorem \\ref{thm:main} to obtain \\verb!Contractive-L2-MHA! $f$, with which we can obtain invertible self-attention $g(\\mathbf{x}) = \\mathbf{x} + f(\\mathbf{x})$. \nSince \\verb!Dropout! is also part of the residual branch along with \\verb!Contractive-L2-MHA!, we should check that it is also contractive.\nAt test time, \\verb!Dropout! multiplies inputs by the dropout keep probability $p < 1$, so it is a contraction with Lipschitz constant $p$ at evaluation time.\nAt training time, \\verb!Dropout! amounts to setting some inputs to zero, while keeping other inputs constant. This can be expressed as right multiplication by a diagonal binary matrix $M$, and for such matrices we can verify $\\|M\\|_p \\coloneqq \\sup_{\\|x\\|_p=1} \\|Mx\\|_p  \\leq 1$.\nNotice that \\verb!LayerNorm! is not part of the residual branch, hence its Lipschitz continuity is not relevant for invertibility; rather, we can replace it with an invertible normalisation such as \\verb!ActNorm! \\cite{kingma2018glow}. \nHowever, architectures that place \\verb!LayerNorm! inside the residual branch (termed \\verb!pre-LN! as opposed to the traditional \\verb!post-LN! in Figure \\ref{fig:transformer_block}) have become more prevalent in the literature \\cite{wang2019learning, xiong2020layer}, and in this case it makes sense to investigate its Lipschitz continuity.\nWe show that \\verb!LayerNorm! is Lipschitz in Appendix \\ref{apd:layernorm}, with a bound on its Lipschitz constant.\n\nIn the next section, we investigate the properties of invertible self-attention and how it compares with the standard dot-product self-attention; we replace \\verb!DP-MHA! in the Transformer with \\verb!Contractive-L2-MHA!, hence replacing the residual self-attention module with invertible self-attention.\nWe are not interested in the modified Transformer per se, but rather in comparing the properties of invertible self-attention to standard self-attention --- we only use the Transformer as a testbed for this purpose, since self-attention is commonly used in a Transformer.\nGiven the theoretical focus of the paper, we believe that a more challenging application of invertible self-attention, such as normalising flow-based modelling, would be more suitable as a separate paper focused on that particular application.\n\n\\section{Experimental Results}\n\\subsection{Asymptotic tightness of the upper bound on $\\boldsymbol{\\lip_{\\infty}(F)}$} \\label{sec:asymptotic}\n\n\\begin{figure}[htb!]\n    \\centering\n    \\includegraphics[width=0.5\\textwidth]{jacobian_opt.pdf}\n    \\caption{Lower and upper bound on $\\lip_{\\infty}(f)$ for L2-MHA $f$, with $H=D=1$ and varying $N$.}\n    \\label{fig:jacobian_opt}\n\\end{figure}\n\nA tight bound on the Lipschitz constant of self-attention is desirable for all listed applications in Section \\ref{sec:intro}; it leads to tighter generalisation bounds, lighter constraints for provable robustness, and better expressiveness in residual flow models.\nHence we investigate the tightness of our bound on the Lipschitz constant of \\verb!L2-MHA!. \nThe Lipschitz constant is a supremum over the space of inputs $X \\in \\mathbb{R}^{N \\times D}$ (c.f.\\ Equation \\eqref{eq:lipschitz_supremum_definition}) and approximating it requires solving an intractable \noptimisation problem. \nHence it is infeasible to estimate accurately in general, especially when $X$ is high-dimensional. \nHowever, we may compute a lower bound on the Lipschitz constant by maximising the norm of the Jacobian $\\|J_f(X)\\|$ with respect to $X$ until convergence.\nThis local optimum will form a lower bound by Theorem \\ref{thm:jacobian}, and we can expect this lower bound to be fairly tight for the low-dimensional case, provided the optimisation is thorough.\n\nWe use this observation to provide empirical evidence for the asymptotic tightness of the upper bound on $\\lip_{\\infty}(f)$ in Theorem \\ref{thm:main}. \nIn Figure \\ref{fig:jacobian_opt}, we show the upper bound as well as the lower bound on $\\lip_{\\infty}(f)$ obtained by optimising $\\|J_f(X)\\|_{\\infty}$ with respect to $X$ for \\verb!L2-MHA! $f$ with 50 different random initialisations of $X$, with $H=D=1$ and $N$ varying between $100$ and $1000$. \nSee Appendix \\ref{apd:experimental_details} for further details.\nNote that we use a log-scale for the x-axis, and recall that the upper bound is $O(\\log N - \\log \\log N)$, dominated by the $O(\\log N)$ term for large $N$.\nHence the plot for the upper bound shows a linear trend.\nWe also observe that the slope of the lower bound is very similar, providing empirical evidence that the $O(\\log N - \\log \\log N)$ upper bound is asymptotically tight.\n\nThere are at least two possible explanations for the gap between the upper and lower bounds.\n\\begin{inparaenum}[(1)]\n\\item The lower bound is only a local optimum --- the true Lipschitz constant is a global optimum across inputs, which can be difficult to attain especially for high values of $N$.\n\\item The multiplicative constant of the upper bound may be loose.\n\\end{inparaenum}\nAssuming asymptotic tightness, it remains an open question whether the multiplicative constant can be tightened.\nWe show the analogous plot for $\\lip_2(F)$ and discuss the results in Appendix \\ref{apd:jacobian_opt2}.\nAdditionally in Appendix \\ref{apd:jacobian_opt_dp}, we show that optimising $\\|J_f(X)\\|_{\\infty}$ w.r.t.~$X$ for \\verb!DP-MHA! $f$ causes the norm to diverge, providing empirical verification of Theorem \\ref{thm:dp_not_lipschitz}, that \\verb!DP-MHA! is indeed \\emph{not} Lipschitz.\n\n\\subsection{Numerical invertibility of MHA residual map}\\label{sec:numerical-invertibility}\n\n\\begin{figure}[htb!]\n    \\centering\n    \\includegraphics[width=0.5\\textwidth]{mha_invertibility_small.pdf}\n    \\caption{Invertibility of $g(\\mathbf{x})= \\mathbf{x} + c f(\\mathbf{x})$ where $f$ is L2-MHA (left) and DP-MHA (right).}\n    \\label{fig:mha_invertibility_small}\n\\end{figure}\n\n\\begin{figure*}[t!] \n  \\includegraphics[width=\\textwidth]{ptb_test.pdf}\n  \\caption{Test NLL curves during training for various LSTM/Transformer models on PTB character level language modelling.} \\label{fig:ptb}\n\\end{figure*}\n\nRecall from Section \\ref{sec:invertible_resnet} that $g(\\mathbf{x}) = \\mathbf{x} + f(\\mathbf{x})$ is invertible if $f$ is contractive. \nHence if $f$ is \\verb!Contractive-L2-MHA!, $g$ is necessarily invertible. \nHowever, technically we do not disprove the invertibility of \\verb!DP-MHA!, since the converse does not hold in general i.e.~if $f$ is \\verb!DP-MHA!, which we have shown is \\emph{not} Lipschitz hence \\emph{not} contractive, it may still be the case that $g$ \\emph{is} invertible.\nTo verify that \\verb!DP-MHA! (with the skip connection) is \\emph{not} invertible in practice, we compare the numerical invertibility of the residual map  $g(\\mathbf{x})= \\mathbf{x} + c f(\\mathbf{x})$ between the cases where $f$ is \\verb!L2-MHA! and \\verb!DP-MHA! in Figure \\ref{fig:mha_invertibility_small}.\nFor each, we take \\verb!MHA! with $8$ heads and randomly initialised weights, and quantify the maximum reconstruction error across a batch of $128$ inputs whose outputs are inverted via the fixed-point iteration described in Section \\ref{sec:invertible_resnet}. We use $N=64$, \n$D=64$,\nand $c \\in \\{0.5,0.7,0.9\\}$\n(see Appendix \\ref{apd:numerical_invertibility} for analogous results for a wider range of $N$ and $D$ and for \\verb!DP-MHA! with trained weights).\nTo highlight the difference between the two types\nof self-attention, recall in the proof of Theorem \\ref{thm:dp_not_lipschitz} (showing that \\verb!DP-MHA! is not Lipschitz) that when one of the inputs $\\mathbf{x}_i$ is $0$, some terms of the Jacobian grow with the sample variance of $\\mathbf{x}_{\\neq i}$. \nHence we check numerical invertibility at a set of $N$ inputs where $\\mathbf{x}_i=0$ and $\\mathbf{x}_{\\neq i}$ are chosen uniformly at random. \n\nIn Figure \\ref{fig:mha_invertibility_small}, we see that \\verb!DP-MHA! is \\emph{not} invertible whereas \\verb!L2-MHA! \\emph{is} invertible for sufficiently small $c$.\nThis shows how not having the theoretical guarantee of $f$ being contractive can cost us invertibility in practice.\nWe note that the figure shows local invertibility at the sampled inputs, as opposed to global invertibility across the whole input space, yet this clearly highlights the difference between the two choices of self-attention.\nExperiments with the globally invertible self-attention obtained by normalising with the Lipschitz upper bound are provided in the next section.\n\n\n\\subsection{Expressiveness of L2-MHA and invertible self-attention}\n\\label{sec:invertible_self-attention}\n\n\nA natural question to ask is: how does the expressiveness of \\verb!L2-MHA! and \\verb!Contractive-L2-MHA! (that leads to invertible self-attention with the skip connection) compare with the original \\verb!DP-MHA!?\nWe expect that the Lipschitz constraint will limit the expressiveness of the Transformer, and would like to find out by how much.\nWe investigate this by comparing the performance of the original Transformer and the Transformer with invertible self-attention (c.f.\\ Figure \\ref{fig:transformer_block}) at character-level language modelling on the Penn Treebank dataset \\citep{marcus1993building}. \nWe compare the test negative log-likelihood (NLL) of a baseline LSTM, the original Transformer (\\verb!DP-MHA!), and a series of models between the original Transformer and the Transformer with invertible self-attention (\\verb!Contractive-L2-MHA!),\nmaking one change at a time and tuning the hyperparameters on a validation set.\nFor \\verb!Contractive-L2-MHA!, we normalise $F=$\\verb!L2-MHA! by the bound on $\\lip_{\\infty}(F)$ as it is tighter than the bound on $\\lip_{2}(F)$. \nDuring training we backpropagate through these contractive blocks $F/\\lip_{\\infty}(F)$ (including the denominator) to update the model parameters.\nWe found that only backpropagating through the numerator (i.e. applying \\verb!stop-gradient! to denominator) gave slightly worse performance.\nSee Appendix \\ref{apd:experimental_details} for experimental details.\n\nThe results are shown in Figure \\ref{fig:ptb}. \nThe first plot shows the best performing LSTM reaching a test NLL of around $1.0$, and the second plot shows the best performing Transformer reaching a slightly improved performance for $3$--$5$ layers of Transformer blocks. \nWe observe instabilities in training for a higher number of layers, requiring careful tuning of the learning rate schedule for stability at the cost of performance, a commonly observed phenomenon in the literature of deep Transformer architectures \\mbox{\\citep{bapna2018training, parisotto2019stabilizing}}.\nThe third plot shows results for the Transformer with \\verb!DP-MHA! replaced with \\verb!L2-MHA! but without tying $W^Q$ and $W^K$, and we observe a very similar test performance. \nThe fourth plot shows the change when we further tie the query and key weights (making $W^Q=W^K$); we see that there is a small degradation in performance.\nHere the number of trainable parameters has been reduced, but in Appendix \\ref{apd:wq_wk_experiment} we show that matching parameter count does not help performance, suggesting that the reduction in performance when tying queries and keys is not solely due to having fewer parameters.\nWe note that performance saturates at around $5$ layers for each Transformer model so far.\nOn the rightmost plot we show results when further dividing self-attention in each block by the upper bound on $\\lip_{\\infty}(F)$, to obtain invertible self-attention. \nThis does give reduced performance for the same number of layers, but we can attain similar performance with more layers, no longer saturating at $5$ layers.\n\n\\begin{table*}[!htb]\n\\centering\n \\begin{tabular}{|c||c|c|c|c|c|c|c|c|c|} \n \\hline\n  Number of Layers & 2 & 4 & 6 & 8 & 10 & 12 & 14 & 16 & 18 \\\\\n \\hline \\hline\n Transformer (\\textbf{DP}) & 1.061 & 1.032 & 1.021 & \\textbf{1.017}  & 1.025 & - & - & - & - \\\\ \n \\hline\nTransformer (\\textbf{L2}), $W^Q=W^K$  & 1.168 & 1.040 & 1.023 & 1.024 & 1.019 & \\textbf{1.008}  & 1.018 & 1.027 & 1.034 \\\\\n \\hline\nTransformer (\\textbf{Contractive-L2}) & 1.246 & 1.135 & 1.103 & 1.079 & 1.072 & 1.060 & 1.039 & \\textbf{1.029} & 1.031 \\\\\n \\hline\n\\end{tabular}\n\\caption{Test NLL for Transformer models trained with \\textbf{fixed learning rate} on PTB character level language modelling.} \\label{tab:ptb_fixed}\n\\end{table*}\n\nThus we conclude the following. \\begin{inparaenum}[(1)]\n\\item Replacing the dot-product with the L2 distance incurs hardly any loss in expressiveness.\n\\item Tying the query and key weights to obtain Lipschitz self-attention incurs a small loss in expressiveness.\n\\item Dividing by the upper bound on $\\lip_{\\infty}(F)$ to obtain invertible self-attention incurs a noticeable loss in expressiveness, but also has a stabilising effect on the optimisation of the Transformer, thus allowing one to compensate for the apparent loss in expressiveness by increasing the number of layers. \n\\end{inparaenum}\n\n\\subsection{Training Stability of DP-MHA vs L2-MHA}\n\\label{sec:stability}\n\nIn Figure \\ref{fig:mha_output_variance}, we compare the output variance of trained \\verb!L2-MHA! against trained \\verb!DP-MHA!, with weights from the one-layer Transformer (L2), $W^Q=W^K$ model and (DP) model used for Figure \\ref{fig:ptb} respectively. We take the same distribution of inputs as used for the numerical invertibility experiment in Section \\ref{sec:numerical-invertibility}, and show the histogram of inputs and outputs after flattening the input/output tensors. We see that the range of outputs remains similar to the range of inputs for Lipschitz \\verb!L2-MHA!, whereas for \\verb!DP-MHA! the outputs have a much wider range, because the Jacobian norm is large for \\verb!DP-MHA! at these inputs. \n\n\\begin{figure}[!htb]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{mha_output_variance.pdf}\n    \\caption{Histogram showing distribution of inputs/outputs of trained L2-MHA and DP-MHA}\n    \\label{fig:mha_output_variance}\n\\end{figure}\n\nIn practice, this leads to instabilities in training for \\verb!DP-MHA!, hence requiring careful tuning of the learning rate schedule for training deeper Transformer models: linear warmup and square root decay, as detailed in Appendix \\ref{apd:experimental_details}. \nWe investigate the behaviour of the different Transformer models on the above PTB task when using a \\textbf{fixed learning rate}.\nWe observe that \\verb!DP-MHA! fails to train at all beyond 10 layers, whereas both \\verb!L2-MHA! ($W^Q=W^K$) (i.e. Lipschitz L2-MHA but not contractive) and \\verb!Contractive-L2-MHA! shows stable training for up to 18 layers (see Appendix \\ref{apd:stability} for the training curves). \nThis was the deepest model we could fit on a single GPU, and we expect to be able to train even deeper models with these two. \nIn Table \\ref{tab:ptb_fixed} we show the best Test NLL across training for each of the Transformer models. Note that for \\verb!DP-MHA! training becomes unstable beyond 10 layers, so we are only able to provide results up to 10 layers. The generalisation performance of the best model for each setting of self-attention is similar.\n\n\\section{Conclusion and Discussion}\n\\label{sec:conclusion}\nWe have shown that the widely used dot-product self-attention is \\emph{not} Lipschitz, and that the proposed L2 self-attention \\emph{is} Lipschitz, by deriving an $O(\\log N - \\log \\log N)$ Lipschitz bound for $p=\\infty$ and an $O(\\sqrt{N} (\\log N - \\log \\log N))$ bound for $p=2$, where $N$ is the input sequence length. \nWe also provided empirical evidence of the asymptotic tightness of the bound for $p=\\infty$.\nWe demonstrated that Lipschitz-constrained self-attention can be used to formulate invertible self-attention, which we experimentally evaluated on a character-level language modelling task.\nAnd finally, we also showed that L2-MHA is more stable during training, allowing the use of fixed learning rate for stable training of deep architectures.\n\nOur approach to Lipschitz self-attention has been to replace the dot-product kernel with an L2 kernel. An alternative would be to constrain the inputs of self-attention to be bounded; if the input space is compact, e.g.\\ $[0,1]^{N \\times D}$, \\emph{any} continuously differentiable function is Lipschitz, including dot-product self-attention. However, while being simple to implement, this solution has its own difficulties.\nFirst, it makes the Lipschitz constant depend on the range of the input, and thus obtaining a tight bound would require non-trivial mathematical work.\nWe stress that a guarantee that the function is Lipschitz does not tell us anything about its Lipschitz constant; without a tight Lipschitz bound, the true Lipschitz constant can be very large, at which point it is unhelpful that the function is Lipschitz.\nSecond, since self-attention is typically applied at multiple layers within a model (e.g.\\ Transformer), the input to each self-attention will live in a different compact set that depends on the parameters of the previous layers, complicating the analysis for subsequent layers. \nA solution is to constrain the inputs of each layer to be in the same compact set, e.g.\\ by passing them through a sigmoid non-linearity. \nThis however can have undesirable side effects such as vanishing gradients when the sigmoids are saturated.\nDespite these difficulties, this could be a worthwhile alternative route for obtaining Lipschitz self-attention to explore in the future.\n\nHaving a provably Lipschitz self-attention module at our disposal makes it possible to use Transformer-based architectures in applications requiring Lipschitz constraints, while enjoying theoretical guarantees.\nA natural application of Lipschitz self-attention is for residual flows \\mbox{\\citep{behrmann2018invertible}}, and for parameterising Neural ODEs \\citep{chen2018neural} where a Lipschitz vector field guarantees the existence of a unique solution to the ODE for all times. \nThese models can be used for density estimation and generative modelling of sets.\nAnother interesting direction for future work would be to analyse different variants of self-attention based on kernels other than dot-product and L2, as  \\cite{tsai2019transformer} do from an experimental perspective, for which we believe the mathematical tools developed in this paper may aid the analysis.\n\n\n\\section*{Acknowledgements}\nWe would like to thank Adam Kosiorek, Arnaud Doucet, Yee Whye Teh, Michalis Titsias, Emilien Dupont and Theophane Weber for helpful discussion and feedback.\n\n\\bibliographystyle{icml2021}\n\\bibliography{refs}\n\n\n\\newpage\n{\n\\appendix\n\\onecolumn\n{\\Large \\bf Appendix}\n\\section{Useful Identities for deriving Jacobian expressions}\n\\label{apd:identities}\nIn this section, we list some useful identities for deriving the Jacobians of the expressions in the paper.\n\nSuppose $\\lambda$ is a scalar, $\\mathbf{u},\\mathbf{v},\\mathbf{x} \\in \\mathbb{R}^{n \\times 1}$ are column vectors, and $f(\\mathbf{u})$ is a vector valued function. We use the standard convention that for $\\mathbf{a} \\in \\mathbb{R}^m$, $\\mathbf{b} \\in \\mathbb{R}^n$, we have $\\frac{\\partial{\\mathbf{a}}}{\\partial{\\mathbf{b}}} \\in \\mathbb{R}^{m \\times n}$. Then we have the following chain rule identities:\n\\begin{itemize}\n    \\item $\\frac{\\partial}{\\partial \\mathbf{x}}[\\lambda \\mathbf{u}] = \\lambda \\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}} + \\mathbf{x} \\frac{\\partial \\lambda}{\\partial \\mathbf{x}}$\n    \\item $\\frac{\\partial f(\\mathbf{u})}{\\partial \\mathbf{x}} = \\frac{\\partial f(\\mathbf{u})}{\\partial \\mathbf{u}} \\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}}$\n    \\item $\\frac{\\partial}{\\partial \\mathbf{x}}[\\mathbf{u}^\\top \\mathbf{v}] = \\mathbf{u}^\\top \\frac{\\partial \\mathbf{v}}{\\partial \\mathbf{x}} + \\mathbf{v}^\\top \\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}}$\n\\end{itemize}\nNote $\\frac{\\partial \\lambda}{\\partial \\mathbf{x}}$ is a row vector, so $\\mathbf{u}\\frac{\\partial \\lambda}{\\partial \\mathbf{x}}$ is a matrix.\n\nThe Jacobian of the softmax is also well-known. Suppose $\\mathbf{v} = \\softmax{\\mathbf{u}} \\in \\mathbb{R}^{n \\times 1}$. Then\n\\begin{equation*}\n    \\frac{\\partial \\mathbf{v}}{\\partial \\mathbf{u}} = \\diag(\\mathbf{v}) - \\mathbf{v} \\mathbf{v}^\\top \n    = \\begin{bmatrix}\n    v_1 (1-v_1) & - v_1 v_2 & \\ldots & - v_1 v_n \\\\\n    - v_2 v_1  & v_2 (1-v_2) & \\ldots & -v_2 v_n \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    -v_n v_1 & -v_n v_2 & \\ldots & v_n(1-v_n)\n    \\end{bmatrix}.\n\\end{equation*}\n\n\\section{Power Iteration} \\label{apd:power_iteration}\nAlthough $\\|W\\|_{\\infty}$ can be computed efficiently in $O(nm)$ time for $W \\in \\mathbb{R}^{m \\times n}$, na{\\\"i}vely computing $\\|W\\|_2= \\sigma_{\\text{max}}(W) \\coloneqq \\sqrt{\\lambda_{\\text{max}}(W^\\top W)}$ requires $O(n^3)$ operations. (By $\\lambda_{\\text{max}}(A)$ we denote the greatest eigenvalue of a symmetric matrix $A$.) We can however obtain an underestimate $\\tilde{\\sigma}(W)$ via \\textit{power iteration}:\n\\begin{equation} \\label{eq:sigma_tilde}\n    b_{k+1} = \\frac{W^\\top W b_k}{\\|W^\\top W b_k\\|_2},\n    \\quad \\tilde{\\sigma}_k(W) = \\sqrt{\\frac{b_k^\\top W^\\top W  b_k}{b_k^\\top b_k}}, \n\\end{equation}\nwith each iteration taking $O(n^2)$ time. Then using $K\\ll n$ iterations gives us an underestimate $\\tilde{\\sigma}_K$ in $O(Kn^2)$ time.\nSince this is an underestimate, the resulting approximation to the Lipschitz constant of the linear map will not be an upper bound. \nHowever the number of power iterations is usually chosen so that $\\tilde{\\sigma}$ is accurate enough --- $K=5$ is shown to be sufficient in the context of fully connected networks or convolutions considered by \\citet{behrmann2018invertible}.\n\nThe iteration will converge if $W^\\top W$ has an eigenvalue that is strictly greater in magnitude than its other eigenvalues, and the starting vector $b_0$ has a nonzero component in the direction of an eigenvector associated with the dominant eigenvalue. \nThis happens with probability $1$ if $b_0$ is chosen at random, and the convergence is geometric with ratio $|\\lambda_2/\\lambda_{\\max}|$ where $\\lambda_2$ is the eigenvalue with second largest magnitude \\citep{mises1929praktische}.\n\n\\newtheorem{innercustomthm}{Theorem}\n\\newenvironment{customthm}[1]\n  {\\renewcommand\\theinnercustomthm{#1}\\innercustomthm}\n  {\\endinnercustomthm}\n\n\\section{Proof of Theorem 3.1 for General $D$} \\label{apd:general_d}\n\\begin{customthm}{3.1}\n\\verb!DP-MHA! is not Lipschitz for any vector $p$-norm $\\|\\cdot\\|_p$ with $p \\in [1, \\infty]$.\n\\end{customthm}\n\\vspace{-4mm}\n\\begin{proof}\nThe mapping $f$ can be written as\n\\vspace{-5mm}\n\\begin{equation}\nf(X) = PX = \\softmax{X A^\\top X^\\top} X = \\begin{bmatrix}\n    f_1(X)^\\top \\\\\n    \\vdots \\\\\n    f_N(X)^\\top\n\\end{bmatrix} \\in \\mathbb{R}^{N \\times D},\n\\end{equation}\nwhere $A = W^K W^{Q^\\top} / \\sqrt{D/H} \\in \\mathbb{R}^{D \\times D}$ and\n$f_i(X) = \\sum_{j=1}^N P_{ij}\\mathbf{x}_j$ with $P_{i:}^\\top = \\softmax{XA\\mathbf{x}_i}$.\nHence $f$ can be interpreted as a map of each $\\mathbf{x}_i$ to a point in the convex hull of ${\\mathbf{x}_1,...,\\mathbf{x}_N}$.\nSince $f$ is a map from $\\mathbb{R}^{N \\times D}$ to $\\mathbb{R}^{N \\times D}$, its Jacobian is\n\\begin{equation}\n    J_f = \\begin{bmatrix}\n    J_{11} & \\dots & J_{1N} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    J_{N1} & \\dots & J_{NN} \\\\\n    \\end{bmatrix}\\in \\mathbb{R}^{ND \\times ND},\n\\end{equation}\nwhere $J_{ij} = \\frac{\\partial f_i(X)}{\\partial \\mathbf{x}_j} \\in \\mathbb{R}^{D \\times D}$. \nBy taking partial derivatives we can show that $J_{ij} = X^\\top P^{(i)} \\left[E_{ji}XA^\\top + XA\\delta_{ij}\\right] + P_{ij}I$\nwhere $E_{ij} \\in \\mathbb{R}^{N \\times N}$ is a binary matrix with zeros everywhere except the $(i,j)$th entry, $\\delta_{ij}$ is the Kronecker delta, and $P^{(i)} \\coloneqq \\diag(P_{i:}) - P_{i:}^\\top P_{i:}$.\nSo for $i=j$:\n\\begin{align}\nJ_{ii} &=X^\\top P^{(i)}E_{ii}XA^\\top + X^\\top P^{(i)}XA + P_{ii}I \\nonumber \\\\\n&= P_{ii}\\left(\\mathbf{x}_i - \\textstyle\\sum_k P_{ik} \\mathbf{x}_k\\right)\\mathbf{x}_i^\\top A^\\top + X^\\top P^{(i)}XA + P_{ii}I. \\label{eq:jac_dot_general}\n\\end{align}\nFor the last equality, note $E_{ii}X$ has all rows equal to zero except for the $i$th row given by $\\mathbf{x}_i^\\top$. We can then verify that $X^\\top P^{(i)}E_{ii}X$ simplifies to $P_{ii}(\\mathbf{x}_i - \\sum_k P_{ik} \\mathbf{x}_k)\\mathbf{x}_i^\\top$.\n\nFor vector $p$-norms, $\\|J_f\\|_p$ is bounded if and only if its entries are bounded, by definition of the operator norm. \nThe entries of $X^\\top P^{(i)}XA$ are bounded for arbitrary $A$ only if the entries of $X^\\top P^{(i)}X$ are bounded.\nSo let us investigate the entries of this $D\\times D$ matrix. \nWriting out each term of the matrix, we observe that it is in fact a covariance matrix of a discrete distribution. Specifically:\n\\begin{equation} \\label{eq:cov_general}\n    [X^\\top P^{(i)}X]_{lm}  = \\textstyle\\sum_k P_{ik} x_{kl} x_{km} - \\left(\\textstyle\\sum_k P_{ik}  x_{kl}\\right)\\left(\\textstyle\\sum_k P_{ik} x_{km}\\right) = \\mathrm{Cov}(\\mathbb{X}_l,\\mathbb{X}_m),\n\\end{equation}\nwhere $\\mathbb{X}$ is a discrete distribution with support at the inputs $\\{\\mathbf{x}_1,\\ldots,\\mathbf{x}_N \\}$ and probability mass function given by their softmax probabilities $\\mathbb{P}(\\mathbb{X}=\\mathbf{x}_j)=P_{ij}$. \nA consequence of this interpretation is that $P^{(i)}$ is \\textit{positive semi-definite} (PSD) since for $D=1$, Equation \\eqref{eq:cov_general} becomes $X^\\top P^{(i)} X = \\mathrm{Var}(\\mathbb{X}) \\geq 0$, with equality if and only if the $\\mathbf{x}_j$ are all equal.\n\nWe use this observation to show that the terms of $J_{ii}$ are unbounded, and so \\verb!DP-MHA! is \\emph{not} Lipschitz.\nConsider the case $\\mathbf{x}_i=0$. Then $P_{i:}^\\top = \\softmax{XA\\mathbf{x}_i} = \\frac{1}{N} \\mathds{1}$, i.e.\\ we have uniform attention regardless of $\\mathbf{x}_{ \\neq i}$. \nThe first term of $J_{ii}$ in Equation \\eqref{eq:jac_dot_general} disappears since $\\mathbf{x}_i=\\mathbf{0}$, and the last term becomes $\\frac{1}{N} I$. For the second term, the entries $[X^\\top P^{(i)}X]_{ll} = \\mathrm{Var}(\\mathbb{X}_l)$ are unbounded since the latter is equal to the sample variance of ${x_{1l},\\ldots,x_{Nl}}$, which can be arbitrarily large.\n\nNote that we have shown that single head dot-product self-atttention ($H=1$) is not Lipschitz, but it is clear that this implies multihead self-attention \\verb!DP-MHA! is also not Lipschitz, since the output of multihead attention is a linear combination of the outputs of each head.\n\\end{proof}\n\n\\section{Bias term in DP Self-Attention} \n\\label{apd:bias}\nA natural question to ask is whether we can add bias terms $b^Q$ to $\\mathbf{x}_i^\\top W^Q$ and $\\mathbf{b}^K$ to $\\mathbf{x}_j^\\top W^K$ to resolve the issue of attention weights $P_{i:}$ becoming uniform when $\\mathbf{x}_i=0$. \nThe answer is \\emph{no} in general. \nIt can again be shown that $J_{ii}$ is unbounded when $\\mathbf{x}_i$ is chosen such that \\smash{$\\mathbf{x}_i^\\top W^Q + \\mathbf{b}^Q=0$} (such a choice is possible assuming $W^Q$ is full rank, a dense set in \\smash{$\\mathbb{R}^{D \\times D/H}$}). Then \\smash{$P_{i:}^\\top=\\frac{1}{N}\\mathds{1}$} again, and the diagonal entries of \\smash{$X^\\top P^{(i)}X$} are unbounded.\n\n\\section{Efficient Computation of L2 Self-Attention} \\label{apd:l2_att_computation}\nDot-product self-attention only requires a few matrix multiplications to compute the logits (i.e.~the inputs to the softmax) between all pairs of inputs, without having to loop over pairs, hence it can be computed efficiently. \nSimilarly, we can show that L2 self-attention can also be computed in an efficient manner. \nUsing the identity $\\|a-b\\|_2^2 = \\|a\\|_2^2 - 2a^\\top b + \\|b\\|_2^2$ we can compute the logits of L2 attention between all pairs via matrix multiplications and computation of row-wise L2 norms, with negligible overhead compared to dot-product self-attention.\nSpecifically, for L2 self-attention we can show that\n\\begin{gather}\nP = \\softmax{-\\frac{\\|XW^Q\\|_{\\text{row}}^2\\mathds{1}^\\top - 2XW^Q (XW^K)^\\top  + \\mathds{1} \\|XW^K\\|_{\\text{row}}^{2\\top}}{\\sqrt{D/H}}}, \\label{eq:L2_W}\n\\end{gather}\nwhere $\\|A\\|_{\\text{row}}^2$ applies the squared L2 norm to each row of $A$, \nso if $A \\in \\mathbb{R}^{m \\times n}$ then $\\|A\\|_{\\text{row}}^2 \\in \\mathbb{R}^m$.\n\nIn Table \\ref{tab:time} we show the wall-clock training times for the Transformer models with different attention functions and a varying number of layers. It is evident that the differences between the models are rather small.\n\\begin{table}[!htb] \n\\centering\n \\begin{tabular}{|c||c|c|c|c|c|} \n \\hline\n  & 1 Layer & 2 Layers & 3 Layers & 4 Layers & 5 Layers \\\\\n \\hline \\hline\n Transformer \\textbf{(DP)} & 37 & 56 & 77 & 92 & 110 \\\\ \n \\hline\nTransformer \\textbf{(L2)} & 35 & 56 & 73 & 99 & 115 \\\\\n \\hline\nTransformer, $W^Q=W^K$ \\textbf{(L2)} & 39 & 58 & 79 & 91 & 108 \\\\\n \\hline\nTransformer,  \\textbf{(Contractive-L2)} & 37 & 60 & 81 & 102 & 127 \\\\\n \\hline\n\\end{tabular}\n\\caption{Wall clock training times for one epoch of training (seconds)} \\label{tab:time}\n\\end{table}\n\n\n\\section{Proof of Theorem \\ref{thm:main}} \\label{apd:proof}\nRecall the formulation of \\verb!L2-MHA!:\n\\begin{align*}\n    F&: \\mathbb{R}^{N \\times D} \\rightarrow \\mathbb{R}^{N \\times D} \\\\\n    F(X) &= \\left[f^1(X)W^{V,1}, \\ldots, f^H(X)W^{V,H}\\right] W^O \\\\\n    f^h(X) &= P^h X A_h \\\\\n    P^h_{ij} \\propto \\exp(L_{ij}) &\\coloneqq \\exp\\left(-\\frac{\\| \\mathbf{x}_i^\\top W^{Q,h} - \\mathbf{x}_j^\\top W^{K,h}\\|_2^2}{\\sqrt{D/H}}\\right), \\spaces \\sum_j P^h_{ij} = 1\n\\end{align*}\nwhere we have that $W^{Q,h}, W^{K,h}, W^{V,h} \\in \\mathbb{R}^{D \\times D/H}$, $W^O \\in \\mathbb{R}^{D \\times D}$, $P^h \\in \\mathbb{R}^{N \\times N}$ and $A_h \\coloneqq W^{Q,h} W^{{Q,h}^\\top} / \\sqrt{D/H} \\in \\mathbb{R}^{D \\times D}$, and the softmax is applied to each row of the input matrix. \nRecall Equation \\eqref{eq:L2_W}:\n\\begin{equation*}\nP^h = \\softmax{-\\frac{\\|XW^{Q,h}\\|_{\\text{row}}^2\\mathds{1}^\\top - 2XW^{Q,h} (XW^{K,h})^\\top  + \\mathds{1} \\|XW^{K,h}\\|_{\\text{row}}^{2^\\top}}{\\sqrt{D/H}}}.\n\\end{equation*}\n\n\\subsection{L2 self-attention is \\emph{not} Lipschitz for general $\\boldsymbol{W^Q, W^K}$}\nLet us first look at the case of $H=1$ and suppress the index $h$ to reduce clutter. \nConsider the map $\\tilde{f}(X) \\coloneqq PX$, so $f(X)=\\tilde{f}(X)A$.\nWe need $\\tilde{f}$ to be Lipschitz for $f$ and hence $F$ to be Lipschitz.\nNote that $P$ is defined as:\n\\begin{equation*}\nP_{ij} \\propto \\exp(L_{ij}) \\coloneqq \\exp\\left(-\\frac{\\| \\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K\\|_2^2}{\\sqrt{D/H}}\\right)\n\\end{equation*}\nand the normalisation constant satisfies $\\sum_j P_{ij} = 1$, for $P \\in \\mathbb{R}^{N \\times N}$, $X \\in \\mathbb{R}^{N \\times D}$.\n\nFor L2 self-attention, we may take partial derivatives and use the chain rule to show that the Jacobian of $\\tilde{f}$ is:\n\\begin{equation}\n    J_{\\tilde{f}} = \\begin{bmatrix}\n    \\tilde{J}_{11} & \\dots & \\tilde{J}_{1N} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    \\tilde{J}_{N1} & \\dots & \\tilde{J}_{NN} \\\\\n    \\end{bmatrix} \\in \\mathbb{R}^{ND \\times ND}\n\\end{equation}\nwith\n\\begin{equation}\n    \\tilde{J}_{ij} = X^\\top P^{(i)} \\frac{\\partial L_{i:}}{\\partial x_j} + P_{ij} I \\in \\mathbb{R}^{D \\times D}\n\\end{equation}\nwhere\n\\begin{equation}\n    \\frac{\\partial L_{i:}}{\\partial \\mathbf{x}_j} = \\frac{2}{\\sqrt{D/H}}\\left[\\left(XW^K - \\mathds{1} \\mathbf{x}_i^\\top W^Q \\right)W^{Q^\\top} \\delta_{ij} + \\left(E_{ji}XW^Q - E_{jj}XW^K\\right)W^{K^\\top}\\right]\n\\end{equation}\nand\n\\begin{equation*}\n    P^{(i)}  \\coloneqq \\diag(P_{i:}) - P_{i:}^\\top P_{i:} =\n    \\begin{bmatrix}\n    P_{i1}(1-P_{i1}) & -P_{i1} P_{i2} & \\dots  & - P_{i1} P_{iN}  \\\\\n    - P_{i2} P_{i1} & P_{i2}(1-P_{i2}) & \\dots  & - P_{i2} P_{iN} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    - P_{iN}P_{i1} & - P_{iN}P_{i2} & \\dots  & P_{iN}(1-P_{iN}) \n    \\end{bmatrix},\n\\end{equation*}\n\\begin{equation*}\n    P_{ij} = \\frac{\\exp\\left(-\\|\\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K\\|_2^2\\right)}{\\sum_k \\exp\\left(-\\|\\mathbf{x}_i^\\top W^Q - \\mathbf{x}_k^\\top W^K\\|_2^2\\right)}.\n\\end{equation*}\nRecall that $E_{ji} \\in \\mathbb{R}^{N \\times N}$ is a binary matrix with zeros everywhere except the $(j,i)$th entry. Hence $E_{ji}X$ has all rows equal to zero except for the $j$th row given by $\\mathbf{x}_i^\\top$. We can then verify:\n\\begin{equation}\n    X^\\top P^{(i)} E_{ji} X = P_{ij}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k) \\mathbf{x}_i^\\top.\n\\end{equation}\nAlso note $P^{(i)}$ is symmetric, and each row/colum sums to $0$, i.e.~$P^{(i)} \\mathds{1} = \\mathds{1}^\\top P^{(i)} = 0$.\nHence we may simplify the Jacobian terms as follows:\n\\begin{align}\n    \\tilde{J}_{ii} &= \\frac{2}{\\sqrt{D/H}}\\left[X^\\top P^{(i)}(XW^K - \\mathds{1}\\mathbf{x}_i^TW^Q)W^{Q^\\top} + X^\\top P^{(i)}E_{ii}X(W^Q-W^K)W^{K^\\top}\\right] + P_{ii} I \\nonumber \\\\\n    &= \\frac{2}{\\sqrt{D/H}}\\left[X^\\top P^{(i)}(XW^K - \\mathds{1}\\mathbf{x}_i^TW^Q)W^{Q^\\top} + P_{ii}(\\mathbf{x}_i - \\sum_k P_{ik} \\mathbf{x}_k)\\mathbf{x}_i^\\top(W^Q-W^K)W^{K^\\top}\\right] + P_{ii} I  \\nonumber \\\\\n    &= \\frac{2}{\\sqrt{D/H}}\\left[X^\\top P^{(i)}XW^K W^{Q^\\top} + P_{ii}(\\mathbf{x}_i - \\sum_k P_{ik} \\mathbf{x}_k)\\mathbf{x}_i^\\top(W^Q-W^K)W^{K^\\top}\\right] + P_{ii} I, \\label{eq:jii}\n\\end{align}\nand for $i\\neq j$:\n\\begin{align}\n    \\tilde{J}_{ij} &= \\frac{2}{\\sqrt{D/H}}X^\\top P^{(i)}(E_{ij}XW^Q - E_{jj}XW^K)W^{K^\\top} + P_{ij} I \\nonumber \\\\\n    &= \\frac{2}{\\sqrt{D/H}} P_{ij}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k)(\\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K)W^{K^\\top} + P_{ij} I. \\label{eq:jij}\n\\end{align}\n\nWe are now ready to show that $\\tilde{f}$ is \\emph{not} Lipschitz for general $W^Q, W^K$:\n\\begin{lemma} \\label{lemma:tie_weights}\nIf $W^K \\in \\mathbb{R}^{D \\times D/H}$ is full rank (i.e.~full column rank), and $W^K \\neq W^Q$, then $J_{ij}$ has terms that are unbounded for $i \\neq j$, hence $\\tilde{f}$ is \\emph{not} Lipschitz. \n\\end{lemma}\n\\begin{proof}\n\nLet us investigate the expression $\\tilde{K}_{ij} \\coloneqq P_{ij} W^{K^\\top}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k)(\\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K) \\in \\mathbb{R}^{\\frac{D}{H} \\times \\frac{D}{H}}$ for $i\\neq j$, which is related to $\\tilde{J}_{ij}$ as follows by Equation \\eqref{eq:jij}:\n\n\\begin{equation*}\n    W^{K^\\top} \\tilde{J}_{ij} = \\left(\\frac{2}{\\sqrt{D/H}} \\tilde{K}_{ij} + P_{ij}I \\right)\n    W^{K^\\top}.\n\\end{equation*}\n\nIt suffices to show that $\\tilde{K}_{ij}$ is unbounded to show that $\\tilde{J}_{ij}$ is unbounded, since $W^K$ is full rank and $P_{ij} \\in [0,1]$. \n\nLet $\\mathbf{y}_j^\\top = \\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K$. \nThen we have:\n\\begin{align*}\n    \\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k \n    &= W^{Q^\\top}\\mathbf{x}_i - W^{K^\\top}\\mathbf{x}_j - \\sum_k P_{ik} (W^{Q^\\top}\\mathbf{x}_i - W^{K^\\top}\\mathbf{x}_k)\\\\\n    &= W^{Q^\\top}\\mathbf{x}_i - W^{K^\\top}\\mathbf{x}_j - (W^{Q^\\top}\\mathbf{x}_i - \\sum_k P_{ik} W^{K^\\top}\\mathbf{x}_k) \\\\\n    &= - W^{K^\\top}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k).\n\\end{align*}\nHence $\\tilde{K}_{ij} = - P_{ij} (\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k) \\mathbf{y}_j^\\top$.\nNote $\\mathbf{y}_i$ can take an arbitrary value in $\\mathbb{R}^{D/H}$, since $W^K \\neq W^Q$ and $W^K$ is full-rank.\n\nFor all $j \\neq i$, let us choose $\\mathbf{x}_j$ such that $\\mathbf{y}_j = -\\mathbf{y}_i$. This is possible for any value of $\\mathbf{y}_i$ since $W^K$ is full-rank.\nNote $\\mathbf{y}_j = - \\mathbf{y}_i$ and not $\\mathbf{y}_i$.\nWe then have that $\\|\\mathbf{y}_j\\|_2^2$ is equal for all $j$, hence $P_{ij} \\coloneqq \\frac{\\exp(-\\|\\mathbf{y}_j\\|_2^2)}{\\sum_k \\exp(-\\|\\mathbf{y}_k\\|_2^2)} = \\frac{1}{N}$ for all $j$. \nThen for $i \\neq j$, $\\tilde{K}_{ij}$ simplifies to\n\\begin{equation*}\n\\tilde{K}_{ij} = - \\frac{1}{N} \\left(-\\mathbf{y}_i - \\frac{1}{N} (N-2) (-\\mathbf{y}_i)\\right) (-\\mathbf{y}_i)^\\top  = - \\frac{2N-2}{N^2} \\mathbf{y}_i \\mathbf{y}_i^\\top \n\\end{equation*}\nwhose entries are unbounded since $\\mathbf{y}_i$ can be any vector in $\\mathbb{R}^{D/H}$ (note we assume $N \\geq 2$ for self-attention to be well-defined, hence $2N-2 \\neq 0$).\n\\end{proof}\n\nThe intuition for this result is as follows: a reason for \\verb!DP-MHA! not being Lipschitz is that for $\\mathbf{x}_i=0$,, the attention weights $P_{ij}$ become uniform regardless of the values of $\\mathbf{x}_j$ for $j \\neq i$. A similar issue arises for \\verb!L2-MHA! with $W^Q \\neq W^K$ and full-rank $W^K$, as shown above: given any $\\mathbf{x}_i$, we can choose $\\mathbf{x}_j$ such that the $P_{ij}$ become uniform. \n\n\\subsection{L2 self-attention is Lipschitz for $\\boldsymbol{W^Q=W^K}$}\n\nHence we impose the restriction that $W^K=W^Q$. With this assumption we have\n\\begin{equation}\nP_{ij} \\propto \\exp\\left(-\\|(\\mathbf{x}_i - \\mathbf{x}_j)^\\top \\sqrt{A}\\|_2^2\\right)\n\\end{equation}\nwhere $A=W^Q W^{Q^\\top} / \\sqrt{D/H} \\in \\mathbb{R}^{D \\times D}$ and $\\sqrt{A}$ is chosen such that $A = \\sqrt{A}\\sqrt{A}^\\top $,\nin particular $\\sqrt{A} \\coloneqq W^Q / (D/H)^{\\frac{1}{4}}$. The terms in the Jacobian of $\\tilde{f}$ simplify to:\n\\begin{align}\n    \\tilde{J}_{ii} &= 2 X^\\top P^{(i)}XA + P_{ii} I \\hspace{2mm}  \\text{(note $P^{(i)} \\mathds{1} = 0$)},\\\\\n    \\tilde{J}_{ij} &= 2 P_{ij}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k)(\\mathbf{x}_i - \\mathbf{x}_j)^\\top A + P_{ij} I  \\hspace{2mm} \\text{for $i \\neq j$}.\n\\end{align}\nLet the Jacobian of $f(X)$ be:\n\\begin{equation}\n    J_{f} = \\begin{bmatrix}\n    J_{11} & \\dots & J_{1N} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    J_{N1} & \\dots & J_{NN} \\\\\n    \\end{bmatrix} \\in \\mathbb{R}^{ND \\times ND}.\n\\end{equation}\nSince $f(X) = \\tilde{f}(X)A$, and by the chain rule $\\frac{\\partial}{\\partial \\mathbf{x}_j}[\\tilde{f}_i(X)A]=A^\\top \\frac{\\partial \\tilde{f}_i(X)}{\\partial \\mathbf{x}_j}=A \\frac{\\partial \\tilde{f}_i(X)}{\\partial \\mathbf{x}_j}$ (by symmetry of $A$), we have that $J_{ij} = A \\tilde{J}_{ij}$. \nHence\n\\begin{align}\n    J_{ii} &= 2 AX^\\top P^{(i)}XA + P_{ii} A \\hspace{2mm}  \\text{(note $P^{(i)} \\mathds{1} = \\mathbf{0}$)},\\\\\n    J_{ij} &= 2 P_{ij} A (\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k)(\\mathbf{x}_i - \\mathbf{x}_j)^\\top A + P_{ij} A  \\hspace{2mm} \\text{for $i \\neq j$}.\n\\end{align}\nNoting $\\lip_p(f) = \\sup_X \\|J_f(X)\\|_p$, we would like to upper bound $\\|J_f\\|_p$.\n\n\\subsubsection{Upper bound on $\\boldsymbol{\\lip_{\\infty}(F)}$ for L2-MHA}\n\nConsider the choice $p=\\infty$, where $\\|J_f\\|_\\infty$ is the maximum absolute row sum of $J_f$. \nA key observation is that if we can bound the $\\infty$-norm of the Jacobian of $f_i$, a single output of $f$, (i.e.~a single block row $\\|[J_{i1},...,J_{iN}]\\|_\\infty$ of $J_f$) then this is also a bound on $\\|J_f\\|_{\\infty}$ due to permutation equivariance of self-attention; all block rows have the same maximal $\\|\\cdot\\|_\\infty$ when each is optimised over the input $X$. \nUsing this, we can prove that $\\|J_f\\|_\\infty$ admits an upper bound that is $O(\\log N - \\log \\log N)$. Below we state and prove lemmas that lead to the proof of this upper bound.\n\nFirst we analyse the term $\\sqrt{A}^\\top X^\\top P^{(i)}X \\sqrt{A}$, that appears in the first term of $J_{ii}$. \nNote that for $Y \\coloneqq X \\sqrt{A}$, so that the rows of $Y$ are $\\mathbf{y}_i^\\top \\coloneqq \\mathbf{x}_i^\\top \\sqrt{A}$, we have \n\\begin{equation}\n    \\sqrt{A}^\\top X^\\top P^{(i)}X \\sqrt{A}= Y^\\top P^{(i)} Y =  \\mathrm{Cov}(\\mathbb{Y})\n\\end{equation}\nwhere $\\mathbb{P}(\\mathbb{Y}=\\mathbf{y}_j)=P_{ij} = \\exp(-\\|\\mathbf{y}_j - \\mathbf{y}_i\\|^2_2)/\\sum_k \\exp(-\\|\\mathbf{y}_k - \\mathbf{y}_i\\|^2_2)$.\nThe last equality uses the observation in Equation \\eqref{eq:cov}.\n\nThe central inequality used throughout the proof of the main theorem is the following:\n\\begin{lemma} \\label{lemma:key}\n$\\Tr(\\mathrm{Cov}(\\mathbb{Y})) = \\sum_j P_{ij}\\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_2^2 \\leq \\sum_j P_{ij}\\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2 \\leq \\phi^{-1}(N-1)$ where $\\phi(c) = c \\exp(c+1)$ is a one-dimensional invertible function on $\\mathbb{R}_{\\geq 0}$.\n\\end{lemma}\n\\begin{proof}\nThe first equality holds since $\\Tr(\\mathrm{Cov}(\\mathbb{Y})) = \\sum_j \\mathrm{Cov}(\\mathbb{Y})_{jj} = \\sum_j \\mathrm{Var}(\\mathbb{Y}_j) = \\sum_j \\mathbb{E}[(\\mathbb{Y}_j -\\mathbb{E}[\\mathbb{Y}_j])^2]$. \nThe next inequality holds since $\\mathrm{Var}(\\mathbb{Y}_j) = \\mathrm{Var}(\\overline{\\mathbb{Y}}_j) = \\mathbb{E}[\\overline{\\mathbb{Y}}_j^2] -\\mathbb{E}[\\overline{\\mathbb{Y}}_j]^2 \\leq \\mathbb{E}[\\overline{\\mathbb{Y}}_j^2]$ where $\\overline{\\mathbb{Y}}= \\mathbb{Y} - y_i$. \nThe final inequality can be proved as follows.\n\nWe would like to bound \n\\begin{equation}\n    \\sum_j P_{ij}\\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2 = \\frac{\\sum_j \\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2 \\exp(-\\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2)}{\\sum_k \\exp(-\\|\\mathbf{y}_k-\\mathbf{y}_i\\|_2^2)}  = \\frac{\\sum_j z_j^2 \\exp(-z_j^2)}{\\sum_k \\exp(-z_k^2)}\n\\end{equation}\nwhere $z_j \\coloneqq \\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2$ (hence $z_i=0$). \nDefine:\n\\begin{equation}\n    g(\\mathbf{z}) \\coloneqq \\frac{\\sum_j z_j^2 \\exp(-z_j^2)}{\\sum_k \\exp(-z_k^2)} = \\frac{\\sum_{j \\neq i} z_j^2 \\exp(-z_j^2)}{1 + \\sum_{k \\neq i} \\exp(-z_k^2)}.\n\\end{equation}\nFirst note that as $z_j \\rightarrow \\infty$, $\\exp(-z_j^2) \\rightarrow 0$ exponentially fast, causing the product $z_j^2 \\exp(-z_j^2) \\rightarrow 0$.\nHence we expect the above quantity to be bounded and attain its maximum.\n\nLet $h(z_j) \\coloneqq \\exp(-z_j^2)$ for notational conciseness, and note $h(z_j) > 0$. By taking partial derivatives with the chain rule, we have that for $j \\neq i$\n\\begin{equation}\n\\frac{\\partial g(\\mathbf{z})}{\\partial z_j} = \\frac{2z_j h(z_j)}{(\\sum_k h(z_k))^2}\\left[(1-z_j^2)\\sum_k h(z_k) + \\sum_k h(z_k)z_k^2\\right].\n\\end{equation}\nHence the derivative is $0$ if and only if $z_j = 0$ or $(1-z_j^2)\\sum_k h(z_k) + \\sum_k h(z_k)z_k^2 = 0$, the latter being equivalent to $z_j^2 = 1 + \\frac{\\sum_k h(z_k)z_k^2}{\\sum_k h(z_k)} = 1 + g(\\mathbf{z})$. \nHence at the maximum, the non-zero values among $\\{z_j\\}_{j=1}^N$ must be equal to one another.\nIt is clear now that the maximum value $c$ is attained when $z_j^2 = 1 + c$ for $j \\neq i$ (and recall $z_i = 0$). \nSo $h(z_j) = \\exp(-1-c)$ for $j \\neq i$.\nSubstituting this into $g(z)$, and rearranging, we obtain $c \\exp(c+1) = N - 1$. Note $\\phi(x) \\coloneqq x \\exp(x+1)$ is increasing for $x > 0$ hence $c = \\phi^{-1}(N-1)$.\n\\end{proof}\nNote $\\phi(\\log N) = (\\log N) \\exp(\\log N + 1) \\geq N \\log N \\geq N -1$ for $N \\geq 3$. Since $\\phi$ is increasing, we have $\\phi^{-1}(N-1) \\leq \\log(N)$ for $N \\geq 3$. In fact, it is known that $\\phi^{-1}(N-1) = O(\\log N - \\log \\log N)$ \\citep{corless1996lambertw}.\n\nNote the $A$ term in $f(X) = \\tilde{f}(X) A$ allows us to use the above inequality, since $Y^\\top P^{(i)}Y = \\mathrm{Cov}(\\mathbb{Y})$ now appears in the terms of $J_f$:\n\\begin{align}\n    J_{ii}\n    &= 2 \\sqrt{A} [Y^\\top P^{(i)}Y]\\sqrt{A}^\\top + P_{ii} A,  \\\\ \n    J_{ij},\n    &= 2 \\sqrt{A} P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top \\sqrt{A}^\\top + P_{ij} A  \\hspace{2mm} \\text{for $i \\neq j$}.\n\\end{align}\n\nUsing the inequalities $\\|BC\\| \\leq \\|B\\| \\|C\\|$, $\\|B + C\\| \\leq \\|B\\| + \\|C\\|$ and $\\|[A_1, \\ldots, A_N]\\| \\leq \\sum_i \\|A_i\\|$, we have: \n\\begin{align*}\n\\|[J_{i1} &, \\ldots, J_{iN}]\\|_{\\infty}  \\\\\n\\leq & \\|J_{ii}\\|_{\\infty} + \\sum_{j \\neq i} \\|J_{ij}\\|_{\\infty} \\\\\n  \\leq & 2 \\|\\sqrt{A}\\|_{\\infty} \\|Y^\\top P^{(i)}Y\\|_{\\infty} \\|\\sqrt{A}^\\top\\|_{\\infty} + P_{ii} \\|A\\|_{\\infty} \\\\\n & + 2 \\sum_{j \\neq i} \\|\\sqrt{A}\\|_{\\infty} \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty \\|\\sqrt{A}^\\top\\|_{\\infty} + P_{ij} \\|A\\|_{\\infty}\\\\\n  = & 2  \\|\\sqrt{A}\\|_{\\infty}\\|\\sqrt{A}^\\top\\|_{\\infty} \n\\bigg(\\|Y^\\top P^{(i)}Y\\|_\\infty \n + \\sum_{j\\neq i} \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty\\bigg) + \\|A\\|_{\\infty} \\\\\n = & 2  \\frac{\\|W^{Q}\\|_{\\infty}\\|W^{Q^\\top}\\|_{\\infty}}{\\sqrt{D/H}} \n\\bigg(\\|Y^\\top P^{(i)}Y\\|_\\infty \n + \\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty\\bigg) + \\frac{\\|W^Q W^{Q^\\top}\\|_{\\infty}}{\\sqrt{D/H}}.\n\\end{align*}\nFor the first equality, note that $\\sum_j P_{ij}=1$. For the second equality, note that the summand for $j=i$ is $0$ because the term $\\mathbf{y}_i - \\mathbf{y}_j=\\mathbf{0}$. \nEach of the terms in the brackets are bounded by the following lemmas:\n\\begin{lemma}\n$\\|Y^\\top P^{(i)}Y\\|_\\infty \\leq  \\phi^{-1}(N-1) \\sqrt{D/H} $ ($\\phi$ defined as in Lemma \\ref{lemma:key}).\n\\end{lemma}\n\\begin{proof}\nRecall that $Y^\\top P^{(i)}Y = \\mathrm{Cov}(\\mathbb{Y})$. Let $\\sigma(\\mathbb{Y}_m)$ denote the standard deviation of $\\mathbb{Y}_m$. Then $[\\mathrm{Cov}(\\mathbb{Y})]_{lm} \\leq \\sigma(\\mathbb{Y}_l)\\sigma(\\mathbb{Y}_m)$.\nHence \n\\begin{align*}\n\\|\\mathrm{Cov}(\\mathbb{Y})\\|_{\\infty} = \\max_l \\sum_m \\left|[\\mathrm{Cov}(\\mathbb{Y})]_{lm}\\right| \n& \\leq  \\max_l \\sigma(\\mathbb{Y}_l) \\sum_m \\sigma(\\mathbb{Y}_m) \\\\\n& \\leq \\sqrt{\\frac{D}{H}} \\sum_m \\sigma^2(\\mathbb{Y}_m)  = \\sqrt{\\frac{D}{H}} \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\\\\n& \\leq \\sqrt{\\frac{D}{H}} \\phi^{-1}(N-1),\n\\end{align*}\nsince $\\sum_m \\sigma(\\mathbb{Y}_m) \\leq \\sqrt{\\frac{D}{H}} \\sqrt{\\sum_m \\sigma^2(\\mathbb{Y}_m)}$ (by e.g.~using the Cauchy--Schwartz inequality on $[\\sigma(\\mathbb{Y}_1), \\ldots, \\sigma(\\mathbb{Y}_{D/H})]$ and $\\mathds{1}$) and $\\max_l \\sigma(\\mathbb{Y}_l) \\leq \\sqrt{\\sum_m \\sigma^2(\\mathbb{Y}_m)}$, and the last inequality is from Lemma \\ref{lemma:key}. \n\\end{proof}\n\n\\begin{lemma} \\label{lemma:low_rank}\n$\\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty \\leq  \\phi^{-1}(N-1) \\sqrt{D/H}$.\n\\end{lemma}\n\\begin{proof}\nNote $\\|\\mathbf{u}\\mathbf{v}^\\top\\|_{\\infty} = \\|\\mathbf{u}\\|_{\\infty} \\|\\mathbf{v}\\|_1$ for real vectors $\\mathbf{u},\\mathbf{v}$. Hence\n\\begin{align*}\n    \\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty & = \\sum_j P_{ij} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_\\infty \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_1 \\\\\n    & = \\mathbf{a}^\\top \\mathbf{b} \\leq \\|\\mathbf{a}\\|_2 \\|\\mathbf{b}\\|_2,\n\\end{align*}\nwhere $a_j = \\sqrt{P_{ij}} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_\\infty$, $b_j = \\sqrt{P_{ij}} \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_1$.\n\nNote $a_j \\leq c_j \\coloneqq  \\sqrt{P_{ij}} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_2$ since $\\|\\mathbf{u}\\|_\\infty \\leq \\|\\mathbf{u}\\|_2$ for vector $\\mathbf{u}$. Hence $\\|\\mathbf{a}\\|_2 \\leq \\|\\mathbf{c}\\|_2$.\n\nAlso $b_j \\leq \\sqrt{\\frac{D}{H}} d_j \\coloneqq  \\sqrt{\\frac{D}{H}} \\sqrt{P_{ij}} \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_2$ since $\\|\\mathbf{u}\\|_1 \\leq \\sqrt{\\frac{D}{H}}\\|\\mathbf{u}\\|_2$ for $\\mathbf{u} \\in \\mathbb{R}^{D/H}$ (e.g.~by the Cauchy--Schwartz inequality on $[|\\mathbf{u}_1|, \\ldots, |\\mathbf{u}_{D/H}|]$ and $\\mathds{1}$). Hence $\\|b\\|_2 \\leq \\sqrt{\\frac{D}{H}}\\|d\\|_2$.\n\nNote $\\|c\\|_2^2 = \\sum_j P_{ij} \\|y_j - \\sum_k P_{ik} y_k\\|_2^2 = \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\phi^{-1}(N-1)$ from Lemma \\ref{lemma:key},\nand $\\|d\\|_2^2 =  \\sum_j P_{ij} \\|y_i - y_j\\|_2^2 \\leq  \\phi^{-1}(N-1)$ also from Lemma \\ref{lemma:key}.\nHence $\\|a\\|_2 \\|b\\|_2 \\leq \\sqrt{\\frac{D}{H}} \\|c\\|_2 \\|d\\|_2 \\leq \\sqrt{\\frac{D}{H}} \\phi^{-1}(N-1)$.\n\\end{proof}\n\n\nPutting the above lemmas altogether, with the observation $\\sup_X \\|J_f(X)\\|_\\infty = \\sup_X \\|[J_{i1}(X), \\ldots, J_{iN}(X)]\\|_\\infty$ by permutation invariance of $\\|J_f\\|_\\infty$ (since $f$ is permutation equivariant and $\\|\\cdot\\|_\\infty$ is the maximum absolute row sum), we have\n\\begin{align}\n\\|J_f\\|_{\\infty}\n& \\leq 4\\|W^Q\\|_{\\infty}\\|W^{Q^\\top}\\|_{\\infty} \\phi^{-1}(N-1)\n+ \\frac{\\|W^Q W^{Q^\\top}\\|_{\\infty}}{\\sqrt{D/H}} \\nonumber\\\\\n& \\leq \\|W^Q\\|_{\\infty}\\|W^{Q^\\top}\\|_{\\infty} \\left(4\\phi^{-1}(N-1) + \\frac{1}{\\sqrt{D/H}}\\right) \\label{ineq:infty}\\\\\n& \\leq \\|W^Q\\|_{\\infty} \\|W^{Q^\\top}\\|_{\\infty} \\left(4\\log N + \\frac{1}{\\sqrt{D/H}}\\right), \\nonumber\n\\end{align}\nwhere the last inequality holds for $N \\geq 3$.\n\nThe full multihead attention map that combines the heads $f^h(X)$ is:\n\\begin{equation*}\nF: X \\mapsto \\left[f^1(X)W^{V,1}, \\ldots f^H(X)W^{V,H}\\right] W^O = g(X) W^V W^O\n\\end{equation*}\nwhere $g:X \\mapsto [f^1(X),\\ldots,f^H(X)]$, $W^O \\in \\mathbb{R}^{D \\times D}$ and\n\\begin{equation*}\n    W^V = \\begin{bmatrix}\n    W^{V,1} & \\dots & 0 \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & W^{V,H} \\\\\n    \\end{bmatrix} \\in \\mathbb{R}^{DH \\times D}.\n\\end{equation*}\nNote the Jacobian $J_g$ is a block matrix whose rows are $J_{f^h}$, hence $\\|J_g\\|_{\\infty} = \\max_h \\|J_{f^h}\\|_{\\infty}$, and similarly $\\|W^{V^\\top}\\|_{\\infty} = \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty}$. Hence we have\n\\begin{equation*}\n    \\lip_{\\infty}(F) \\leq \\max_h \\|J_{f^h}\\|_{\\infty} \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty} \\|W^{O^\\top}\\|_{\\infty}.\n\\end{equation*}\n\nCombining this with Inequality (\\ref{ineq:infty}), we have:\n\\begin{equation*}\n    \\lip_{\\infty}(F)  \\leq \\left(4 \\phi^{-1}(N-1) + \\frac{1}{\\sqrt{D/H}}\\right) \\max_h \\|W^{Q,h}\\|_{\\infty} \\|W^{{Q,h}^\\top}\\|_{\\infty} \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty} \\ \\|W^{O^\\top}\\|_{\\infty}.\n\\end{equation*}\n\n\\subsubsection{Upper bound on $\\boldsymbol{\\lip_2(F)}$ for L2-MHA}\nFor $p=2$, we use the following lemma:\n\\begin{lemma} \\label{lemma:block_rows}\nLet A be a block matrix with block rows $A_1, \\ldots, A_N$. Then $\\|A\\|_2 \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2}$, and equality holds if and only if the first right singular vectors of the $A_i$ align.\n\\end{lemma}\n\\begin{proof}\n\\begin{equation*}\n\\|A\\|_2^2 = \\left\\Vert \\begin{bmatrix} A_1 \\\\ \\vdots \\\\ A_N \\\\ \\end{bmatrix}\\right\\Vert_2^2 = \\sup_{\\|\\mathbf{x}\\|_2=1} \\left\\Vert\\begin{bmatrix} A_1 \\\\ \\vdots \\\\ A_N \\\\ \\end{bmatrix} \\mathbf{x}\\right\\Vert_2^2 = \\sup_{\\|\\mathbf{x}\\|_2=1} \\sum_i \\|A_i \\mathbf{x}\\|_2^2 \\leq \\sum_i \\sup_{\\|\\mathbf{x}\\|_2=1} \\|A_i \\mathbf{x}\\|_2^2 = \\sum_i \\|A_i\\|_2^2.\n\\end{equation*}\nNote that equality holds if and only if the first right singular vectors of the $A_i$ align.\n\\end{proof}\nHence a bound on the spectral norm of each block row of $J_f$ can give us an $O(\\sqrt{N})$ bound on $\\|J_f\\|_2$, which may be loose, and it remains an open question as to whether this bound can be tightened.\n\nTo bound the $\\|\\cdot\\|_2$ norm of each row of $J_f$, we use the following lemmas:\n\\begin{lemma}\n$\\|Y^\\top P^{(i)}Y\\|_2 \\leq \\phi^{-1}(N-1)$\n\\end{lemma}\n\\begin{proof}\n$\\|Y^\\top P^{(i)}Y\\|_2=\\|\\mathrm{Cov}(\\mathbb{Y})\\|_2 = \\lambda_{\\max}(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\phi^{-1}(N-1)$, where the first equality holds by symmetry of $\\mathrm{Cov}(\\mathbb{Y})$ and the next holds by $\\mathrm{Cov}(\\mathbb{Y})$ being positive semi-definite, so all its eigenvalues are non-negative, and hence the maximal eigenvalue is bounded by the sum of the eigenvalues, equal to its trace. The final inequality is from Lemma \\ref{lemma:key}.\n\\end{proof}\n\n\\begin{lemma}\n$\\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_2 \\leq  \\phi^{-1}(N-1)$\n\\end{lemma}\n\\begin{proof}\nDirectly use Cauchy--Schwartz on $c$ and $d$ in the proof of Lemma \\ref{lemma:low_rank}. \n\\end{proof}\nAgain using the inequalities $\\|BC\\| \\leq \\|B\\| \\|C\\|$, $\\|B + C\\| \\leq \\|B\\| + \\|C\\|$ and $\\|[A_1, \\ldots, A_N]\\| \\leq \\sum_i \\|A_i\\|$, with the additional equality $\\|B^\\top\\|_2 = \\|B\\|_2$, we have the bound: \n\\begin{align*}\n&\\|[J_{i1}, \\ldots, J_{iN}]\\|_2 \\\\\n & \\leq  2  \\frac{\\|W^Q\\|_2\\|W^{Q^\\top}\\|_2}{\\sqrt{D/H}} \n\\bigg(\\|Y^\\top P^{(i)}Y\\|_2\n + \\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_2 \\bigg) + \\frac{\\|W^Q W^{Q^\\top}\\|_2}{\\sqrt{D/H}} \\\\\n & \\leq  4\\phi^{-1}(N-1) \\frac{\\|W^Q\\|_2^2}{\\sqrt{D/H}}  + \\frac{\\|W^Q W^{Q^\\top}\\|_2}{\\sqrt{D/H}} \\\\\n & \\leq  \\frac{\\|W^Q\\|_2^2}{\\sqrt{D/H}} \\bigg(4\\phi^{-1}(N-1)+1 \\bigg).\n\\end{align*}\nUsing Lemma \\ref{lemma:block_rows}, we have that\n\\begin{align}\n    \\|J_f\\|_2 & \\leq \\frac{\\sqrt{N}\\|W^Q\\|_2^2}{\\sqrt{D/H}} \\bigg(4\\phi^{-1}(N-1)+1 \\bigg) \\label{ineq:2} \\\\\n    & \\leq \\frac{\\sqrt{N}\\|W^Q\\|_2^2}{\\sqrt{D/H}}(4\\log N+1). \\nonumber\n\\end{align}\nTo obtain the final result for the full multihead self-attention $F$, we need a final lemma:\n\\begin{lemma} \\label{lemma:block_cols}\nLet A be a block matrix with block columns $A_1, \\ldots, A_N$. Then $\\|A\\|_2 \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2}$.\n\\end{lemma}\n\\begin{proof}\n\\begin{align*}\n\\|A\\|_2 &= \\|[A_1, \\ldots, A_N]\\|_2 = \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1}  \\left\\Vert [A_1, \\ldots, A_N] \\begin{bmatrix} \\mathbf{x}_1\\\\ \\vdots \\\\ \\mathbf{x}_N \\\\ \\end{bmatrix} \\right\\Vert_2^2 = \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1} \\|\\sum_i A_i \\mathbf{x}_i\\|_2 \\\\ \n& \\leq \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1} \\sum_i \\|A_i \\mathbf{x}_i\\|_2 = \\sup_{\\|\\mathbf{e}_i\\|_2=1, \\sum_i \\lambda_i^2 =1} \\sum_i \\lambda_i \\|A_i \\mathbf{e}_i\\|_2 = \\sup_{\\sum_i \\lambda_i^2 =1} \\sum_i \\lambda_i \\|A_i\\|_2 \\\\\n& \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2},\n\\end{align*}\nwhere we are using the substitution $\\mathbf{x}_i = \\lambda_i \\mathbf{e}_i$, and the last inequality holds by e.g.~Cauchy--Schwartz inequality on $[\\lambda_1, \\ldots, \\lambda_N]$ and $[\\|A_1\\|_2, \\ldots, \\|A_N\\|_2]$.\n\\end{proof}\nRecall that \n\\begin{equation*}\nF: X \\mapsto \\left[f^1(X)W^{V,1}, \\ldots, f^H(X)W^{V,H}\\right] W^O.\n\\end{equation*}\nSince $\\|f^h(X)W^{V,h}\\|_2 \\leq \\|J_{f^h}\\|_2 \\|W^{V,h}\\|_2$, by Lemma \\ref{lemma:block_cols} we have that\n\\begin{equation*}\n    \\left\\|[f^1(X)W^{V,1}, \\ldots, f^H(X)W^{V,H}]\\right\\|_2 \\leq \\sqrt{\\sum_h \\|J_{f^h}\\|_2^2 \\|W^{V,h}\\|_2^2}\n\\end{equation*} and hence\n\\begin{equation}\n    \\lip_2(F) \n    \\leq \\left(\\sqrt{\\sum_h \\|J_{f^h}\\|_2^2 \\|W^{V,h}\\|_2^2}\\right) \\|W^O\\|_2.\n\\end{equation}\nCombining this with Inequality (\\ref{ineq:2}), we have:\n\\begin{equation*}\n    \\lip_2(F) \\leq \\frac{\\sqrt{N}}{\\sqrt{D/H}}\n    \\left(4 \\phi^{-1}(N-1) + 1 \\right) \\left(\\sqrt{\\textstyle\\sum_h \\|W^{Q,h}\\|_2^2\\, \\|W^{V,h}\\|_2^2}\\right) \\|W^O\\|_2.\n\\end{equation*}\n\n\\section{The Case with Masking} \\label{apd:masking}\nSince self-attention is often used with \\textit{masking}, a natural question is how masking affects the derived bounds. In self-attention (for any choice of attention function), masking is implemented as follows: given a set of mask indices $\\mathcal{M}  \n\\subset \\{1, \\ldots, N\\} \\times \\{1, \\ldots, N\\}$, the logits (i.e.~the inputs to the softmax) are set to $-\\infty$ at the mask indices. That is,\n\\begin{equation*}\n    L_{ij}= \n\\begin{cases}\n    \\tilde{L}_{ij} & \\text{if } (i,j) \\notin \\mathcal{M}\\\\\n    -\\infty        & \\text{if } (i,j) \\in \\mathcal{M}\n\\end{cases}\n\\end{equation*}\nwhere $\\tilde{L}_{ij}$ is the original logit (e.g.~for L2 self-attention, $\\tilde{L}_{ij} = -(\\mathbf{x}_i - \\mathbf{x}_j)^\\top A (\\mathbf{x}_i - \\mathbf{x}_j)$). \n\nMasking implies $f_i(X)$ is not a function of $\\mathbf{x}_j$ for $(i,j) \\in \\mathcal{M}$, hence  $J_{ij} = 0$ for $(i,j) \\in \\mathcal{M}$.\nThus $f_i(X)$ is equal to the $i$th output for self-attention with inputs restricted to $\\{\\mathbf{x}_j: (i,j) \\notin \\mathcal{M}\\}$, the unmasked inputs with respect to the $i$th output. \nHence $J_{ij}$ will no longer contribute to the bound on $\\|[J_{i1}, \\ldots, J_{iN}]\\|$, and hence the bound for the unmasked case will continue to hold as long as $(i,i) \\in \\mathcal{M}$ i.e.~$\\mathbf{x}_i$ attends to itself (this is necessary for the proof of Lemma \\ref{lemma:key} to hold). \nThe bound can in fact be tightened by replacing $N$ with $|\\{\\mathbf{x}_j: (i,j) \\notin \\mathcal{M}\\}|$, the number of unmasked inputs with respect to the $i$th output.\n\n\\section{Experimental Details} \\label{apd:experimental_details}\nFor the experiment in Section \\ref{sec:asymptotic}, showing the asymptotic tightness of the upper bound on $\\lip_{\\infty}(F)$ where $F$ is \\verb!L2-MHA!, we fix all free parameters of $F$ (namely $W^Q, W^V$) to be the identity, and only optimise the input $X$. \nWe use $50$ random initialisations of $X$ for each $N$, where $X_{ij} \\sim \\unif [-c, c]$ for $c \\sim \\unif [0,10]$ (we observed that having $c$ itself be random improves optimisation). We display the top $5$ results for each value of $N$ after optimising each random initialisation till convergence using Adam \\citep{kingma2014adam} with a learning rate of $0.1$. \n\nFor the experiments in Section \\ref{sec:invertible_self-attention}, we comparing the performance of the original Transformer and the Transformer with Lipschitz/invertible self-attention at character-level language modelling on the Penn Treebank dataset \\citep{marcus1993building}.\\footnote{We use the standard training-validation-test split, and the dataset can be found at e.g.\\ \\url{https://github.com/harvardnlp/TextFlow/tree/master/data/ptb}.}\nEach training example is a sentence represented as a variable-length sequence of characters, and examples are batched according to length such that padding is minimised, with the maximum sequence length set to $288$. \nAll models are autoregressive, outputting the logits for the categorical likelihood predicting the next character, and are trained using maximum likelihood (cross-entropy loss) with a batch size of $64$. \nThe LSTM models have the dimensionality of the hidden state equal to the dimensionality $D$ of the cell state (the usual default implementation). \nThe Transformer models are trained with a varying number of blocks (number of layers) with $H=8$ heads and $D=512$, tuning hyperparameters for dropout rate in $\\{0,0.1,0.2\\}$ and base learning rate $\\gamma \\in \\{0.2,0.4,0.6,0.8,1.0,1.5,2.0\\}$ with number of warmup iterations $w \\in \\{1000,2000,4000,8000\\}$ for the standard custom learning rate schedule in \\citet{vaswani2017attention}:\n\\begin{equation*}\n  \\epsilon_t = \\frac{\\gamma}{\\sqrt{D}} \\min(t^{-1/2}, t w^{-3/2}),\n\\end{equation*}\nwhere $\\epsilon_t$ is the learning rate at training iteration $t$. Hence the learning rate linearly increases from $0$ to $(Dw)^{-1/2}$ over $w$ iterations, then decays proportionally to $t^{-1/2}$.\nWe use Glorot Uniform initialisation \\citep{glorot2010understanding} for all weights ($U\\left[-\\sqrt{\\frac{1}{d_{in} + d_{out}}}, \\sqrt{\\frac{1}{d_{in} + d_{out}}}\\right]$), except for weights in \\verb!L2-MHA! that are initialised from $U\\left[-\\frac{s}{\\sqrt{D}},\\frac{s}{\\sqrt{D}}\\right]$, and $s$ is a hyperparameter. For $D=512$, we used $s=\\frac{1}{2^4}$. All experiments were done in Tensorflow 1.14 \\citep{abadi2016tensorflow} on single Nvidia Tesla V100 GPUs.\n\n\\section{Numerical Invertibility of MHA Residual Map} \\label{apd:numerical_invertibility}\nFollowing Section \\ref{sec:numerical-invertibility}, Figure \\ref{fig:trained_dp_mha_invertibility} confirms that numerical invertibility does not hold for trained weights for dot-product multihead self-attention (DP-MHA) (obtained from one-layer Transformer (DP) model used for Figure \\ref{fig:ptb}), similar to the randomly initialised weight case.\nFigure \\ref{fig:mha_invertibility} shows additional results for different values of $N$ and $D$. \n\n\\begin{figure}[!htb]\n    \\centering\n    \\includegraphics[width=0.35\\textwidth]{trained_dp_mha_invertibility.pdf}\n    \\caption{Invertibility of $g(\\mathbf{x})=\\mathbf{x} + cf(\\mathbf{x})$ for trained DP-MHA $f$.}\n    \\label{fig:trained_dp_mha_invertibility}\n\\end{figure}\n\n\\begin{figure}[!htb]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{mha_invertibility.pdf}\n    \\caption{Numerical invertibility of $g(\\mathbf{x})= \\mathbf{x} + c f(\\mathbf{x})$ where $f$ is L2-MHA(left) or DP-MHA  (right), for different values of $N$ and $D$.}\n    \\label{fig:mha_invertibility}\n\\end{figure}\n\n\n\n\\newpage\n\\section{Behaviour of Lower Bound on $\\boldsymbol{\\lip_2(F)}$}\n\\label{apd:jacobian_opt2}\n\\begin{figure}[!htb]\n    \\centering\n    \\includegraphics[width=0.6\\columnwidth]{jacobian_opt2.pdf}\n    \\caption{Lower bound on $\\lip_{2}(F)$ where $F$ is L2-MHA, with $D=1$ and varying $N$, obtained by optimising $\\|J_F(X)\\|_{2}$ with respect to $X$, with $50$ random initialisations of $X$ for each $N$.}\n    \\label{fig:jacobian_opt2}\n\\end{figure}\nIn Figure \\ref{fig:jacobian_opt2}, we show the lower bound on $\\lip_{2}(F)$ obtained by optimising $\\|J_F(X)\\|_{2}$ using the same optimisation procedure as for Figure \\ref{fig:jacobian_opt} of Section \\ref{sec:asymptotic}. Here the optimisation is more difficult, evident in the variance of the top $5$ values, and the trend is less clear, but it appears that $\\lip_{2}(f)$ grows at a rate of $O(\\log N)$. The message is less clear here, and there are at least two possibilities: \n\\begin{enumerate}[label=(\\arabic*), leftmargin=*]\n    \\item The optimisation is difficult even for small values of $N$, hence Figure \\ref{fig:jacobian_opt2} shows a loose lower bound.\n    \\item If the lower bound is tight, this suggests that the $O(\\sqrt{N} \\log N)$ bound in Theorem \\ref{thm:main} is not asymptotically tight, and could be improved to $O(\\log N)$ (or $O(\\log N - \\log \\log N)$ as for $p=\\infty$).\n\\end{enumerate}\n\n\\section{Optimising the norm of the Jacobian of DP-MHA}\n\\label{apd:jacobian_opt_dp}\nIn Figure \\ref{fig:trained_dp_mha_lower_bound}, we show how the norm of the Jacobian $\\|J_f(X)\\|_{\\infty}$ for \\verb!DP-MHA! $f$ keeps increasing when being optimised with respect to $X$. This is a useful sanity check validating our theoretical result of Theorem \\ref{thm:dp_not_lipschitz}, that \\verb!DP-MHA! is \\emph{not} Lipshchitz. The oscillations are likely due to momentum term of Adam optimizer that was used to optimise the norm. \n\n\\begin{figure}[!htb]\n    \\centering\n    \\includegraphics[width=0.4\\columnwidth]{trained_dp_mha_lower_bound.pdf}\n    \\caption{Optimise $\\|J_f(X)\\|_{\\infty}$ w.r.t.~$X$ for trained DP-MHA $f$.}\n    \\label{fig:trained_dp_mha_lower_bound}\n\\end{figure}\n\n\\section{Experiment tying keys and queries of L2-MHA but preserving parameter count}\n\\label{apd:wq_wk_experiment}\nIn Figure \\ref{fig:ptb} of Section \\ref{sec:invertible_self-attention}, we have shown that there is a clear reduction in performance when tying the keys and queries. To test whether this can be attributed to the reduction in parameter count, we tried doubling the number of columns of $W^Q$ when the keys and queries are shared (i.e. from $D/H$ to $2D/H$) so that the shared model has the same number of parameters as the unshared model. In Figure \\ref{fig:ptb_wq_wk}, the third column shows results for shared \\verb!L2-MHA!, but with the same number of parameters as the unshared \\verb!L2-MHA! i.e.~without tying the keys and queries. The performance is similar to the second column (tying with a reduced number of parameters), suggesting that there is an inherent limitation in expressiveness to tying the keys and queries, and that the reduction in number of parameters is an insufficient explanation this phenomenon.\n\n\\begin{figure}[!htb]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{ptb_wq_wk_experiment.pdf}\n    \\caption{Experiment tying keys/queries but preserving parameter count.}\n    \\label{fig:ptb_wq_wk}\n\\end{figure}\n\n\n\\section{Training curves for fixed learning rate DP-MHA vs L2-MHA} \\label{apd:stability}\n\n\\begin{figure}[!ht]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{stability_experiment.pdf}\n    \\caption{Train NLL for Transformer (DP), Transformer (L2) and Transformer (Contractive-L2)}\n    \\label{fig:stability}\n\\end{figure}\n\n\\section{The Lipschitz constant of LayerNorm}\n\\label{apd:layernorm}\nIn this section, we show that \\verb!LayerNorm! is Lipschitz, with a loose bound on its Lipschitz constant w.r.t. to the $\\infty$-norm.\n\\verb!LayerNorm! is defined as follows:\n\\begin{align*}\n    \\text{LN}(\\mathbf{x}) &= \\frac{\\mathbf{x}-\\mu(\\mathbf{x})}{\\sqrt{\\sigma^2(\\mathbf{x}) + \\epsilon}} \\odot \\boldsymbol\\gamma + \\boldsymbol\\beta \\\\\n    \\mu(\\mathbf{x}) &= \\frac{1}{D} \\sum_{d=1}^D x_d \\\\\n    \\sigma^2(\\mathbf{x}) &= \\frac{1}{D}\\sum_{d=1}^D (x_d - \\mu(\\mathbf{x}))^2 \n\\end{align*}\nwhere $\\mathbf{x}, \\boldsymbol\\beta, \\boldsymbol\\gamma \\in \\mathbb{R}^D$. We will omit dependence on $x$ to write $\\mu, \\sigma^2$ in cases when there is no ambiguity to reduce clutter.\n\nIn the trivial case where $x_d$ are all equal or when $D=1$, $\\mathbf{x} = \\mu$ hence $LN(\\mathbf{x})=\\boldsymbol\\beta$, so its Lipschitz constant is 0. Thus let us assume $D > 2$ and not all $x_d$ are equal.\n\nFirst let us compute the derivative of $\\mu$ and $\\sigma^2$ w.r.t $x$:\n\\begin{align*}\n    \\frac{\\partial \\mu}{\\partial \\mathbf{x}} &= \\frac{1}{D} \\mathds{1}^\\top \\\\\n    \\frac{\\partial \\sigma^2}{\\partial \\mathbf{x}} &= \\frac{1}{D} \\sum_d 2(x_d - \\mu) \\frac{\\partial}{\\partial \\mathbf{x}}(x_d - \\mu) \\\\\n    &= \\frac{2}{D} \\sum_d (x_d - \\mu)(\\mathbf{e}_d - \\frac{1}{D}\\mathds{1})^\\top \\\\\n    &= \\frac{2}{D} \\bigg[\\sum_d (x_d - \\mu) \\mathbf{e}_d - \\frac{1}{D} \\mathds{1} \\sum_d (x_d - \\mu)\\bigg]^\\top \\\\\n    &= \\frac{2}{D}\\sum_d (x_d - \\mu) \\mathbf{e}_d^\\top \\\\\n    &= \\frac{2}{D}(\\mathbf{x} - \\mu)^\\top\n\\end{align*}\nwhere $\\mathbf{e}_d \\in \\mathbb{R}^D$ is a one-hot vector with $1$ at the $d$th element. Note the penultimate equality holds because $\\sum_d (x_d - \\mu) = 0$.\n\nNow the derivative of $\\text{LN}(\\mathbf{x})_d$, the $d$th element of $\\text{LN}(\\mathbf{x})$, w.r.t.$\\mathbf{x}$ is\n\\begin{align*}\n    \\frac{\\partial \\text{LN}(\\mathbf{x})_d}{\\partial \\mathbf{x}} &= \\gamma_d \\bigg[\\frac{\\partial}{\\partial \\mathbf{x}}(x_d - \\mu)(\\sigma^2 + \\epsilon)^{-\\frac{1}{2}} + (x_d - \\mu)\\Big(-\\frac{1}{2}(\\sigma^2 + \\epsilon)^{-\\frac{3}{2}}\\Big)\\frac{\\partial \\sigma^2}{\\partial \\mathbf{x}}\\bigg] \\\\\n    &= \\gamma_d (\\sigma^2 + \\epsilon)^{-\\frac{1}{2}} \\bigg[(\\mathbf{e}_d - \\frac{1}{D}\\mathds{1})^\\top - \\frac{1}{2} (x_d - \\mu)(\\sigma^2 + \\epsilon)^{-1} \\frac{2}{D}(\\mathbf{x} - \\mu)^\\top \\bigg] \\\\\n    &= \\gamma_d (\\sigma^2 + \\epsilon)^{-\\frac{1}{2}} \\bigg[(\\mathbf{e}_d - \\frac{1}{D}\\mathds{1})^\\top - \\frac{1}{D} (\\sigma^2 + \\epsilon)^{-1} (x_d - \\mu)(\\mathbf{x} - \\mu)^\\top \\bigg].\n\\end{align*}\n\nHence\n\\begin{align*}\n    \\frac{\\partial \\text{LN}(\\mathbf{x})}{\\partial \\mathbf{x}} &= (\\sigma^2 + \\epsilon)^{-\\frac{1}{2}} \\bigg[ \\text{diag}(\\boldsymbol\\gamma) - \\frac{1}{D}\\boldsymbol\\gamma \\mathds{1}^\\top - \\frac{1}{D} (\\sigma^2 + \\epsilon)^{-1}\\text{diag}(\\boldsymbol\\gamma)(\\mathbf{x} - \\mu)(\\mathbf{x} - \\mu)^\\top \\bigg].\n\\end{align*}\n\nNote\n\\begin{equation*}\n    \\text{diag}(\\boldsymbol\\gamma) - \\frac{1}{D}\\boldsymbol\\gamma \\mathds{1}^\\top = \\begin{bmatrix}\n    \\gamma_1 (D-1)/D & -\\gamma_1 / D & \\dots & -\\gamma_1 / D \\\\\n    - \\gamma_2 / D & \\gamma_2 (D-1)/D & \\dots & -\\gamma_2 / D \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    -\\gamma_D / D & -\\gamma_D / D & \\dots & \\gamma_D (D-1)/D\n    \\end{bmatrix},\n\\end{equation*}\n\nhence\n\\begin{equation} \\label{eq:first_terms_inf_norm}\n    \\left\\Vert \\text{diag}(\\boldsymbol\\gamma) - \\frac{1}{D}\\boldsymbol\\gamma \\mathds{1}^\\top \\right\\Vert_{\\infty} = \\frac{2(D-1)}{D}\\max_d |\\gamma_d|,\n\\end{equation}\nrecalling that $\\left\\Vert \\cdot \\right\\Vert_{\\infty}$ is the maximum absolute row sum.\n\nLet $z_d \\coloneqq x_d - \\mu$. Hence $\\sum_d z_d = 0$, $\\sigma^2 = \\frac{1}{D} \\sum_d z_d^2$ and\n\\begin{equation*}\n    \\mathrm{Cov}(\\mathbf{x}) = \n    (\\mathbf{x} - \\mu)(\\mathbf{x} - \\mu)^\\top =\n    \\begin{bmatrix}\n    z_1^2 & \\dots & z_1 z_D \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    z_D z_1 & \\dots & z_D^2 \\\\\n    \\end{bmatrix}.\n\\end{equation*}\n\nHence\n\\begin{equation*}\n\\frac{\\left\\Vert \\mathrm{Cov}(\\mathbf{x})\\right\\Vert_{\\infty}}{\\sigma^2} = \\frac{\\max_d |z_d| \\sum_{d'} |z_{d'}|}{\\frac{1}{D}\\sum_d z_d^2}.    \n\\end{equation*}\nNoting that this expression is scale-invariant in $\\mathbf{z}$, we may assume WLOG $\\max_d |z_d| = z_D = 1$, since we are assuming not all $x_d$ are equal and hence at least one $z_d$ is non-zero.\n\nThe expression now becomes\n\\begin{equation} \\label{eq:cov_expression}\n\\frac{\\left\\Vert \\mathrm{Cov}(\\mathbf{x})\\right\\Vert_{\\infty}}{\\sigma^2} =  D\\bigg(\\frac{1 + \\sum_{d < D} |z_d|}{1 + \\sum_{d<D} z_d^2}\\bigg). \n\\end{equation}\nSince all terms $|z_d| \\leq 1$ are bounded, this continuous expression reaches a global maximum for some value of $\\mathbf{z}$ with $z_D = 1$.\n\nIt is easy to see that at the global maximum, $z_d \\neq 0$ $\\forall d$:\nsuppose this were to be true, WLOG $z_1 = 0$.\nThen let us see how the quantity \\eqref{eq:cov_expression} changes when $z_1=0$ is increased by $0< \\delta < 1$ and $z_D=1$ is decreased by $\\delta$, keeping the sum constant.\nIt is easy to see that the numerator $\\sum_d |z_d|$ stays constant, but the denominator $\\sum_d z_d^2$ changes by $2\\delta^2 - 2\\delta < 0$.\nSince for small $\\delta$, the numerator of \\eqref{eq:cov_expression} stays constant but the denominator decreases, the quantity \\eqref{eq:cov_expression} increases, contradicting that the global max is obtained for $z_1 = 0$.\nHence we may assume that $z_d \\neq 0$ $\\forall d$.\n\nHence the quantity \\eqref{eq:cov_expression} (in particular, $\\sum_d {|z_d|}$) is differentiable at the global maximum, at which the partial derivatives of the following Lagrangian are zero:\n\\begin{equation*}\n    \\mathcal{L}(z_1,\\ldots,z_{D-1}, \\lambda) = \\frac{1 + \\sum_{d < D} |z_d|}{1 + \\sum_{d<D} z_d^2} - \\lambda(\\sum_{d<D} z_d + 1).\n\\end{equation*}\nFrom now on let us write $\\sum$ for $\\sum_{d < D}$ below to reduce clutter. Setting $\\frac{\\partial \\mathcal{L}}{\\partial z_k} = 0$ and noting $\\frac{d|z_k|}{dz_k} = \\text{sgn}(z_k)$, we obtain\n\\begin{align*}\n    & \\frac{\\text{sgn}(z_k)(1 + \\sum z_d^2) - 2z_k(1 + \\sum |z_d|)}{(1+\\sum z_d^2)^2} - \\lambda = 0 \\\\\n    \\iff & \\text{sgn}(z_k)(1 + \\sum z_d^2) - 2z_k(1 + \\sum |z_d|) = \\lambda (1+\\sum z_d^2)^2 \\\\\n    \\iff & z_k = \\frac{\\text{sgn}(z_k)(1 + \\sum z_d^2) - \\lambda (1+\\sum z_d^2)^2}{2(1 + \\sum |z_d|)} \\\\\n    \\iff & z_k = \\frac{(\\text{sgn}(z_k) - \\lambda (1+\\sum z_d^2))(1 + \\sum z_d^2)}{2(1 + \\sum |z_d|)}\n\\end{align*}\nHence at the global maximum, $z_k$ takes one of two values $a > 0$ and $b < 0$.\nFurther we have that\n\\begin{align} \\label{eq:max_expression}\n    \\frac{1 + \\sum|z_d|}{1 + \\sum z_d^2} = \\frac{\\text{sgn}(z_k) - \\lambda (1+\\sum z_d^2)}{2z_k}\n\\end{align}\nIf both $a$ and $b$ are among the $z_k$, we have that $\\frac{1 - \\lambda (1+\\sum z_d^2)}{2a} = \\frac{- 1 - \\lambda (1+\\sum z_d^2)}{2b}$.\nSolving for $\\lambda(1+\\sum z_d^2)$ and plugging it in back to Equation \\eqref{eq:max_expression}, we get:\n\\begin{align*}\n    \\frac{1 + \\sum|z_d|}{1 + \\sum z_d^2} = \\frac{1}{a-b}\n\\end{align*}\nSince $a > 0$, $b < 0$ and $\\sum z_d = -1$, $a-b$ is minimised when only one of the $z_d$ is $a$ and the rest are $b$. Hence a crude lower bound on $a-b$ is $\\frac{1}{D-2}$, giving a bound:\n\\begin{equation} \\label{eq:second_term_inf_norm}\n    \\frac{\\left\\Vert \\mathrm{Cov}(\\mathbf{x})\\right\\Vert_{\\infty}}{\\sigma^2}\n    \\leq D(D-2)\n\\end{equation}\n\nHowever we conjecture that the true global maximum is attained when $z_d = - \\frac{1}{D-1}$ $\\forall d < D$ (i.e. all the $z_d$ for $d < D$ are equal to $b < 0$), for which it is easy to show that $\\frac{1 + \\sum_{d < D} |z_d|}{1 + \\sum_{d<D} z_d^2} = 2(D-1)/D$.\n\nPutting together the above, we have:\n\\begin{align*}\n    \\left\\Vert \\frac{\\partial \\text{LN}(\\mathbf{x})}{\\partial \\mathbf{x}} \\right\\Vert_{\\infty} &= (\\sigma^2 + \\epsilon)^{-\\frac{1}{2}} \\left\\Vert   \\text{diag}(\\boldsymbol\\gamma) - \\frac{1}{D}\\boldsymbol\\gamma \\mathds{1}^\\top - \\frac{1}{D} (\\sigma^2 + \\epsilon)^{-1}\\text{diag}(\\boldsymbol\\gamma)(\\mathbf{x} - \\mu)(\\mathbf{x} - \\mu)^\\top \\right\\Vert_{\\infty} \\\\\n    &\\leq \\epsilon^{-\\frac{1}{2}} \\bigg( \\left\\Vert \\text{diag}(\\boldsymbol\\gamma) - \\frac{1}{D}\\boldsymbol\\gamma \\mathds{1}^\\top \\right\\Vert_{\\infty} + \\frac{1}{D}\\left\\Vert  \\text{diag}(\\boldsymbol\\gamma) \\right\\Vert_{\\infty} \\left\\Vert  (\\sigma^2 + \\epsilon)^{-1}(\\mathbf{x} - \\mu)(\\mathbf{x} - \\mu)^\\top \\right\\Vert_{\\infty} \\bigg) \\\\\n    &\\leq \\epsilon^{-\\frac{1}{2}} \\bigg( \\left\\Vert \\text{diag}(\\boldsymbol\\gamma) - \\frac{1}{D}\\boldsymbol\\gamma \\mathds{1}^\\top \\right\\Vert_{\\infty} + \\frac{1}{D}\\left\\Vert  \\text{diag}(\\boldsymbol\\gamma) \\right\\Vert_{\\infty} \\left\\Vert  \\mathrm{Cov}(\\mathbf{x}) / \\sigma^2 \\right\\Vert_{\\infty} \\bigg) \\\\\n    &\\leq \\epsilon^{-\\frac{1}{2}} \\bigg( \\frac{2(D-1)}{D}\\max_d |\\gamma_d| + \\frac{1}{D} \\max_d |\\gamma_d| D(D-2) \\bigg) \\\\\n    &= \\epsilon^{-\\frac{1}{2}} \\max_d |\\gamma_d| \\bigg(\\frac{2(D-1)}{D} + D-2 \\bigg) \\\\\n    &= \\epsilon^{-\\frac{1}{2}} \\max_d |\\gamma_d| \\bigg(\\frac{D^2-2}{D}\\bigg).\n\\end{align*}\n\n\n}\n\\end{document}\n\n==== END OF /2006.04710/main.tex ====",
            "processed_original_tex": "==== BEGINNING OF /2006.04710/main.tex ====\n\\documentclass{article}\n\\usepackage{times}\n\\usepackage[utf8]{inputenc} \n\\usepackage[T1]{fontenc}    \n\\usepackage{hyperref}       \n\\usepackage{url}            \n\\usepackage{booktabs}       \n\\usepackage{amsfonts}       \n\\usepackage{nicefrac}       \n\\usepackage{microtype}      \n\\usepackage{graphicx}\n\\usepackage{amsmath}\n\\usepackage{mathtools} \n\\usepackage{amsthm}\n\\usepackage{verbatim}\n\\usepackage{dsfont}  \n\\usepackage{wrapfig}  \n\\usepackage{caption}\n\\usepackage{subcaption}\n\\usepackage{xcolor}\n\\usepackage{amsbsy}\n\\usepackage{paralist}\n\\usepackage{enumitem}\n\n\\newtheorem{theorem}{Theorem}[section]\n\\newtheorem{lemma}{Lemma}[section]\n\\theoremstyle{definition}\n\\newtheorem{definition}{Definition}[section]\n\\newtheorem{corollary}{Corollary}[section]\n\n\\DeclareMathOperator{\\Tr}{Tr}\n\\newcommand{\\spaces}{\\hspace{2mm}}\n\\DeclareMathOperator{\\softmaxOp}{softmax}\n\\newcommand{\\softmax}[1]{\\softmaxOp\\left(#1\\right)}\n\\DeclareMathOperator{\\lip}{Lip}\n\\DeclareMathAlphabet{\\pazocal}{OMS}{zplm}{m}{n}\n\\newcommand{\\unif}{\\pazocal{U}}\n\\DeclareMathOperator{\\diag}{diag}\n\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n\n\\newcommand{\\andriy}[1]{\\textcolor{blue}{[AM: #1]}}\n\\newcommand{\\hyunjik}[1]{\\textcolor{red}{[HK: #1]}}\n\\newcommand{\\george}[1]{\\textcolor{magenta}{[George: #1]}}\n\n\n\\usepackage[accepted]{icml2021}\n\n\n\n\n\n\n\\icmltitlerunning{The Lipschitz Constant of Self-Attention}\n\n\\begin{document}\n\n\\twocolumn[\n\\icmltitle{The Lipschitz Constant of Self-Attention}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\icmlsetsymbol{equal}{*}\n\n\\begin{icmlauthorlist}\n\\icmlauthor{Hyunjik Kim}{dm}\n\\icmlauthor{George Papamakarios}{dm}\n\\icmlauthor{Andriy Mnih}{dm}\n\\end{icmlauthorlist}\n\n\\icmlaffiliation{dm}{DeepMind, UK}\n\n\\icmlcorrespondingauthor{Hyunjik Kim}{hyunjikk@google.com}\n\n\n\n\n\\icmlkeywords{Machine Learning, ICML}\n\n\\vskip 0.3in\n]\n\n\n\n\n\n\n\n\n\n\n\\printAffiliationsAndNotice{} \n\n\n\\begin{abstract}\nLipschitz constants of neural networks have been explored in various contexts in deep learning, such as provable adversarial robustness, estimating Wasserstein distance, stabilising training of GANs, and formulating invertible neural networks.\nSuch works have focused on bounding the Lipschitz constant of fully connected\nor convolutional networks, composed of linear maps and pointwise non-linearities. \nIn this paper, we investigate the Lipschitz constant of\nself-attention, a non-linear neural network module widely used in sequence modelling.\nWe prove that the standard dot-product self-attention is \\emph{not} Lipschitz for unbounded input domain, and propose an alternative L2 self-attention that \\emph{is} Lipschitz. \nWe derive an upper bound on the Lipschitz constant of L2 self-attention and provide empirical evidence for its asymptotic tightness. \nTo demonstrate the practical relevance of our theoretical work, we formulate invertible self-attention and use it in a Transformer-based architecture for a character-level language modelling task.\n\\end{abstract}\n\n\\section{Introduction}\n\\label{sec:intro}\nLipschitz continuity is a strong form of continuity for functions. \nLoosely speaking, a function is \\textit{Lipschitz continuous} if changing its input by a certain amount cannot change its output by more than $K$ times that amount. \nThe constant $K$ is a hard constraint on how rapidly the function's output can vary, and the smallest such $K$ is known as the function's \\textit{Lipschitz constant}.\nFor example, \\smash{$f_1(x) = \\sqrt{|x|}$} and $f_2(x) = \\exp(x)$ for $x\\in\\mathbb{R}$ are not Lipschitz continuous, because their output can change arbitrarily fast as $x$ approaches $0$ and $+\\infty$ respectively. \nOn the other hand, $g_1(x) = \\tanh(x)$ and $g_2(x) = \\alpha x$ are Lipschitz continuous, because their rate of change (derivative) is bounded.\n\nIn deep learning, we often use Lipschitz continuity as a constraint for neural networks, to control how much a network's output can change relative to its input. \nSuch Lipschitz constraints are useful in several contexts. \nFor example, Lipschitz constraints can endow models with provable robustness against adversarial pertubations \\citep{cisse2017parseval, tsuzuku2018lipschitz, anil2019sorting}, and guaranteed generalisation bounds \\citep{sokolic2017robust}.\nMoreover, the dual form of the Wasserstein distance is defined as a supremum over Lipschitz functions with a given Lipschitz constant, hence Lipschitz-constrained networks are used for estimating Wasserstein distances \\citep{peyre2019computational}. \nFurther, Lipschitz-constrained networks can stabilise training for GANs, an example being spectral normalisation \\citep{miyato2018spectral}.\nFinally, Lipschitz-constrained networks are also used to construct invertible models and normalising flows. \nFor example, Lipschitz-constrained networks can be used as a building block for invertible residual networks and hence flow-based generative models \\citep{behrmann2018invertible, chen2019residual}.\nAdditionally, Neural ODEs \\citep{chen2018neural, grathwohl2018ffjord} are typically defined using vector fields parameterized via Lipschitz networks, so that the flow generated by the vector field is guaranteed to exist for all times.\n\nNonetheless, designing Lipschitz-continuous neural networks and computing (or even upper-bounding) their Lipschitz constant is a hard problem. \nPrevious work mostly focused on fully-connected and convolutional networks, not only because they are common in deep learning, but also because they are relatively simple to analyze, as compositions of linear maps and pointwise non-linearities. \nEven in this case however, exact evaluation of the Lipschitz constant\nof fully-connected and convolutional networks is NP-hard \\citep{virmaux2018lipschitz} and obtaining a tight upper bound remains a challenging task \\citep{virmaux2018lipschitz, fazlyab2019efficient, Latorre2020Lipschitz}.\n\nFully-connected and convolutional networks are not the only neural networks worthy of interest.\nRecently, \\textit{self-attention} \\citep{vaswani2017attention} has become a popular alternative to recurrent neural networks. Self-attention is a key component of the Transformer \\citep{vaswani2017attention}, that has found success as a building block in models of various data modalities, starting with natural-language processing \\citep{vaswani2017attention, devlin2018bert, brown2020language} and extending to computer vision \\mbox{\\citep{zhang2018self, ramachandran2019stand}}, audio generation \\citep{huang2018music}, and reinforcement learning \\citep{parisotto2019stabilizing}. However, so far no previous work has analysed the Lipschitz properties of self-attention, and thus it has been unclear whether self-attention is a viable option in applications that require Lipschitz constraints.\nIn this work, we address this gap in the theory of self-attention by providing a thorough analysis of its Lipschitz properties. In particular, we make the following contributions: \n\\begin{itemize}[leftmargin=*]\n    \\item We prove that the widely used \\textit{dot-product self-attention} is \\emph{not} Lipschitz, and therefore not suitable to use in applications requiring Lipschitz constraints.\n    \\item We formulate \\textit{L2 self-attention} as an alternative, and show that it \\emph{is} Lipschitz.\n    \\item We derive a theoretical upper bound on the Lipschitz constant of L2 self-attention, and provide empirical evidence of the asymptotic tightness of the bound.\n    \\item As a practical demonstration of the theory, we use this bound to formulate invertible self-attention, and explore its use in a Transformer architecture for character-level language modelling. We compare its test log-likelihood and stability to dot-product self-attention.\n\\end{itemize}\n\n\\section{Lipschitz Constant of Fully-Connected/Convolutional Layers}\nWe first define the notion of Lipschitz continuity, and proceed to define the Lipschitz constant.\n\\begin{definition}\\label{Lipschitz_definition}\nGiven two metric spaces $(\\mathcal{X}, d_{\\mathcal{X}})$ and $(\\mathcal{Y}, d_{\\mathcal{Y}})$, a function $f:\\mathcal{X} \\rightarrow \\mathcal{Y}$ is called \\textit{Lipschitz continuous} (or $K$-\\textit{Lipschitz}) if there exists a constant $K\\geq 0$ such that \n\\begin{equation}\nd_{\\mathcal{Y}}(f(\\mathbf{x}),f(\\mathbf{x'})) \\leq K d_{\\mathcal{X}}(\\mathbf{x},\\mathbf{x'}) \\spaces\\spaces \\text{for all } \\mathbf{x},\\mathbf{x'} \\in \\mathcal{X}.\n\\end{equation}\nThe smallest such $K$ is the \\textit{Lipschitz constant} of $f$, denoted $\\lip(f)$.\n\\end{definition}\nIn this paper, we focus on the common case where $\\mathcal{X} = \\mathbb{R}^n$, $\\mathcal{Y} = \\mathbb{R}^m$, and $d_{\\mathcal{X}}, d_{\\mathcal{Y}}$ are induced by a $p$-norm \\smash{$\\|\\mathbf{x}\\|_p \\coloneqq (\\sum_{i} |x_i|^p)^{{1}/{p}}$}. \nWe will primarily consider the cases $p=2$ and $p=\\infty$, where $\\|\\mathbf{x}\\|_{\\infty} \\coloneqq \\max_i |x_i|$.\nTo emphasise the dependence of the Lipschitz constant on the choice of $p$-norm, we will often denote it by $\\lip_p(f)$. \nIn this case, it follows directly from Definition \\ref{Lipschitz_definition} that the Lipschitz constant is given by\n\\begin{equation}\\label{eq:lipschitz_supremum_definition}\n    \\lip_p(f) = \\sup_{\\mathbf{x}\\neq \\mathbf{x'} \\in \\mathbb{R}^n} \\frac{\\|f(\\mathbf{x})-f(\\mathbf{x'})\\|_p}{\\|\\mathbf{x}-\\mathbf{x'}\\|_p}.\n\\end{equation}\nNext, we outline some basic results that are useful for estimating Lipschitz constants, also covered in related works \\citep{virmaux2018lipschitz, behrmann2018invertible}. \nWe describe how these results are used to provide bounds on the Lipschitz constant of fully-connected networks (\\verb!FCN!) and convolutional neural networks (\\verb!CNN!), using the fact that both are compositions of linear maps and pointwise non-linearities.\nTo begin with, the following theorem suggests a way to bound $\\lip_p(f)$ for a differentiable Lipschitz function $f$:\n\\begin{theorem}[\\citealp{federer1969geometric}] \\label{thm:jacobian} Let $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ be differentiable and Lipschitz continuous under a choice of $p$-norm $\\|\\cdot\\|_p$. \nLet $J_f(x)$ denote its total derivative (Jacobian) at $x$. Then $\\lip_p(f) = \\sup_{\\mathbf{x}\\in \\mathbb{R}^n} \\|J_f(\\mathbf{x})\\|_p$ where $\\|J_f(\\mathbf{x})\\|_p$ is the induced operator norm on $J_f(\\mathbf{x})$. \n\\end{theorem}\nHence if $f$ is a linear map represented by a matrix $W$ then\n\\begin{align*}\n    \\lip_p(f)&= \\|W\\|_p \\coloneqq \\sup_{\\|\\mathbf{x}\\|_p=1} \\|W\\mathbf{x}\\|_p \\\\\n    &=\n    \\begin{cases}\n        \\sigma_{\\max}(W), & \\text{if } p=2\\\\\n        \\max_i \\sum_j |W_{ij}|      & \\text{if } p = \\infty\n    \\end{cases}\n\\end{align*}\nwhere $\\|W\\|_p$ is the operator norm on matrices induced by the vector $p$-norm, and $\\sigma_{\\max}(W)$ is the largest singular value of $W$. \nUnder this choice of norm, many common non-linearities (including \\verb!relu!, \\verb!sigmoid!, \\verb!tanh!, \\verb!elu!) are $1$-Lipschitz. \n$\\|W\\|_2= \\sigma_{\\text{max}}(W)$ is usually estimated via \\textit{power iteration}; we provide details on how this is done in Appendix \\ref{apd:power_iteration}.\n\nSince we now know the Lipschitz constants of the components of both \\verb!FCN! and \\verb!CNN!, we can bound their Lipschitz constants by applying the following lemma:\n\\begin{lemma}[\\citealp{federer1969geometric}]\nLet $g,h$ be two composable Lipschitz functions. Then $g \\circ h$ is also Lipschitz with $\\lip(g \\circ h) \\leq \\lip(g) \\lip(h)$.\n\\end{lemma}\n\\begin{corollary} \\label{cor:lip_conv}\nFor a fully-connected network (\\verb!FCN!) or a convolutional neural network (\\verb!CNN!) $f=W_K \\circ \\rho_{K-1} \\circ W_{K-1} \\circ \\ldots \\circ \\rho_1 \\circ W_1$, we have $\\lip_p(f) \\leq \\prod_k \\|W_k\\|_p$ under a choice of $p$-norm with $1$-Lipschitz non-linearities $\\rho_k$.\n\\end{corollary}\nThe above bound is not necessarily tight; there are various works that compute tighter bounds for \\verb!FCN! and \\verb!CNN! \\citep[e.g.][]{virmaux2018lipschitz, fazlyab2019efficient, Latorre2020Lipschitz}.\n\n\\section{Lipschitz Constant of Self-Attention}\n\n\\subsection{Dot-product self-attention is \\emph{not} Lipschitz}\n\nMoving on, we investigate whether self-attention is Lipschitz. We first consider the widely used \\textit{(scaled) dot-product multihead self-attention}  as formulated by \\citet{vaswani2017attention}.\nLet $\\mathbf{x}_1, \\ldots, \\mathbf{x}_N$ be a sequence of $N$ elements, where $\\mathbf{x}_i \\in \\mathbb{R}^D$ for $i=1,\\ldots,N$.\nWe represent this sequence as a matrix $X$:\n\\begin{equation}\n    X = \\begin{bmatrix}\n    \\text{---} & \\mathbf{x}_1^\\top & \\text{---}\\\\\n    & \\vdots & \\\\\n    \\text{---} & \\mathbf{x}_N^\\top & \\text{---} \\\\\n    \\end{bmatrix}\\in \\mathbb{R}^{N \\times D},\n\\end{equation}\n\nDot-product multihead self-attention (\\verb!DP-MHA!) is a map from $\\mathbb{R}^{N \\times D}$ to $\\mathbb{R}^{N \\times D}$ consisting of $H$ `heads', where $H$ is chosen to divide $D$. Each head is a map from $\\mathbb{R}^{N \\times D}$ to $\\mathbb{R}^{N \\times D/H}$ defined by\n\\begin{align*} \\label{eq:dot_product_self_attention_definition}\n    \\mathit{DP}(X) & \\coloneqq \\softmax{\\frac{X W^Q (X W^K)^\\top}{\\sqrt{D/H}}} X W^V \\\\\n    &= P X W^V, \n\\end{align*}\nwhere $W^Q, W^K, W^V \\in \\mathbb{R}^{D \\times D/H}$ are learnable parameters specific to each head, and $P \\in \\mathbb{R}^{N \\times N}$ is the output of the softmax (we suppress the dependence of $P$ on $X$ to reduce clutter below). The input to the softmax is an $N\\times N$ matrix of pairwise dot products (hence \\textit{dot-product} self-attention), and the softmax is applied to each row of this matrix. Finally, the outputs of all heads are concatenated into an $N\\times D$ matrix and are right multiplied by $W^O\\in \\mathbb{R}^{D \\times D}$, thus \\verb!DP-MHA! is defined by\n\\begin{equation}\n    \\mathit{MHA}_{DP}(X) \\coloneqq \\left[\\mathit{DP}^1(X), \\ldots, \\mathit{DP}^H(X)\\right] W^O.\n\\end{equation}\nIn what follows, we will prove that $\\mathit{MHA}$ as defined above is  \\emph{not} Lipschitz, assuming that the $\\mathit{MHA}$ map is non-trivial, i.e.~$W^Q, W^K, W^V, W^O \\neq 0$. It is sufficient to show that a single head $\\mathit{DP}$ is not Lipschitz, since $\\mathit{MHA}$ is a linear combination of the outputs of each head. Also note that $P$ is a stochastic matrix, i.e.~its entries are non-negative and its rows sum to $1$.\nSince the rows of $X$ are the $\\mathbf{x}_i$'s, a linear transformation of each $\\mathbf{x}_i$ by some matrix $A$ is equivalent to right multiplication of $X$ by $A^\\top$. \nSo right multiplication of $X$ by $W^V$ is a linear map and thus Lipschitz.\nTherefore, we are interested in the mapping $f(X) = PX$; this is \\emph{not} a linear mapping because $P$ itself is a non-linear function of $X$. \nIn fact, we show that $f$ \nis \\emph{not} Lipschitz, thus proving the first main result of the paper:\n\\begin{theorem} \\label{thm:dp_not_lipschitz}\n\\verb!DP-MHA! is not Lipschitz for any vector $p$-norm $\\|\\cdot\\|_p$ with $p \\in [1, \\infty]$.\n\\end{theorem}\n\\textit{Summary of Proof}. We use Theorem \\ref{thm:jacobian}, noting that if the supremum of the norm of the Jacobian is infinite, then the mapping is not Lipschitz.\nIn particular, we show that when $\\mathbf{x}_i=\\mathbf{0}$ for some $i$, some elements of the Jacobian of $f$ grow proportionally to the sample variance of $\\mathbf{x}_{\\neq i}$, which is unbounded.\n\\begin{proof}\nWe show the proof for the case $D=H=1$ (i.e.~$X \\in \\mathbb{R}^{N \\times 1}$, a column vector, and $x_i \\in \\mathbb{R}$) for readability. See Appendix \\ref{apd:general_d} for the general case, which follows the same logic. \n\nThe mapping $f$ can be written as\n\\begin{align*}\nf(X) = PX = & \\softmax{a X X^\\top} X = \\begin{bmatrix}\n    f_1(X) \\\\\n    \\vdots \\\\\n    f_N(X)\n\\end{bmatrix} \\in \\mathbb{R}^{N \\times 1}, \\\\\n\\text{where} \\quad & f_i(X) = \\sum_{j=1}^N P_{ij}x_j \\in \\mathbb{R}\n\\end{align*}\nand $a = W^K W^Q \\in \\mathbb{R}$ (we assume $a \\neq 0$ such that self-attention is non-trivial).\nHence $f$ can be interpreted as a map of each $x_i$ to a point in the convex hull of ${x_1,...,x_N}$.\nSince $f$ is a map from $\\mathbb{R}^{N \\times 1}$ to $\\mathbb{R}^{N \\times 1}$, its Jacobian is\n\\begin{equation}\n    J_f = \\begin{bmatrix}\n    J_{11} & \\dots & J_{1N} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    J_{N1} & \\dots & J_{NN} \\\\\n    \\end{bmatrix}\\in \\mathbb{R}^{N \\times N},\n\\end{equation}\nwhere $\\smash{J_{ij} = \\frac{\\partial f_i(X)}{\\partial x_j} \\in \\mathbb{R}}$. \nBy taking partial derivatives we can show that\n\\begin{equation*}\n    J_{ij} = a X^\\top P^{(i)} \\left[E_{ji}X + \\delta_{ij}X \\right] + P_{ij}I\n\\end{equation*}\nwhere \n\\begin{itemize}\n    \\item $E_{ij} \\in \\mathbb{R}^{N \\times N}$ is a binary matrix with zeros everywhere except the $(i,j)$th entry\n    \\item $\\delta_{ij} \\in \\{0,1\\}$ is the Kronecker delta\n    \\item $P^{(i)} \\coloneqq \\diag(P_{i:}) - P_{i:}^\\top P_{i:} \\in \\mathbb{R}^{N \\times N}$.\n\\end{itemize}\nSee Appendix \\ref{apd:identities} for useful identities in deriving the above Jacobian.\n\nSo for $i=j$:\n\\begin{align}\nJ_{ii} =\n a X^\\top P^{(i)} e_{ii} X + a X^\\top P^{(i)} X + P_{ii} \\label{eq:jac_dot}\n\\end{align}\n\nLet us investigate the scalar $X^\\top P^{(i)}X$. We observe that it is in fact a variance of a discrete distribution. Specifically:\n\\begin{equation} \\label{eq:cov}\n    X^\\top P^{(i)}X  = \\textstyle\\sum_k P_{ik} x_k^2 - \\left(\\textstyle\\sum_k P_{ik}  x_k\\right)^2 = \\mathrm{Var}(\\mathbb{X}),\n\\end{equation}\nwhere $\\mathbb{X}$ is a discrete distribution with support at the inputs $\\{x_1,\\ldots,x_N \\}$ and probability mass function given by their softmax probabilities $\\mathbb{P}(\\mathbb{X}=x_j)=P_{ij}$. \nA consequence of this interpretation is that $P^{(i)}$ is \\textit{positive semi-definite} (PSD) since $X^\\top P^{(i)} X = \\mathrm{Var}(\\mathbb{X}) \\geq 0$, with equality if and only if the $x_j$ are all equal.\n\nWe use this observation to show that $J_{ii}$ is unbounded, and so $\\|J_f\\|_p$ is unbounded, hence \\verb!DP-MHA! is \\emph{not} Lipschitz.\nConsider the case $x_i=0$. Then \n\\begin{equation*}\n    P_{i:}^\\top = \\softmax{XAx_i} = \\frac{1}{N} \\mathds{1},\n\\end{equation*}\ni.e.\\ we have uniform attention regardless of $x_{ \\neq i}$. \nThe first term of $J_{ii}$ in Equation \\eqref{eq:jac_dot} disappears since $e_{ii} X = [0, \\ldots, x_i, \\ldots, 0] = \\mathbf{0}$, and the last term becomes $\\frac{1}{N} I$. Now consider the second term $a X^\\top P^{(i)}X = a \\mathrm{Var}(\\mathbb{X}_l)$. Note $\\mathbb{X}$ is uniformly distributed, since $\\mathbb{P}(\\mathbb{X}=x_j)=P_{ij}= 1/N$. Hence the second term is equal to $a$ times the sample variance of ${x_1,\\ldots,x_N}$, which can be arbitrarily large. Hence $J_{ii}$ can become arbitrarily large, so the full Jacobian $J_f$ is unbounded.\n\\end{proof}\n\\textit{High-level intuition for proof.}\nAt $x_i=0$, $f_i(X) = \\frac{1}{N} \\sum_{k} x_k$, the mean of the inputs. \nThe rate of change of $f_i$ is governed by how fast the softmax saturates when $x_i$ is perturbed, which is determined by how spread out the $x_{\\neq i}$ are. \nThe more spread out they are (the higher the sample variance), the greater the rate of saturation of the softmax, and the faster the rate of change of $f_i$.\nSince the sample variance of $x_{\\neq i}$ can be arbitrarily large, the rate of change of $f_i$ can also be arbitrarily large, i.e.~the entries of the Jacobian (and hence its $p$-norm) can become arbitrarily large. In Appendix \\ref{apd:bias}, we show that adding bias terms to $\\mathbf{x}_i^\\top W^Q$ and $\\mathbf{x}_j^\\top W^K$ does \\emph{not} resolve the issue.\n\nThe implications of this result are the following.\n\\begin{inparaenum}[(1)]\n\\item There can be undesirable behaviour (e.g.~training instabilities) for the Transformer when some inputs are close to zero and others have large magnitude.\n\\item Dot-product self-attention (and hence the standard Transformer) is not a suitable choice when we require a Lipschitz neural network, such as for formulating invertible residual networks \\citep{behrmann2018invertible}.\n\\end{inparaenum}\nTherefore, to use self-attention and Transformers in such applications, a Lipschitz formulation of self-attention is required, together with an explicit (ideally tight) upper bound to its Lipschitz constant, to quantify how much the output can change with respect to changes in the input.\n\nOne method to make dot-product self-attention Lipschitz is by ensuring its inputs are bounded. Indeed, if the input space is compact, e.g.\\ $[0,1]^{N \\times D}$, any continuously differentiable function is Lipschitz, including dot-product self-attention.\nHowever, as we further discuss in Section \\ref{sec:conclusion}, such an approach has its own challenges, since it makes the Lipschitz constant depend on the input range. Instead, in the next section we formulate a version of self-attention that is provably Lipschitz on all of $\\mathbb{R}^{N\\times D}$, allowing us to derive an upper bound that holds for any subset of $\\mathbb{R}^{N\\times D}$.\n\n\\subsection{L2 self-attention: a Lipschitz formulation of self-attention}\nThe pathology in dot-product self-attention arises because the softmax probabilities $P_{i:}$ are constant with respect to $\\mathbf{x}_{\\neq i}$ when $\\mathbf{x}_i=0$. \nThis behaviour can be undesirable as we want $P_{ij}$ to vary according to $\\mathbf{x}_j$, regardless of whether $\\mathbf{x}_i$ is zero or not.\nHence we propose an alternative form of self-attention based on L2 distance:\n\\begin{align}\\label{eq:L2_self_attention_definition}\nP_{ij} \\propto \\exp(L_{ij}) \\coloneqq \\exp\\left(-\\frac{\\norm{ \\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K}_2^2}{\\sqrt{D/H}}\\right),\n\\end{align}\nwith the normalisation constant ensuring that $\\sum_j P_{ij} = 1$. \nWe will refer to it as \\textit{L2 self-attention}. \nIt is reminiscent of the standard squared-exponential kernel, but with softmax normalisation that ensures that each row of the kernel matrix sums to $1$. \nNormalisation is usually necessary to deal with inputs of varying length $N$ \\citep{wang2018non}, hence we keep the softmax for L2 self-attention. Similarly to dot-product self-attention, L2 self-attention can be computed efficiently with matrix operations; see Appendix \\ref{apd:l2_att_computation} for details, with a comparison of wall-clock runtimes between different choices of attention. \n\nWe first state the mathematical formulation of L2 multihead self-attention (\\verb!L2-MHA!) before proving the main result --- the upper bound of its Lipschitz constant with respect to $\\|\\cdot\\|_p$ for $p=2, \\infty$. The full \\verb!L2-MHA! map $F: \\mathbb{R}^{N \\times D} \\rightarrow \\mathbb{R}^{N \\times D}$ is defined as\n\\begin{align*}\n    F(X) &\\coloneqq \\left[f^1(X)W^{V,1}, \\ldots, f^H(X)W^{V,H}\\right] W^O \\\\\n    & \\quad\\text{where}\\quad\n    f^h(X) \\coloneqq P^h X A_h.\n\\end{align*}\nIn the above, $W^{V,h} \\in \\mathbb{R}^{D \\times D/H}$, $W^O \\in \\mathbb{R}^{D \\times D}$, $P^h$ is defined as in Equation \\eqref{eq:L2_self_attention_definition} with $W^{Q,h}=W^{K,h} \\in \\mathbb{R}^{D \\times D/H}$, and $A_h \\coloneqq W^{Q,h} W^{{Q,h}^\\top} / \\sqrt{D/H} \\in \\mathbb{R}^{D \\times D}$. \nThere are two changes from the usual form of multihead self-attention:\n\\begin{enumerate}[label=(\\arabic*), leftmargin=*]\n    \\item We require $W^{Q,h} = W^{K,h}$ for each head $f^h(X)$ to be Lipschitz. In Lemma \\ref{lemma:tie_weights} of Appendix \\ref{apd:proof} we show that \\verb!L2-MHA! is \\emph{not} Lipschitz for arbitrary $W^{Q,h}$, $W^{K,h}$, and that tying $W^{Q,h} = W^{K,h}$ is sufficient for \\verb!L2-MHA! to be Lipschitz, with intuition for why tying is sufficient.\n    \\item In each head of the self-attention $f^h(X)$, right multiplication by $A_h$ has been included for the theorem below to hold (details are in the proof). In practice, there is little harm done by this extra linear transformation, since when the heads are combined together in $F$, each $f^h(X)$ is additionally transformed by $W^{V,h}$, a free parameter.\n\\end{enumerate}\n\nThe second main result of the paper is the following:\n\\begin{theorem} \\label{thm:main}\n\\verb!L2-MHA! is Lipschitz, with the following bound on $\\lip_{\\infty}(F)$:\n\\begin{align*}\n    \\lip_{\\infty}(F)  \\leq &\\left(4 \\phi^{-1}(N-1) + \\frac{1}{\\sqrt{D/H}}\\right) \\|W^{O^\\top}\\|_{\\infty} \\\\\n    &\\max_h \\|W^{Q,h}\\|_{\\infty} \\|W^{{Q,h}^\\top}\\|_{\\infty} \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty} \n\\end{align*}\nand the following bound on $\\lip_{2}(F)$:\n\\begin{align*}\n    \\lip_2(F) \\leq & \\frac{\\sqrt{N}}{\\sqrt{D/H}}\n    \\left(4 \\phi^{-1}(N-1) + 1 \\right) \\\\ \n    & \\left(\\sqrt{\\textstyle\\sum_h \\|W^{Q,h}\\|_2^2\\, \\|W^{V,h}\\|_2^2}\\right) \\|W^O\\|_2 \n\\end{align*}\nwhere $\\phi(x) \\coloneqq x\\exp(x+1)$ is an invertible univariate function on $x > 0$, and $N$ is the input sequence length.\n\nSpecifically, $\\phi^{-1}(N-1) = W_0(\\frac{N}{e})$ where $W_0$ is the Lambert $W$-function, which grows sub-logarithmically as $O(\\log N - \\log \\log N)$ \\citep{corless1996lambertw}. Hence the above bounds can be simplified to $O(\\log N)$ for $p=\\infty$ and $O(\\sqrt{N} \\log N)$ for $p=2$.\n\\end{theorem}\n\\begin{proof}\nSee Appendix \\ref{apd:proof}, which uses the key observation that $X^\\top P^{(i)}X$ is a covariance matrix (c.f.\\ Equation \\eqref{eq:cov}) to bound $\\|J_F\\|_p$, the norm of the Jacobian of $F$. Appendix \\ref{apd:masking} shows how the argument can be modified to prove the analogous result for the case with masking in the self-attention.\n\\end{proof}\n\nThese bounds are complemented by the concurrent work of \\citet{vuckovic2020attention}, which provides a $O(\\sqrt{D\\log N})$ bound on $\\lip_1(F)$ using measure-theoretic tools.\n\\section{Application: Invertible Self-Attention}\n\\subsection{Invertible residual network} \\label{sec:invertible_resnet}\n\n\nConsider the residual function $g(x) \\coloneqq \\mathbf{x} + f(\\mathbf{x})$. \\citet{behrmann2018invertible} give the following sufficient condition for its invertibility: \nif $f$ is a \\textit{contraction} with respect to some\nmetric, i.e.~if $\\lip(f) < 1$, and the metric space on which $f$ is defined is complete,\nthen $g$ is invertible. \n(A Euclidean space with a metric induced by a $p$-norm $\\|\\cdot\\|_p$ for $p \\in [1, \\infty]$ is always complete.)\nSpecifically, the inverse $g^{-1}(\\mathbf{y})$ is the unique fixed point of the recursion $\\mathbf{x}^{i+1} \\coloneqq \\mathbf{y} - f(\\mathbf{x}^i)$, since by the definition of the inverse we have $\\mathbf{y} = g^{-1}(\\mathbf{y}) + f(g^{-1}(\\mathbf{y}))$. \nBecause $f$ is a contraction, \\textit{Banach's Fixed Point Theorem} guarantees that this fixed point exists and is unique for all $\\mathbf{y}$, and that the recursion converges for all initial values $\\mathbf{x}^0$ (often set to $\\mathbf{y}$ in practice) exponentially fast. \nHence the inverse can be computed to arbitrary accuracy (up to numerical precision in practice) by the above fixed-point iteration.\n\nNote that a composition of such invertible residual blocks is also invertible. \n\\citet{behrmann2018invertible} use this observation to design invertible ResNets: they take $f$ to be a \\verb!CNN! normalised by an upper bound on $\\lip(f)$ given by Corollary \\ref{cor:lip_conv}, making the resulting function \\textit{contractive}. \nFor the $2$-norm $\\|\\cdot\\|_2$, a hyperparameter $c < 1$ is chosen and each linear map (convolution) $W$ in the \\verb!CNN! is multiplied by $c/\\|W\\|_2$ if $c < \\|W\\|_2$ where $\\|W\\|_2$ is estimated by power iteration (c.f.\\ Appendix \\ref{apd:power_iteration}).\nThis multiplicative factor determines the scale of the Lipschitz constant of the normalised function.\n\n\n\\subsection{Invertible self-attention}\n\n\\begin{wrapfigure}{r}{0.2\\textwidth}\n    \\centering\n    \\includegraphics[width=0.2\\textwidth]{transformer_block_single.pdf}\n    \\caption{Transformer block.}\n    \\label{fig:transformer_block}\n\\end{wrapfigure}\n\nThe standard use case of self-attention is with a skip connection inside the Transformer. A Transformer block is composed of residual blocks of multihead self-attention (\\verb!MHA!) and fully-connected (\\verb!FCN!) layers (Figure \\ref{fig:transformer_block}).\nHence similarly to invertible ResNets, we can normalise \\verb!L2-MHA! by the upper bounds given in Theorem \\ref{thm:main} to obtain \\verb!Contractive-L2-MHA! $f$, with which we can obtain invertible self-attention $g(\\mathbf{x}) = \\mathbf{x} + f(\\mathbf{x})$. \nSince \\verb!Dropout! is also part of the residual branch along with \\verb!Contractive-L2-MHA!, we should check that it is also contractive.\nAt test time, \\verb!Dropout! multiplies inputs by the dropout keep probability $p < 1$, so it is a contraction with Lipschitz constant $p$ at evaluation time.\nAt training time, \\verb!Dropout! amounts to setting some inputs to zero, while keeping other inputs constant. This can be expressed as right multiplication by a diagonal binary matrix $M$, and for such matrices we can verify $\\|M\\|_p \\coloneqq \\sup_{\\|x\\|_p=1} \\|Mx\\|_p  \\leq 1$.\nNotice that \\verb!LayerNorm! is not part of the residual branch, hence its Lipschitz continuity is not relevant for invertibility; rather, we can replace it with an invertible normalisation such as \\verb!ActNorm! \\cite{kingma2018glow}. \nHowever, architectures that place \\verb!LayerNorm! inside the residual branch (termed \\verb!pre-LN! as opposed to the traditional \\verb!post-LN! in Figure \\ref{fig:transformer_block}) have become more prevalent in the literature \\cite{wang2019learning, xiong2020layer}, and in this case it makes sense to investigate its Lipschitz continuity.\nWe show that \\verb!LayerNorm! is Lipschitz in Appendix \\ref{apd:layernorm}, with a bound on its Lipschitz constant.\n\nIn the next section, we investigate the properties of invertible self-attention and how it compares with the standard dot-product self-attention; we replace \\verb!DP-MHA! in the Transformer with \\verb!Contractive-L2-MHA!, hence replacing the residual self-attention module with invertible self-attention.\nWe are not interested in the modified Transformer per se, but rather in comparing the properties of invertible self-attention to standard self-attention --- we only use the Transformer as a testbed for this purpose, since self-attention is commonly used in a Transformer.\nGiven the theoretical focus of the paper, we believe that a more challenging application of invertible self-attention, such as normalising flow-based modelling, would be more suitable as a separate paper focused on that particular application.\n\n\\section{Experimental Results}\n\\subsection{Asymptotic tightness of the upper bound on $\\boldsymbol{\\lip_{\\infty}(F)}$} \\label{sec:asymptotic}\n\n\\begin{figure}[htb!]\n    \\centering\n    \\includegraphics[width=0.5\\textwidth]{jacobian_opt.pdf}\n    \\caption{Lower and upper bound on $\\lip_{\\infty}(f)$ for L2-MHA $f$, with $H=D=1$ and varying $N$.}\n    \\label{fig:jacobian_opt}\n\\end{figure}\n\nA tight bound on the Lipschitz constant of self-attention is desirable for all listed applications in Section \\ref{sec:intro}; it leads to tighter generalisation bounds, lighter constraints for provable robustness, and better expressiveness in residual flow models.\nHence we investigate the tightness of our bound on the Lipschitz constant of \\verb!L2-MHA!. \nThe Lipschitz constant is a supremum over the space of inputs $X \\in \\mathbb{R}^{N \\times D}$ (c.f.\\ Equation \\eqref{eq:lipschitz_supremum_definition}) and approximating it requires solving an intractable \noptimisation problem. \nHence it is infeasible to estimate accurately in general, especially when $X$ is high-dimensional. \nHowever, we may compute a lower bound on the Lipschitz constant by maximising the norm of the Jacobian $\\|J_f(X)\\|$ with respect to $X$ until convergence.\nThis local optimum will form a lower bound by Theorem \\ref{thm:jacobian}, and we can expect this lower bound to be fairly tight for the low-dimensional case, provided the optimisation is thorough.\n\nWe use this observation to provide empirical evidence for the asymptotic tightness of the upper bound on $\\lip_{\\infty}(f)$ in Theorem \\ref{thm:main}. \nIn Figure \\ref{fig:jacobian_opt}, we show the upper bound as well as the lower bound on $\\lip_{\\infty}(f)$ obtained by optimising $\\|J_f(X)\\|_{\\infty}$ with respect to $X$ for \\verb!L2-MHA! $f$ with 50 different random initialisations of $X$, with $H=D=1$ and $N$ varying between $100$ and $1000$. \nSee Appendix \\ref{apd:experimental_details} for further details.\nNote that we use a log-scale for the x-axis, and recall that the upper bound is $O(\\log N - \\log \\log N)$, dominated by the $O(\\log N)$ term for large $N$.\nHence the plot for the upper bound shows a linear trend.\nWe also observe that the slope of the lower bound is very similar, providing empirical evidence that the $O(\\log N - \\log \\log N)$ upper bound is asymptotically tight.\n\nThere are at least two possible explanations for the gap between the upper and lower bounds.\n\\begin{inparaenum}[(1)]\n\\item The lower bound is only a local optimum --- the true Lipschitz constant is a global optimum across inputs, which can be difficult to attain especially for high values of $N$.\n\\item The multiplicative constant of the upper bound may be loose.\n\\end{inparaenum}\nAssuming asymptotic tightness, it remains an open question whether the multiplicative constant can be tightened.\nWe show the analogous plot for $\\lip_2(F)$ and discuss the results in Appendix \\ref{apd:jacobian_opt2}.\nAdditionally in Appendix \\ref{apd:jacobian_opt_dp}, we show that optimising $\\|J_f(X)\\|_{\\infty}$ w.r.t.~$X$ for \\verb!DP-MHA! $f$ causes the norm to diverge, providing empirical verification of Theorem \\ref{thm:dp_not_lipschitz}, that \\verb!DP-MHA! is indeed \\emph{not} Lipschitz.\n\n\\subsection{Numerical invertibility of MHA residual map}\\label{sec:numerical-invertibility}\n\n\\begin{figure}[htb!]\n    \\centering\n    \\includegraphics[width=0.5\\textwidth]{mha_invertibility_small.pdf}\n    \\caption{Invertibility of $g(\\mathbf{x})= \\mathbf{x} + c f(\\mathbf{x})$ where $f$ is L2-MHA (left) and DP-MHA (right).}\n    \\label{fig:mha_invertibility_small}\n\\end{figure}\n\n\\begin{figure*}[t!] \n  \\includegraphics[width=\\textwidth]{ptb_test.pdf}\n  \\caption{Test NLL curves during training for various LSTM/Transformer models on PTB character level language modelling.} \\label{fig:ptb}\n\\end{figure*}\n\nRecall from Section \\ref{sec:invertible_resnet} that $g(\\mathbf{x}) = \\mathbf{x} + f(\\mathbf{x})$ is invertible if $f$ is contractive. \nHence if $f$ is \\verb!Contractive-L2-MHA!, $g$ is necessarily invertible. \nHowever, technically we do not disprove the invertibility of \\verb!DP-MHA!, since the converse does not hold in general i.e.~if $f$ is \\verb!DP-MHA!, which we have shown is \\emph{not} Lipschitz hence \\emph{not} contractive, it may still be the case that $g$ \\emph{is} invertible.\nTo verify that \\verb!DP-MHA! (with the skip connection) is \\emph{not} invertible in practice, we compare the numerical invertibility of the residual map  $g(\\mathbf{x})= \\mathbf{x} + c f(\\mathbf{x})$ between the cases where $f$ is \\verb!L2-MHA! and \\verb!DP-MHA! in Figure \\ref{fig:mha_invertibility_small}.\nFor each, we take \\verb!MHA! with $8$ heads and randomly initialised weights, and quantify the maximum reconstruction error across a batch of $128$ inputs whose outputs are inverted via the fixed-point iteration described in Section \\ref{sec:invertible_resnet}. We use $N=64$, \n$D=64$,\nand $c \\in \\{0.5,0.7,0.9\\}$\n(see Appendix \\ref{apd:numerical_invertibility} for analogous results for a wider range of $N$ and $D$ and for \\verb!DP-MHA! with trained weights).\nTo highlight the difference between the two types\nof self-attention, recall in the proof of Theorem \\ref{thm:dp_not_lipschitz} (showing that \\verb!DP-MHA! is not Lipschitz) that when one of the inputs $\\mathbf{x}_i$ is $0$, some terms of the Jacobian grow with the sample variance of $\\mathbf{x}_{\\neq i}$. \nHence we check numerical invertibility at a set of $N$ inputs where $\\mathbf{x}_i=0$ and $\\mathbf{x}_{\\neq i}$ are chosen uniformly at random. \n\nIn Figure \\ref{fig:mha_invertibility_small}, we see that \\verb!DP-MHA! is \\emph{not} invertible whereas \\verb!L2-MHA! \\emph{is} invertible for sufficiently small $c$.\nThis shows how not having the theoretical guarantee of $f$ being contractive can cost us invertibility in practice.\nWe note that the figure shows local invertibility at the sampled inputs, as opposed to global invertibility across the whole input space, yet this clearly highlights the difference between the two choices of self-attention.\nExperiments with the globally invertible self-attention obtained by normalising with the Lipschitz upper bound are provided in the next section.\n\n\n\\subsection{Expressiveness of L2-MHA and invertible self-attention}\n\\label{sec:invertible_self-attention}\n\n\nA natural question to ask is: how does the expressiveness of \\verb!L2-MHA! and \\verb!Contractive-L2-MHA! (that leads to invertible self-attention with the skip connection) compare with the original \\verb!DP-MHA!?\nWe expect that the Lipschitz constraint will limit the expressiveness of the Transformer, and would like to find out by how much.\nWe investigate this by comparing the performance of the original Transformer and the Transformer with invertible self-attention (c.f.\\ Figure \\ref{fig:transformer_block}) at character-level language modelling on the Penn Treebank dataset \\citep{marcus1993building}. \nWe compare the test negative log-likelihood (NLL) of a baseline LSTM, the original Transformer (\\verb!DP-MHA!), and a series of models between the original Transformer and the Transformer with invertible self-attention (\\verb!Contractive-L2-MHA!),\nmaking one change at a time and tuning the hyperparameters on a validation set.\nFor \\verb!Contractive-L2-MHA!, we normalise $F=$\\verb!L2-MHA! by the bound on $\\lip_{\\infty}(F)$ as it is tighter than the bound on $\\lip_{2}(F)$. \nDuring training we backpropagate through these contractive blocks $F/\\lip_{\\infty}(F)$ (including the denominator) to update the model parameters.\nWe found that only backpropagating through the numerator (i.e. applying \\verb!stop-gradient! to denominator) gave slightly worse performance.\nSee Appendix \\ref{apd:experimental_details} for experimental details.\n\nThe results are shown in Figure \\ref{fig:ptb}. \nThe first plot shows the best performing LSTM reaching a test NLL of around $1.0$, and the second plot shows the best performing Transformer reaching a slightly improved performance for $3$--$5$ layers of Transformer blocks. \nWe observe instabilities in training for a higher number of layers, requiring careful tuning of the learning rate schedule for stability at the cost of performance, a commonly observed phenomenon in the literature of deep Transformer architectures \\mbox{\\citep{bapna2018training, parisotto2019stabilizing}}.\nThe third plot shows results for the Transformer with \\verb!DP-MHA! replaced with \\verb!L2-MHA! but without tying $W^Q$ and $W^K$, and we observe a very similar test performance. \nThe fourth plot shows the change when we further tie the query and key weights (making $W^Q=W^K$); we see that there is a small degradation in performance.\nHere the number of trainable parameters has been reduced, but in Appendix \\ref{apd:wq_wk_experiment} we show that matching parameter count does not help performance, suggesting that the reduction in performance when tying queries and keys is not solely due to having fewer parameters.\nWe note that performance saturates at around $5$ layers for each Transformer model so far.\nOn the rightmost plot we show results when further dividing self-attention in each block by the upper bound on $\\lip_{\\infty}(F)$, to obtain invertible self-attention. \nThis does give reduced performance for the same number of layers, but we can attain similar performance with more layers, no longer saturating at $5$ layers.\n\n\\begin{table*}[!htb]\n\\centering\n \\begin{tabular}{|c||c|c|c|c|c|c|c|c|c|} \n \\hline\n  Number of Layers & 2 & 4 & 6 & 8 & 10 & 12 & 14 & 16 & 18 \\\\\n \\hline \\hline\n Transformer (\\textbf{DP}) & 1.061 & 1.032 & 1.021 & \\textbf{1.017}  & 1.025 & - & - & - & - \\\\ \n \\hline\nTransformer (\\textbf{L2}), $W^Q=W^K$  & 1.168 & 1.040 & 1.023 & 1.024 & 1.019 & \\textbf{1.008}  & 1.018 & 1.027 & 1.034 \\\\\n \\hline\nTransformer (\\textbf{Contractive-L2}) & 1.246 & 1.135 & 1.103 & 1.079 & 1.072 & 1.060 & 1.039 & \\textbf{1.029} & 1.031 \\\\\n \\hline\n\\end{tabular}\n\\caption{Test NLL for Transformer models trained with \\textbf{fixed learning rate} on PTB character level language modelling.} \\label{tab:ptb_fixed}\n\\end{table*}\n\nThus we conclude the following. \\begin{inparaenum}[(1)]\n\\item Replacing the dot-product with the L2 distance incurs hardly any loss in expressiveness.\n\\item Tying the query and key weights to obtain Lipschitz self-attention incurs a small loss in expressiveness.\n\\item Dividing by the upper bound on $\\lip_{\\infty}(F)$ to obtain invertible self-attention incurs a noticeable loss in expressiveness, but also has a stabilising effect on the optimisation of the Transformer, thus allowing one to compensate for the apparent loss in expressiveness by increasing the number of layers. \n\\end{inparaenum}\n\n\\subsection{Training Stability of DP-MHA vs L2-MHA}\n\\label{sec:stability}\n\nIn Figure \\ref{fig:mha_output_variance}, we compare the output variance of trained \\verb!L2-MHA! against trained \\verb!DP-MHA!, with weights from the one-layer Transformer (L2), $W^Q=W^K$ model and (DP) model used for Figure \\ref{fig:ptb} respectively. We take the same distribution of inputs as used for the numerical invertibility experiment in Section \\ref{sec:numerical-invertibility}, and show the histogram of inputs and outputs after flattening the input/output tensors. We see that the range of outputs remains similar to the range of inputs for Lipschitz \\verb!L2-MHA!, whereas for \\verb!DP-MHA! the outputs have a much wider range, because the Jacobian norm is large for \\verb!DP-MHA! at these inputs. \n\n\\begin{figure}[!htb]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{mha_output_variance.pdf}\n    \\caption{Histogram showing distribution of inputs/outputs of trained L2-MHA and DP-MHA}\n    \\label{fig:mha_output_variance}\n\\end{figure}\n\nIn practice, this leads to instabilities in training for \\verb!DP-MHA!, hence requiring careful tuning of the learning rate schedule for training deeper Transformer models: linear warmup and square root decay, as detailed in Appendix \\ref{apd:experimental_details}. \nWe investigate the behaviour of the different Transformer models on the above PTB task when using a \\textbf{fixed learning rate}.\nWe observe that \\verb!DP-MHA! fails to train at all beyond 10 layers, whereas both \\verb!L2-MHA! ($W^Q=W^K$) (i.e. Lipschitz L2-MHA but not contractive) and \\verb!Contractive-L2-MHA! shows stable training for up to 18 layers (see Appendix \\ref{apd:stability} for the training curves). \nThis was the deepest model we could fit on a single GPU, and we expect to be able to train even deeper models with these two. \nIn Table \\ref{tab:ptb_fixed} we show the best Test NLL across training for each of the Transformer models. Note that for \\verb!DP-MHA! training becomes unstable beyond 10 layers, so we are only able to provide results up to 10 layers. The generalisation performance of the best model for each setting of self-attention is similar.\n\n\\section{Conclusion and Discussion}\n\\label{sec:conclusion}\nWe have shown that the widely used dot-product self-attention is \\emph{not} Lipschitz, and that the proposed L2 self-attention \\emph{is} Lipschitz, by deriving an $O(\\log N - \\log \\log N)$ Lipschitz bound for $p=\\infty$ and an $O(\\sqrt{N} (\\log N - \\log \\log N))$ bound for $p=2$, where $N$ is the input sequence length. \nWe also provided empirical evidence of the asymptotic tightness of the bound for $p=\\infty$.\nWe demonstrated that Lipschitz-constrained self-attention can be used to formulate invertible self-attention, which we experimentally evaluated on a character-level language modelling task.\nAnd finally, we also showed that L2-MHA is more stable during training, allowing the use of fixed learning rate for stable training of deep architectures.\n\nOur approach to Lipschitz self-attention has been to replace the dot-product kernel with an L2 kernel. An alternative would be to constrain the inputs of self-attention to be bounded; if the input space is compact, e.g.\\ $[0,1]^{N \\times D}$, \\emph{any} continuously differentiable function is Lipschitz, including dot-product self-attention. However, while being simple to implement, this solution has its own difficulties.\nFirst, it makes the Lipschitz constant depend on the range of the input, and thus obtaining a tight bound would require non-trivial mathematical work.\nWe stress that a guarantee that the function is Lipschitz does not tell us anything about its Lipschitz constant; without a tight Lipschitz bound, the true Lipschitz constant can be very large, at which point it is unhelpful that the function is Lipschitz.\nSecond, since self-attention is typically applied at multiple layers within a model (e.g.\\ Transformer), the input to each self-attention will live in a different compact set that depends on the parameters of the previous layers, complicating the analysis for subsequent layers. \nA solution is to constrain the inputs of each layer to be in the same compact set, e.g.\\ by passing them through a sigmoid non-linearity. \nThis however can have undesirable side effects such as vanishing gradients when the sigmoids are saturated.\nDespite these difficulties, this could be a worthwhile alternative route for obtaining Lipschitz self-attention to explore in the future.\n\nHaving a provably Lipschitz self-attention module at our disposal makes it possible to use Transformer-based architectures in applications requiring Lipschitz constraints, while enjoying theoretical guarantees.\nA natural application of Lipschitz self-attention is for residual flows \\mbox{\\citep{behrmann2018invertible}}, and for parameterising Neural ODEs \\citep{chen2018neural} where a Lipschitz vector field guarantees the existence of a unique solution to the ODE for all times. \nThese models can be used for density estimation and generative modelling of sets.\nAnother interesting direction for future work would be to analyse different variants of self-attention based on kernels other than dot-product and L2, as  \\cite{tsai2019transformer} do from an experimental perspective, for which we believe the mathematical tools developed in this paper may aid the analysis.\n\n\n\\section*{Acknowledgements}\nWe would like to thank Adam Kosiorek, Arnaud Doucet, Yee Whye Teh, Michalis Titsias, Emilien Dupont and Theophane Weber for helpful discussion and feedback.\n\n\\bibliographystyle{icml2021}\n\\bibliography{refs}\n\n\n\\newpage\n{\n\\appendix\n\\onecolumn\n{\\Large \\bf Appendix}\n\\section{Useful Identities for deriving Jacobian expressions}\n\\label{apd:identities}\nIn this section, we list some useful identities for deriving the Jacobians of the expressions in the paper.\n\nSuppose $\\lambda$ is a scalar, $\\mathbf{u},\\mathbf{v},\\mathbf{x} \\in \\mathbb{R}^{n \\times 1}$ are column vectors, and $f(\\mathbf{u})$ is a vector valued function. We use the standard convention that for $\\mathbf{a} \\in \\mathbb{R}^m$, $\\mathbf{b} \\in \\mathbb{R}^n$, we have $\\frac{\\partial{\\mathbf{a}}}{\\partial{\\mathbf{b}}} \\in \\mathbb{R}^{m \\times n}$. Then we have the following chain rule identities:\n\\begin{itemize}\n    \\item $\\frac{\\partial}{\\partial \\mathbf{x}}[\\lambda \\mathbf{u}] = \\lambda \\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}} + \\mathbf{x} \\frac{\\partial \\lambda}{\\partial \\mathbf{x}}$\n    \\item $\\frac{\\partial f(\\mathbf{u})}{\\partial \\mathbf{x}} = \\frac{\\partial f(\\mathbf{u})}{\\partial \\mathbf{u}} \\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}}$\n    \\item $\\frac{\\partial}{\\partial \\mathbf{x}}[\\mathbf{u}^\\top \\mathbf{v}] = \\mathbf{u}^\\top \\frac{\\partial \\mathbf{v}}{\\partial \\mathbf{x}} + \\mathbf{v}^\\top \\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}}$\n\\end{itemize}\nNote $\\frac{\\partial \\lambda}{\\partial \\mathbf{x}}$ is a row vector, so $\\mathbf{u}\\frac{\\partial \\lambda}{\\partial \\mathbf{x}}$ is a matrix.\n\nThe Jacobian of the softmax is also well-known. Suppose $\\mathbf{v} = \\softmax{\\mathbf{u}} \\in \\mathbb{R}^{n \\times 1}$. Then\n\\begin{equation*}\n    \\frac{\\partial \\mathbf{v}}{\\partial \\mathbf{u}} = \\diag(\\mathbf{v}) - \\mathbf{v} \\mathbf{v}^\\top \n    = \\begin{bmatrix}\n    v_1 (1-v_1) & - v_1 v_2 & \\ldots & - v_1 v_n \\\\\n    - v_2 v_1  & v_2 (1-v_2) & \\ldots & -v_2 v_n \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    -v_n v_1 & -v_n v_2 & \\ldots & v_n(1-v_n)\n    \\end{bmatrix}.\n\\end{equation*}\n\n\\section{Power Iteration} \\label{apd:power_iteration}\nAlthough $\\|W\\|_{\\infty}$ can be computed efficiently in $O(nm)$ time for $W \\in \\mathbb{R}^{m \\times n}$, na{\\\"i}vely computing $\\|W\\|_2= \\sigma_{\\text{max}}(W) \\coloneqq \\sqrt{\\lambda_{\\text{max}}(W^\\top W)}$ requires $O(n^3)$ operations. (By $\\lambda_{\\text{max}}(A)$ we denote the greatest eigenvalue of a symmetric matrix $A$.) We can however obtain an underestimate $\\tilde{\\sigma}(W)$ via \\textit{power iteration}:\n\\begin{equation} \\label{eq:sigma_tilde}\n    b_{k+1} = \\frac{W^\\top W b_k}{\\|W^\\top W b_k\\|_2},\n    \\quad \\tilde{\\sigma}_k(W) = \\sqrt{\\frac{b_k^\\top W^\\top W  b_k}{b_k^\\top b_k}}, \n\\end{equation}\nwith each iteration taking $O(n^2)$ time. Then using $K\\ll n$ iterations gives us an underestimate $\\tilde{\\sigma}_K$ in $O(Kn^2)$ time.\nSince this is an underestimate, the resulting approximation to the Lipschitz constant of the linear map will not be an upper bound. \nHowever the number of power iterations is usually chosen so that $\\tilde{\\sigma}$ is accurate enough --- $K=5$ is shown to be sufficient in the context of fully connected networks or convolutions considered by \\citet{behrmann2018invertible}.\n\nThe iteration will converge if $W^\\top W$ has an eigenvalue that is strictly greater in magnitude than its other eigenvalues, and the starting vector $b_0$ has a nonzero component in the direction of an eigenvector associated with the dominant eigenvalue. \nThis happens with probability $1$ if $b_0$ is chosen at random, and the convergence is geometric with ratio $|\\lambda_2/\\lambda_{\\max}|$ where $\\lambda_2$ is the eigenvalue with second largest magnitude \\citep{mises1929praktische}.\n\n\\newtheorem{innercustomthm}{Theorem}\n\\newenvironment{customthm}[1]\n  {\\renewcommand\\theinnercustomthm{#1}\\innercustomthm}\n  {\\endinnercustomthm}\n\n\\section{Proof of Theorem 3.1 for General $D$} \\label{apd:general_d}\n\\begin{customthm}{3.1}\n\\verb!DP-MHA! is not Lipschitz for any vector $p$-norm $\\|\\cdot\\|_p$ with $p \\in [1, \\infty]$.\n\\end{customthm}\n\\vspace{-4mm}\n\\begin{proof}\nThe mapping $f$ can be written as\n\\vspace{-5mm}\n\\begin{equation}\nf(X) = PX = \\softmax{X A^\\top X^\\top} X = \\begin{bmatrix}\n    f_1(X)^\\top \\\\\n    \\vdots \\\\\n    f_N(X)^\\top\n\\end{bmatrix} \\in \\mathbb{R}^{N \\times D},\n\\end{equation}\nwhere $A = W^K W^{Q^\\top} / \\sqrt{D/H} \\in \\mathbb{R}^{D \\times D}$ and\n$f_i(X) = \\sum_{j=1}^N P_{ij}\\mathbf{x}_j$ with $P_{i:}^\\top = \\softmax{XA\\mathbf{x}_i}$.\nHence $f$ can be interpreted as a map of each $\\mathbf{x}_i$ to a point in the convex hull of ${\\mathbf{x}_1,...,\\mathbf{x}_N}$.\nSince $f$ is a map from $\\mathbb{R}^{N \\times D}$ to $\\mathbb{R}^{N \\times D}$, its Jacobian is\n\\begin{equation}\n    J_f = \\begin{bmatrix}\n    J_{11} & \\dots & J_{1N} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    J_{N1} & \\dots & J_{NN} \\\\\n    \\end{bmatrix}\\in \\mathbb{R}^{ND \\times ND},\n\\end{equation}\nwhere $J_{ij} = \\frac{\\partial f_i(X)}{\\partial \\mathbf{x}_j} \\in \\mathbb{R}^{D \\times D}$. \nBy taking partial derivatives we can show that $J_{ij} = X^\\top P^{(i)} \\left[E_{ji}XA^\\top + XA\\delta_{ij}\\right] + P_{ij}I$\nwhere $E_{ij} \\in \\mathbb{R}^{N \\times N}$ is a binary matrix with zeros everywhere except the $(i,j)$th entry, $\\delta_{ij}$ is the Kronecker delta, and $P^{(i)} \\coloneqq \\diag(P_{i:}) - P_{i:}^\\top P_{i:}$.\nSo for $i=j$:\n\\begin{align}\nJ_{ii} &=X^\\top P^{(i)}E_{ii}XA^\\top + X^\\top P^{(i)}XA + P_{ii}I \\nonumber \\\\\n&= P_{ii}\\left(\\mathbf{x}_i - \\textstyle\\sum_k P_{ik} \\mathbf{x}_k\\right)\\mathbf{x}_i^\\top A^\\top + X^\\top P^{(i)}XA + P_{ii}I. \\label{eq:jac_dot_general}\n\\end{align}\nFor the last equality, note $E_{ii}X$ has all rows equal to zero except for the $i$th row given by $\\mathbf{x}_i^\\top$. We can then verify that $X^\\top P^{(i)}E_{ii}X$ simplifies to $P_{ii}(\\mathbf{x}_i - \\sum_k P_{ik} \\mathbf{x}_k)\\mathbf{x}_i^\\top$.\n\nFor vector $p$-norms, $\\|J_f\\|_p$ is bounded if and only if its entries are bounded, by definition of the operator norm. \nThe entries of $X^\\top P^{(i)}XA$ are bounded for arbitrary $A$ only if the entries of $X^\\top P^{(i)}X$ are bounded.\nSo let us investigate the entries of this $D\\times D$ matrix. \nWriting out each term of the matrix, we observe that it is in fact a covariance matrix of a discrete distribution. Specifically:\n\\begin{equation} \\label{eq:cov_general}\n    [X^\\top P^{(i)}X]_{lm}  = \\textstyle\\sum_k P_{ik} x_{kl} x_{km} - \\left(\\textstyle\\sum_k P_{ik}  x_{kl}\\right)\\left(\\textstyle\\sum_k P_{ik} x_{km}\\right) = \\mathrm{Cov}(\\mathbb{X}_l,\\mathbb{X}_m),\n\\end{equation}\nwhere $\\mathbb{X}$ is a discrete distribution with support at the inputs $\\{\\mathbf{x}_1,\\ldots,\\mathbf{x}_N \\}$ and probability mass function given by their softmax probabilities $\\mathbb{P}(\\mathbb{X}=\\mathbf{x}_j)=P_{ij}$. \nA consequence of this interpretation is that $P^{(i)}$ is \\textit{positive semi-definite} (PSD) since for $D=1$, Equation \\eqref{eq:cov_general} becomes $X^\\top P^{(i)} X = \\mathrm{Var}(\\mathbb{X}) \\geq 0$, with equality if and only if the $\\mathbf{x}_j$ are all equal.\n\nWe use this observation to show that the terms of $J_{ii}$ are unbounded, and so \\verb!DP-MHA! is \\emph{not} Lipschitz.\nConsider the case $\\mathbf{x}_i=0$. Then $P_{i:}^\\top = \\softmax{XA\\mathbf{x}_i} = \\frac{1}{N} \\mathds{1}$, i.e.\\ we have uniform attention regardless of $\\mathbf{x}_{ \\neq i}$. \nThe first term of $J_{ii}$ in Equation \\eqref{eq:jac_dot_general} disappears since $\\mathbf{x}_i=\\mathbf{0}$, and the last term becomes $\\frac{1}{N} I$. For the second term, the entries $[X^\\top P^{(i)}X]_{ll} = \\mathrm{Var}(\\mathbb{X}_l)$ are unbounded since the latter is equal to the sample variance of ${x_{1l},\\ldots,x_{Nl}}$, which can be arbitrarily large.\n\nNote that we have shown that single head dot-product self-atttention ($H=1$) is not Lipschitz, but it is clear that this implies multihead self-attention \\verb!DP-MHA! is also not Lipschitz, since the output of multihead attention is a linear combination of the outputs of each head.\n\\end{proof}\n\n\\section{Bias term in DP Self-Attention} \n\\label{apd:bias}\nA natural question to ask is whether we can add bias terms $b^Q$ to $\\mathbf{x}_i^\\top W^Q$ and $\\mathbf{b}^K$ to $\\mathbf{x}_j^\\top W^K$ to resolve the issue of attention weights $P_{i:}$ becoming uniform when $\\mathbf{x}_i=0$. \nThe answer is \\emph{no} in general. \nIt can again be shown that $J_{ii}$ is unbounded when $\\mathbf{x}_i$ is chosen such that \\smash{$\\mathbf{x}_i^\\top W^Q + \\mathbf{b}^Q=0$} (such a choice is possible assuming $W^Q$ is full rank, a dense set in \\smash{$\\mathbb{R}^{D \\times D/H}$}). Then \\smash{$P_{i:}^\\top=\\frac{1}{N}\\mathds{1}$} again, and the diagonal entries of \\smash{$X^\\top P^{(i)}X$} are unbounded.\n\n\\section{Efficient Computation of L2 Self-Attention} \\label{apd:l2_att_computation}\nDot-product self-attention only requires a few matrix multiplications to compute the logits (i.e.~the inputs to the softmax) between all pairs of inputs, without having to loop over pairs, hence it can be computed efficiently. \nSimilarly, we can show that L2 self-attention can also be computed in an efficient manner. \nUsing the identity $\\|a-b\\|_2^2 = \\|a\\|_2^2 - 2a^\\top b + \\|b\\|_2^2$ we can compute the logits of L2 attention between all pairs via matrix multiplications and computation of row-wise L2 norms, with negligible overhead compared to dot-product self-attention.\nSpecifically, for L2 self-attention we can show that\n\\begin{gather}\nP = \\softmax{-\\frac{\\|XW^Q\\|_{\\text{row}}^2\\mathds{1}^\\top - 2XW^Q (XW^K)^\\top  + \\mathds{1} \\|XW^K\\|_{\\text{row}}^{2\\top}}{\\sqrt{D/H}}}, \\label{eq:L2_W}\n\\end{gather}\nwhere $\\|A\\|_{\\text{row}}^2$ applies the squared L2 norm to each row of $A$, \nso if $A \\in \\mathbb{R}^{m \\times n}$ then $\\|A\\|_{\\text{row}}^2 \\in \\mathbb{R}^m$.\n\nIn Table \\ref{tab:time} we show the wall-clock training times for the Transformer models with different attention functions and a varying number of layers. It is evident that the differences between the models are rather small.\n\\begin{table}[!htb] \n\\centering\n \\begin{tabular}{|c||c|c|c|c|c|} \n \\hline\n  & 1 Layer & 2 Layers & 3 Layers & 4 Layers & 5 Layers \\\\\n \\hline \\hline\n Transformer \\textbf{(DP)} & 37 & 56 & 77 & 92 & 110 \\\\ \n \\hline\nTransformer \\textbf{(L2)} & 35 & 56 & 73 & 99 & 115 \\\\\n \\hline\nTransformer, $W^Q=W^K$ \\textbf{(L2)} & 39 & 58 & 79 & 91 & 108 \\\\\n \\hline\nTransformer,  \\textbf{(Contractive-L2)} & 37 & 60 & 81 & 102 & 127 \\\\\n \\hline\n\\end{tabular}\n\\caption{Wall clock training times for one epoch of training (seconds)} \\label{tab:time}\n\\end{table}\n\n\n\\section{Proof of Theorem \\ref{thm:main}} \\label{apd:proof}\nRecall the formulation of \\verb!L2-MHA!:\n\\begin{align*}\n    F&: \\mathbb{R}^{N \\times D} \\rightarrow \\mathbb{R}^{N \\times D} \\\\\n    F(X) &= \\left[f^1(X)W^{V,1}, \\ldots, f^H(X)W^{V,H}\\right] W^O \\\\\n    f^h(X) &= P^h X A_h \\\\\n    P^h_{ij} \\propto \\exp(L_{ij}) &\\coloneqq \\exp\\left(-\\frac{\\| \\mathbf{x}_i^\\top W^{Q,h} - \\mathbf{x}_j^\\top W^{K,h}\\|_2^2}{\\sqrt{D/H}}\\right), \\spaces \\sum_j P^h_{ij} = 1\n\\end{align*}\nwhere we have that $W^{Q,h}, W^{K,h}, W^{V,h} \\in \\mathbb{R}^{D \\times D/H}$, $W^O \\in \\mathbb{R}^{D \\times D}$, $P^h \\in \\mathbb{R}^{N \\times N}$ and $A_h \\coloneqq W^{Q,h} W^{{Q,h}^\\top} / \\sqrt{D/H} \\in \\mathbb{R}^{D \\times D}$, and the softmax is applied to each row of the input matrix. \nRecall Equation \\eqref{eq:L2_W}:\n\\begin{equation*}\nP^h = \\softmax{-\\frac{\\|XW^{Q,h}\\|_{\\text{row}}^2\\mathds{1}^\\top - 2XW^{Q,h} (XW^{K,h})^\\top  + \\mathds{1} \\|XW^{K,h}\\|_{\\text{row}}^{2^\\top}}{\\sqrt{D/H}}}.\n\\end{equation*}\n\n\\subsection{L2 self-attention is \\emph{not} Lipschitz for general $\\boldsymbol{W^Q, W^K}$}\nLet us first look at the case of $H=1$ and suppress the index $h$ to reduce clutter. \nConsider the map $\\tilde{f}(X) \\coloneqq PX$, so $f(X)=\\tilde{f}(X)A$.\nWe need $\\tilde{f}$ to be Lipschitz for $f$ and hence $F$ to be Lipschitz.\nNote that $P$ is defined as:\n\\begin{equation*}\nP_{ij} \\propto \\exp(L_{ij}) \\coloneqq \\exp\\left(-\\frac{\\| \\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K\\|_2^2}{\\sqrt{D/H}}\\right)\n\\end{equation*}\nand the normalisation constant satisfies $\\sum_j P_{ij} = 1$, for $P \\in \\mathbb{R}^{N \\times N}$, $X \\in \\mathbb{R}^{N \\times D}$.\n\nFor L2 self-attention, we may take partial derivatives and use the chain rule to show that the Jacobian of $\\tilde{f}$ is:\n\\begin{equation}\n    J_{\\tilde{f}} = \\begin{bmatrix}\n    \\tilde{J}_{11} & \\dots & \\tilde{J}_{1N} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    \\tilde{J}_{N1} & \\dots & \\tilde{J}_{NN} \\\\\n    \\end{bmatrix} \\in \\mathbb{R}^{ND \\times ND}\n\\end{equation}\nwith\n\\begin{equation}\n    \\tilde{J}_{ij} = X^\\top P^{(i)} \\frac{\\partial L_{i:}}{\\partial x_j} + P_{ij} I \\in \\mathbb{R}^{D \\times D}\n\\end{equation}\nwhere\n\\begin{equation}\n    \\frac{\\partial L_{i:}}{\\partial \\mathbf{x}_j} = \\frac{2}{\\sqrt{D/H}}\\left[\\left(XW^K - \\mathds{1} \\mathbf{x}_i^\\top W^Q \\right)W^{Q^\\top} \\delta_{ij} + \\left(E_{ji}XW^Q - E_{jj}XW^K\\right)W^{K^\\top}\\right]\n\\end{equation}\nand\n\\begin{equation*}\n    P^{(i)}  \\coloneqq \\diag(P_{i:}) - P_{i:}^\\top P_{i:} =\n    \\begin{bmatrix}\n    P_{i1}(1-P_{i1}) & -P_{i1} P_{i2} & \\dots  & - P_{i1} P_{iN}  \\\\\n    - P_{i2} P_{i1} & P_{i2}(1-P_{i2}) & \\dots  & - P_{i2} P_{iN} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    - P_{iN}P_{i1} & - P_{iN}P_{i2} & \\dots  & P_{iN}(1-P_{iN}) \n    \\end{bmatrix},\n\\end{equation*}\n\\begin{equation*}\n    P_{ij} = \\frac{\\exp\\left(-\\|\\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K\\|_2^2\\right)}{\\sum_k \\exp\\left(-\\|\\mathbf{x}_i^\\top W^Q - \\mathbf{x}_k^\\top W^K\\|_2^2\\right)}.\n\\end{equation*}\nRecall that $E_{ji} \\in \\mathbb{R}^{N \\times N}$ is a binary matrix with zeros everywhere except the $(j,i)$th entry. Hence $E_{ji}X$ has all rows equal to zero except for the $j$th row given by $\\mathbf{x}_i^\\top$. We can then verify:\n\\begin{equation}\n    X^\\top P^{(i)} E_{ji} X = P_{ij}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k) \\mathbf{x}_i^\\top.\n\\end{equation}\nAlso note $P^{(i)}$ is symmetric, and each row/colum sums to $0$, i.e.~$P^{(i)} \\mathds{1} = \\mathds{1}^\\top P^{(i)} = 0$.\nHence we may simplify the Jacobian terms as follows:\n\\begin{align}\n    \\tilde{J}_{ii} &= \\frac{2}{\\sqrt{D/H}}\\left[X^\\top P^{(i)}(XW^K - \\mathds{1}\\mathbf{x}_i^TW^Q)W^{Q^\\top} + X^\\top P^{(i)}E_{ii}X(W^Q-W^K)W^{K^\\top}\\right] + P_{ii} I \\nonumber \\\\\n    &= \\frac{2}{\\sqrt{D/H}}\\left[X^\\top P^{(i)}(XW^K - \\mathds{1}\\mathbf{x}_i^TW^Q)W^{Q^\\top} + P_{ii}(\\mathbf{x}_i - \\sum_k P_{ik} \\mathbf{x}_k)\\mathbf{x}_i^\\top(W^Q-W^K)W^{K^\\top}\\right] + P_{ii} I  \\nonumber \\\\\n    &= \\frac{2}{\\sqrt{D/H}}\\left[X^\\top P^{(i)}XW^K W^{Q^\\top} + P_{ii}(\\mathbf{x}_i - \\sum_k P_{ik} \\mathbf{x}_k)\\mathbf{x}_i^\\top(W^Q-W^K)W^{K^\\top}\\right] + P_{ii} I, \\label{eq:jii}\n\\end{align}\nand for $i\\neq j$:\n\\begin{align}\n    \\tilde{J}_{ij} &= \\frac{2}{\\sqrt{D/H}}X^\\top P^{(i)}(E_{ij}XW^Q - E_{jj}XW^K)W^{K^\\top} + P_{ij} I \\nonumber \\\\\n    &= \\frac{2}{\\sqrt{D/H}} P_{ij}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k)(\\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K)W^{K^\\top} + P_{ij} I. \\label{eq:jij}\n\\end{align}\n\nWe are now ready to show that $\\tilde{f}$ is \\emph{not} Lipschitz for general $W^Q, W^K$:\n\\begin{lemma} \\label{lemma:tie_weights}\nIf $W^K \\in \\mathbb{R}^{D \\times D/H}$ is full rank (i.e.~full column rank), and $W^K \\neq W^Q$, then $J_{ij}$ has terms that are unbounded for $i \\neq j$, hence $\\tilde{f}$ is \\emph{not} Lipschitz. \n\\end{lemma}\n\\begin{proof}\n\nLet us investigate the expression $\\tilde{K}_{ij} \\coloneqq P_{ij} W^{K^\\top}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k)(\\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K) \\in \\mathbb{R}^{\\frac{D}{H} \\times \\frac{D}{H}}$ for $i\\neq j$, which is related to $\\tilde{J}_{ij}$ as follows by Equation \\eqref{eq:jij}:\n\n\\begin{equation*}\n    W^{K^\\top} \\tilde{J}_{ij} = \\left(\\frac{2}{\\sqrt{D/H}} \\tilde{K}_{ij} + P_{ij}I \\right)\n    W^{K^\\top}.\n\\end{equation*}\n\nIt suffices to show that $\\tilde{K}_{ij}$ is unbounded to show that $\\tilde{J}_{ij}$ is unbounded, since $W^K$ is full rank and $P_{ij} \\in [0,1]$. \n\nLet $\\mathbf{y}_j^\\top = \\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K$. \nThen we have:\n\\begin{align*}\n    \\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k \n    &= W^{Q^\\top}\\mathbf{x}_i - W^{K^\\top}\\mathbf{x}_j - \\sum_k P_{ik} (W^{Q^\\top}\\mathbf{x}_i - W^{K^\\top}\\mathbf{x}_k)\\\\\n    &= W^{Q^\\top}\\mathbf{x}_i - W^{K^\\top}\\mathbf{x}_j - (W^{Q^\\top}\\mathbf{x}_i - \\sum_k P_{ik} W^{K^\\top}\\mathbf{x}_k) \\\\\n    &= - W^{K^\\top}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k).\n\\end{align*}\nHence $\\tilde{K}_{ij} = - P_{ij} (\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k) \\mathbf{y}_j^\\top$.\nNote $\\mathbf{y}_i$ can take an arbitrary value in $\\mathbb{R}^{D/H}$, since $W^K \\neq W^Q$ and $W^K$ is full-rank.\n\nFor all $j \\neq i$, let us choose $\\mathbf{x}_j$ such that $\\mathbf{y}_j = -\\mathbf{y}_i$. This is possible for any value of $\\mathbf{y}_i$ since $W^K$ is full-rank.\nNote $\\mathbf{y}_j = - \\mathbf{y}_i$ and not $\\mathbf{y}_i$.\nWe then have that $\\|\\mathbf{y}_j\\|_2^2$ is equal for all $j$, hence $P_{ij} \\coloneqq \\frac{\\exp(-\\|\\mathbf{y}_j\\|_2^2)}{\\sum_k \\exp(-\\|\\mathbf{y}_k\\|_2^2)} = \\frac{1}{N}$ for all $j$. \nThen for $i \\neq j$, $\\tilde{K}_{ij}$ simplifies to\n\\begin{equation*}\n\\tilde{K}_{ij} = - \\frac{1}{N} \\left(-\\mathbf{y}_i - \\frac{1}{N} (N-2) (-\\mathbf{y}_i)\\right) (-\\mathbf{y}_i)^\\top  = - \\frac{2N-2}{N^2} \\mathbf{y}_i \\mathbf{y}_i^\\top \n\\end{equation*}\nwhose entries are unbounded since $\\mathbf{y}_i$ can be any vector in $\\mathbb{R}^{D/H}$ (note we assume $N \\geq 2$ for self-attention to be well-defined, hence $2N-2 \\neq 0$).\n\\end{proof}\n\nThe intuition for this result is as follows: a reason for \\verb!DP-MHA! not being Lipschitz is that for $\\mathbf{x}_i=0$,, the attention weights $P_{ij}$ become uniform regardless of the values of $\\mathbf{x}_j$ for $j \\neq i$. A similar issue arises for \\verb!L2-MHA! with $W^Q \\neq W^K$ and full-rank $W^K$, as shown above: given any $\\mathbf{x}_i$, we can choose $\\mathbf{x}_j$ such that the $P_{ij}$ become uniform. \n\n\\subsection{L2 self-attention is Lipschitz for $\\boldsymbol{W^Q=W^K}$}\n\nHence we impose the restriction that $W^K=W^Q$. With this assumption we have\n\\begin{equation}\nP_{ij} \\propto \\exp\\left(-\\|(\\mathbf{x}_i - \\mathbf{x}_j)^\\top \\sqrt{A}\\|_2^2\\right)\n\\end{equation}\nwhere $A=W^Q W^{Q^\\top} / \\sqrt{D/H} \\in \\mathbb{R}^{D \\times D}$ and $\\sqrt{A}$ is chosen such that $A = \\sqrt{A}\\sqrt{A}^\\top $,\nin particular $\\sqrt{A} \\coloneqq W^Q / (D/H)^{\\frac{1}{4}}$. The terms in the Jacobian of $\\tilde{f}$ simplify to:\n\\begin{align}\n    \\tilde{J}_{ii} &= 2 X^\\top P^{(i)}XA + P_{ii} I \\hspace{2mm}  \\text{(note $P^{(i)} \\mathds{1} = 0$)},\\\\\n    \\tilde{J}_{ij} &= 2 P_{ij}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k)(\\mathbf{x}_i - \\mathbf{x}_j)^\\top A + P_{ij} I  \\hspace{2mm} \\text{for $i \\neq j$}.\n\\end{align}\nLet the Jacobian of $f(X)$ be:\n\\begin{equation}\n    J_{f} = \\begin{bmatrix}\n    J_{11} & \\dots & J_{1N} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    J_{N1} & \\dots & J_{NN} \\\\\n    \\end{bmatrix} \\in \\mathbb{R}^{ND \\times ND}.\n\\end{equation}\nSince $f(X) = \\tilde{f}(X)A$, and by the chain rule $\\frac{\\partial}{\\partial \\mathbf{x}_j}[\\tilde{f}_i(X)A]=A^\\top \\frac{\\partial \\tilde{f}_i(X)}{\\partial \\mathbf{x}_j}=A \\frac{\\partial \\tilde{f}_i(X)}{\\partial \\mathbf{x}_j}$ (by symmetry of $A$), we have that $J_{ij} = A \\tilde{J}_{ij}$. \nHence\n\\begin{align}\n    J_{ii} &= 2 AX^\\top P^{(i)}XA + P_{ii} A \\hspace{2mm}  \\text{(note $P^{(i)} \\mathds{1} = \\mathbf{0}$)},\\\\\n    J_{ij} &= 2 P_{ij} A (\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k)(\\mathbf{x}_i - \\mathbf{x}_j)^\\top A + P_{ij} A  \\hspace{2mm} \\text{for $i \\neq j$}.\n\\end{align}\nNoting $\\lip_p(f) = \\sup_X \\|J_f(X)\\|_p$, we would like to upper bound $\\|J_f\\|_p$.\n\n\\subsubsection{Upper bound on $\\boldsymbol{\\lip_{\\infty}(F)}$ for L2-MHA}\n\nConsider the choice $p=\\infty$, where $\\|J_f\\|_\\infty$ is the maximum absolute row sum of $J_f$. \nA key observation is that if we can bound the $\\infty$-norm of the Jacobian of $f_i$, a single output of $f$, (i.e.~a single block row $\\|[J_{i1},...,J_{iN}]\\|_\\infty$ of $J_f$) then this is also a bound on $\\|J_f\\|_{\\infty}$ due to permutation equivariance of self-attention; all block rows have the same maximal $\\|\\cdot\\|_\\infty$ when each is optimised over the input $X$. \nUsing this, we can prove that $\\|J_f\\|_\\infty$ admits an upper bound that is $O(\\log N - \\log \\log N)$. Below we state and prove lemmas that lead to the proof of this upper bound.\n\nFirst we analyse the term $\\sqrt{A}^\\top X^\\top P^{(i)}X \\sqrt{A}$, that appears in the first term of $J_{ii}$. \nNote that for $Y \\coloneqq X \\sqrt{A}$, so that the rows of $Y$ are $\\mathbf{y}_i^\\top \\coloneqq \\mathbf{x}_i^\\top \\sqrt{A}$, we have \n\\begin{equation}\n    \\sqrt{A}^\\top X^\\top P^{(i)}X \\sqrt{A}= Y^\\top P^{(i)} Y =  \\mathrm{Cov}(\\mathbb{Y})\n\\end{equation}\nwhere $\\mathbb{P}(\\mathbb{Y}=\\mathbf{y}_j)=P_{ij} = \\exp(-\\|\\mathbf{y}_j - \\mathbf{y}_i\\|^2_2)/\\sum_k \\exp(-\\|\\mathbf{y}_k - \\mathbf{y}_i\\|^2_2)$.\nThe last equality uses the observation in Equation \\eqref{eq:cov}.\n\nThe central inequality used throughout the proof of the main theorem is the following:\n\\begin{lemma} \\label{lemma:key}\n$\\Tr(\\mathrm{Cov}(\\mathbb{Y})) = \\sum_j P_{ij}\\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_2^2 \\leq \\sum_j P_{ij}\\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2 \\leq \\phi^{-1}(N-1)$ where $\\phi(c) = c \\exp(c+1)$ is a one-dimensional invertible function on $\\mathbb{R}_{\\geq 0}$.\n\\end{lemma}\n\\begin{proof}\nThe first equality holds since $\\Tr(\\mathrm{Cov}(\\mathbb{Y})) = \\sum_j \\mathrm{Cov}(\\mathbb{Y})_{jj} = \\sum_j \\mathrm{Var}(\\mathbb{Y}_j) = \\sum_j \\mathbb{E}[(\\mathbb{Y}_j -\\mathbb{E}[\\mathbb{Y}_j])^2]$. \nThe next inequality holds since $\\mathrm{Var}(\\mathbb{Y}_j) = \\mathrm{Var}(\\overline{\\mathbb{Y}}_j) = \\mathbb{E}[\\overline{\\mathbb{Y}}_j^2] -\\mathbb{E}[\\overline{\\mathbb{Y}}_j]^2 \\leq \\mathbb{E}[\\overline{\\mathbb{Y}}_j^2]$ where $\\overline{\\mathbb{Y}}= \\mathbb{Y} - y_i$. \nThe final inequality can be proved as follows.\n\nWe would like to bound \n\\begin{equation}\n    \\sum_j P_{ij}\\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2 = \\frac{\\sum_j \\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2 \\exp(-\\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2)}{\\sum_k \\exp(-\\|\\mathbf{y}_k-\\mathbf{y}_i\\|_2^2)}  = \\frac{\\sum_j z_j^2 \\exp(-z_j^2)}{\\sum_k \\exp(-z_k^2)}\n\\end{equation}\nwhere $z_j \\coloneqq \\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2$ (hence $z_i=0$). \nDefine:\n\\begin{equation}\n    g(\\mathbf{z}) \\coloneqq \\frac{\\sum_j z_j^2 \\exp(-z_j^2)}{\\sum_k \\exp(-z_k^2)} = \\frac{\\sum_{j \\neq i} z_j^2 \\exp(-z_j^2)}{1 + \\sum_{k \\neq i} \\exp(-z_k^2)}.\n\\end{equation}\nFirst note that as $z_j \\rightarrow \\infty$, $\\exp(-z_j^2) \\rightarrow 0$ exponentially fast, causing the product $z_j^2 \\exp(-z_j^2) \\rightarrow 0$.\nHence we expect the above quantity to be bounded and attain its maximum.\n\nLet $h(z_j) \\coloneqq \\exp(-z_j^2)$ for notational conciseness, and note $h(z_j) > 0$. By taking partial derivatives with the chain rule, we have that for $j \\neq i$\n\\begin{equation}\n\\frac{\\partial g(\\mathbf{z})}{\\partial z_j} = \\frac{2z_j h(z_j)}{(\\sum_k h(z_k))^2}\\left[(1-z_j^2)\\sum_k h(z_k) + \\sum_k h(z_k)z_k^2\\right].\n\\end{equation}\nHence the derivative is $0$ if and only if $z_j = 0$ or $(1-z_j^2)\\sum_k h(z_k) + \\sum_k h(z_k)z_k^2 = 0$, the latter being equivalent to $z_j^2 = 1 + \\frac{\\sum_k h(z_k)z_k^2}{\\sum_k h(z_k)} = 1 + g(\\mathbf{z})$. \nHence at the maximum, the non-zero values among $\\{z_j\\}_{j=1}^N$ must be equal to one another.\nIt is clear now that the maximum value $c$ is attained when $z_j^2 = 1 + c$ for $j \\neq i$ (and recall $z_i = 0$). \nSo $h(z_j) = \\exp(-1-c)$ for $j \\neq i$.\nSubstituting this into $g(z)$, and rearranging, we obtain $c \\exp(c+1) = N - 1$. Note $\\phi(x) \\coloneqq x \\exp(x+1)$ is increasing for $x > 0$ hence $c = \\phi^{-1}(N-1)$.\n\\end{proof}\nNote $\\phi(\\log N) = (\\log N) \\exp(\\log N + 1) \\geq N \\log N \\geq N -1$ for $N \\geq 3$. Since $\\phi$ is increasing, we have $\\phi^{-1}(N-1) \\leq \\log(N)$ for $N \\geq 3$. In fact, it is known that $\\phi^{-1}(N-1) = O(\\log N - \\log \\log N)$ \\citep{corless1996lambertw}.\n\nNote the $A$ term in $f(X) = \\tilde{f}(X) A$ allows us to use the above inequality, since $Y^\\top P^{(i)}Y = \\mathrm{Cov}(\\mathbb{Y})$ now appears in the terms of $J_f$:\n\\begin{align}\n    J_{ii}\n    &= 2 \\sqrt{A} [Y^\\top P^{(i)}Y]\\sqrt{A}^\\top + P_{ii} A,  \\\\ \n    J_{ij},\n    &= 2 \\sqrt{A} P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top \\sqrt{A}^\\top + P_{ij} A  \\hspace{2mm} \\text{for $i \\neq j$}.\n\\end{align}\n\nUsing the inequalities $\\|BC\\| \\leq \\|B\\| \\|C\\|$, $\\|B + C\\| \\leq \\|B\\| + \\|C\\|$ and $\\|[A_1, \\ldots, A_N]\\| \\leq \\sum_i \\|A_i\\|$, we have: \n\\begin{align*}\n\\|[J_{i1} &, \\ldots, J_{iN}]\\|_{\\infty}  \\\\\n\\leq & \\|J_{ii}\\|_{\\infty} + \\sum_{j \\neq i} \\|J_{ij}\\|_{\\infty} \\\\\n  \\leq & 2 \\|\\sqrt{A}\\|_{\\infty} \\|Y^\\top P^{(i)}Y\\|_{\\infty} \\|\\sqrt{A}^\\top\\|_{\\infty} + P_{ii} \\|A\\|_{\\infty} \\\\\n & + 2 \\sum_{j \\neq i} \\|\\sqrt{A}\\|_{\\infty} \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty \\|\\sqrt{A}^\\top\\|_{\\infty} + P_{ij} \\|A\\|_{\\infty}\\\\\n  = & 2  \\|\\sqrt{A}\\|_{\\infty}\\|\\sqrt{A}^\\top\\|_{\\infty} \n\\bigg(\\|Y^\\top P^{(i)}Y\\|_\\infty \n + \\sum_{j\\neq i} \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty\\bigg) + \\|A\\|_{\\infty} \\\\\n = & 2  \\frac{\\|W^{Q}\\|_{\\infty}\\|W^{Q^\\top}\\|_{\\infty}}{\\sqrt{D/H}} \n\\bigg(\\|Y^\\top P^{(i)}Y\\|_\\infty \n + \\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty\\bigg) + \\frac{\\|W^Q W^{Q^\\top}\\|_{\\infty}}{\\sqrt{D/H}}.\n\\end{align*}\nFor the first equality, note that $\\sum_j P_{ij}=1$. For the second equality, note that the summand for $j=i$ is $0$ because the term $\\mathbf{y}_i - \\mathbf{y}_j=\\mathbf{0}$. \nEach of the terms in the brackets are bounded by the following lemmas:\n\\begin{lemma}\n$\\|Y^\\top P^{(i)}Y\\|_\\infty \\leq  \\phi^{-1}(N-1) \\sqrt{D/H} $ ($\\phi$ defined as in Lemma \\ref{lemma:key}).\n\\end{lemma}\n\\begin{proof}\nRecall that $Y^\\top P^{(i)}Y = \\mathrm{Cov}(\\mathbb{Y})$. Let $\\sigma(\\mathbb{Y}_m)$ denote the standard deviation of $\\mathbb{Y}_m$. Then $[\\mathrm{Cov}(\\mathbb{Y})]_{lm} \\leq \\sigma(\\mathbb{Y}_l)\\sigma(\\mathbb{Y}_m)$.\nHence \n\\begin{align*}\n\\|\\mathrm{Cov}(\\mathbb{Y})\\|_{\\infty} = \\max_l \\sum_m \\left|[\\mathrm{Cov}(\\mathbb{Y})]_{lm}\\right| \n& \\leq  \\max_l \\sigma(\\mathbb{Y}_l) \\sum_m \\sigma(\\mathbb{Y}_m) \\\\\n& \\leq \\sqrt{\\frac{D}{H}} \\sum_m \\sigma^2(\\mathbb{Y}_m)  = \\sqrt{\\frac{D}{H}} \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\\\\n& \\leq \\sqrt{\\frac{D}{H}} \\phi^{-1}(N-1),\n\\end{align*}\nsince $\\sum_m \\sigma(\\mathbb{Y}_m) \\leq \\sqrt{\\frac{D}{H}} \\sqrt{\\sum_m \\sigma^2(\\mathbb{Y}_m)}$ (by e.g.~using the Cauchy--Schwartz inequality on $[\\sigma(\\mathbb{Y}_1), \\ldots, \\sigma(\\mathbb{Y}_{D/H})]$ and $\\mathds{1}$) and $\\max_l \\sigma(\\mathbb{Y}_l) \\leq \\sqrt{\\sum_m \\sigma^2(\\mathbb{Y}_m)}$, and the last inequality is from Lemma \\ref{lemma:key}. \n\\end{proof}\n\n\\begin{lemma} \\label{lemma:low_rank}\n$\\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty \\leq  \\phi^{-1}(N-1) \\sqrt{D/H}$.\n\\end{lemma}\n\\begin{proof}\nNote $\\|\\mathbf{u}\\mathbf{v}^\\top\\|_{\\infty} = \\|\\mathbf{u}\\|_{\\infty} \\|\\mathbf{v}\\|_1$ for real vectors $\\mathbf{u},\\mathbf{v}$. Hence\n\\begin{align*}\n    \\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty & = \\sum_j P_{ij} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_\\infty \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_1 \\\\\n    & = \\mathbf{a}^\\top \\mathbf{b} \\leq \\|\\mathbf{a}\\|_2 \\|\\mathbf{b}\\|_2,\n\\end{align*}\nwhere $a_j = \\sqrt{P_{ij}} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_\\infty$, $b_j = \\sqrt{P_{ij}} \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_1$.\n\nNote $a_j \\leq c_j \\coloneqq  \\sqrt{P_{ij}} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_2$ since $\\|\\mathbf{u}\\|_\\infty \\leq \\|\\mathbf{u}\\|_2$ for vector $\\mathbf{u}$. Hence $\\|\\mathbf{a}\\|_2 \\leq \\|\\mathbf{c}\\|_2$.\n\nAlso $b_j \\leq \\sqrt{\\frac{D}{H}} d_j \\coloneqq  \\sqrt{\\frac{D}{H}} \\sqrt{P_{ij}} \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_2$ since $\\|\\mathbf{u}\\|_1 \\leq \\sqrt{\\frac{D}{H}}\\|\\mathbf{u}\\|_2$ for $\\mathbf{u} \\in \\mathbb{R}^{D/H}$ (e.g.~by the Cauchy--Schwartz inequality on $[|\\mathbf{u}_1|, \\ldots, |\\mathbf{u}_{D/H}|]$ and $\\mathds{1}$). Hence $\\|b\\|_2 \\leq \\sqrt{\\frac{D}{H}}\\|d\\|_2$.\n\nNote $\\|c\\|_2^2 = \\sum_j P_{ij} \\|y_j - \\sum_k P_{ik} y_k\\|_2^2 = \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\phi^{-1}(N-1)$ from Lemma \\ref{lemma:key},\nand $\\|d\\|_2^2 =  \\sum_j P_{ij} \\|y_i - y_j\\|_2^2 \\leq  \\phi^{-1}(N-1)$ also from Lemma \\ref{lemma:key}.\nHence $\\|a\\|_2 \\|b\\|_2 \\leq \\sqrt{\\frac{D}{H}} \\|c\\|_2 \\|d\\|_2 \\leq \\sqrt{\\frac{D}{H}} \\phi^{-1}(N-1)$.\n\\end{proof}\n\n\nPutting the above lemmas altogether, with the observation $\\sup_X \\|J_f(X)\\|_\\infty = \\sup_X \\|[J_{i1}(X), \\ldots, J_{iN}(X)]\\|_\\infty$ by permutation invariance of $\\|J_f\\|_\\infty$ (since $f$ is permutation equivariant and $\\|\\cdot\\|_\\infty$ is the maximum absolute row sum), we have\n\\begin{align}\n\\|J_f\\|_{\\infty}\n& \\leq 4\\|W^Q\\|_{\\infty}\\|W^{Q^\\top}\\|_{\\infty} \\phi^{-1}(N-1)\n+ \\frac{\\|W^Q W^{Q^\\top}\\|_{\\infty}}{\\sqrt{D/H}} \\nonumber\\\\\n& \\leq \\|W^Q\\|_{\\infty}\\|W^{Q^\\top}\\|_{\\infty} \\left(4\\phi^{-1}(N-1) + \\frac{1}{\\sqrt{D/H}}\\right) \\label{ineq:infty}\\\\\n& \\leq \\|W^Q\\|_{\\infty} \\|W^{Q^\\top}\\|_{\\infty} \\left(4\\log N + \\frac{1}{\\sqrt{D/H}}\\right), \\nonumber\n\\end{align}\nwhere the last inequality holds for $N \\geq 3$.\n\nThe full multihead attention map that combines the heads $f^h(X)$ is:\n\\begin{equation*}\nF: X \\mapsto \\left[f^1(X)W^{V,1}, \\ldots f^H(X)W^{V,H}\\right] W^O = g(X) W^V W^O\n\\end{equation*}\nwhere $g:X \\mapsto [f^1(X),\\ldots,f^H(X)]$, $W^O \\in \\mathbb{R}^{D \\times D}$ and\n\\begin{equation*}\n    W^V = \\begin{bmatrix}\n    W^{V,1} & \\dots & 0 \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & W^{V,H} \\\\\n    \\end{bmatrix} \\in \\mathbb{R}^{DH \\times D}.\n\\end{equation*}\nNote the Jacobian $J_g$ is a block matrix whose rows are $J_{f^h}$, hence $\\|J_g\\|_{\\infty} = \\max_h \\|J_{f^h}\\|_{\\infty}$, and similarly $\\|W^{V^\\top}\\|_{\\infty} = \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty}$. Hence we have\n\\begin{equation*}\n    \\lip_{\\infty}(F) \\leq \\max_h \\|J_{f^h}\\|_{\\infty} \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty} \\|W^{O^\\top}\\|_{\\infty}.\n\\end{equation*}\n\nCombining this with Inequality (\\ref{ineq:infty}), we have:\n\\begin{equation*}\n    \\lip_{\\infty}(F)  \\leq \\left(4 \\phi^{-1}(N-1) + \\frac{1}{\\sqrt{D/H}}\\right) \\max_h \\|W^{Q,h}\\|_{\\infty} \\|W^{{Q,h}^\\top}\\|_{\\infty} \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty} \\ \\|W^{O^\\top}\\|_{\\infty}.\n\\end{equation*}\n\n\\subsubsection{Upper bound on $\\boldsymbol{\\lip_2(F)}$ for L2-MHA}\nFor $p=2$, we use the following lemma:\n\\begin{lemma} \\label{lemma:block_rows}\nLet A be a block matrix with block rows $A_1, \\ldots, A_N$. Then $\\|A\\|_2 \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2}$, and equality holds if and only if the first right singular vectors of the $A_i$ align.\n\\end{lemma}\n\\begin{proof}\n\\begin{equation*}\n\\|A\\|_2^2 = \\left\\Vert \\begin{bmatrix} A_1 \\\\ \\vdots \\\\ A_N \\\\ \\end{bmatrix}\\right\\Vert_2^2 = \\sup_{\\|\\mathbf{x}\\|_2=1} \\left\\Vert\\begin{bmatrix} A_1 \\\\ \\vdots \\\\ A_N \\\\ \\end{bmatrix} \\mathbf{x}\\right\\Vert_2^2 = \\sup_{\\|\\mathbf{x}\\|_2=1} \\sum_i \\|A_i \\mathbf{x}\\|_2^2 \\leq \\sum_i \\sup_{\\|\\mathbf{x}\\|_2=1} \\|A_i \\mathbf{x}\\|_2^2 = \\sum_i \\|A_i\\|_2^2.\n\\end{equation*}\nNote that equality holds if and only if the first right singular vectors of the $A_i$ align.\n\\end{proof}\nHence a bound on the spectral norm of each block row of $J_f$ can give us an $O(\\sqrt{N})$ bound on $\\|J_f\\|_2$, which may be loose, and it remains an open question as to whether this bound can be tightened.\n\nTo bound the $\\|\\cdot\\|_2$ norm of each row of $J_f$, we use the following lemmas:\n\\begin{lemma}\n$\\|Y^\\top P^{(i)}Y\\|_2 \\leq \\phi^{-1}(N-1)$\n\\end{lemma}\n\\begin{proof}\n$\\|Y^\\top P^{(i)}Y\\|_2=\\|\\mathrm{Cov}(\\mathbb{Y})\\|_2 = \\lambda_{\\max}(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\phi^{-1}(N-1)$, where the first equality holds by symmetry of $\\mathrm{Cov}(\\mathbb{Y})$ and the next holds by $\\mathrm{Cov}(\\mathbb{Y})$ being positive semi-definite, so all its eigenvalues are non-negative, and hence the maximal eigenvalue is bounded by the sum of the eigenvalues, equal to its trace. The final inequality is from Lemma \\ref{lemma:key}.\n\\end{proof}\n\n\\begin{lemma}\n$\\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_2 \\leq  \\phi^{-1}(N-1)$\n\\end{lemma}\n\\begin{proof}\nDirectly use Cauchy--Schwartz on $c$ and $d$ in the proof of Lemma \\ref{lemma:low_rank}. \n\\end{proof}\nAgain using the inequalities $\\|BC\\| \\leq \\|B\\| \\|C\\|$, $\\|B + C\\| \\leq \\|B\\| + \\|C\\|$ and $\\|[A_1, \\ldots, A_N]\\| \\leq \\sum_i \\|A_i\\|$, with the additional equality $\\|B^\\top\\|_2 = \\|B\\|_2$, we have the bound: \n\\begin{align*}\n&\\|[J_{i1}, \\ldots, J_{iN}]\\|_2 \\\\\n & \\leq  2  \\frac{\\|W^Q\\|_2\\|W^{Q^\\top}\\|_2}{\\sqrt{D/H}} \n\\bigg(\\|Y^\\top P^{(i)}Y\\|_2\n + \\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_2 \\bigg) + \\frac{\\|W^Q W^{Q^\\top}\\|_2}{\\sqrt{D/H}} \\\\\n & \\leq  4\\phi^{-1}(N-1) \\frac{\\|W^Q\\|_2^2}{\\sqrt{D/H}}  + \\frac{\\|W^Q W^{Q^\\top}\\|_2}{\\sqrt{D/H}} \\\\\n & \\leq  \\frac{\\|W^Q\\|_2^2}{\\sqrt{D/H}} \\bigg(4\\phi^{-1}(N-1)+1 \\bigg).\n\\end{align*}\nUsing Lemma \\ref{lemma:block_rows}, we have that\n\\begin{align}\n    \\|J_f\\|_2 & \\leq \\frac{\\sqrt{N}\\|W^Q\\|_2^2}{\\sqrt{D/H}} \\bigg(4\\phi^{-1}(N-1)+1 \\bigg) \\label{ineq:2} \\\\\n    & \\leq \\frac{\\sqrt{N}\\|W^Q\\|_2^2}{\\sqrt{D/H}}(4\\log N+1). \\nonumber\n\\end{align}\nTo obtain the final result for the full multihead self-attention $F$, we need a final lemma:\n\\begin{lemma} \\label{lemma:block_cols}\nLet A be a block matrix with block columns $A_1, \\ldots, A_N$. Then $\\|A\\|_2 \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2}$.\n\\end{lemma}\n\\begin{proof}\n\\begin{align*}\n\\|A\\|_2 &= \\|[A_1, \\ldots, A_N]\\|_2 = \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1}  \\left\\Vert [A_1, \\ldots, A_N] \\begin{bmatrix} \\mathbf{x}_1\\\\ \\vdots \\\\ \\mathbf{x}_N \\\\ \\end{bmatrix} \\right\\Vert_2^2 = \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1} \\|\\sum_i A_i \\mathbf{x}_i\\|_2 \\\\ \n& \\leq \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1} \\sum_i \\|A_i \\mathbf{x}_i\\|_2 = \\sup_{\\|\\mathbf{e}_i\\|_2=1, \\sum_i \\lambda_i^2 =1} \\sum_i \\lambda_i \\|A_i \\mathbf{e}_i\\|_2 = \\sup_{\\sum_i \\lambda_i^2 =1} \\sum_i \\lambda_i \\|A_i\\|_2 \\\\\n& \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2},\n\\end{align*}\nwhere we are using the substitution $\\mathbf{x}_i = \\lambda_i \\mathbf{e}_i$, and the last inequality holds by e.g.~Cauchy--Schwartz inequality on $[\\lambda_1, \\ldots, \\lambda_N]$ and $[\\|A_1\\|_2, \\ldots, \\|A_N\\|_2]$.\n\\end{proof}\nRecall that \n\\begin{equation*}\nF: X \\mapsto \\left[f^1(X)W^{V,1}, \\ldots, f^H(X)W^{V,H}\\right] W^O.\n\\end{equation*}\nSince $\\|f^h(X)W^{V,h}\\|_2 \\leq \\|J_{f^h}\\|_2 \\|W^{V,h}\\|_2$, by Lemma \\ref{lemma:block_cols} we have that\n\\begin{equation*}\n    \\left\\|[f^1(X)W^{V,1}, \\ldots, f^H(X)W^{V,H}]\\right\\|_2 \\leq \\sqrt{\\sum_h \\|J_{f^h}\\|_2^2 \\|W^{V,h}\\|_2^2}\n\\end{equation*} and hence\n\\begin{equation}\n    \\lip_2(F) \n    \\leq \\left(\\sqrt{\\sum_h \\|J_{f^h}\\|_2^2 \\|W^{V,h}\\|_2^2}\\right) \\|W^O\\|_2.\n\\end{equation}\nCombining this with Inequality (\\ref{ineq:2}), we have:\n\\begin{equation*}\n    \\lip_2(F) \\leq \\frac{\\sqrt{N}}{\\sqrt{D/H}}\n    \\left(4 \\phi^{-1}(N-1) + 1 \\right) \\left(\\sqrt{\\textstyle\\sum_h \\|W^{Q,h}\\|_2^2\\, \\|W^{V,h}\\|_2^2}\\right) \\|W^O\\|_2.\n\\end{equation*}\n\n\\section{The Case with Masking} \\label{apd:masking}\nSince self-attention is often used with \\textit{masking}, a natural question is how masking affects the derived bounds. In self-attention (for any choice of attention function), masking is implemented as follows: given a set of mask indices $\\mathcal{M}  \n\\subset \\{1, \\ldots, N\\} \\times \\{1, \\ldots, N\\}$, the logits (i.e.~the inputs to the softmax) are set to $-\\infty$ at the mask indices. That is,\n\\begin{equation*}\n    L_{ij}= \n\\begin{cases}\n    \\tilde{L}_{ij} & \\text{if } (i,j) \\notin \\mathcal{M}\\\\\n    -\\infty        & \\text{if } (i,j) \\in \\mathcal{M}\n\\end{cases}\n\\end{equation*}\nwhere $\\tilde{L}_{ij}$ is the original logit (e.g.~for L2 self-attention, $\\tilde{L}_{ij} = -(\\mathbf{x}_i - \\mathbf{x}_j)^\\top A (\\mathbf{x}_i - \\mathbf{x}_j)$). \n\nMasking implies $f_i(X)$ is not a function of $\\mathbf{x}_j$ for $(i,j) \\in \\mathcal{M}$, hence  $J_{ij} = 0$ for $(i,j) \\in \\mathcal{M}$.\nThus $f_i(X)$ is equal to the $i$th output for self-attention with inputs restricted to $\\{\\mathbf{x}_j: (i,j) \\notin \\mathcal{M}\\}$, the unmasked inputs with respect to the $i$th output. \nHence $J_{ij}$ will no longer contribute to the bound on $\\|[J_{i1}, \\ldots, J_{iN}]\\|$, and hence the bound for the unmasked case will continue to hold as long as $(i,i) \\in \\mathcal{M}$ i.e.~$\\mathbf{x}_i$ attends to itself (this is necessary for the proof of Lemma \\ref{lemma:key} to hold). \nThe bound can in fact be tightened by replacing $N$ with $|\\{\\mathbf{x}_j: (i,j) \\notin \\mathcal{M}\\}|$, the number of unmasked inputs with respect to the $i$th output.\n\n\\section{Experimental Details} \\label{apd:experimental_details}\nFor the experiment in Section \\ref{sec:asymptotic}, showing the asymptotic tightness of the upper bound on $\\lip_{\\infty}(F)$ where $F$ is \\verb!L2-MHA!, we fix all free parameters of $F$ (namely $W^Q, W^V$) to be the identity, and only optimise the input $X$. \nWe use $50$ random initialisations of $X$ for each $N$, where $X_{ij} \\sim \\unif [-c, c]$ for $c \\sim \\unif [0,10]$ (we observed that having $c$ itself be random improves optimisation). We display the top $5$ results for each value of $N$ after optimising each random initialisation till convergence using Adam \\citep{kingma2014adam} with a learning rate of $0.1$. \n\nFor the experiments in Section \\ref{sec:invertible_self-attention}, we comparing the performance of the original Transformer and the Transformer with Lipschitz/invertible self-attention at character-level language modelling on the Penn Treebank dataset \\citep{marcus1993building}.\\footnote{We use the standard training-validation-test split, and the dataset can be found at e.g.\\ \\url{https://github.com/harvardnlp/TextFlow/tree/master/data/ptb}.}\nEach training example is a sentence represented as a variable-length sequence of characters, and examples are batched according to length such that padding is minimised, with the maximum sequence length set to $288$. \nAll models are autoregressive, outputting the logits for the categorical likelihood predicting the next character, and are trained using maximum likelihood (cross-entropy loss) with a batch size of $64$. \nThe LSTM models have the dimensionality of the hidden state equal to the dimensionality $D$ of the cell state (the usual default implementation). \nThe Transformer models are trained with a varying number of blocks (number of layers) with $H=8$ heads and $D=512$, tuning hyperparameters for dropout rate in $\\{0,0.1,0.2\\}$ and base learning rate $\\gamma \\in \\{0.2,0.4,0.6,0.8,1.0,1.5,2.0\\}$ with number of warmup iterations $w \\in \\{1000,2000,4000,8000\\}$ for the standard custom learning rate schedule in \\citet{vaswani2017attention}:\n\\begin{equation*}\n  \\epsilon_t = \\frac{\\gamma}{\\sqrt{D}} \\min(t^{-1/2}, t w^{-3/2}),\n\\end{equation*}\nwhere $\\epsilon_t$ is the learning rate at training iteration $t$. Hence the learning rate linearly increases from $0$ to $(Dw)^{-1/2}$ over $w$ iterations, then decays proportionally to $t^{-1/2}$.\nWe use Glorot Uniform initialisation \\citep{glorot2010understanding} for all weights ($U\\left[-\\sqrt{\\frac{1}{d_{in} + d_{out}}}, \\sqrt{\\frac{1}{d_{in} + d_{out}}}\\right]$), except for weights in \\verb!L2-MHA! that are initialised from $U\\left[-\\frac{s}{\\sqrt{D}},\\frac{s}{\\sqrt{D}}\\right]$, and $s$ is a hyperparameter. For $D=512$, we used $s=\\frac{1}{2^4}$. All experiments were done in Tensorflow 1.14 \\citep{abadi2016tensorflow} on single Nvidia Tesla V100 GPUs.\n\n\\section{Numerical Invertibility of MHA Residual Map} \\label{apd:numerical_invertibility}\nFollowing Section \\ref{sec:numerical-invertibility}, Figure \\ref{fig:trained_dp_mha_invertibility} confirms that numerical invertibility does not hold for trained weights for dot-product multihead self-attention (DP-MHA) (obtained from one-layer Transformer (DP) model used for Figure \\ref{fig:ptb}), similar to the randomly initialised weight case.\nFigure \\ref{fig:mha_invertibility} shows additional results for different values of $N$ and $D$. \n\n\\begin{figure}[!htb]\n    \\centering\n    \\includegraphics[width=0.35\\textwidth]{trained_dp_mha_invertibility.pdf}\n    \\caption{Invertibility of $g(\\mathbf{x})=\\mathbf{x} + cf(\\mathbf{x})$ for trained DP-MHA $f$.}\n    \\label{fig:trained_dp_mha_invertibility}\n\\end{figure}\n\n\\begin{figure}[!htb]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{mha_invertibility.pdf}\n    \\caption{Numerical invertibility of $g(\\mathbf{x})= \\mathbf{x} + c f(\\mathbf{x})$ where $f$ is L2-MHA(left) or DP-MHA  (right), for different values of $N$ and $D$.}\n    \\label{fig:mha_invertibility}\n\\end{figure}\n\n\n\n\\newpage\n\\section{Behaviour of Lower Bound on $\\boldsymbol{\\lip_2(F)}$}\n\\label{apd:jacobian_opt2}\n\\begin{figure}[!htb]\n    \\centering\n    \\includegraphics[width=0.6\\columnwidth]{jacobian_opt2.pdf}\n    \\caption{Lower bound on $\\lip_{2}(F)$ where $F$ is L2-MHA, with $D=1$ and varying $N$, obtained by optimising $\\|J_F(X)\\|_{2}$ with respect to $X$, with $50$ random initialisations of $X$ for each $N$.}\n    \\label{fig:jacobian_opt2}\n\\end{figure}\nIn Figure \\ref{fig:jacobian_opt2}, we show the lower bound on $\\lip_{2}(F)$ obtained by optimising $\\|J_F(X)\\|_{2}$ using the same optimisation procedure as for Figure \\ref{fig:jacobian_opt} of Section \\ref{sec:asymptotic}. Here the optimisation is more difficult, evident in the variance of the top $5$ values, and the trend is less clear, but it appears that $\\lip_{2}(f)$ grows at a rate of $O(\\log N)$. The message is less clear here, and there are at least two possibilities: \n\\begin{enumerate}[label=(\\arabic*), leftmargin=*]\n    \\item The optimisation is difficult even for small values of $N$, hence Figure \\ref{fig:jacobian_opt2} shows a loose lower bound.\n    \\item If the lower bound is tight, this suggests that the $O(\\sqrt{N} \\log N)$ bound in Theorem \\ref{thm:main} is not asymptotically tight, and could be improved to $O(\\log N)$ (or $O(\\log N - \\log \\log N)$ as for $p=\\infty$).\n\\end{enumerate}\n\n\\section{Optimising the norm of the Jacobian of DP-MHA}\n\\label{apd:jacobian_opt_dp}\nIn Figure \\ref{fig:trained_dp_mha_lower_bound}, we show how the norm of the Jacobian $\\|J_f(X)\\|_{\\infty}$ for \\verb!DP-MHA! $f$ keeps increasing when being optimised with respect to $X$. This is a useful sanity check validating our theoretical result of Theorem \\ref{thm:dp_not_lipschitz}, that \\verb!DP-MHA! is \\emph{not} Lipshchitz. The oscillations are likely due to momentum term of Adam optimizer that was used to optimise the norm. \n\n\\begin{figure}[!htb]\n    \\centering\n    \\includegraphics[width=0.4\\columnwidth]{trained_dp_mha_lower_bound.pdf}\n    \\caption{Optimise $\\|J_f(X)\\|_{\\infty}$ w.r.t.~$X$ for trained DP-MHA $f$.}\n    \\label{fig:trained_dp_mha_lower_bound}\n\\end{figure}\n\n\\section{Experiment tying keys and queries of L2-MHA but preserving parameter count}\n\\label{apd:wq_wk_experiment}\nIn Figure \\ref{fig:ptb} of Section \\ref{sec:invertible_self-attention}, we have shown that there is a clear reduction in performance when tying the keys and queries. To test whether this can be attributed to the reduction in parameter count, we tried doubling the number of columns of $W^Q$ when the keys and queries are shared (i.e. from $D/H$ to $2D/H$) so that the shared model has the same number of parameters as the unshared model. In Figure \\ref{fig:ptb_wq_wk}, the third column shows results for shared \\verb!L2-MHA!, but with the same number of parameters as the unshared \\verb!L2-MHA! i.e.~without tying the keys and queries. The performance is similar to the second column (tying with a reduced number of parameters), suggesting that there is an inherent limitation in expressiveness to tying the keys and queries, and that the reduction in number of parameters is an insufficient explanation this phenomenon.\n\n\\begin{figure}[!htb]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{ptb_wq_wk_experiment.pdf}\n    \\caption{Experiment tying keys/queries but preserving parameter count.}\n    \\label{fig:ptb_wq_wk}\n\\end{figure}\n\n\n\\section{Training curves for fixed learning rate DP-MHA vs L2-MHA} \\label{apd:stability}\n\n\\begin{figure}[!ht]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{stability_experiment.pdf}\n    \\caption{Train NLL for Transformer (DP), Transformer (L2) and Transformer (Contractive-L2)}\n    \\label{fig:stability}\n\\end{figure}\n\n\\section{The Lipschitz constant of LayerNorm}\n\\label{apd:layernorm}\nIn this section, we show that \\verb!LayerNorm! is Lipschitz, with a loose bound on its Lipschitz constant w.r.t. to the $\\infty$-norm.\n\\verb!LayerNorm! is defined as follows:\n\\begin{align*}\n    \\text{LN}(\\mathbf{x}) &= \\frac{\\mathbf{x}-\\mu(\\mathbf{x})}{\\sqrt{\\sigma^2(\\mathbf{x}) + \\epsilon}} \\odot \\boldsymbol\\gamma + \\boldsymbol\\beta \\\\\n    \\mu(\\mathbf{x}) &= \\frac{1}{D} \\sum_{d=1}^D x_d \\\\\n    \\sigma^2(\\mathbf{x}) &= \\frac{1}{D}\\sum_{d=1}^D (x_d - \\mu(\\mathbf{x}))^2 \n\\end{align*}\nwhere $\\mathbf{x}, \\boldsymbol\\beta, \\boldsymbol\\gamma \\in \\mathbb{R}^D$. We will omit dependence on $x$ to write $\\mu, \\sigma^2$ in cases when there is no ambiguity to reduce clutter.\n\nIn the trivial case where $x_d$ are all equal or when $D=1$, $\\mathbf{x} = \\mu$ hence $LN(\\mathbf{x})=\\boldsymbol\\beta$, so its Lipschitz constant is 0. Thus let us assume $D > 2$ and not all $x_d$ are equal.\n\nFirst let us compute the derivative of $\\mu$ and $\\sigma^2$ w.r.t $x$:\n\\begin{align*}\n    \\frac{\\partial \\mu}{\\partial \\mathbf{x}} &= \\frac{1}{D} \\mathds{1}^\\top \\\\\n    \\frac{\\partial \\sigma^2}{\\partial \\mathbf{x}} &= \\frac{1}{D} \\sum_d 2(x_d - \\mu) \\frac{\\partial}{\\partial \\mathbf{x}}(x_d - \\mu) \\\\\n    &= \\frac{2}{D} \\sum_d (x_d - \\mu)(\\mathbf{e}_d - \\frac{1}{D}\\mathds{1})^\\top \\\\\n    &= \\frac{2}{D} \\bigg[\\sum_d (x_d - \\mu) \\mathbf{e}_d - \\frac{1}{D} \\mathds{1} \\sum_d (x_d - \\mu)\\bigg]^\\top \\\\\n    &= \\frac{2}{D}\\sum_d (x_d - \\mu) \\mathbf{e}_d^\\top \\\\\n    &= \\frac{2}{D}(\\mathbf{x} - \\mu)^\\top\n\\end{align*}\nwhere $\\mathbf{e}_d \\in \\mathbb{R}^D$ is a one-hot vector with $1$ at the $d$th element. Note the penultimate equality holds because $\\sum_d (x_d - \\mu) = 0$.\n\nNow the derivative of $\\text{LN}(\\mathbf{x})_d$, the $d$th element of $\\text{LN}(\\mathbf{x})$, w.r.t.$\\mathbf{x}$ is\n\\begin{align*}\n    \\frac{\\partial \\text{LN}(\\mathbf{x})_d}{\\partial \\mathbf{x}} &= \\gamma_d \\bigg[\\frac{\\partial}{\\partial \\mathbf{x}}(x_d - \\mu)(\\sigma^2 + \\epsilon)^{-\\frac{1}{2}} + (x_d - \\mu)\\Big(-\\frac{1}{2}(\\sigma^2 + \\epsilon)^{-\\frac{3}{2}}\\Big)\\frac{\\partial \\sigma^2}{\\partial \\mathbf{x}}\\bigg] \\\\\n    &= \\gamma_d (\\sigma^2 + \\epsilon)^{-\\frac{1}{2}} \\bigg[(\\mathbf{e}_d - \\frac{1}{D}\\mathds{1})^\\top - \\frac{1}{2} (x_d - \\mu)(\\sigma^2 + \\epsilon)^{-1} \\frac{2}{D}(\\mathbf{x} - \\mu)^\\top \\bigg] \\\\\n    &= \\gamma_d (\\sigma^2 + \\epsilon)^{-\\frac{1}{2}} \\bigg[(\\mathbf{e}_d - \\frac{1}{D}\\mathds{1})^\\top - \\frac{1}{D} (\\sigma^2 + \\epsilon)^{-1} (x_d - \\mu)(\\mathbf{x} - \\mu)^\\top \\bigg].\n\\end{align*}\n\nHence\n\\begin{align*}\n    \\frac{\\partial \\text{LN}(\\mathbf{x})}{\\partial \\mathbf{x}} &= (\\sigma^2 + \\epsilon)^{-\\frac{1}{2}} \\bigg[ \\text{diag}(\\boldsymbol\\gamma) - \\frac{1}{D}\\boldsymbol\\gamma \\mathds{1}^\\top - \\frac{1}{D} (\\sigma^2 + \\epsilon)^{-1}\\text{diag}(\\boldsymbol\\gamma)(\\mathbf{x} - \\mu)(\\mathbf{x} - \\mu)^\\top \\bigg].\n\\end{align*}\n\nNote\n\\begin{equation*}\n    \\text{diag}(\\boldsymbol\\gamma) - \\frac{1}{D}\\boldsymbol\\gamma \\mathds{1}^\\top = \\begin{bmatrix}\n    \\gamma_1 (D-1)/D & -\\gamma_1 / D & \\dots & -\\gamma_1 / D \\\\\n    - \\gamma_2 / D & \\gamma_2 (D-1)/D & \\dots & -\\gamma_2 / D \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    -\\gamma_D / D & -\\gamma_D / D & \\dots & \\gamma_D (D-1)/D\n    \\end{bmatrix},\n\\end{equation*}\n\nhence\n\\begin{equation} \\label{eq:first_terms_inf_norm}\n    \\left\\Vert \\text{diag}(\\boldsymbol\\gamma) - \\frac{1}{D}\\boldsymbol\\gamma \\mathds{1}^\\top \\right\\Vert_{\\infty} = \\frac{2(D-1)}{D}\\max_d |\\gamma_d|,\n\\end{equation}\nrecalling that $\\left\\Vert \\cdot \\right\\Vert_{\\infty}$ is the maximum absolute row sum.\n\nLet $z_d \\coloneqq x_d - \\mu$. Hence $\\sum_d z_d = 0$, $\\sigma^2 = \\frac{1}{D} \\sum_d z_d^2$ and\n\\begin{equation*}\n    \\mathrm{Cov}(\\mathbf{x}) = \n    (\\mathbf{x} - \\mu)(\\mathbf{x} - \\mu)^\\top =\n    \\begin{bmatrix}\n    z_1^2 & \\dots & z_1 z_D \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    z_D z_1 & \\dots & z_D^2 \\\\\n    \\end{bmatrix}.\n\\end{equation*}\n\nHence\n\\begin{equation*}\n\\frac{\\left\\Vert \\mathrm{Cov}(\\mathbf{x})\\right\\Vert_{\\infty}}{\\sigma^2} = \\frac{\\max_d |z_d| \\sum_{d'} |z_{d'}|}{\\frac{1}{D}\\sum_d z_d^2}.    \n\\end{equation*}\nNoting that this expression is scale-invariant in $\\mathbf{z}$, we may assume WLOG $\\max_d |z_d| = z_D = 1$, since we are assuming not all $x_d$ are equal and hence at least one $z_d$ is non-zero.\n\nThe expression now becomes\n\\begin{equation} \\label{eq:cov_expression}\n\\frac{\\left\\Vert \\mathrm{Cov}(\\mathbf{x})\\right\\Vert_{\\infty}}{\\sigma^2} =  D\\bigg(\\frac{1 + \\sum_{d < D} |z_d|}{1 + \\sum_{d<D} z_d^2}\\bigg). \n\\end{equation}\nSince all terms $|z_d| \\leq 1$ are bounded, this continuous expression reaches a global maximum for some value of $\\mathbf{z}$ with $z_D = 1$.\n\nIt is easy to see that at the global maximum, $z_d \\neq 0$ $\\forall d$:\nsuppose this were to be true, WLOG $z_1 = 0$.\nThen let us see how the quantity \\eqref{eq:cov_expression} changes when $z_1=0$ is increased by $0< \\delta < 1$ and $z_D=1$ is decreased by $\\delta$, keeping the sum constant.\nIt is easy to see that the numerator $\\sum_d |z_d|$ stays constant, but the denominator $\\sum_d z_d^2$ changes by $2\\delta^2 - 2\\delta < 0$.\nSince for small $\\delta$, the numerator of \\eqref{eq:cov_expression} stays constant but the denominator decreases, the quantity \\eqref{eq:cov_expression} increases, contradicting that the global max is obtained for $z_1 = 0$.\nHence we may assume that $z_d \\neq 0$ $\\forall d$.\n\nHence the quantity \\eqref{eq:cov_expression} (in particular, $\\sum_d {|z_d|}$) is differentiable at the global maximum, at which the partial derivatives of the following Lagrangian are zero:\n\\begin{equation*}\n    \\mathcal{L}(z_1,\\ldots,z_{D-1}, \\lambda) = \\frac{1 + \\sum_{d < D} |z_d|}{1 + \\sum_{d<D} z_d^2} - \\lambda(\\sum_{d<D} z_d + 1).\n\\end{equation*}\nFrom now on let us write $\\sum$ for $\\sum_{d < D}$ below to reduce clutter. Setting $\\frac{\\partial \\mathcal{L}}{\\partial z_k} = 0$ and noting $\\frac{d|z_k|}{dz_k} = \\text{sgn}(z_k)$, we obtain\n\\begin{align*}\n    & \\frac{\\text{sgn}(z_k)(1 + \\sum z_d^2) - 2z_k(1 + \\sum |z_d|)}{(1+\\sum z_d^2)^2} - \\lambda = 0 \\\\\n    \\iff & \\text{sgn}(z_k)(1 + \\sum z_d^2) - 2z_k(1 + \\sum |z_d|) = \\lambda (1+\\sum z_d^2)^2 \\\\\n    \\iff & z_k = \\frac{\\text{sgn}(z_k)(1 + \\sum z_d^2) - \\lambda (1+\\sum z_d^2)^2}{2(1 + \\sum |z_d|)} \\\\\n    \\iff & z_k = \\frac{(\\text{sgn}(z_k) - \\lambda (1+\\sum z_d^2))(1 + \\sum z_d^2)}{2(1 + \\sum |z_d|)}\n\\end{align*}\nHence at the global maximum, $z_k$ takes one of two values $a > 0$ and $b < 0$.\nFurther we have that\n\\begin{align} \\label{eq:max_expression}\n    \\frac{1 + \\sum|z_d|}{1 + \\sum z_d^2} = \\frac{\\text{sgn}(z_k) - \\lambda (1+\\sum z_d^2)}{2z_k}\n\\end{align}\nIf both $a$ and $b$ are among the $z_k$, we have that $\\frac{1 - \\lambda (1+\\sum z_d^2)}{2a} = \\frac{- 1 - \\lambda (1+\\sum z_d^2)}{2b}$.\nSolving for $\\lambda(1+\\sum z_d^2)$ and plugging it in back to Equation \\eqref{eq:max_expression}, we get:\n\\begin{align*}\n    \\frac{1 + \\sum|z_d|}{1 + \\sum z_d^2} = \\frac{1}{a-b}\n\\end{align*}\nSince $a > 0$, $b < 0$ and $\\sum z_d = -1$, $a-b$ is minimised when only one of the $z_d$ is $a$ and the rest are $b$. Hence a crude lower bound on $a-b$ is $\\frac{1}{D-2}$, giving a bound:\n\\begin{equation} \\label{eq:second_term_inf_norm}\n    \\frac{\\left\\Vert \\mathrm{Cov}(\\mathbf{x})\\right\\Vert_{\\infty}}{\\sigma^2}\n    \\leq D(D-2)\n\\end{equation}\n\nHowever we conjecture that the true global maximum is attained when $z_d = - \\frac{1}{D-1}$ $\\forall d < D$ (i.e. all the $z_d$ for $d < D$ are equal to $b < 0$), for which it is easy to show that $\\frac{1 + \\sum_{d < D} |z_d|}{1 + \\sum_{d<D} z_d^2} = 2(D-1)/D$.\n\nPutting together the above, we have:\n\\begin{align*}\n    \\left\\Vert \\frac{\\partial \\text{LN}(\\mathbf{x})}{\\partial \\mathbf{x}} \\right\\Vert_{\\infty} &= (\\sigma^2 + \\epsilon)^{-\\frac{1}{2}} \\left\\Vert   \\text{diag}(\\boldsymbol\\gamma) - \\frac{1}{D}\\boldsymbol\\gamma \\mathds{1}^\\top - \\frac{1}{D} (\\sigma^2 + \\epsilon)^{-1}\\text{diag}(\\boldsymbol\\gamma)(\\mathbf{x} - \\mu)(\\mathbf{x} - \\mu)^\\top \\right\\Vert_{\\infty} \\\\\n    &\\leq \\epsilon^{-\\frac{1}{2}} \\bigg( \\left\\Vert \\text{diag}(\\boldsymbol\\gamma) - \\frac{1}{D}\\boldsymbol\\gamma \\mathds{1}^\\top \\right\\Vert_{\\infty} + \\frac{1}{D}\\left\\Vert  \\text{diag}(\\boldsymbol\\gamma) \\right\\Vert_{\\infty} \\left\\Vert  (\\sigma^2 + \\epsilon)^{-1}(\\mathbf{x} - \\mu)(\\mathbf{x} - \\mu)^\\top \\right\\Vert_{\\infty} \\bigg) \\\\\n    &\\leq \\epsilon^{-\\frac{1}{2}} \\bigg( \\left\\Vert \\text{diag}(\\boldsymbol\\gamma) - \\frac{1}{D}\\boldsymbol\\gamma \\mathds{1}^\\top \\right\\Vert_{\\infty} + \\frac{1}{D}\\left\\Vert  \\text{diag}(\\boldsymbol\\gamma) \\right\\Vert_{\\infty} \\left\\Vert  \\mathrm{Cov}(\\mathbf{x}) / \\sigma^2 \\right\\Vert_{\\infty} \\bigg) \\\\\n    &\\leq \\epsilon^{-\\frac{1}{2}} \\bigg( \\frac{2(D-1)}{D}\\max_d |\\gamma_d| + \\frac{1}{D} \\max_d |\\gamma_d| D(D-2) \\bigg) \\\\\n    &= \\epsilon^{-\\frac{1}{2}} \\max_d |\\gamma_d| \\bigg(\\frac{2(D-1)}{D} + D-2 \\bigg) \\\\\n    &= \\epsilon^{-\\frac{1}{2}} \\max_d |\\gamma_d| \\bigg(\\frac{D^2-2}{D}\\bigg).\n\\end{align*}\n\n\n}\n\\end{document}\n\n==== END OF /2006.04710/main.tex ====",
            "statements": {
                "definitions": [
                    {
                        "statement_id": "5f8a3b08-c740-47c4-bdc4-78d01193d98e",
                        "paper_id": "895bbef7-305d-4454-850a-7279bb832d9d",
                        "library_nr": 1,
                        "library_name": "Definition 1",
                        "title": "Lipschitz Continuity",
                        "statement_original_tex": "\\begin{definition}\\label{Lipschitz_definition}\nGiven two metric spaces $(\\mathcal{X}, d_{\\mathcal{X}})$ and $(\\mathcal{Y}, d_{\\mathcal{Y}})$, a function $f:\\mathcal{X} \\rightarrow \\mathcal{Y}$ is called \\textit{Lipschitz continuous} (or $K$-\\textit{Lipschitz}) if there exists a constant $K\\geq 0$ such that \n\\begin{equation}\nd_{\\mathcal{Y}}(f(\\mathbf{x}),f(\\mathbf{x'})) \\leq K d_{\\mathcal{X}}(\\mathbf{x},\\mathbf{x'}) \\spaces\\spaces \\text{for all } \\mathbf{x},\\mathbf{x'} \\in \\mathcal{X}.\n\\end{equation}\nThe smallest such $K$ is the \\textit{Lipschitz constant} of $f$, denoted $\\lip(f)$.\n\\end{definition}",
                        "statement_html": "Given two metric spaces $(\\mathcal{X}, d_{\\mathcal{X}})$ and $(\\mathcal{Y}, d_{\\mathcal{Y}})$, a function $f:\\mathcal{X} \\rightarrow \\mathcal{Y}$ is called $\\textit{Lipschitz continuous}$ (or $K$-$\\textit{Lipschitz}$) if there exists a constant $K\\geq 0$ such that \n\\begin{equation}\nd_{\\mathcal{Y}}(f(\\mathbf{x}),f(\\mathbf{x'})) \\leq K d_{\\mathcal{X}}(\\mathbf{x},\\mathbf{x'}) \\spaces\\spaces \\text{for all } \\mathbf{x},\\mathbf{x'} \\in \\mathcal{X}.\n\\end{equation}\nThe smallest such $K$ is the $\\textit{Lipschitz constant}$ of $f$, denoted $\\lip(f)$.",
                        "statement_type": "definition",
                        "statement_motivation_html": "Lipschitz continuity is a crucial concept in analysis and applied mathematics. It ensures that the function $f$ does not change too rapidly, providing a controlled rate of change. This property is particularly useful in numerical analysis, optimization, and differential equations, where stability and convergence of algorithms are often guaranteed under Lipschitz conditions. If you need to ensure that small changes in the input of a function result in proportionally small changes in the output, verifying Lipschitz continuity is essential.",
                        "html_url": "library/definitions/definition_1/index.html"
                    }
                ],
                "axioms": [],
                "lemmas": [
                    {
                        "statement_id": "34220ba0-537b-49c3-9c5a-992547e4eb62",
                        "paper_id": "895bbef7-305d-4454-850a-7279bb832d9d",
                        "library_nr": 1,
                        "library_name": "Lemma 1",
                        "title": "Lipschitz Composition Lemma",
                        "statement_original_tex": "\\begin{lemma}[\\citealp{federer1969geometric}]\nLet $g,h$ be two composable Lipschitz functions. Then $g \\circ h$ is also Lipschitz with $\\lip(g \\circ h) \\leq \\lip(g) \\lip(h)$.\n\\end{lemma}",
                        "statement_html": "Let $g,h$ be two composable Lipschitz functions. Then $g \\circ h$ is also Lipschitz with $\\lip(g \\circ h) \\leq \\lip(g) \\lip(h)$.",
                        "statement_type": "lemma",
                        "statement_motivation_html": "The composition of Lipschitz functions theorem is useful in analysis and applied mathematics, particularly when dealing with functions that have bounded rates of change. If you know that two functions $g$ and $h$ are Lipschitz, this theorem allows you to conclude that their composition $g \\circ h$ is also Lipschitz. This is particularly valuable in scenarios where you need to ensure that the composed function does not exhibit wild fluctuations, and it provides an upper bound for the Lipschitz constant of the composition.",
                        "html_url": "library/lemmas/lemma_1/index.html",
                        "corollary_ids": [
                            "1a102984-a1c3-49ec-a94b-73deff56b917"
                        ],
                        "proof": null
                    },
                    {
                        "statement_id": "e51ba6db-a83d-4b4b-b069-b115b5302efe",
                        "paper_id": "895bbef7-305d-4454-850a-7279bb832d9d",
                        "library_nr": 2,
                        "library_name": "Lemma 2",
                        "title": "Non-Lipschitz Condition for Distinct Full Rank Matrices",
                        "statement_original_tex": "\\begin{lemma} \\label{lemma:tie_weights}\nIf $W^K \\in \\mathbb{R}^{D \\times D/H}$ is full rank (i.e.~full column rank), and $W^K \\neq W^Q$, then $J_{ij}$ has terms that are unbounded for $i \\neq j$, hence $\\tilde{f}$ is \\emph{not} Lipschitz. \n\\end{lemma}",
                        "statement_html": "If $W^K \\in \\mathbb{R}^{D \\times D/H}$ is full rank (i.e.~full column rank), and $W^K \\neq W^Q$, then $J_{ij}$ has terms that are unbounded for $i \\neq j$, hence $\\tilde{f}$ is $\\emph{not}$ Lipschitz.",
                        "statement_type": "lemma",
                        "statement_motivation_html": "This statement is useful in the context of analyzing the properties of neural networks, particularly in understanding the behavior of the attention mechanism in transformer models. If the weight matrices $W^K$ and $W^Q$ are distinct and $W^K$ is full rank, the resulting function $\\tilde{f}$ is not Lipschitz continuous. This insight is crucial for understanding the stability and convergence properties of the model, as non-Lipschitz functions can lead to unbounded gradients and potentially unstable training dynamics.",
                        "html_url": "library/lemmas/lemma_2/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "db2f6268-497d-48c3-b25d-5984126c4979",
                            "paper_id": "895bbef7-305d-4454-850a-7279bb832d9d",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\n\nLet us investigate the expression $\\tilde{K}_{ij} \\coloneqq P_{ij} W^{K^\\top}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k)(\\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K) \\in \\mathbb{R}^{\\frac{D}{H} \\times \\frac{D}{H}}$ for $i\\neq j$, which is related to $\\tilde{J}_{ij}$ as follows by Equation \\eqref{eq:jij}:\n\n\\begin{equation*}\n    W^{K^\\top} \\tilde{J}_{ij} = \\left(\\frac{2}{\\sqrt{D/H}} \\tilde{K}_{ij} + P_{ij}I \\right)\n    W^{K^\\top}.\n\\end{equation*}\n\nIt suffices to show that $\\tilde{K}_{ij}$ is unbounded to show that $\\tilde{J}_{ij}$ is unbounded, since $W^K$ is full rank and $P_{ij} \\in [0,1]$. \n\nLet $\\mathbf{y}_j^\\top = \\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K$. \nThen we have:\n\\begin{align*}\n    \\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k \n    &= W^{Q^\\top}\\mathbf{x}_i - W^{K^\\top}\\mathbf{x}_j - \\sum_k P_{ik} (W^{Q^\\top}\\mathbf{x}_i - W^{K^\\top}\\mathbf{x}_k)\\\\\n    &= W^{Q^\\top}\\mathbf{x}_i - W^{K^\\top}\\mathbf{x}_j - (W^{Q^\\top}\\mathbf{x}_i - \\sum_k P_{ik} W^{K^\\top}\\mathbf{x}_k) \\\\\n    &= - W^{K^\\top}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k).\n\\end{align*}\nHence $\\tilde{K}_{ij} = - P_{ij} (\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k) \\mathbf{y}_j^\\top$.\nNote $\\mathbf{y}_i$ can take an arbitrary value in $\\mathbb{R}^{D/H}$, since $W^K \\neq W^Q$ and $W^K$ is full-rank.\n\nFor all $j \\neq i$, let us choose $\\mathbf{x}_j$ such that $\\mathbf{y}_j = -\\mathbf{y}_i$. This is possible for any value of $\\mathbf{y}_i$ since $W^K$ is full-rank.\nNote $\\mathbf{y}_j = - \\mathbf{y}_i$ and not $\\mathbf{y}_i$.\nWe then have that $\\|\\mathbf{y}_j\\|_2^2$ is equal for all $j$, hence $P_{ij} \\coloneqq \\frac{\\exp(-\\|\\mathbf{y}_j\\|_2^2)}{\\sum_k \\exp(-\\|\\mathbf{y}_k\\|_2^2)} = \\frac{1}{N}$ for all $j$. \nThen for $i \\neq j$, $\\tilde{K}_{ij}$ simplifies to\n\\begin{equation*}\n\\tilde{K}_{ij} = - \\frac{1}{N} \\left(-\\mathbf{y}_i - \\frac{1}{N} (N-2) (-\\mathbf{y}_i)\\right) (-\\mathbf{y}_i)^\\top  = - \\frac{2N-2}{N^2} \\mathbf{y}_i \\mathbf{y}_i^\\top \n\\end{equation*}\nwhose entries are unbounded since $\\mathbf{y}_i$ can be any vector in $\\mathbb{R}^{D/H}$ (note we assume $N \\geq 2$ for self-attention to be well-defined, hence $2N-2 \\neq 0$).\n\\end{proof}",
                            "statement_html": "Let us investigate the expression $\\tilde{K}_{ij} \\coloneqq P_{ij} W^{K^\\top}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k)(\\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K) \\in \\mathbb{R}^{\\frac{D}{H} \\times \\frac{D}{H}}$ for $i\\neq j$, which is related to $\\tilde{J}_{ij}$ as follows by Equation $\\eqref{eq:jij}$:\n\n\\begin{equation*}\n    W^{K^\\top} \\tilde{J}_{ij} = \\left(\\frac{2}{\\sqrt{D/H}} \\tilde{K}_{ij} + P_{ij}I \\right)\n    W^{K^\\top}.\n\\end{equation*}\n\nIt suffices to show that $\\tilde{K}_{ij}$ is unbounded to show that $\\tilde{J}_{ij}$ is unbounded, since $W^K$ is full rank and $P_{ij} \\in [0,1]$. \n\nLet $\\mathbf{y}_j^\\top = \\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K$. \nThen we have:\n\\begin{align*}\n    \\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k \n    &= W^{Q^\\top}\\mathbf{x}_i - W^{K^\\top}\\mathbf{x}_j - \\sum_k P_{ik} (W^{Q^\\top}\\mathbf{x}_i - W^{K^\\top}\\mathbf{x}_k)\\\\\n    &= W^{Q^\\top}\\mathbf{x}_i - W^{K^\\top}\\mathbf{x}_j - (W^{Q^\\top}\\mathbf{x}_i - \\sum_k P_{ik} W^{K^\\top}\\mathbf{x}_k) \\\\\n    &= - W^{K^\\top}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k).\n\\end{align*}\nHence $\\tilde{K}_{ij} = - P_{ij} (\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k) \\mathbf{y}_j^\\top$.\nNote $\\mathbf{y}_i$ can take an arbitrary value in $\\mathbb{R}^{D/H}$, since $W^K \\neq W^Q$ and $W^K$ is full-rank.\n\nFor all $j \\neq i$, let us choose $\\mathbf{x}_j$ such that $\\mathbf{y}_j = -\\mathbf{y}_i$. This is possible for any value of $\\mathbf{y}_i$ since $W^K$ is full-rank.\nNote $\\mathbf{y}_j = - \\mathbf{y}_i$ and not $\\mathbf{y}_i$.\nWe then have that $\\|\\mathbf{y}_j\\|_2^2$ is equal for all $j$, hence $P_{ij} \\coloneqq \\frac{\\exp(-\\|\\mathbf{y}_j\\|_2^2)}{\\sum_k \\exp(-\\|\\mathbf{y}_k\\|_2^2)} = \\frac{1}{N}$ for all $j$. \nThen for $i \\neq j$, $\\tilde{K}_{ij}$ simplifies to\n\\begin{equation*}\n\\tilde{K}_{ij} = - \\frac{1}{N} \\left(-\\mathbf{y}_i - \\frac{1}{N} (N-2) (-\\mathbf{y}_i)\\right) (-\\mathbf{y}_i)^\\top  = - \\frac{2N-2}{N^2} \\mathbf{y}_i \\mathbf{y}_i^\\top \n\\end{equation*}\nwhose entries are unbounded since $\\mathbf{y}_i$ can be any vector in $\\mathbb{R}^{D/H}$ (note we assume $N \\geq 2$ for self-attention to be well-defined, hence $2N-2 \\neq 0$).",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n<br>\n<br>1. <i>Expression Definition</i>: The proof starts by defining the expression \\(\\tilde{K}_{ij}\\) in terms of \\(P_{ij}\\), \\(W^{K^\\top}\\), and \\(\\mathbf{x}_j\\):\n<br>   \\[\n   \\tilde{K}_{ij} \\coloneqq P_{ij} W^{K^\\top}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k)(\\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K)\n   \\]\n<br>\n<br>2. <i>Relation to \\(\\tilde{J}_{ij}\\)</i>: It is shown that \\(\\tilde{K}_{ij}\\) is related to \\(\\tilde{J}_{ij}\\) by the equation:\n<br>   \\[\n   W^{K^\\top} \\tilde{J}_{ij} = \\left(\\frac{2}{\\sqrt{D/H}} \\tilde{K}_{ij} + P_{ij}I \\right) W^{K^\\top}\n   \\]\n<br>\n<br>3. <i>Unboundedness of \\(\\tilde{K}_{ij}\\)</i>: To prove \\(\\tilde{J}_{ij}\\) is unbounded, it suffices to show \\(\\tilde{K}_{ij}\\) is unbounded, given \\(W^K\\) is full rank and \\(P_{ij} \\in [0,1]\\).\n<br>\n<br>4. <i>Substitution of \\(\\mathbf{y}_j\\)</i>: Define \\(\\mathbf{y}_j^\\top = \\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K\\). Then:\n<br>   \\[\n   \\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k = - W^{K^\\top}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k)\n   \\]\n<br>\n<br>5. <i>Expression for \\(\\tilde{K}_{ij}\\)</i>: Using the above substitution, \\(\\tilde{K}_{ij}\\) is expressed as:\n<br>   \\[\n   \\tilde{K}_{ij} = - P_{ij} (\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k) \\mathbf{y}_j^\\top\n   \\]\n<br>\n<br>6. <i>Choice of \\(\\mathbf{x}_j\\)</i>: Choose \\(\\mathbf{x}_j\\) such that \\(\\mathbf{y}_j = -\\mathbf{y}_i\\). This is possible since \\(W^K\\) is full-rank.\n<br>\n<br>7. <i>Equal Norms and Simplification</i>: With \\(\\mathbf{y}_j = -\\mathbf{y}_i\\), the norms \\(\\|\\mathbf{y}_j\\|_2^2\\) are equal for all \\(j\\), leading to:\n<br>   \\[\n   P_{ij} = \\frac{1}{N}\n   \\]\n<br>\n<br>8. <i>Final Simplification</i>: For \\(i \\neq j\\), \\(\\tilde{K}_{ij}\\) simplifies to:\n<br>   \\[\n   \\tilde{K}_{ij} = - \\frac{2N-2}{N^2} \\mathbf{y}_i \\mathbf{y}_i^\\top\n   \\]\n<br>\n<br>9. <i>Unbounded Entries</i>: The entries of \\(\\tilde{K}_{ij}\\) are unbounded since \\(\\mathbf{y}_i\\) can be any vector in \\(\\mathbb{R}^{D/H}\\), and \\(2N-2 \\neq 0\\) for \\(N \\geq 2\\)."
                        }
                    },
                    {
                        "statement_id": "5099566b-88d9-4731-a228-b2b8ef75b56e",
                        "paper_id": "895bbef7-305d-4454-850a-7279bb832d9d",
                        "library_nr": 3,
                        "library_name": "Lemma 3",
                        "title": "Covariance Trace Bound",
                        "statement_original_tex": "\\begin{lemma} \\label{lemma:key}\n$\\Tr(\\mathrm{Cov}(\\mathbb{Y})) = \\sum_j P_{ij}\\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_2^2 \\leq \\sum_j P_{ij}\\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2 \\leq \\phi^{-1}(N-1)$ where $\\phi(c) = c \\exp(c+1)$ is a one-dimensional invertible function on $\\mathbb{R}_{\\geq 0}$.\n\\end{lemma}",
                        "statement_html": "$\\Tr(\\mathrm{Cov}(\\mathbb{Y})) = \\sum_j P_{ij}\\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_2^2 \\leq \\sum_j P_{ij}\\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2 \\leq \\phi^{-1}(N-1)$ where $\\phi(c) = c \\exp(c+1)$ is a one-dimensional invertible function on $\\mathbb{R}_{\\geq 0}$.",
                        "statement_type": "lemma",
                        "statement_motivation_html": "This inequality provides a bound on the trace of the covariance matrix of a set of vectors $\\mathbb{Y}$, which is crucial in understanding the spread and variability of the data. It is particularly useful in fields such as machine learning and statistics, where controlling the variance is important for ensuring the stability and performance of algorithms. The function $\\phi(c)$ helps in deriving these bounds, making it easier to manage and interpret the data's dispersion.",
                        "html_url": "library/lemmas/lemma_3/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "b56e159b-30dd-4e6b-9457-1067195cc0d2",
                            "paper_id": "895bbef7-305d-4454-850a-7279bb832d9d",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\nThe first equality holds since $\\Tr(\\mathrm{Cov}(\\mathbb{Y})) = \\sum_j \\mathrm{Cov}(\\mathbb{Y})_{jj} = \\sum_j \\mathrm{Var}(\\mathbb{Y}_j) = \\sum_j \\mathbb{E}[(\\mathbb{Y}_j -\\mathbb{E}[\\mathbb{Y}_j])^2]$. \nThe next inequality holds since $\\mathrm{Var}(\\mathbb{Y}_j) = \\mathrm{Var}(\\overline{\\mathbb{Y}}_j) = \\mathbb{E}[\\overline{\\mathbb{Y}}_j^2] -\\mathbb{E}[\\overline{\\mathbb{Y}}_j]^2 \\leq \\mathbb{E}[\\overline{\\mathbb{Y}}_j^2]$ where $\\overline{\\mathbb{Y}}= \\mathbb{Y} - y_i$. \nThe final inequality can be proved as follows.\n\nWe would like to bound \n\\begin{equation}\n    \\sum_j P_{ij}\\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2 = \\frac{\\sum_j \\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2 \\exp(-\\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2)}{\\sum_k \\exp(-\\|\\mathbf{y}_k-\\mathbf{y}_i\\|_2^2)}  = \\frac{\\sum_j z_j^2 \\exp(-z_j^2)}{\\sum_k \\exp(-z_k^2)}\n\\end{equation}\nwhere $z_j \\coloneqq \\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2$ (hence $z_i=0$). \nDefine:\n\\begin{equation}\n    g(\\mathbf{z}) \\coloneqq \\frac{\\sum_j z_j^2 \\exp(-z_j^2)}{\\sum_k \\exp(-z_k^2)} = \\frac{\\sum_{j \\neq i} z_j^2 \\exp(-z_j^2)}{1 + \\sum_{k \\neq i} \\exp(-z_k^2)}.\n\\end{equation}\nFirst note that as $z_j \\rightarrow \\infty$, $\\exp(-z_j^2) \\rightarrow 0$ exponentially fast, causing the product $z_j^2 \\exp(-z_j^2) \\rightarrow 0$.\nHence we expect the above quantity to be bounded and attain its maximum.\n\nLet $h(z_j) \\coloneqq \\exp(-z_j^2)$ for notational conciseness, and note $h(z_j) > 0$. By taking partial derivatives with the chain rule, we have that for $j \\neq i$\n\\begin{equation}\n\\frac{\\partial g(\\mathbf{z})}{\\partial z_j} = \\frac{2z_j h(z_j)}{(\\sum_k h(z_k))^2}\\left[(1-z_j^2)\\sum_k h(z_k) + \\sum_k h(z_k)z_k^2\\right].\n\\end{equation}\nHence the derivative is $0$ if and only if $z_j = 0$ or $(1-z_j^2)\\sum_k h(z_k) + \\sum_k h(z_k)z_k^2 = 0$, the latter being equivalent to $z_j^2 = 1 + \\frac{\\sum_k h(z_k)z_k^2}{\\sum_k h(z_k)} = 1 + g(\\mathbf{z})$. \nHence at the maximum, the non-zero values among $\\{z_j\\}_{j=1}^N$ must be equal to one another.\nIt is clear now that the maximum value $c$ is attained when $z_j^2 = 1 + c$ for $j \\neq i$ (and recall $z_i = 0$). \nSo $h(z_j) = \\exp(-1-c)$ for $j \\neq i$.\nSubstituting this into $g(z)$, and rearranging, we obtain $c \\exp(c+1) = N - 1$. Note $\\phi(x) \\coloneqq x \\exp(x+1)$ is increasing for $x > 0$ hence $c = \\phi^{-1}(N-1)$.\n\\end{proof}",
                            "statement_html": "The first equality holds since $\\Tr(\\mathrm{Cov}(\\mathbb{Y})) = \\sum_j \\mathrm{Cov}(\\mathbb{Y})_{jj} = \\sum_j \\mathrm{Var}(\\mathbb{Y}_j) = \\sum_j \\mathbb{E}[(\\mathbb{Y}_j -\\mathbb{E}[\\mathbb{Y}_j])^2]$. \nThe next inequality holds since $\\mathrm{Var}(\\mathbb{Y}_j) = \\mathrm{Var}(\\overline{\\mathbb{Y}}_j) = \\mathbb{E}[\\overline{\\mathbb{Y}}_j^2] -\\mathbb{E}[\\overline{\\mathbb{Y}}_j]^2 \\leq \\mathbb{E}[\\overline{\\mathbb{Y}}_j^2]$ where $\\overline{\\mathbb{Y}}= \\mathbb{Y} - y_i$. \nThe final inequality can be proved as follows.\n\nWe would like to bound \n\\begin{equation}\n    \\sum_j P_{ij}\\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2 = \\frac{\\sum_j \\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2 \\exp(-\\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2)}{\\sum_k \\exp(-\\|\\mathbf{y}_k-\\mathbf{y}_i\\|_2^2)}  = \\frac{\\sum_j z_j^2 \\exp(-z_j^2)}{\\sum_k \\exp(-z_k^2)}\n\\end{equation}\nwhere $z_j \\coloneqq \\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2$ (hence $z_i=0$). \nDefine:\n\\begin{equation}\n    g(\\mathbf{z}) \\coloneqq \\frac{\\sum_j z_j^2 \\exp(-z_j^2)}{\\sum_k \\exp(-z_k^2)} = \\frac{\\sum_{j \\neq i} z_j^2 \\exp(-z_j^2)}{1 + \\sum_{k \\neq i} \\exp(-z_k^2)}.\n\\end{equation}\nFirst note that as $z_j \\rightarrow \\infty$, $\\exp(-z_j^2) \\rightarrow 0$ exponentially fast, causing the product $z_j^2 \\exp(-z_j^2) \\rightarrow 0$.\nHence we expect the above quantity to be bounded and attain its maximum.\n\nLet $h(z_j) \\coloneqq \\exp(-z_j^2)$ for notational conciseness, and note $h(z_j) > 0$. By taking partial derivatives with the chain rule, we have that for $j \\neq i$\n\\begin{equation}\n\\frac{\\partial g(\\mathbf{z})}{\\partial z_j} = \\frac{2z_j h(z_j)}{(\\sum_k h(z_k))^2}\\left[(1-z_j^2)\\sum_k h(z_k) + \\sum_k h(z_k)z_k^2\\right].\n\\end{equation}\nHence the derivative is $0$ if and only if $z_j = 0$ or $(1-z_j^2)\\sum_k h(z_k) + \\sum_k h(z_k)z_k^2 = 0$, the latter being equivalent to $z_j^2 = 1 + \\frac{\\sum_k h(z_k)z_k^2}{\\sum_k h(z_k)} = 1 + g(\\mathbf{z})$. \nHence at the maximum, the non-zero values among $\\{z_j\\}_{j=1}^N$ must be equal to one another.\nIt is clear now that the maximum value $c$ is attained when $z_j^2 = 1 + c$ for $j \\neq i$ (and recall $z_i = 0$). \nSo $h(z_j) = \\exp(-1-c)$ for $j \\neq i$.\nSubstituting this into $g(z)$, and rearranging, we obtain $c \\exp(c+1) = N - 1$. Note $\\phi(x) \\coloneqq x \\exp(x+1)$ is increasing for $x > 0$ hence $c = \\phi^{-1}(N-1)$.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n<br>\n<br>1. <i>Trace of Covariance</i>: The proof starts by expressing the trace of the covariance matrix \\(\\mathrm{Cov}(\\mathbb{Y})\\) in terms of the variances of its components:\n<br>   \\[\n   \\Tr(\\mathrm{Cov}(\\mathbb{Y})) = \\sum_j \\mathrm{Cov}(\\mathbb{Y})_{jj} = \\sum_j \\mathrm{Var}(\\mathbb{Y}_j) = \\sum_j \\mathbb{E}[(\\mathbb{Y}_j -\\mathbb{E}[\\mathbb{Y}_j])^2]\n   \\]\n<br>\n<br>2. <i>Variance Inequality</i>: The next step involves an inequality for the variance of \\(\\mathbb{Y}_j\\). By defining \\(\\overline{\\mathbb{Y}} = \\mathbb{Y} - y_i\\), we have:\n<br>   \\[\n   \\mathrm{Var}(\\mathbb{Y}_j) = \\mathrm{Var}(\\overline{\\mathbb{Y}}_j) = \\mathbb{E}[\\overline{\\mathbb{Y}}_j^2] -\\mathbb{E}[\\overline{\\mathbb{Y}}_j]^2 \\leq \\mathbb{E}[\\overline{\\mathbb{Y}}_j^2]\n   \\]\n<br>\n<br>3. <i>Bounding the Sum</i>: The goal is to bound the following sum:\n<br>   \\[\n   \\sum_j P_{ij}\\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2 = \\frac{\\sum_j \\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2 \\exp(-\\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2)}{\\sum_k \\exp(-\\|\\mathbf{y}_k-\\mathbf{y}_i\\|_2^2)} = \\frac{\\sum_j z_j^2 \\exp(-z_j^2)}{\\sum_k \\exp(-z_k^2)}\n   \\]\n<br>   where \\(z_j \\coloneqq \\|\\mathbf{y}_j-\\mathbf{y}_i\\|\\) and \\(z_i = 0\\).\n<br>\n<br>4. <i>Function Definition</i>: Define the function \\(g(\\mathbf{z})\\) for notational convenience:\n<br>   \\[\n   g(\\mathbf{z}) \\coloneqq \\frac{\\sum_j z_j^2 \\exp(-z_j^2)}{\\sum_k \\exp(-z_k^2)} = \\frac{\\sum_{j \\neq i} z_j^2 \\exp(-z_j^2)}{1 + \\sum_{k \\neq i} \\exp(-z_k^2)}\n   \\]\n<br>\n<br>5. <i>Behavior at Infinity</i>: As \\(z_j \\rightarrow \\infty\\), \\(\\exp(-z_j^2) \\rightarrow 0\\) exponentially fast, causing \\(z_j^2 \\exp(-z_j^2) \\rightarrow 0\\). This suggests that \\(g(\\mathbf{z})\\) is bounded and attains its maximum.\n<br>\n<br>6. <i>Partial Derivatives</i>: Define \\(h(z_j) \\coloneqq \\exp(-z_j^2)\\). By taking partial derivatives using the chain rule, we get:\n<br>   \\[\n   \\frac{\\partial g(\\mathbf{z})}{\\partial z_j} = \\frac{2z_j h(z_j)}{(\\sum_k h(z_k))^2}\\left[(1-z_j^2)\\sum_k h(z_k) + \\sum_k h(z_k)z_k^2\\right]\n   \\]\n<br>   The derivative is zero if and only if \\(z_j = 0\\) or \\(z_j^2 = 1 + g(\\mathbf{z})\\).\n<br>\n<br>7. <i>Maximum Condition</i>: At the maximum, the non-zero values among \\(\\{z_j\\}_{j=1}^N\\) must be equal. The maximum value \\(c\\) is attained when \\(z_j^2 = 1 + c\\) for \\(j \\neq i\\). Thus, \\(h(z_j) = \\exp(-1-c)\\) for \\(j \\neq i\\).\n<br>\n<br>8. <i>Final Rearrangement</i>: Substituting into \\(g(z)\\) and rearranging, we obtain:\n<br>   \\[\n   c \\exp(c+1) = N - 1\n   \\]\n<br>   Since \\(\\phi(x) \\coloneqq x \\exp(x+1)\\) is increasing for \\(x > 0\\), we have \\(c = \\phi^{-1}(N-1)\\)."
                        }
                    },
                    {
                        "statement_id": "eba227d5-5ce7-4364-a484-b4b5debced71",
                        "paper_id": "895bbef7-305d-4454-850a-7279bb832d9d",
                        "library_nr": 4,
                        "library_name": "Lemma 4",
                        "title": "Matrix Norm Bound Lemma",
                        "statement_original_tex": "\\begin{lemma}\n$\\|Y^\\top P^{(i)}Y\\|_\\infty \\leq  \\phi^{-1}(N-1) \\sqrt{D/H} $ ($\\phi$ defined as in Lemma \\ref{lemma:key}).\n\\end{lemma}",
                        "statement_html": "$\\|Y^\\top P^{(i)}Y\\|_\\infty \\leq  \\phi^{-1}(N-1) \\sqrt{D/H} $ ($\\phi$ defined as in Lemma <a href=\"library/lemmas/lemma_3/index.html#lemma%3Akey\">Lemma 3</a>).",
                        "statement_type": "lemma",
                        "statement_motivation_html": "This inequality is useful in the context of matrix analysis and optimization. It provides an upper bound on the infinity norm of the product of a transposed matrix $Y^\\top$, a matrix $P^{(i)}$, and the matrix $Y$. This can be particularly helpful when assessing the stability and performance of algorithms that involve such matrix products, especially in high-dimensional settings where direct computation might be infeasible.",
                        "html_url": "library/lemmas/lemma_4/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "adc140ac-9549-4ae7-93e1-5617fc6122a4",
                            "paper_id": "895bbef7-305d-4454-850a-7279bb832d9d",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\nRecall that $Y^\\top P^{(i)}Y = \\mathrm{Cov}(\\mathbb{Y})$. Let $\\sigma(\\mathbb{Y}_m)$ denote the standard deviation of $\\mathbb{Y}_m$. Then $[\\mathrm{Cov}(\\mathbb{Y})]_{lm} \\leq \\sigma(\\mathbb{Y}_l)\\sigma(\\mathbb{Y}_m)$.\nHence \n\\begin{align*}\n\\|\\mathrm{Cov}(\\mathbb{Y})\\|_{\\infty} = \\max_l \\sum_m \\left|[\\mathrm{Cov}(\\mathbb{Y})]_{lm}\\right| \n& \\leq  \\max_l \\sigma(\\mathbb{Y}_l) \\sum_m \\sigma(\\mathbb{Y}_m) \\\\\n& \\leq \\sqrt{\\frac{D}{H}} \\sum_m \\sigma^2(\\mathbb{Y}_m)  = \\sqrt{\\frac{D}{H}} \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\\\\n& \\leq \\sqrt{\\frac{D}{H}} \\phi^{-1}(N-1),\n\\end{align*}\nsince $\\sum_m \\sigma(\\mathbb{Y}_m) \\leq \\sqrt{\\frac{D}{H}} \\sqrt{\\sum_m \\sigma^2(\\mathbb{Y}_m)}$ (by e.g.~using the Cauchy--Schwartz inequality on $[\\sigma(\\mathbb{Y}_1), \\ldots, \\sigma(\\mathbb{Y}_{D/H})]$ and $\\mathds{1}$) and $\\max_l \\sigma(\\mathbb{Y}_l) \\leq \\sqrt{\\sum_m \\sigma^2(\\mathbb{Y}_m)}$, and the last inequality is from Lemma \\ref{lemma:key}. \n\\end{proof}",
                            "statement_html": "Recall that $Y^\\top P^{(i)}Y = \\mathrm{Cov}(\\mathbb{Y})$. Let $\\sigma(\\mathbb{Y}_m)$ denote the standard deviation of $\\mathbb{Y}_m$. Then $[\\mathrm{Cov}(\\mathbb{Y})]_{lm} \\leq \\sigma(\\mathbb{Y}_l)\\sigma(\\mathbb{Y}_m)$. Hence \n\\begin{align*}\n\\|\\mathrm{Cov}(\\mathbb{Y})\\|_{\\infty} = \\max_l \\sum_m \\left|[\\mathrm{Cov}(\\mathbb{Y})]_{lm}\\right| \n& \\leq  \\max_l \\sigma(\\mathbb{Y}_l) \\sum_m \\sigma(\\mathbb{Y}_m) \\\\\n& \\leq \\sqrt{\\frac{D}{H}} \\sum_m \\sigma^2(\\mathbb{Y}_m)  = \\sqrt{\\frac{D}{H}} \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\\\\n& \\leq \\sqrt{\\frac{D}{H}} \\phi^{-1}(N-1),\n\\end{align*}\nsince $\\sum_m \\sigma(\\mathbb{Y}_m) \\leq \\sqrt{\\frac{D}{H}} \\sqrt{\\sum_m \\sigma^2(\\mathbb{Y}_m)}$ (by e.g.~using the Cauchy--Schwartz inequality on $[\\sigma(\\mathbb{Y}_1), \\ldots, \\sigma(\\mathbb{Y}_{D/H})]$ and $\\mathds{1}$) and $\\max_l \\sigma(\\mathbb{Y}_l) \\leq \\sqrt{\\sum_m \\sigma^2(\\mathbb{Y}_m)}$, and the last inequality is from Lemma <a href=\"library/lemmas/lemma_3/index.html#lemma%3Akey\">Lemma 3</a>.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n<br>\n<br>1. <i>Covariance Definition</i>: The proof starts by recalling that the covariance matrix of \\(\\mathbb{Y}\\) is given by \\(Y^\\top P^{(i)}Y = \\mathrm{Cov}(\\mathbb{Y})\\).\n<br>\n<br>2. <i>Standard Deviation Bound</i>: It is noted that the covariance element \\([\\mathrm{Cov}(\\mathbb{Y})]_{lm}\\) is bounded by the product of the standard deviations of \\(\\mathbb{Y}_l\\) and \\(\\mathbb{Y}_m\\):\n<br>   \\[\n   [\\mathrm{Cov}(\\mathbb{Y})]_{lm} \\leq \\sigma(\\mathbb{Y}_l)\\sigma(\\mathbb{Y}_m)\n   \\]\n<br>\n<br>3. <i>Infinity Norm of Covariance</i>: The infinity norm of the covariance matrix is then expressed as:\n<br>   \\[\n   \\|\\mathrm{Cov}(\\mathbb{Y})\\|_{\\infty} = \\max_l \\sum_m \\left|[\\mathrm{Cov}(\\mathbb{Y})]_{lm}\\right|\n   \\]\n<br>   This is bounded by:\n<br>   \\[\n   \\max_l \\sigma(\\mathbb{Y}_l) \\sum_m \\sigma(\\mathbb{Y}_m)\n   \\]\n<br>\n<br>4. <i>Sum of Standard Deviations</i>: The sum of the standard deviations \\(\\sum_m \\sigma(\\mathbb{Y}_m)\\) is further bounded using the Cauchy-Schwarz inequality:\n<br>   \\[\n   \\sum_m \\sigma(\\mathbb{Y}_m) \\leq \\sqrt{\\frac{D}{H}} \\sqrt{\\sum_m \\sigma^2(\\mathbb{Y}_m)}\n   \\]\n<br>\n<br>5. <i>Trace of Covariance</i>: Recognizing that \\(\\sum_m \\sigma^2(\\mathbb{Y}_m)\\) is the trace of the covariance matrix, we get:\n<br>   \\[\n   \\sqrt{\\frac{D}{H}} \\sum_m \\sigma^2(\\mathbb{Y}_m) = \\sqrt{\\frac{D}{H}} \\Tr(\\mathrm{Cov}(\\mathbb{Y}))\n   \\]\n<br>\n<br>6. <i>Final Bound</i>: Finally, using the bound from Lemma 3, the trace of the covariance matrix is bounded by \\(\\phi^{-1}(N-1)\\):\n<br>   \\[\n   \\sqrt{\\frac{D}{H}} \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\sqrt{\\frac{D}{H}} \\phi^{-1}(N-1)\n   \\]\n<br>\n<br>Thus, the proof shows that:\n<br>\\[\n\\|\\mathrm{Cov}(\\mathbb{Y})\\|_{\\infty} \\leq \\sqrt{\\frac{D}{H}} \\phi^{-1}(N-1)\n\\]"
                        }
                    },
                    {
                        "statement_id": "7686508f-1252-4a52-bbd2-97ec9bb2027a",
                        "paper_id": "895bbef7-305d-4454-850a-7279bb832d9d",
                        "library_nr": 5,
                        "library_name": "Lemma 5",
                        "title": "Low-Rank Approximation Bound",
                        "statement_original_tex": "\\begin{lemma} \\label{lemma:low_rank}\n$\\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty \\leq  \\phi^{-1}(N-1) \\sqrt{D/H}$.\n\\end{lemma}",
                        "statement_html": "$\\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty \\leq  \\phi^{-1}(N-1) \\sqrt{D/H}$.",
                        "statement_type": "lemma",
                        "statement_motivation_html": "This inequality is useful in the context of high-dimensional data analysis and machine learning, particularly in the study of the stability and robustness of algorithms. It provides a bound on the influence of individual data points on the overall system, which is crucial for understanding the behavior of algorithms under perturbations or noise. Use this inequality when you need to ensure that your model or algorithm remains stable and performs reliably even when subjected to small changes in the input data.",
                        "html_url": "library/lemmas/lemma_5/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "3207d43d-d796-4eeb-99ea-9b3ef3b8a4a3",
                            "paper_id": "895bbef7-305d-4454-850a-7279bb832d9d",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\nNote $\\|\\mathbf{u}\\mathbf{v}^\\top\\|_{\\infty} = \\|\\mathbf{u}\\|_{\\infty} \\|\\mathbf{v}\\|_1$ for real vectors $\\mathbf{u},\\mathbf{v}$. Hence\n\\begin{align*}\n    \\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty & = \\sum_j P_{ij} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_\\infty \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_1 \\\\\n    & = \\mathbf{a}^\\top \\mathbf{b} \\leq \\|\\mathbf{a}\\|_2 \\|\\mathbf{b}\\|_2,\n\\end{align*}\nwhere $a_j = \\sqrt{P_{ij}} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_\\infty$, $b_j = \\sqrt{P_{ij}} \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_1$.\n\nNote $a_j \\leq c_j \\coloneqq  \\sqrt{P_{ij}} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_2$ since $\\|\\mathbf{u}\\|_\\infty \\leq \\|\\mathbf{u}\\|_2$ for vector $\\mathbf{u}$. Hence $\\|\\mathbf{a}\\|_2 \\leq \\|\\mathbf{c}\\|_2$.\n\nAlso $b_j \\leq \\sqrt{\\frac{D}{H}} d_j \\coloneqq  \\sqrt{\\frac{D}{H}} \\sqrt{P_{ij}} \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_2$ since $\\|\\mathbf{u}\\|_1 \\leq \\sqrt{\\frac{D}{H}}\\|\\mathbf{u}\\|_2$ for $\\mathbf{u} \\in \\mathbb{R}^{D/H}$ (e.g.~by the Cauchy--Schwartz inequality on $[|\\mathbf{u}_1|, \\ldots, |\\mathbf{u}_{D/H}|]$ and $\\mathds{1}$). Hence $\\|b\\|_2 \\leq \\sqrt{\\frac{D}{H}}\\|d\\|_2$.\n\nNote $\\|c\\|_2^2 = \\sum_j P_{ij} \\|y_j - \\sum_k P_{ik} y_k\\|_2^2 = \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\phi^{-1}(N-1)$ from Lemma \\ref{lemma:key},\nand $\\|d\\|_2^2 =  \\sum_j P_{ij} \\|y_i - y_j\\|_2^2 \\leq  \\phi^{-1}(N-1)$ also from Lemma \\ref{lemma:key}.\nHence $\\|a\\|_2 \\|b\\|_2 \\leq \\sqrt{\\frac{D}{H}} \\|c\\|_2 \\|d\\|_2 \\leq \\sqrt{\\frac{D}{H}} \\phi^{-1}(N-1)$.\n\\end{proof}",
                            "statement_html": "Note $\\|\\mathbf{u}\\mathbf{v}^\\top\\|_{\\infty} = \\|\\mathbf{u}\\|_{\\infty} \\|\\mathbf{v}\\|_1$ for real vectors $\\mathbf{u},\\mathbf{v}$. Hence\n\\begin{align*}\n    \\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty & = \\sum_j P_{ij} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_\\infty \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_1 \\\\\n    & = \\mathbf{a}^\\top \\mathbf{b} \\leq \\|\\mathbf{a}\\|_2 \\|\\mathbf{b}\\|_2,\n\\end{align*}\nwhere $a_j = \\sqrt{P_{ij}} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_\\infty$, $b_j = \\sqrt{P_{ij}} \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_1$.\n\nNote $a_j \\leq c_j \\coloneqq  \\sqrt{P_{ij}} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_2$ since $\\|\\mathbf{u}\\|_\\infty \\leq \\|\\mathbf{u}\\|_2$ for vector $\\mathbf{u}$. Hence $\\|\\mathbf{a}\\|_2 \\leq \\|\\mathbf{c}\\|_2$.\n\nAlso $b_j \\leq \\sqrt{\\frac{D}{H}} d_j \\coloneqq  \\sqrt{\\frac{D}{H}} \\sqrt{P_{ij}} \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_2$ since $\\|\\mathbf{u}\\|_1 \\leq \\sqrt{\\frac{D}{H}}\\|\\mathbf{u}\\|_2$ for $\\mathbf{u} \\in \\mathbb{R}^{D/H}$ (e.g.~by the Cauchy--Schwartz inequality on $[|\\mathbf{u}_1|, \\ldots, |\\mathbf{u}_{D/H}|]$ and $\\mathds{1}$). Hence $\\|b\\|_2 \\leq \\sqrt{\\frac{D}{H}}\\|d\\|_2$.\n\nNote $\\|c\\|_2^2 = \\sum_j P_{ij} \\|y_j - \\sum_k P_{ik} y_k\\|_2^2 = \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\phi^{-1}(N-1)$ from Lemma <a href=\"library/lemmas/lemma_3/index.html#lemma%3Akey\">Lemma 3</a>,\nand $\\|d\\|_2^2 =  \\sum_j P_{ij} \\|y_i - y_j\\|_2^2 \\leq  \\phi^{-1}(N-1)$ also from Lemma <a href=\"library/lemmas/lemma_3/index.html#lemma%3Akey\">Lemma 3</a>.\nHence $\\|a\\|_2 \\|b\\|_2 \\leq \\sqrt{\\frac{D}{H}} \\|c\\|_2 \\|d\\|_2 \\leq \\sqrt{\\frac{D}{H}} \\phi^{-1}(N-1)$.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n<br>\n<br>1. <i>Infinity Norm of Outer Product</i>: The proof starts by noting that the infinity norm of the outer product of two real vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) is given by:\n<br>   \\[\n   \\|\\mathbf{u}\\mathbf{v}^\\top\\|_{\\infty} = \\|\\mathbf{u}\\|_{\\infty} \\|\\mathbf{v}\\|_1\n   \\]\n<br>\n<br>2. <i>Summation and Norms</i>: Using the above property, the proof considers the sum involving \\(P_{ij}\\) and the norms of vectors \\(\\mathbf{y}_j\\), \\(\\mathbf{y}_i\\), and \\(\\mathbf{y}_k\\):\n<br>   \\[\n   \\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty = \\sum_j P_{ij} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_\\infty \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_1\n   \\]\n<br>\n<br>3. <i>Vector Definitions</i>: The proof introduces vectors \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) defined as:\n<br>   \\[\n   a_j = \\sqrt{P_{ij}} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_\\infty, \\quad b_j = \\sqrt{P_{ij}} \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_1\n   \\]\n<br>\n<br>4. <i>Inner Product and Cauchy-Schwarz</i>: The inner product \\(\\mathbf{a}^\\top \\mathbf{b}\\) is then considered and bounded using the Cauchy-Schwarz inequality:\n<br>   \\[\n   \\mathbf{a}^\\top \\mathbf{b} \\leq \\|\\mathbf{a}\\|_2 \\|\\mathbf{b}\\|_2\n   \\]\n<br>\n<br>5. <i>Bounding \\(\\mathbf{a}\\)</i>: The proof shows that \\(a_j \\leq c_j\\) where:\n<br>   \\[\n   c_j \\coloneqq  \\sqrt{P_{ij}} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_2\n   \\]\n<br>   This follows from the property \\(\\|\\mathbf{u}\\|_\\infty \\leq \\|\\mathbf{u}\\|_2\\) for any vector \\(\\mathbf{u}\\). Hence, \\(\\|\\mathbf{a}\\|_2 \\leq \\|\\mathbf{c}\\|_2\\).\n<br>\n<br>6. <i>Bounding \\(\\mathbf{b}\\)</i>: Similarly, \\(b_j \\leq \\sqrt{\\frac{D}{H}} d_j\\) where:\n<br>   \\[\n   d_j \\coloneqq  \\sqrt{\\frac{D}{H}} \\sqrt{P_{ij}} \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_2\n   \\]\n<br>   This follows from the property \\(\\|\\mathbf{u}\\|_1 \\leq \\sqrt{\\frac{D}{H}}\\|\\mathbf{u}\\|_2\\) for \\(\\mathbf{u} \\in \\mathbb{R}^{D/H}\\). Hence, \\(\\|\\mathbf{b}\\|_2 \\leq \\sqrt{\\frac{D}{H}}\\|\\mathbf{d}\\|_2\\).\n<br>\n<br>7. <i>Bounding \\(\\|\\mathbf{c}\\|_2\\) and \\(\\|\\mathbf{d}\\|_2\\)</i>: The proof uses Lemma 3 to bound \\(\\|\\mathbf{c}\\|_2^2\\) and \\(\\|\\mathbf{d}\\|_2^2\\):\n<br>   \\[\n   \\|c\\|_2^2 = \\sum_j P_{ij} \\|y_j - \\sum_k P_{ik} y_k\\|_2^2 = \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\phi^{-1}(N-1)\n   \\]\n<br>   \\[\n   \\|d\\|_2^2 =  \\sum_j P_{ij} \\|y_i - y_j\\|_2^2 \\leq  \\phi^{-1}(N-1)\n   \\]\n<br>\n<br>8. <i>Final Bound</i>: Combining the above results, the proof concludes with:\n<br>   \\[\n   \\|\\mathbf{a}\\|_2 \\|\\mathbf{b}\\|_2 \\leq \\sqrt{\\frac{D}{H}} \\|\\mathbf{c}\\|_2 \\|\\mathbf{d}\\|_2 \\leq \\sqrt{\\frac{D}{H}} \\phi^{-1}(N-1)\n   \\]"
                        }
                    },
                    {
                        "statement_id": "7c31f984-ae3e-4a11-97b9-b5d31c19626b",
                        "paper_id": "895bbef7-305d-4454-850a-7279bb832d9d",
                        "library_nr": 6,
                        "library_name": "Lemma 6",
                        "title": "Block row norm inequality",
                        "statement_original_tex": "\\begin{lemma} \\label{lemma:block_rows}\nLet A be a block matrix with block rows $A_1, \\ldots, A_N$. Then $\\|A\\|_2 \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2}$, and equality holds if and only if the first right singular vectors of the $A_i$ align.\n\\end{lemma}",
                        "statement_html": "Let $A$ be a block matrix with block rows $A_1, \\ldots, A_N$. Then $\\|A\\|_2 \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2}$, and equality holds if and only if the first right singular vectors of the $A_i$ align.",
                        "statement_type": "lemma",
                        "statement_motivation_html": "The given inequality for block matrices is particularly useful in numerical linear algebra and matrix analysis. It provides an upper bound for the 2-norm of a block matrix in terms of the 2-norms of its block rows. This can be applied in scenarios where the matrix is too large to handle directly, but its blocks are more manageable. Additionally, the condition for equality helps in understanding the structure of the matrix and the alignment of its singular vectors, which can be crucial for optimization problems and in the study of matrix decompositions.",
                        "html_url": "library/lemmas/lemma_6/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "8b22598b-e986-4ce0-a0ab-85d414f15c25",
                            "paper_id": "895bbef7-305d-4454-850a-7279bb832d9d",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\n\\begin{equation*}\n\\|A\\|_2^2 = \\left\\Vert \\begin{bmatrix} A_1 \\\\ \\vdots \\\\ A_N \\\\ \\end{bmatrix}\\right\\Vert_2^2 = \\sup_{\\|\\mathbf{x}\\|_2=1} \\left\\Vert\\begin{bmatrix} A_1 \\\\ \\vdots \\\\ A_N \\\\ \\end{bmatrix} \\mathbf{x}\\right\\Vert_2^2 = \\sup_{\\|\\mathbf{x}\\|_2=1} \\sum_i \\|A_i \\mathbf{x}\\|_2^2 \\leq \\sum_i \\sup_{\\|\\mathbf{x}\\|_2=1} \\|A_i \\mathbf{x}\\|_2^2 = \\sum_i \\|A_i\\|_2^2.\n\\end{equation*}\nNote that equality holds if and only if the first right singular vectors of the $A_i$ align.\n\\end{proof}",
                            "statement_html": "\\begin{equation*}\n\\|A\\|_2^2 = \\left\\Vert \\begin{bmatrix} A_1 \\\\ \\vdots \\\\ A_N \\\\ \\end{bmatrix}\\right\\Vert_2^2 = \\sup_{\\|\\mathbf{x}\\|_2=1} \\left\\Vert\\begin{bmatrix} A_1 \\\\ \\vdots \\\\ A_N \\\\ \\end{bmatrix} \\mathbf{x}\\right\\Vert_2^2 = \\sup_{\\|\\mathbf{x}\\|_2=1} \\sum_i \\|A_i \\mathbf{x}\\|_2^2 \\leq \\sum_i \\sup_{\\|\\mathbf{x}\\|_2=1} \\|A_i \\mathbf{x}\\|_2^2 = \\sum_i \\|A_i\\|_2^2.\n\\end{equation*}\nNote that equality holds if and only if the first right singular vectors of the $A_i$ align.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n<br>\n<br>1. <i>Norm Definition</i>: The proof starts by defining the squared 2-norm of matrix \\(A\\) as the squared 2-norm of a block matrix composed of submatrices \\(A_i\\):\n<br>   \\[\n   \\|A\\|_2^2 = \\left\\Vert \\begin{bmatrix} A_1 \\\\ \\vdots \\\\ A_N \\\\ \\end{bmatrix}\\right\\Vert_2^2\n   \\]\n<br>\n<br>2. <i>Supremum Representation</i>: The squared 2-norm is then expressed as the supremum over all unit vectors \\(\\mathbf{x}\\):\n<br>   \\[\n   \\sup_{\\|\\mathbf{x}\\|_2=1} \\left\\Vert\\begin{bmatrix} A_1 \\\\ \\vdots \\\\ A_N \\\\ \\end{bmatrix} \\mathbf{x}\\right\\Vert_2^2\n   \\]\n<br>\n<br>3. <i>Sum of Norms</i>: This supremum is expanded into a sum of squared norms of the submatrices \\(A_i\\) applied to \\(\\mathbf{x}\\):\n<br>   \\[\n   \\sup_{\\|\\mathbf{x}\\|_2=1} \\sum_i \\|A_i \\mathbf{x}\\|_2^2\n   \\]\n<br>\n<br>4. <i>Supremum of Each Term</i>: The sum of supremums is then bounded by the sum of the supremums of each individual term:\n<br>   \\[\n   \\leq \\sum_i \\sup_{\\|\\mathbf{x}\\|_2=1} \\|A_i \\mathbf{x}\\|_2^2\n   \\]\n<br>\n<br>5. <i>Norm of Submatrices</i>: Finally, each supremum is recognized as the squared 2-norm of the submatrix \\(A_i\\):\n<br>   \\[\n   = \\sum_i \\|A_i\\|_2^2\n   \\]\n<br>\n<br>Thus, the proof shows that:\n<br>\\[\n\\|A\\|_2^2 \\leq \\sum_i \\|A_i\\|_2^2\n\\]\n<br>\n<br>Note that equality holds if and only if the first right singular vectors of the \\(A_i\\) align."
                        }
                    },
                    {
                        "statement_id": "32b22b0c-56d2-467d-9b6a-07b9a6a73561",
                        "paper_id": "895bbef7-305d-4454-850a-7279bb832d9d",
                        "library_nr": 7,
                        "library_name": "Lemma 7",
                        "title": "Matrix Norm Bound Lemma",
                        "statement_original_tex": "\\begin{lemma}\n$\\|Y^\\top P^{(i)}Y\\|_2 \\leq \\phi^{-1}(N-1)$\n\\end{lemma}",
                        "statement_html": "$\\|Y^\\top P^{(i)}Y\\|_2 \\leq \\phi^{-1}(N-1)$",
                        "statement_type": "lemma",
                        "statement_motivation_html": "The given inequality $\\|Y^\\top P^{(i)}Y\\|_2 \\leq \\phi^{-1}(N-1)$ is useful in the context of matrix analysis and optimization. It provides a bound on the spectral norm of a transformed matrix, which can be critical when assessing the stability and performance of numerical algorithms. This statement is particularly valuable when dealing with large-scale data or complex systems where ensuring that certain matrix properties are within specific limits is essential for accurate and efficient computations.",
                        "html_url": "library/lemmas/lemma_7/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "5f2b07df-2a77-4e7c-9ed0-39ee45d6ce77",
                            "paper_id": "895bbef7-305d-4454-850a-7279bb832d9d",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\n$\\|Y^\\top P^{(i)}Y\\|_2=\\|\\mathrm{Cov}(\\mathbb{Y})\\|_2 = \\lambda_{\\max}(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\phi^{-1}(N-1)$, where the first equality holds by symmetry of $\\mathrm{Cov}(\\mathbb{Y})$ and the next holds by $\\mathrm{Cov}(\\mathbb{Y})$ being positive semi-definite, so all its eigenvalues are non-negative, and hence the maximal eigenvalue is bounded by the sum of the eigenvalues, equal to its trace. The final inequality is from Lemma \\ref{lemma:key}.\n\\end{proof}",
                            "statement_html": "$\\|Y^\\top P^{(i)}Y\\|_2=\\|\\mathrm{Cov}(\\mathbb{Y})\\|_2 = \\lambda_{\\max}(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\phi^{-1}(N-1)$, where the first equality holds by symmetry of $\\mathrm{Cov}(\\mathbb{Y})$ and the next holds by $\\mathrm{Cov}(\\mathbb{Y})$ being positive semi-definite, so all its eigenvalues are non-negative, and hence the maximal eigenvalue is bounded by the sum of the eigenvalues, equal to its trace. The final inequality is from Lemma <a href=\"library/lemmas/lemma_3/index.html#lemma%3Akey\">Lemma 3</a>.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n<br>\n<br>1. <i>Norm of Transformed Matrix</i>: The proof begins by considering the norm of the transformed matrix \\(Y^\\top P^{(i)}Y\\). This is expressed as:\n<br>   \\[\n   \\|Y^\\top P^{(i)}Y\\|_2\n   \\]\n<br>\n<br>2. <i>Covariance Representation</i>: Next, this norm is equated to the norm of the covariance matrix of \\(\\mathbb{Y}\\):\n<br>   \\[\n   \\|\\mathrm{Cov}(\\mathbb{Y})\\|_2\n   \\]\n<br>\n<br>3. <i>Maximal Eigenvalue</i>: The norm of the covariance matrix is then represented by its maximal eigenvalue:\n<br>   \\[\n   \\lambda_{\\max}(\\mathrm{Cov}(\\mathbb{Y}))\n   \\]\n<br>\n<br>4. <i>Trace Bound</i>: Since \\(\\mathrm{Cov}(\\mathbb{Y})\\) is positive semi-definite, all its eigenvalues are non-negative. Therefore, the maximal eigenvalue is bounded by the sum of the eigenvalues, which is equal to the trace of the covariance matrix:\n<br>   \\[\n   \\lambda_{\\max}(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\Tr(\\mathrm{Cov}(\\mathbb{Y}))\n   \\]\n<br>\n<br>5. <i>Application of Lemma</i>: Finally, the trace of the covariance matrix is bounded by \\(\\phi^{-1}(N-1)\\) as stated in Lemma 3:\n<br>   \\[\n   \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\phi^{-1}(N-1)\n   \\]\n<br>\n<br>Thus, the proof shows that:\n<br>\\[\n\\|Y^\\top P^{(i)}Y\\|_2 \\leq \\phi^{-1}(N-1)\n\\]"
                        }
                    },
                    {
                        "statement_id": "443abf64-9890-4dc3-9118-2140536895ab",
                        "paper_id": "895bbef7-305d-4454-850a-7279bb832d9d",
                        "library_nr": 8,
                        "library_name": "Lemma 8",
                        "title": "'Projection Inequality'",
                        "statement_original_tex": "\\begin{lemma}\n$\\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_2 \\leq  \\phi^{-1}(N-1)$\n\\end{lemma}",
                        "statement_html": "$\\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_2 \\leq  \\phi^{-1}(N-1)$",
                        "statement_type": "lemma",
                        "statement_motivation_html": "The given inequality is useful in the context of analyzing the stability and convergence of iterative algorithms, particularly in optimization and machine learning. It provides a bound on the sum of certain matrix norms, which can be critical for ensuring that the algorithm behaves as expected. This type of bound is often used to guarantee that the updates in an iterative process do not diverge, thereby ensuring the reliability and robustness of the algorithm.",
                        "html_url": "library/lemmas/lemma_8/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "48d1bb14-a8c4-46e0-9645-fc2fdccd3221",
                            "paper_id": "895bbef7-305d-4454-850a-7279bb832d9d",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\nDirectly use Cauchy--Schwartz on $c$ and $d$ in the proof of Lemma \\ref{lemma:low_rank}. \n\\end{proof}",
                            "statement_html": "Directly use Cauchy--Schwartz on $c$ and $d$ in the proof of Lemma <a href=\"library/lemmas/lemma_5/index.html#lemma%3Alow_rank\">Lemma 5</a>.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n<br>\n<br>1. <i>Application of Cauchy--Schwartz Inequality</i>: The proof begins by directly applying the Cauchy--Schwartz inequality to the variables \\(c\\) and \\(d\\).\n<br>\n<br>2. <i>Reference to Lemma 5</i>: This application is done in the context of the proof of Lemma 5, which can be found in the provided link.\n<br>\n<br>3. <i>Low Rank Condition</i>: The specific condition or property being utilized from Lemma 5 is the low rank condition, which is crucial for the application of the Cauchy--Schwartz inequality in this context.\n<br>\n<br>Thus, the proof shows that by using the Cauchy--Schwartz inequality on \\(c\\) and \\(d\\) within the framework of Lemma 5, the desired result is obtained."
                        }
                    },
                    {
                        "statement_id": "2c11fbd6-9bed-45c9-912d-9e9caffaae73",
                        "paper_id": "895bbef7-305d-4454-850a-7279bb832d9d",
                        "library_nr": 9,
                        "library_name": "Lemma 9",
                        "title": "Block Column Norm Inequality",
                        "statement_original_tex": "\\begin{lemma} \\label{lemma:block_cols}\nLet A be a block matrix with block columns $A_1, \\ldots, A_N$. Then $\\|A\\|_2 \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2}$.\n\\end{lemma}",
                        "statement_html": "Let $A$ be a block matrix with block columns $A_1, \\ldots, A_N$. Then $\\|A\\|_2 \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2}$.",
                        "statement_type": "lemma",
                        "statement_motivation_html": "The given inequality is useful in the context of block matrices, particularly when estimating the spectral norm of a matrix. It provides an upper bound for the spectral norm of the entire matrix in terms of the spectral norms of its block columns. This can be particularly helpful in numerical linear algebra and matrix analysis, where dealing with smaller submatrices can simplify computations and provide insights into the properties of the larger matrix.",
                        "html_url": "library/lemmas/lemma_9/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "46c3b6e1-7659-46bd-97e3-f70323c7b94a",
                            "paper_id": "895bbef7-305d-4454-850a-7279bb832d9d",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\n\\begin{align*}\n\\|A\\|_2 &= \\|[A_1, \\ldots, A_N]\\|_2 = \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1}  \\left\\Vert [A_1, \\ldots, A_N] \\begin{bmatrix} \\mathbf{x}_1\\\\ \\vdots \\\\ \\mathbf{x}_N \\\\ \\end{bmatrix} \\right\\Vert_2^2 = \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1} \\|\\sum_i A_i \\mathbf{x}_i\\|_2 \\\\ \n& \\leq \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1} \\sum_i \\|A_i \\mathbf{x}_i\\|_2 = \\sup_{\\|\\mathbf{e}_i\\|_2=1, \\sum_i \\lambda_i^2 =1} \\sum_i \\lambda_i \\|A_i \\mathbf{e}_i\\|_2 = \\sup_{\\sum_i \\lambda_i^2 =1} \\sum_i \\lambda_i \\|A_i\\|_2 \\\\\n& \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2},\n\\end{align*}\nwhere we are using the substitution $\\mathbf{x}_i = \\lambda_i \\mathbf{e}_i$, and the last inequality holds by e.g.~Cauchy--Schwartz inequality on $[\\lambda_1, \\ldots, \\lambda_N]$ and $[\\|A_1\\|_2, \\ldots, \\|A_N\\|_2]$.\n\\end{proof}",
                            "statement_html": "\\begin{align*}\n\\|A\\|_2 &= \\|[A_1, \\ldots, A_N]\\|_2 = \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1}  \\left\\Vert [A_1, \\ldots, A_N] \\begin{bmatrix} \\mathbf{x}_1\\\\ \\vdots \\\\ \\mathbf{x}_N \\\\ \\end{bmatrix} \\right\\Vert_2^2 = \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1} \\|\\sum_i A_i \\mathbf{x}_i\\|_2 \\\\ \n& \\leq \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1} \\sum_i \\|A_i \\mathbf{x}_i\\|_2 = \\sup_{\\|\\mathbf{e}_i\\|_2=1, \\sum_i \\lambda_i^2 =1} \\sum_i \\lambda_i \\|A_i \\mathbf{e}_i\\|_2 = \\sup_{\\sum_i \\lambda_i^2 =1} \\sum_i \\lambda_i \\|A_i\\|_2 \\\\\n& \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2},\n\\end{align*}\nwhere we are using the substitution $\\mathbf{x}_i = \\lambda_i \\mathbf{e}_i$, and the last inequality holds by e.g.~Cauchy--Schwartz inequality on $[\\lambda_1, \\ldots, \\lambda_N]$ and $[\\|A_1\\|_2, \\ldots, \\|A_N\\|_2]$.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n<br>\n<br>1. <i>Norm Definition</i>: The proof starts by defining the 2-norm of matrix \\(A\\) as:\n<br>   \\[\n   \\|A\\|_2 = \\|[A_1, \\ldots, A_N]\\|_2\n   \\]\n<br>\n<br>2. <i>Supremum of Quadratic Form</i>: The 2-norm is expressed as the supremum of a quadratic form involving vectors \\(\\mathbf{x}_i\\) such that their squared norms sum to 1:\n<br>   \\[\n   \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1}  \\left\\Vert [A_1, \\ldots, A_N] \\begin{bmatrix} \\mathbf{x}_1\\\\ \\vdots \\\\ \\mathbf{x}_N \\\\ \\end{bmatrix} \\right\\Vert_2^2\n   \\]\n<br>\n<br>3. <i>Vector Summation</i>: This expression is simplified to the norm of the sum of \\(A_i \\mathbf{x}_i\\):\n<br>   \\[\n   \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1} \\|\\sum_i A_i \\mathbf{x}_i\\|_2\n   \\]\n<br>\n<br>4. <i>Triangle Inequality</i>: The norm of the sum is bounded by the sum of the norms:\n<br>   \\[\n   \\leq \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1} \\sum_i \\|A_i \\mathbf{x}_i\\|_2\n   \\]\n<br>\n<br>5. <i>Substitution</i>: The vectors \\(\\mathbf{x}_i\\) are substituted with \\(\\lambda_i \\mathbf{e}_i\\), where \\(\\|\\mathbf{e}_i\\|_2=1\\) and \\(\\sum_i \\lambda_i^2 =1\\):\n<br>   \\[\n   = \\sup_{\\|\\mathbf{e}_i\\|_2=1, \\sum_i \\lambda_i^2 =1} \\sum_i \\lambda_i \\|A_i \\mathbf{e}_i\\|_2\n   \\]\n<br>\n<br>6. <i>Norm Property</i>: This is further simplified to:\n<br>   \\[\n   = \\sup_{\\sum_i \\lambda_i^2 =1} \\sum_i \\lambda_i \\|A_i\\|_2\n   \\]\n<br>\n<br>7. <i>Cauchy-Schwarz Inequality</i>: Finally, the sum is bounded using the Cauchy-Schwarz inequality:\n<br>   \\[\n   \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2}\n   \\]\n<br>\n<br>Thus, the proof shows that:\n<br>\\[\n\\|A\\|_2 \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2}\n\\]"
                        }
                    }
                ],
                "theorems": [
                    {
                        "statement_id": "3b665c1b-367e-40ba-9e8d-20b4b7a194c0",
                        "paper_id": "895bbef7-305d-4454-850a-7279bb832d9d",
                        "library_nr": 1,
                        "library_name": "Theorem 1",
                        "title": "Jacobian Lipschitz Norm Theorem",
                        "statement_original_tex": "\\begin{theorem}[\\citealp{federer1969geometric}] \\label{thm:jacobian} Let $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ be differentiable and Lipschitz continuous under a choice of $p$-norm $\\|\\cdot\\|_p$. \nLet $J_f(x)$ denote its total derivative (Jacobian) at $x$. Then $\\lip_p(f) = \\sup_{\\mathbf{x}\\in \\mathbb{R}^n} \\|J_f(\\mathbf{x})\\|_p$ where $\\|J_f(\\mathbf{x})\\|_p$ is the induced operator norm on $J_f(\\mathbf{x})$. \n\\end{theorem}",
                        "statement_html": "Let $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ be differentiable and Lipschitz continuous under a choice of $p$-norm $\\|\\cdot\\|_p$. \nLet $J_f(x)$ denote its total derivative (Jacobian) at $x$. Then $\\lip_p(f) = \\sup_{\\mathbf{x}\\in \\mathbb{R}^n} \\|J_f(\\mathbf{x})\\|_p$ where $\\|J_f(\\mathbf{x})\\|_p$ is the induced operator norm on $J_f(\\mathbf{x})$.",
                        "statement_type": "theorem",
                        "statement_motivation_html": "Understanding the Lipschitz constant $\\lip_p(f)$ of a differentiable and Lipschitz continuous function $f$ is crucial in various fields such as optimization, numerical analysis, and machine learning. It provides a bound on how much the function can change with respect to changes in its input, ensuring stability and robustness. The relationship between the Lipschitz constant and the Jacobian matrix $J_f(x)$ allows us to compute this bound efficiently using the induced operator norm. This is particularly useful when analyzing the sensitivity of systems and algorithms to perturbations in their input.",
                        "html_url": "library/theorems/theorem_1/index.html",
                        "corollary_ids": [],
                        "proof": null
                    },
                    {
                        "statement_id": "2c82dd47-8a5d-41de-a6ca-9e52bcf91e12",
                        "paper_id": "895bbef7-305d-4454-850a-7279bb832d9d",
                        "library_nr": 2,
                        "library_name": "Theorem 2",
                        "title": "Non-Lipschitz Property of DP-MHA",
                        "statement_original_tex": "\\begin{theorem} \\label{thm:dp_not_lipschitz}\n\\verb!DP-MHA! is not Lipschitz for any vector $p$-norm $\\|\\cdot\\|_p$ with $p \\in [1, \\infty]$.\n\\end{theorem}",
                        "statement_html": "$\\verb!DP-MHA!$ is not Lipschitz for any vector $p$-norm $\\|\\cdot\\|_p$ with $p \\in [1, \\infty]$.",
                        "statement_type": "theorem",
                        "statement_motivation_html": "The fact that $\\verb!DP-MHA!$ is not Lipschitz for any vector $p$-norm $\\|\\cdot\\|_p$ with $p \\in [1, \\infty]$ is crucial in understanding the limitations of certain algorithms in machine learning and optimization. This result highlights that the stability and robustness of $\\verb!DP-MHA!$ cannot be guaranteed under these norms, which is important when analyzing the performance and convergence of algorithms that rely on Lipschitz continuity. This insight is particularly useful when designing or choosing algorithms for applications where stability is a critical factor.",
                        "html_url": "library/theorems/theorem_2/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "0c845543-7173-4672-ba50-44df1c1d1cc2",
                            "paper_id": "895bbef7-305d-4454-850a-7279bb832d9d",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\nWe show the proof for the case $D=H=1$ (i.e.~$X \\in \\mathbb{R}^{N \\times 1}$, a column vector, and $x_i \\in \\mathbb{R}$) for readability. See Appendix \\ref{apd:general_d} for the general case, which follows the same logic. \n\nThe mapping $f$ can be written as\n\\begin{align*}\nf(X) = PX = & \\softmax{a X X^\\top} X = \\begin{bmatrix}\n    f_1(X) \\\\\n    \\vdots \\\\\n    f_N(X)\n\\end{bmatrix} \\in \\mathbb{R}^{N \\times 1}, \\\\\n\\text{where} \\quad & f_i(X) = \\sum_{j=1}^N P_{ij}x_j \\in \\mathbb{R}\n\\end{align*}\nand $a = W^K W^Q \\in \\mathbb{R}$ (we assume $a \\neq 0$ such that self-attention is non-trivial).\nHence $f$ can be interpreted as a map of each $x_i$ to a point in the convex hull of ${x_1,...,x_N}$.\nSince $f$ is a map from $\\mathbb{R}^{N \\times 1}$ to $\\mathbb{R}^{N \\times 1}$, its Jacobian is\n\\begin{equation}\n    J_f = \\begin{bmatrix}\n    J_{11} & \\dots & J_{1N} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    J_{N1} & \\dots & J_{NN} \\\\\n    \\end{bmatrix}\\in \\mathbb{R}^{N \\times N},\n\\end{equation}\nwhere $\\smash{J_{ij} = \\frac{\\partial f_i(X)}{\\partial x_j} \\in \\mathbb{R}}$. \nBy taking partial derivatives we can show that\n\\begin{equation*}\n    J_{ij} = a X^\\top P^{(i)} \\left[E_{ji}X + \\delta_{ij}X \\right] + P_{ij}I\n\\end{equation*}\nwhere \n\\begin{itemize}\n    \\item $E_{ij} \\in \\mathbb{R}^{N \\times N}$ is a binary matrix with zeros everywhere except the $(i,j)$th entry\n    \\item $\\delta_{ij} \\in \\{0,1\\}$ is the Kronecker delta\n    \\item $P^{(i)} \\coloneqq \\diag(P_{i:}) - P_{i:}^\\top P_{i:} \\in \\mathbb{R}^{N \\times N}$.\n\\end{itemize}\nSee Appendix \\ref{apd:identities} for useful identities in deriving the above Jacobian.\n\nSo for $i=j$:\n\\begin{align}\nJ_{ii} =\n a X^\\top P^{(i)} e_{ii} X + a X^\\top P^{(i)} X + P_{ii} \\label{eq:jac_dot}\n\\end{align}\n\nLet us investigate the scalar $X^\\top P^{(i)}X$. We observe that it is in fact a variance of a discrete distribution. Specifically:\n\\begin{equation} \\label{eq:cov}\n    X^\\top P^{(i)}X  = \\textstyle\\sum_k P_{ik} x_k^2 - \\left(\\textstyle\\sum_k P_{ik}  x_k\\right)^2 = \\mathrm{Var}(\\mathbb{X}),\n\\end{equation}\nwhere $\\mathbb{X}$ is a discrete distribution with support at the inputs $\\{x_1,\\ldots,x_N \\}$ and probability mass function given by their softmax probabilities $\\mathbb{P}(\\mathbb{X}=x_j)=P_{ij}$. \nA consequence of this interpretation is that $P^{(i)}$ is \\textit{positive semi-definite} (PSD) since $X^\\top P^{(i)} X = \\mathrm{Var}(\\mathbb{X}) \\geq 0$, with equality if and only if the $x_j$ are all equal.\n\nWe use this observation to show that $J_{ii}$ is unbounded, and so $\\|J_f\\|_p$ is unbounded, hence \\verb!DP-MHA! is \\emph{not} Lipschitz.\nConsider the case $x_i=0$. Then \n\\begin{equation*}\n    P_{i:}^\\top = \\softmax{XAx_i} = \\frac{1}{N} \\mathds{1},\n\\end{equation*}\ni.e.\\ we have uniform attention regardless of $x_{ \\neq i}$. \nThe first term of $J_{ii}$ in Equation \\eqref{eq:jac_dot} disappears since $e_{ii} X = [0, \\ldots, x_i, \\ldots, 0] = \\mathbf{0}$, and the last term becomes $\\frac{1}{N} I$. Now consider the second term $a X^\\top P^{(i)}X = a \\mathrm{Var}(\\mathbb{X}_l)$. Note $\\mathbb{X}$ is uniformly distributed, since $\\mathbb{P}(\\mathbb{X}=x_j)=P_{ij}= 1/N$. Hence the second term is equal to $a$ times the sample variance of ${x_1,\\ldots,x_N}$, which can be arbitrarily large. Hence $J_{ii}$ can become arbitrarily large, so the full Jacobian $J_f$ is unbounded.\n\\end{proof}",
                            "statement_html": "We show the proof for the case $D=H=1$ (i.e.~$X \\in \\mathbb{R}^{N \\times 1}$, a column vector, and $x_i \\in \\mathbb{R}$) for readability. See Appendix $\\ref{apd:general_d}$ for the general case, which follows the same logic. \n\nThe mapping $f$ can be written as\n\\begin{align*}\nf(X) = PX = & \\softmax{a X X^\\top} X = \\begin{bmatrix}\n    f_1(X) \\\\\n    \\vdots \\\\\n    f_N(X)\n\\end{bmatrix} \\in \\mathbb{R}^{N \\times 1}, \\\\\n\\text{where} \\quad & f_i(X) = \\sum_{j=1}^N P_{ij}x_j \\in \\mathbb{R}\n\\end{align*}\nand $a = W^K W^Q \\in \\mathbb{R}$ (we assume $a \\neq 0$ such that self-attention is non-trivial).\nHence $f$ can be interpreted as a map of each $x_i$ to a point in the convex hull of ${x_1,...,x_N}$.\nSince $f$ is a map from $\\mathbb{R}^{N \\times 1}$ to $\\mathbb{R}^{N \\times 1}$, its Jacobian is\n\\begin{equation}\n    J_f = \\begin{bmatrix}\n    J_{11} & \\dots & J_{1N} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    J_{N1} & \\dots & J_{NN} \\\\\n    \\end{bmatrix}\\in \\mathbb{R}^{N \\times N},\n\\end{equation}\nwhere $\\smash{J_{ij} = \\frac{\\partial f_i(X)}{\\partial x_j} \\in \\mathbb{R}}$. \nBy taking partial derivatives we can show that\n\\begin{equation*}\n    J_{ij} = a X^\\top P^{(i)} \\left[E_{ji}X + \\delta_{ij}X \\right] + P_{ij}I\n\\end{equation*}\nwhere \n<ul>\n    <li>$E_{ij} \\in \\mathbb{R}^{N \\times N}$ is a binary matrix with zeros everywhere except the $(i,j)$th entry</li>\n    <li>$\\delta_{ij} \\in \\{0,1\\}$ is the Kronecker delta</li>\n    <li>$P^{(i)} \\coloneqq \\diag(P_{i:}) - P_{i:}^\\top P_{i:} \\in \\mathbb{R}^{N \\times N}$.</li>\n</ul>\nSee Appendix $\\ref{apd:identities}$ for useful identities in deriving the above Jacobian.\n\nSo for $i=j$:\n\\begin{align}\nJ_{ii} =\n a X^\\top P^{(i)} e_{ii} X + a X^\\top P^{(i)} X + P_{ii} \\label{eq:jac_dot}\n\\end{align}\n\nLet us investigate the scalar $X^\\top P^{(i)}X$. We observe that it is in fact a variance of a discrete distribution. Specifically:\n\\begin{equation} \\label{eq:cov}\n    X^\\top P^{(i)}X  = \\textstyle\\sum_k P_{ik} x_k^2 - \\left(\\textstyle\\sum_k P_{ik}  x_k\\right)^2 = \\mathrm{Var}(\\mathbb{X}),\n\\end{equation}\nwhere $\\mathbb{X}$ is a discrete distribution with support at the inputs $\\{x_1,\\ldots,x_N \\}$ and probability mass function given by their softmax probabilities $\\mathbb{P}(\\mathbb{X}=x_j)=P_{ij}$. \nA consequence of this interpretation is that $P^{(i)}$ is \\textit{positive semi-definite} (PSD) since $X^\\top P^{(i)} X = \\mathrm{Var}(\\mathbb{X}) \\geq 0$, with equality if and only if the $x_j$ are all equal.\n\nWe use this observation to show that $J_{ii}$ is unbounded, and so $\\|J_f\\|_p$ is unbounded, hence $\\verb!DP-MHA!$ is $\\emph{not}$ Lipschitz.\nConsider the case $x_i=0$. Then \n\\begin{equation*}\n    P_{i:}^\\top = \\softmax{XAx_i} = \\frac{1}{N} \\mathds{1},\n\\end{equation*}\ni.e.\\ we have uniform attention regardless of $x_{ \\neq i}$. \nThe first term of $J_{ii}$ in Equation $\\eqref{eq:jac_dot}$ disappears since $e_{ii} X = [0, \\ldots, x_i, \\ldots, 0] = \\mathbf{0}$, and the last term becomes $\\frac{1}{N} I$. Now consider the second term $a X^\\top P^{(i)}X = a \\mathrm{Var}(\\mathbb{X}_l)$. Note $\\mathbb{X}$ is uniformly distributed, since $\\mathbb{P}(\\mathbb{X}=x_j)=P_{ij}= 1/N$. Hence the second term is equal to $a$ times the sample variance of ${x_1,\\ldots,x_N}$, which can be arbitrarily large. Hence $J_{ii}$ can become arbitrarily large, so the full Jacobian $J_f$ is unbounded.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n<br>\n<br>1. <i>Setup and Assumptions</i>: The proof begins by considering the case where \\(D=H=1\\), meaning \\(X\\) is a column vector in \\(\\mathbb{R}^{N \\times 1}\\) and each \\(x_i \\in \\mathbb{R}\\). The general case is discussed in the appendix.\n<br>\n<br>2. <i>Mapping Definition</i>: The mapping \\(f\\) is defined as:\n<br>   \\[\n   f(X) = PX = \\softmax{a X X^\\top} X = \\begin{bmatrix}\n       f_1(X) \\\\\n       \\vdots \\\\\n       f_N(X)\n   \\end{bmatrix} \\in \\mathbb{R}^{N \\times 1},\n   \\]\n<br>   where \\(f_i(X) = \\sum_{j=1}^N P_{ij}x_j \\in \\mathbb{R}\\) and \\(a = W^K W^Q \\in \\mathbb{R}\\).\n<br>\n<br>3. <i>Jacobian Matrix</i>: The Jacobian \\(J_f\\) of the mapping \\(f\\) is given by:\n<br>   \\[\n   J_f = \\begin{bmatrix}\n       J_{11} & \\dots & J_{1N} \\\\\n       \\vdots & \\ddots & \\vdots \\\\\n       J_{N1} & \\dots & J_{NN} \\\\\n   \\end{bmatrix}\\in \\mathbb{R}^{N \\times N},\n   \\]\n<br>   where \\(J_{ij} = \\frac{\\partial f_i(X)}{\\partial x_j} \\in \\mathbb{R}\\).\n<br>\n<br>4. <i>Partial Derivatives</i>: By taking partial derivatives, it is shown that:\n<br>   \\[\n   J_{ij} = a X^\\top P^{(i)} \\left[E_{ji}X + \\delta_{ij}X \\right] + P_{ij}I,\n   \\]\n<br>   where:\n<ul>\n    <li>\\(E_{ij} \\in \\mathbb{R}^{N \\times N}\\) is a binary matrix with zeros everywhere except the \\((i,j)\\)th entry.</li>\n    <li>\\(\\delta_{ij} \\in \\{0,1\\}\\) is the Kronecker delta.</li>\n    <li>\\(P^{(i)} \\coloneqq \\diag(P_{i:}) - P_{i:}^\\top P_{i:} \\in \\mathbb{R}^{N \\times N}\\).</li>\n</ul>\n<br>\n<br>5. <i>Diagonal Elements</i>: For \\(i=j\\), the Jacobian element \\(J_{ii}\\) is:\n<br>   \\[\n   J_{ii} = a X^\\top P^{(i)} e_{ii} X + a X^\\top P^{(i)} X + P_{ii}.\n   \\]\n<br>\n<br>6. <i>Variance Interpretation</i>: The term \\(X^\\top P^{(i)}X\\) is interpreted as the variance of a discrete distribution:\n<br>   \\[\n   X^\\top P^{(i)}X  = \\textstyle\\sum_k P_{ik} x_k^2 - \\left(\\textstyle\\sum_k P_{ik}  x_k\\right)^2 = \\mathrm{Var}(\\mathbb{X}),\n   \\]\n<br>   where \\(\\mathbb{X}\\) is a discrete distribution with support at \\(\\{x_1,\\ldots,x_N \\}\\) and probability mass function \\(\\mathbb{P}(\\mathbb{X}=x_j)=P_{ij}\\).\n<br>\n<br>7. <i>Positive Semi-Definiteness</i>: It is noted that \\(P^{(i)}\\) is positive semi-definite (PSD) since \\(X^\\top P^{(i)} X = \\mathrm{Var}(\\mathbb{X}) \\geq 0\\).\n<br>\n<br>8. <i>Unbounded Jacobian</i>: To show that \\(J_{ii}\\) is unbounded, consider \\(x_i=0\\). Then:\n<br>   \\[\n   P_{i:}^\\top = \\softmax{XAx_i} = \\frac{1}{N} \\mathds{1},\n   \\]\n<br>   leading to uniform attention. The first term of \\(J_{ii}\\) disappears, and the last term becomes \\(\\frac{1}{N} I\\). The second term \\(a X^\\top P^{(i)}X = a \\mathrm{Var}(\\mathbb{X}_l)\\) can be arbitrarily large, making \\(J_{ii}\\) unbounded.\n<br>\n<br>Thus, the full Jacobian \\(J_f\\) is unbounded, showing that \\(\\verb!DP-MHA!\\) is not Lipschitz."
                        }
                    },
                    {
                        "statement_id": "690f1ffc-65aa-4c74-94fb-4c845111c43b",
                        "paper_id": "895bbef7-305d-4454-850a-7279bb832d9d",
                        "library_nr": 3,
                        "library_name": "Theorem 3",
                        "title": "Lipschitz Bound for L2-MHA",
                        "statement_original_tex": "\\begin{theorem} \\label{thm:main}\n\\verb!L2-MHA! is Lipschitz, with the following bound on $\\lip_{\\infty}(F)$:\n\\begin{align*}\n    \\lip_{\\infty}(F)  \\leq &\\left(4 \\phi^{-1}(N-1) + \\frac{1}{\\sqrt{D/H}}\\right) \\|W^{O^\\top}\\|_{\\infty} \\\\\n    &\\max_h \\|W^{Q,h}\\|_{\\infty} \\|W^{{Q,h}^\\top}\\|_{\\infty} \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty} \n\\end{align*}\nand the following bound on $\\lip_{2}(F)$:\n\\begin{align*}\n    \\lip_2(F) \\leq & \\frac{\\sqrt{N}}{\\sqrt{D/H}}\n    \\left(4 \\phi^{-1}(N-1) + 1 \\right) \\\\ \n    & \\left(\\sqrt{\\textstyle\\sum_h \\|W^{Q,h}\\|_2^2\\, \\|W^{V,h}\\|_2^2}\\right) \\|W^O\\|_2 \n\\end{align*}\nwhere $\\phi(x) \\coloneqq x\\exp(x+1)$ is an invertible univariate function on $x > 0$, and $N$ is the input sequence length.\n\nSpecifically, $\\phi^{-1}(N-1) = W_0(\\frac{N}{e})$ where $W_0$ is the Lambert $W$-function, which grows sub-logarithmically as $O(\\log N - \\log \\log N)$ \\citep{corless1996lambertw}. Hence the above bounds can be simplified to $O(\\log N)$ for $p=\\infty$ and $O(\\sqrt{N} \\log N)$ for $p=2$.\n\\end{theorem}",
                        "statement_html": "$\\verb!L2-MHA!$ is Lipschitz, with the following bound on $\\lip_{\\infty}(F)$:\n\\begin{align*}\n    \\lip_{\\infty}(F)  \\leq &\\left(4 \\phi^{-1}(N-1) + \\frac{1}{\\sqrt{D/H}}\\right) \\|W^{O^\\top}\\|_{\\infty} \\\\\n    &\\max_h \\|W^{Q,h}\\|_{\\infty} \\|W^{{Q,h}^\\top}\\|_{\\infty} \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty} \n\\end{align*}\nand the following bound on $\\lip_{2}(F)$:\n\\begin{align*}\n    \\lip_2(F) \\leq & \\frac{\\sqrt{N}}{\\sqrt{D/H}}\n    \\left(4 \\phi^{-1}(N-1) + 1 \\right) \\\\ \n    & \\left(\\sqrt{\\textstyle\\sum_h \\|W^{Q,h}\\|_2^2\\, \\|W^{V,h}\\|_2^2}\\right) \\|W^O\\|_2 \n\\end{align*}\nwhere $\\phi(x) \\coloneqq x\\exp(x+1)$ is an invertible univariate function on $x > 0$, and $N$ is the input sequence length.\n\nSpecifically, $\\phi^{-1}(N-1) = W_0(\\frac{N}{e})$ where $W_0$ is the Lambert $W$-function, which grows sub-logarithmically as $O(\\log N - \\log \\log N)$ \\citep{corless1996lambertw}. Hence the above bounds can be simplified to $O(\\log N)$ for $p=\\infty$ and $O(\\sqrt{N} \\log N)$ for $p=2$.",
                        "statement_type": "theorem",
                        "statement_motivation_html": "Understanding the Lipschitz properties of $\\verb!L2-MHA!$ is crucial for analyzing the stability and robustness of neural networks, particularly in the context of multi-head attention mechanisms. These bounds provide insights into how the output of the network changes in response to small perturbations in the input, which is essential for ensuring reliable performance in various applications, such as natural language processing and computer vision. The bounds also help in designing networks that are less sensitive to noise and adversarial attacks, making them more robust and trustworthy.",
                        "html_url": "library/theorems/theorem_3/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "f6c8e006-c135-47b5-8241-487a57adfd30",
                            "paper_id": "895bbef7-305d-4454-850a-7279bb832d9d",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\nThe mapping $f$ can be written as\n\\vspace{-5mm}\n\\begin{equation}\nf(X) = PX = \\softmax{X A^\\top X^\\top} X = \\begin{bmatrix}\n    f_1(X)^\\top \\\\\n    \\vdots \\\\\n    f_N(X)^\\top\n\\end{bmatrix} \\in \\mathbb{R}^{N \\times D},\n\\end{equation}\nwhere $A = W^K W^{Q^\\top} / \\sqrt{D/H} \\in \\mathbb{R}^{D \\times D}$ and\n$f_i(X) = \\sum_{j=1}^N P_{ij}\\mathbf{x}_j$ with $P_{i:}^\\top = \\softmax{XA\\mathbf{x}_i}$.\nHence $f$ can be interpreted as a map of each $\\mathbf{x}_i$ to a point in the convex hull of ${\\mathbf{x}_1,...,\\mathbf{x}_N}$.\nSince $f$ is a map from $\\mathbb{R}^{N \\times D}$ to $\\mathbb{R}^{N \\times D}$, its Jacobian is\n\\begin{equation}\n    J_f = \\begin{bmatrix}\n    J_{11} & \\dots & J_{1N} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    J_{N1} & \\dots & J_{NN} \\\\\n    \\end{bmatrix}\\in \\mathbb{R}^{ND \\times ND},\n\\end{equation}\nwhere $J_{ij} = \\frac{\\partial f_i(X)}{\\partial \\mathbf{x}_j} \\in \\mathbb{R}^{D \\times D}$. \nBy taking partial derivatives we can show that $J_{ij} = X^\\top P^{(i)} \\left[E_{ji}XA^\\top + XA\\delta_{ij}\\right] + P_{ij}I$\nwhere $E_{ij} \\in \\mathbb{R}^{N \\times N}$ is a binary matrix with zeros everywhere except the $(i,j)$th entry, $\\delta_{ij}$ is the Kronecker delta, and $P^{(i)} \\coloneqq \\diag(P_{i:}) - P_{i:}^\\top P_{i:}$.\nSo for $i=j$:\n\\begin{align}\nJ_{ii} &=X^\\top P^{(i)}E_{ii}XA^\\top + X^\\top P^{(i)}XA + P_{ii}I \\nonumber \\\\\n&= P_{ii}\\left(\\mathbf{x}_i - \\textstyle\\sum_k P_{ik} \\mathbf{x}_k\\right)\\mathbf{x}_i^\\top A^\\top + X^\\top P^{(i)}XA + P_{ii}I. \\label{eq:jac_dot_general}\n\\end{align}\nFor the last equality, note $E_{ii}X$ has all rows equal to zero except for the $i$th row given by $\\mathbf{x}_i^\\top$. We can then verify that $X^\\top P^{(i)}E_{ii}X$ simplifies to $P_{ii}(\\mathbf{x}_i - \\sum_k P_{ik} \\mathbf{x}_k)\\mathbf{x}_i^\\top$.\n\nFor vector $p$-norms, $\\|J_f\\|_p$ is bounded if and only if its entries are bounded, by definition of the operator norm. \nThe entries of $X^\\top P^{(i)}XA$ are bounded for arbitrary $A$ only if the entries of $X^\\top P^{(i)}X$ are bounded.\nSo let us investigate the entries of this $D\\times D$ matrix. \nWriting out each term of the matrix, we observe that it is in fact a covariance matrix of a discrete distribution. Specifically:\n\\begin{equation} \\label{eq:cov_general}\n    [X^\\top P^{(i)}X]_{lm}  = \\textstyle\\sum_k P_{ik} x_{kl} x_{km} - \\left(\\textstyle\\sum_k P_{ik}  x_{kl}\\right)\\left(\\textstyle\\sum_k P_{ik} x_{km}\\right) = \\mathrm{Cov}(\\mathbb{X}_l,\\mathbb{X}_m),\n\\end{equation}\nwhere $\\mathbb{X}$ is a discrete distribution with support at the inputs $\\{\\mathbf{x}_1,\\ldots,\\mathbf{x}_N \\}$ and probability mass function given by their softmax probabilities $\\mathbb{P}(\\mathbb{X}=\\mathbf{x}_j)=P_{ij}$. \nA consequence of this interpretation is that $P^{(i)}$ is \\textit{positive semi-definite} (PSD) since for $D=1$, Equation \\eqref{eq:cov_general} becomes $X^\\top P^{(i)} X = \\mathrm{Var}(\\mathbb{X}) \\geq 0$, with equality if and only if the $\\mathbf{x}_j$ are all equal.\n\nWe use this observation to show that the terms of $J_{ii}$ are unbounded, and so \\verb!DP-MHA! is \\emph{not} Lipschitz.\nConsider the case $\\mathbf{x}_i=0$. Then $P_{i:}^\\top = \\softmax{XA\\mathbf{x}_i} = \\frac{1}{N} \\mathds{1}$, i.e.\\ we have uniform attention regardless of $\\mathbf{x}_{ \\neq i}$. \nThe first term of $J_{ii}$ in Equation \\eqref{eq:jac_dot_general} disappears since $\\mathbf{x}_i=\\mathbf{0}$, and the last term becomes $\\frac{1}{N} I$. For the second term, the entries $[X^\\top P^{(i)}X]_{ll} = \\mathrm{Var}(\\mathbb{X}_l)$ are unbounded since the latter is equal to the sample variance of ${x_{1l},\\ldots,x_{Nl}}$, which can be arbitrarily large.\n\nNote that we have shown that single head dot-product self-atttention ($H=1$) is not Lipschitz, but it is clear that this implies multihead self-attention \\verb!DP-MHA! is also not Lipschitz, since the output of multihead attention is a linear combination of the outputs of each head.\n\\end{proof}",
                            "statement_html": "The mapping $f$ can be written as\n\\vspace{-5mm}\n\\begin{equation}\nf(X) = PX = \\softmax{X A^\\top X^\\top} X = \\begin{bmatrix}\n    f_1(X)^\\top \\\\\n    \\vdots \\\\\n    f_N(X)^\\top\n\\end{bmatrix} \\in \\mathbb{R}^{N \\times D},\n\\end{equation}\nwhere $A = W^K W^{Q^\\top} / \\sqrt{D/H} \\in \\mathbb{R}^{D \\times D}$ and\n$f_i(X) = \\sum_{j=1}^N P_{ij}\\mathbf{x}_j$ with $P_{i:}^\\top = \\softmax{XA\\mathbf{x}_i}$.\nHence $f$ can be interpreted as a map of each $\\mathbf{x}_i$ to a point in the convex hull of ${\\mathbf{x}_1,...,\\mathbf{x}_N}$.\nSince $f$ is a map from $\\mathbb{R}^{N \\times D}$ to $\\mathbb{R}^{N \\times D}$, its Jacobian is\n\\begin{equation}\n    J_f = \\begin{bmatrix}\n    J_{11} & \\dots & J_{1N} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    J_{N1} & \\dots & J_{NN} \\\\\n    \\end{bmatrix}\\in \\mathbb{R}^{ND \\times ND},\n\\end{equation}\nwhere $J_{ij} = \\frac{\\partial f_i(X)}{\\partial \\mathbf{x}_j} \\in \\mathbb{R}^{D \\times D}$. \nBy taking partial derivatives we can show that $J_{ij} = X^\\top P^{(i)} \\left[E_{ji}XA^\\top + XA\\delta_{ij}\\right] + P_{ij}I$\nwhere $E_{ij} \\in \\mathbb{R}^{N \\times N}$ is a binary matrix with zeros everywhere except the $(i,j)$th entry, $\\delta_{ij}$ is the Kronecker delta, and $P^{(i)} \\coloneqq \\diag(P_{i:}) - P_{i:}^\\top P_{i:}$.\nSo for $i=j$:\n\\begin{align}\nJ_{ii} &=X^\\top P^{(i)}E_{ii}XA^\\top + X^\\top P^{(i)}XA + P_{ii}I \\nonumber \\\\\n&= P_{ii}\\left(\\mathbf{x}_i - \\textstyle\\sum_k P_{ik} \\mathbf{x}_k\\right)\\mathbf{x}_i^\\top A^\\top + X^\\top P^{(i)}XA + P_{ii}I. \\label{eq:jac_dot_general}\n\\end{align}\nFor the last equality, note $E_{ii}X$ has all rows equal to zero except for the $i$th row given by $\\mathbf{x}_i^\\top$. We can then verify that $X^\\top P^{(i)}E_{ii}X$ simplifies to $P_{ii}(\\mathbf{x}_i - \\sum_k P_{ik} \\mathbf{x}_k)\\mathbf{x}_i^\\top$.\n\nFor vector $p$-norms, $\\|J_f\\|_p$ is bounded if and only if its entries are bounded, by definition of the operator norm. \nThe entries of $X^\\top P^{(i)}XA$ are bounded for arbitrary $A$ only if the entries of $X^\\top P^{(i)}X$ are bounded.\nSo let us investigate the entries of this $D\\times D$ matrix. \nWriting out each term of the matrix, we observe that it is in fact a covariance matrix of a discrete distribution. Specifically:\n\\begin{equation} \\label{eq:cov_general}\n    [X^\\top P^{(i)}X]_{lm}  = \\textstyle\\sum_k P_{ik} x_{kl} x_{km} - \\left(\\textstyle\\sum_k P_{ik}  x_{kl}\\right)\\left(\\textstyle\\sum_k P_{ik} x_{km}\\right) = \\mathrm{Cov}(\\mathbb{X}_l,\\mathbb{X}_m),\n\\end{equation}\nwhere $\\mathbb{X}$ is a discrete distribution with support at the inputs $\\{\\mathbf{x}_1,\\ldots,\\mathbf{x}_N \\}$ and probability mass function given by their softmax probabilities $\\mathbb{P}(\\mathbb{X}=\\mathbf{x}_j)=P_{ij}$. \nA consequence of this interpretation is that $P^{(i)}$ is \\textit{positive semi-definite} (PSD) since for $D=1$, Equation \\eqref{eq:cov_general} becomes $X^\\top P^{(i)} X = \\mathrm{Var}(\\mathbb{X}) \\geq 0$, with equality if and only if the $\\mathbf{x}_j$ are all equal.\n\nWe use this observation to show that the terms of $J_{ii}$ are unbounded, and so $\\verb!DP-MHA!$ is $\\emph{not}$ Lipschitz.\nConsider the case $\\mathbf{x}_i=0$. Then $P_{i:}^\\top = \\softmax{XA\\mathbf{x}_i} = \\frac{1}{N} \\mathds{1}$, i.e.\\ we have uniform attention regardless of $\\mathbf{x}_{ \\neq i}$. \nThe first term of $J_{ii}$ in Equation \\eqref{eq:jac_dot_general} disappears since $\\mathbf{x}_i=\\mathbf{0}$, and the last term becomes $\\frac{1}{N} I$. For the second term, the entries $[X^\\top P^{(i)}X]_{ll} = \\mathrm{Var}(\\mathbb{X}_l)$ are unbounded since the latter is equal to the sample variance of ${x_{1l},\\ldots,x_{Nl}}$, which can be arbitrarily large.\n\nNote that we have shown that single head dot-product self-atttention ($H=1$) is not Lipschitz, but it is clear that this implies multihead self-attention $\\verb!DP-MHA!$ is also not Lipschitz, since the output of multihead attention is a linear combination of the outputs of each head.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n<br>\n<br>1. <i>Mapping Definition</i>: The proof starts by defining the mapping \\( f \\) as:\n<br>   \\[\n   f(X) = PX = \\softmax{X A^\\top X^\\top} X = \\begin{bmatrix}\n       f_1(X)^\\top \\\\\n       \\vdots \\\\\n       f_N(X)^\\top\n   \\end{bmatrix} \\in \\mathbb{R}^{N \\times D},\n   \\]\n<br>   where \\( A = W^K W^{Q^\\top} / \\sqrt{D/H} \\in \\mathbb{R}^{D \\times D} \\) and \\( f_i(X) = \\sum_{j=1}^N P_{ij}\\mathbf{x}_j \\) with \\( P_{i:}^\\top = \\softmax{XA\\mathbf{x}_i} \\).\n<br>\n<br>2. <i>Interpretation of \\( f \\)</i>: It is noted that \\( f \\) maps each \\( \\mathbf{x}_i \\) to a point in the convex hull of \\( \\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_N\\} \\).\n<br>\n<br>3. <i>Jacobian Definition</i>: The Jacobian of \\( f \\) is defined as:\n<br>   \\[\n   J_f = \\begin{bmatrix}\n       J_{11} & \\dots & J_{1N} \\\\\n       \\vdots & \\ddots & \\vdots \\\\\n       J_{N1} & \\dots & J_{NN} \\\\\n   \\end{bmatrix} \\in \\mathbb{R}^{ND \\times ND},\n   \\]\n<br>   where \\( J_{ij} = \\frac{\\partial f_i(X)}{\\partial \\mathbf{x}_j} \\in \\mathbb{R}^{D \\times D} \\).\n<br>\n<br>4. <i>Partial Derivatives</i>: By taking partial derivatives, it is shown that:\n<br>   \\[\n   J_{ij} = X^\\top P^{(i)} \\left[E_{ji}XA^\\top + XA\\delta_{ij}\\right] + P_{ij}I,\n   \\]\n<br>   where \\( E_{ij} \\) is a binary matrix, \\( \\delta_{ij} \\) is the Kronecker delta, and \\( P^{(i)} \\coloneqq \\diag(P_{i:}) - P_{i:}^\\top P_{i:} \\).\n<br>\n<br>5. <i>Case \\( i = j \\)</i>: For \\( i = j \\), the expression simplifies to:\n<br>   \\[\n   J_{ii} = P_{ii}\\left(\\mathbf{x}_i - \\textstyle\\sum_k P_{ik} \\mathbf{x}_k\\right)\\mathbf{x}_i^\\top A^\\top + X^\\top P^{(i)}XA + P_{ii}I.\n   \\]\n<br>   It is noted that \\( E_{ii}X \\) has all rows equal to zero except for the \\( i \\)th row given by \\( \\mathbf{x}_i^\\top \\).\n<br>\n<br>6. <i>Boundedness of Entries</i>: For vector \\( p \\)-norms, \\( \\|J_f\\|_p \\) is bounded if and only if its entries are bounded. The entries of \\( X^\\top P^{(i)}XA \\) are bounded for arbitrary \\( A \\) only if the entries of \\( X^\\top P^{(i)}X \\) are bounded.\n<br>\n<br>7. <i>Covariance Matrix</i>: The entries of \\( X^\\top P^{(i)}X \\) are shown to form a covariance matrix:\n<br>   \\[\n   [X^\\top P^{(i)}X]_{lm} = \\textstyle\\sum_k P_{ik} x_{kl} x_{km} - \\left(\\textstyle\\sum_k P_{ik} x_{kl}\\right)\\left(\\textstyle\\sum_k P_{ik} x_{km}\\right) = \\mathrm{Cov}(\\mathbb{X}_l, \\mathbb{X}_m),\n   \\]\n<br>   where \\( \\mathbb{X} \\) is a discrete distribution with support at the inputs \\( \\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\} \\) and probability mass function given by their softmax probabilities.\n<br>\n<br>8. <i>Positive Semi-Definiteness</i>: It is observed that \\( P^{(i)} \\) is positive semi-definite (PSD) since for \\( D=1 \\), \\( X^\\top P^{(i)} X = \\mathrm{Var}(\\mathbb{X}) \\geq 0 \\).\n<br>\n<br>9. <i>Unbounded Terms</i>: The terms of \\( J_{ii} \\) are shown to be unbounded, indicating that \\( \\verb!DP-MHA! \\) is not Lipschitz. Specifically, for \\( \\mathbf{x}_i = 0 \\), the entries \\( [X^\\top P^{(i)}X]_{ll} = \\mathrm{Var}(\\mathbb{X}_l) \\) are unbounded.\n<br>\n<br>10. <i>Multihead Attention</i>: It is concluded that single head dot-product self-attention (\\( H=1 \\)) is not Lipschitz, implying that multihead self-attention (\\( \\verb!DP-MHA! \\)) is also not Lipschitz, as the output of multihead attention is a linear combination of the outputs of each head."
                        }
                    }
                ],
                "corollaries": [
                    {
                        "statement_id": "1a102984-a1c3-49ec-a94b-73deff56b917",
                        "paper_id": "895bbef7-305d-4454-850a-7279bb832d9d",
                        "library_nr": 1,
                        "library_name": "Corollary 1",
                        "title": "Lipschitz Bound for Neural Networks",
                        "statement_original_tex": "\\begin{corollary} \\label{cor:lip_conv}\nFor a fully-connected network (\\verb!FCN!) or a convolutional neural network (\\verb!CNN!) $f=W_K \\circ \\rho_{K-1} \\circ W_{K-1} \\circ \\ldots \\circ \\rho_1 \\circ W_1$, we have $\\lip_p(f) \\leq \\prod_k \\|W_k\\|_p$ under a choice of $p$-norm with $1$-Lipschitz non-linearities $\\rho_k$.\n\\end{corollary}",
                        "statement_html": "For a fully-connected network ($\\verb!FCN!$) or a convolutional neural network ($\\verb!CNN!$) $f=W_K \\circ \\rho_{K-1} \\circ W_{K-1} \\circ \\ldots \\circ \\rho_1 \\circ W_1$, we have $\\lip_p(f) \\leq \\prod_k \\|W_k\\|_p$ under a choice of $p$-norm with $1$-Lipschitz non-linearities $\\rho_k$.",
                        "statement_type": "corollary",
                        "statement_motivation_html": "Understanding the Lipschitz constant of a neural network is crucial for analyzing its robustness and generalization capabilities. The given inequality provides an upper bound on the Lipschitz constant of the entire network in terms of the product of the norms of its weight matrices. This is particularly useful when designing or evaluating neural networks, as it helps in ensuring that small changes in the input do not lead to disproportionately large changes in the output, thereby promoting stability and reliability in the network's predictions.",
                        "html_url": "library/corollaries/corollary_1/index.html",
                        "proof": null,
                        "parent_id": "34220ba0-537b-49c3-9c5a-992547e4eb62"
                    }
                ]
            },
            "mathjax_macros": [
                "emph: [\"\\\\textit{#1}\", 1]",
                "mathds: [\"\\\\mathbf{#1}\", 1]",
                "bm: [\"\\\\boldsymbol{\\\\mathbf{#1}}\", 1]",
                "Tr: \"\\\\operatorname{Tr}\"",
                "softmaxOp: \"\\\\operatorname{softmax}\"",
                "lip: \"\\\\operatorname{Lip}\"",
                "diag: \"\\\\operatorname{diag}\"",
                "spaces: \"\\\\hspace{2mm}\"",
                "unif: \"\\\\pazocal{U}\"",
                "softmax: [\"\\\\softmaxOp\\\\left(#1\\\\right)\", 1]",
                "norm: [\"\\\\left\\\\lVert#1\\\\right\\\\rVert\", 1]",
                "andriy: [\"\\\\textcolor{blue}{[AM: #1]}\", 1]",
                "hyunjik: [\"\\\\textcolor{red}{[HK: #1]}\", 1]",
                "george: [\"\\\\textcolor{magenta}{[George: #1]}\", 1]"
            ],
            "label2statementid": {
                "Lipschitz_definition": "5f8a3b08-c740-47c4-bdc4-78d01193d98e",
                "lemma:tie_weights": "e51ba6db-a83d-4b4b-b069-b115b5302efe",
                "lemma:key": "5099566b-88d9-4731-a228-b2b8ef75b56e",
                "lemma:low_rank": "7686508f-1252-4a52-bbd2-97ec9bb2027a",
                "lemma:block_rows": "7c31f984-ae3e-4a11-97b9-b5d31c19626b",
                "lemma:block_cols": "2c11fbd6-9bed-45c9-912d-9e9caffaae73",
                "thm:jacobian": "3b665c1b-367e-40ba-9e8d-20b4b7a194c0",
                "thm:dp_not_lipschitz": "2c82dd47-8a5d-41de-a6ca-9e52bcf91e12",
                "eq:jac_dot": "2c82dd47-8a5d-41de-a6ca-9e52bcf91e12",
                "eq:cov": "2c82dd47-8a5d-41de-a6ca-9e52bcf91e12",
                "thm:main": "690f1ffc-65aa-4c74-94fb-4c845111c43b",
                "eq:jac_dot_general": "690f1ffc-65aa-4c74-94fb-4c845111c43b",
                "eq:cov_general": "690f1ffc-65aa-4c74-94fb-4c845111c43b",
                "cor:lip_conv": "1a102984-a1c3-49ec-a94b-73deff56b917"
            }
        }
    ],
    "extraction_dir": "/tmp/paper_extraction",
    "pages_root": null
}
{
    "db_path": "/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory/lm_theory/data/paper_database.json",
    "papers": [
        {
            "paper_id": "083fc279-e9c8-48f3-94e5-2b3a363832b3",
            "title": "The Lipschitz Constant of Self-Attention",
            "authors": [
                "Hyunjik Kim",
                "George Papamakarios",
                "Andriy Mnih"
            ],
            "year": 2021,
            "source_url": "https://arxiv.org/abs/2006.04710",
            "html_url": "library/papers/the_lipschitz_constant_of_self-attention/index.html",
            "bibtex": "@misc{kim2021lipschitzconstantselfattention,\n\ttitle={The Lipschitz Constant of Self-Attention},\n\tauthor={Hyunjik Kim and George Papamakarios and Andriy Mnih},\n\tyear={2021},\n\teprint={2006.04710},\n\tarchivePrefix={arXiv},\n\tprimaryClass={stat.ML},\n\turl={https://arxiv.org/abs/2006.04710}\n}",
            "original_tex": "==== BEGINNING OF /2006.04710/main.tex ====\n\\documentclass{article}\n\\usepackage{times}\n\\usepackage[utf8]{inputenc} % allow utf-8 input\n\\usepackage[T1]{fontenc}    % use 8-bit T1 fonts\n\\usepackage{hyperref}       % hyperlinks\n\\usepackage{url}            % simple URL typesetting\n\\usepackage{booktabs}       % professional-quality tables\n\\usepackage{amsfonts}       % blackboard math symbols\n\\usepackage{nicefrac}       % compact symbols for 1/2, etc.\n\\usepackage{microtype}      % microtypography\n\\usepackage{graphicx}\n\\usepackage{amsmath}\n\\usepackage{mathtools} % for \\coloneqq\n\\usepackage{amsthm}\n\\usepackage{verbatim}\n\\usepackage{dsfont}  % for \\mathds\n\\usepackage{wrapfig}  % for \\begin{wrapfigure}\n\\usepackage{caption}\n\\usepackage{subcaption}\n\\usepackage{xcolor}\n\\usepackage{amsbsy}\n\\usepackage{paralist}\n\\usepackage{enumitem}\n\n\\newtheorem{theorem}{Theorem}[section]\n\\newtheorem{lemma}{Lemma}[section]\n\\theoremstyle{definition}\n\\newtheorem{definition}{Definition}[section]\n\\newtheorem{corollary}{Corollary}[section]\n\n\\DeclareMathOperator{\\Tr}{Tr}\n\\newcommand{\\spaces}{\\hspace{2mm}}\n\\DeclareMathOperator{\\softmaxOp}{softmax}\n\\newcommand{\\softmax}[1]{\\softmaxOp\\left(#1\\right)}\n\\DeclareMathOperator{\\lip}{Lip}\n\\DeclareMathAlphabet{\\pazocal}{OMS}{zplm}{m}{n}\n\\newcommand{\\unif}{\\pazocal{U}}\n\\DeclareMathOperator{\\diag}{diag}\n\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n\n\\newcommand{\\andriy}[1]{\\textcolor{blue}{[AM: #1]}}\n\\newcommand{\\hyunjik}[1]{\\textcolor{red}{[HK: #1]}}\n\\newcommand{\\george}[1]{\\textcolor{magenta}{[George: #1]}}\n\n% Use the following line for the initial blind version submitted for review:\n\\usepackage[accepted]{icml2021}\n\n% If accepted, instead use the following line for the camera-ready submission:\n%\\usepackage[accepted]{icml2021}\n\n% The \\icmltitle you define below is probably too long as a header.\n% Therefore, a short form for the running title is supplied here:\n\\icmltitlerunning{The Lipschitz Constant of Self-Attention}\n\n\\begin{document}\n\n\\twocolumn[\n\\icmltitle{The Lipschitz Constant of Self-Attention}\n\n% It is OKAY to include author information, even for blind\n% submissions: the style file will automatically remove it for you\n% unless you've provided the [accepted] option to the icml2021\n% package.\n\n% List of affiliations: The first argument should be a (short)\n% identifier you will use later to specify author affiliations\n% Academic affiliations should list Department, University, City, Region, Country\n% Industry affiliations should list Company, City, Region, Country\n\n% You can specify symbols, otherwise they are numbered in order.\n% Ideally, you should not use this facility. Affiliations will be numbered\n% in order of appearance and this is the preferred way.\n\\icmlsetsymbol{equal}{*}\n\n\\begin{icmlauthorlist}\n\\icmlauthor{Hyunjik Kim}{dm}\n\\icmlauthor{George Papamakarios}{dm}\n\\icmlauthor{Andriy Mnih}{dm}\n\\end{icmlauthorlist}\n\n\\icmlaffiliation{dm}{DeepMind, UK}\n\n\\icmlcorrespondingauthor{Hyunjik Kim}{hyunjikk@google.com}\n\n% You may provide any keywords that you\n% find helpful for describing your paper; these are used to populate\n% the \"keywords\" metadata in the PDF but will not be shown in the document\n\\icmlkeywords{Machine Learning, ICML}\n\n\\vskip 0.3in\n]\n\n% this must go after the closing bracket ] following \\twocolumn[ ...\n\n% This command actually creates the footnote in the first column\n% listing the affiliations and the copyright notice.\n% The command takes one argument, which is text to display at the start of the footnote.\n% The \\icmlEqualContribution command is standard text for equal contribution.\n% Remove it (just {}) if you do not need this facility.\n\n%\\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution\n\\printAffiliationsAndNotice{} % otherwise use the standard text.\n\n\n\\begin{abstract}\nLipschitz constants of neural networks have been explored in various contexts in deep learning, such as provable adversarial robustness, estimating Wasserstein distance, stabilising training of GANs, and formulating invertible neural networks.\nSuch works have focused on bounding the Lipschitz constant of fully connected\nor convolutional networks, composed of linear maps and pointwise non-linearities. \nIn this paper, we investigate the Lipschitz constant of\nself-attention, a non-linear neural network module widely used in sequence modelling.\nWe prove that the standard dot-product self-attention is \\emph{not} Lipschitz for unbounded input domain, and propose an alternative L2 self-attention that \\emph{is} Lipschitz. \nWe derive an upper bound on the Lipschitz constant of L2 self-attention and provide empirical evidence for its asymptotic tightness. \nTo demonstrate the practical relevance of our theoretical work, we formulate invertible self-attention and use it in a Transformer-based architecture for a character-level language modelling task.\n\\end{abstract}\n\n\\section{Introduction}\n\\label{sec:intro}\nLipschitz continuity is a strong form of continuity for functions. \nLoosely speaking, a function is \\textit{Lipschitz continuous} if changing its input by a certain amount cannot change its output by more than $K$ times that amount. \nThe constant $K$ is a hard constraint on how rapidly the function's output can vary, and the smallest such $K$ is known as the function's \\textit{Lipschitz constant}.\nFor example, \\smash{$f_1(x) = \\sqrt{|x|}$} and $f_2(x) = \\exp(x)$ for $x\\in\\mathbb{R}$ are not Lipschitz continuous, because their output can change arbitrarily fast as $x$ approaches $0$ and $+\\infty$ respectively. \nOn the other hand, $g_1(x) = \\tanh(x)$ and $g_2(x) = \\alpha x$ are Lipschitz continuous, because their rate of change (derivative) is bounded.\n\nIn deep learning, we often use Lipschitz continuity as a constraint for neural networks, to control how much a network's output can change relative to its input. \nSuch Lipschitz constraints are useful in several contexts. \nFor example, Lipschitz constraints can endow models with provable robustness against adversarial pertubations \\citep{cisse2017parseval, tsuzuku2018lipschitz, anil2019sorting}, and guaranteed generalisation bounds \\citep{sokolic2017robust}.\nMoreover, the dual form of the Wasserstein distance is defined as a supremum over Lipschitz functions with a given Lipschitz constant, hence Lipschitz-constrained networks are used for estimating Wasserstein distances \\citep{peyre2019computational}. \nFurther, Lipschitz-constrained networks can stabilise training for GANs, an example being spectral normalisation \\citep{miyato2018spectral}.\nFinally, Lipschitz-constrained networks are also used to construct invertible models and normalising flows. \nFor example, Lipschitz-constrained networks can be used as a building block for invertible residual networks and hence flow-based generative models \\citep{behrmann2018invertible, chen2019residual}.\nAdditionally, Neural ODEs \\citep{chen2018neural, grathwohl2018ffjord} are typically defined using vector fields parameterized via Lipschitz networks, so that the flow generated by the vector field is guaranteed to exist for all times.\n\nNonetheless, designing Lipschitz-continuous neural networks and computing (or even upper-bounding) their Lipschitz constant is a hard problem. \nPrevious work mostly focused on fully-connected and convolutional networks, not only because they are common in deep learning, but also because they are relatively simple to analyze, as compositions of linear maps and pointwise non-linearities. \nEven in this case however, exact evaluation of the Lipschitz constant\nof fully-connected and convolutional networks is NP-hard \\citep{virmaux2018lipschitz} and obtaining a tight upper bound remains a challenging task \\citep{virmaux2018lipschitz, fazlyab2019efficient, Latorre2020Lipschitz}.\n\nFully-connected and convolutional networks are not the only neural networks worthy of interest.\nRecently, \\textit{self-attention} \\citep{vaswani2017attention} has become a popular alternative to recurrent neural networks. Self-attention is a key component of the Transformer \\citep{vaswani2017attention}, that has found success as a building block in models of various data modalities, starting with natural-language processing \\citep{vaswani2017attention, devlin2018bert, brown2020language} and extending to computer vision \\mbox{\\citep{zhang2018self, ramachandran2019stand}}, audio generation \\citep{huang2018music}, and reinforcement learning \\citep{parisotto2019stabilizing}. However, so far no previous work has analysed the Lipschitz properties of self-attention, and thus it has been unclear whether self-attention is a viable option in applications that require Lipschitz constraints.\nIn this work, we address this gap in the theory of self-attention by providing a thorough analysis of its Lipschitz properties. In particular, we make the following contributions: \n\\begin{itemize}[leftmargin=*]\n    \\item We prove that the widely used \\textit{dot-product self-attention} is \\emph{not} Lipschitz, and therefore not suitable to use in applications requiring Lipschitz constraints.\n    \\item We formulate \\textit{L2 self-attention} as an alternative, and show that it \\emph{is} Lipschitz.\n    \\item We derive a theoretical upper bound on the Lipschitz constant of L2 self-attention, and provide empirical evidence of the asymptotic tightness of the bound.\n    \\item As a practical demonstration of the theory, we use this bound to formulate invertible self-attention, and explore its use in a Transformer architecture for character-level language modelling. We compare its test log-likelihood and stability to dot-product self-attention.\n\\end{itemize}\n\n\\section{Lipschitz Constant of Fully-Connected/Convolutional Layers}\nWe first define the notion of Lipschitz continuity, and proceed to define the Lipschitz constant.\n\\begin{definition}\\label{Lipschitz_definition}\nGiven two metric spaces $(\\mathcal{X}, d_{\\mathcal{X}})$ and $(\\mathcal{Y}, d_{\\mathcal{Y}})$, a function $f:\\mathcal{X} \\rightarrow \\mathcal{Y}$ is called \\textit{Lipschitz continuous} (or $K$-\\textit{Lipschitz}) if there exists a constant $K\\geq 0$ such that \n\\begin{equation}\nd_{\\mathcal{Y}}(f(\\mathbf{x}),f(\\mathbf{x'})) \\leq K d_{\\mathcal{X}}(\\mathbf{x},\\mathbf{x'}) \\spaces\\spaces \\text{for all } \\mathbf{x},\\mathbf{x'} \\in \\mathcal{X}.\n\\end{equation}\nThe smallest such $K$ is the \\textit{Lipschitz constant} of $f$, denoted $\\lip(f)$.\n\\end{definition}\nIn this paper, we focus on the common case where $\\mathcal{X} = \\mathbb{R}^n$, $\\mathcal{Y} = \\mathbb{R}^m$, and $d_{\\mathcal{X}}, d_{\\mathcal{Y}}$ are induced by a $p$-norm \\smash{$\\|\\mathbf{x}\\|_p \\coloneqq (\\sum_{i} |x_i|^p)^{{1}/{p}}$}. \nWe will primarily consider the cases $p=2$ and $p=\\infty$, where $\\|\\mathbf{x}\\|_{\\infty} \\coloneqq \\max_i |x_i|$.\nTo emphasise the dependence of the Lipschitz constant on the choice of $p$-norm, we will often denote it by $\\lip_p(f)$. \nIn this case, it follows directly from Definition \\ref{Lipschitz_definition} that the Lipschitz constant is given by\n\\begin{equation}\\label{eq:lipschitz_supremum_definition}\n    \\lip_p(f) = \\sup_{\\mathbf{x}\\neq \\mathbf{x'} \\in \\mathbb{R}^n} \\frac{\\|f(\\mathbf{x})-f(\\mathbf{x'})\\|_p}{\\|\\mathbf{x}-\\mathbf{x'}\\|_p}.\n\\end{equation}\nNext, we outline some basic results that are useful for estimating Lipschitz constants, also covered in related works \\citep{virmaux2018lipschitz, behrmann2018invertible}. \nWe describe how these results are used to provide bounds on the Lipschitz constant of fully-connected networks (\\verb!FCN!) and convolutional neural networks (\\verb!CNN!), using the fact that both are compositions of linear maps and pointwise non-linearities.\nTo begin with, the following theorem suggests a way to bound $\\lip_p(f)$ for a differentiable Lipschitz function $f$:\n\\begin{theorem}[\\citealp{federer1969geometric}] \\label{thm:jacobian} Let $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ be differentiable and Lipschitz continuous under a choice of $p$-norm $\\|\\cdot\\|_p$. \nLet $J_f(x)$ denote its total derivative (Jacobian) at $x$. Then $\\lip_p(f) = \\sup_{\\mathbf{x}\\in \\mathbb{R}^n} \\|J_f(\\mathbf{x})\\|_p$ where $\\|J_f(\\mathbf{x})\\|_p$ is the induced operator norm on $J_f(\\mathbf{x})$. \n\\end{theorem}\nHence if $f$ is a linear map represented by a matrix $W$ then\n\\begin{align*}\n    \\lip_p(f)&= \\|W\\|_p \\coloneqq \\sup_{\\|\\mathbf{x}\\|_p=1} \\|W\\mathbf{x}\\|_p \\\\\n    &=\n    \\begin{cases}\n        \\sigma_{\\max}(W), & \\text{if } p=2\\\\\n        \\max_i \\sum_j |W_{ij}|      & \\text{if } p = \\infty\n    \\end{cases}\n\\end{align*}\nwhere $\\|W\\|_p$ is the operator norm on matrices induced by the vector $p$-norm, and $\\sigma_{\\max}(W)$ is the largest singular value of $W$. \nUnder this choice of norm, many common non-linearities (including \\verb!relu!, \\verb!sigmoid!, \\verb!tanh!, \\verb!elu!) are $1$-Lipschitz. \n$\\|W\\|_2= \\sigma_{\\text{max}}(W)$ is usually estimated via \\textit{power iteration}; we provide details on how this is done in Appendix \\ref{apd:power_iteration}.\n\nSince we now know the Lipschitz constants of the components of both \\verb!FCN! and \\verb!CNN!, we can bound their Lipschitz constants by applying the following lemma:\n\\begin{lemma}[\\citealp{federer1969geometric}]\nLet $g,h$ be two composable Lipschitz functions. Then $g \\circ h$ is also Lipschitz with $\\lip(g \\circ h) \\leq \\lip(g) \\lip(h)$.\n\\end{lemma}\n\\begin{corollary} \\label{cor:lip_conv}\nFor a fully-connected network (\\verb!FCN!) or a convolutional neural network (\\verb!CNN!) $f=W_K \\circ \\rho_{K-1} \\circ W_{K-1} \\circ \\ldots \\circ \\rho_1 \\circ W_1$, we have $\\lip_p(f) \\leq \\prod_k \\|W_k\\|_p$ under a choice of $p$-norm with $1$-Lipschitz non-linearities $\\rho_k$.\n\\end{corollary}\nThe above bound is not necessarily tight; there are various works that compute tighter bounds for \\verb!FCN! and \\verb!CNN! \\citep[e.g.][]{virmaux2018lipschitz, fazlyab2019efficient, Latorre2020Lipschitz}.\n\n\\section{Lipschitz Constant of Self-Attention}\n\n\\subsection{Dot-product self-attention is \\emph{not} Lipschitz}\n\nMoving on, we investigate whether self-attention is Lipschitz. We first consider the widely used \\textit{(scaled) dot-product multihead self-attention}  as formulated by \\citet{vaswani2017attention}.\nLet $\\mathbf{x}_1, \\ldots, \\mathbf{x}_N$ be a sequence of $N$ elements, where $\\mathbf{x}_i \\in \\mathbb{R}^D$ for $i=1,\\ldots,N$.\nWe represent this sequence as a matrix $X$:\n\\begin{equation}\n    X = \\begin{bmatrix}\n    \\text{---} & \\mathbf{x}_1^\\top & \\text{---}\\\\\n    & \\vdots & \\\\\n    \\text{---} & \\mathbf{x}_N^\\top & \\text{---} \\\\\n    \\end{bmatrix}\\in \\mathbb{R}^{N \\times D},\n\\end{equation}\n\nDot-product multihead self-attention (\\verb!DP-MHA!) is a map from $\\mathbb{R}^{N \\times D}$ to $\\mathbb{R}^{N \\times D}$ consisting of $H$ `heads', where $H$ is chosen to divide $D$. Each head is a map from $\\mathbb{R}^{N \\times D}$ to $\\mathbb{R}^{N \\times D/H}$ defined by\n\\begin{align*} \\label{eq:dot_product_self_attention_definition}\n    \\mathit{DP}(X) & \\coloneqq \\softmax{\\frac{X W^Q (X W^K)^\\top}{\\sqrt{D/H}}} X W^V \\\\\n    &= P X W^V, \n\\end{align*}\nwhere $W^Q, W^K, W^V \\in \\mathbb{R}^{D \\times D/H}$ are learnable parameters specific to each head, and $P \\in \\mathbb{R}^{N \\times N}$ is the output of the softmax (we suppress the dependence of $P$ on $X$ to reduce clutter below). The input to the softmax is an $N\\times N$ matrix of pairwise dot products (hence \\textit{dot-product} self-attention), and the softmax is applied to each row of this matrix. Finally, the outputs of all heads are concatenated into an $N\\times D$ matrix and are right multiplied by $W^O\\in \\mathbb{R}^{D \\times D}$, thus \\verb!DP-MHA! is defined by\n\\begin{equation}\n    \\mathit{MHA}_{DP}(X) \\coloneqq \\left[\\mathit{DP}^1(X), \\ldots, \\mathit{DP}^H(X)\\right] W^O.\n\\end{equation}\nIn what follows, we will prove that $\\mathit{MHA}$ as defined above is  \\emph{not} Lipschitz, assuming that the $\\mathit{MHA}$ map is non-trivial, i.e.~$W^Q, W^K, W^V, W^O \\neq 0$. It is sufficient to show that a single head $\\mathit{DP}$ is not Lipschitz, since $\\mathit{MHA}$ is a linear combination of the outputs of each head. Also note that $P$ is a stochastic matrix, i.e.~its entries are non-negative and its rows sum to $1$.\nSince the rows of $X$ are the $\\mathbf{x}_i$'s, a linear transformation of each $\\mathbf{x}_i$ by some matrix $A$ is equivalent to right multiplication of $X$ by $A^\\top$. \nSo right multiplication of $X$ by $W^V$ is a linear map and thus Lipschitz.\nTherefore, we are interested in the mapping $f(X) = PX$; this is \\emph{not} a linear mapping because $P$ itself is a non-linear function of $X$. \nIn fact, we show that $f$ \nis \\emph{not} Lipschitz, thus proving the first main result of the paper:\n\\begin{theorem} \\label{thm:dp_not_lipschitz}\n\\verb!DP-MHA! is not Lipschitz for any vector $p$-norm $\\|\\cdot\\|_p$ with $p \\in [1, \\infty]$.\n\\end{theorem}\n\\textit{Summary of Proof}. We use Theorem \\ref{thm:jacobian}, noting that if the supremum of the norm of the Jacobian is infinite, then the mapping is not Lipschitz.\nIn particular, we show that when $\\mathbf{x}_i=\\mathbf{0}$ for some $i$, some elements of the Jacobian of $f$ grow proportionally to the sample variance of $\\mathbf{x}_{\\neq i}$, which is unbounded.\n\\begin{proof}\nWe show the proof for the case $D=H=1$ (i.e.~$X \\in \\mathbb{R}^{N \\times 1}$, a column vector, and $x_i \\in \\mathbb{R}$) for readability. See Appendix \\ref{apd:general_d} for the general case, which follows the same logic. \n\nThe mapping $f$ can be written as\n\\begin{align*}\nf(X) = PX = & \\softmax{a X X^\\top} X = \\begin{bmatrix}\n    f_1(X) \\\\\n    \\vdots \\\\\n    f_N(X)\n\\end{bmatrix} \\in \\mathbb{R}^{N \\times 1}, \\\\\n\\text{where} \\quad & f_i(X) = \\sum_{j=1}^N P_{ij}x_j \\in \\mathbb{R}\n\\end{align*}\nand $a = W^K W^Q \\in \\mathbb{R}$ (we assume $a \\neq 0$ such that self-attention is non-trivial).\nHence $f$ can be interpreted as a map of each $x_i$ to a point in the convex hull of ${x_1,...,x_N}$.\nSince $f$ is a map from $\\mathbb{R}^{N \\times 1}$ to $\\mathbb{R}^{N \\times 1}$, its Jacobian is\n\\begin{equation}\n    J_f = \\begin{bmatrix}\n    J_{11} & \\dots & J_{1N} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    J_{N1} & \\dots & J_{NN} \\\\\n    \\end{bmatrix}\\in \\mathbb{R}^{N \\times N},\n\\end{equation}\nwhere $\\smash{J_{ij} = \\frac{\\partial f_i(X)}{\\partial x_j} \\in \\mathbb{R}}$. \nBy taking partial derivatives we can show that\n\\begin{equation*}\n    J_{ij} = a X^\\top P^{(i)} \\left[E_{ji}X + \\delta_{ij}X \\right] + P_{ij}I\n\\end{equation*}\nwhere \n\\begin{itemize}\n    \\item $E_{ij} \\in \\mathbb{R}^{N \\times N}$ is a binary matrix with zeros everywhere except the $(i,j)$th entry\n    \\item $\\delta_{ij} \\in \\{0,1\\}$ is the Kronecker delta\n    \\item $P^{(i)} \\coloneqq \\diag(P_{i:}) - P_{i:}^\\top P_{i:} \\in \\mathbb{R}^{N \\times N}$.\n\\end{itemize}\nSee Appendix \\ref{apd:identities} for useful identities in deriving the above Jacobian.\n\nSo for $i=j$:\n\\begin{align}\nJ_{ii} =\n a X^\\top P^{(i)} e_{ii} X + a X^\\top P^{(i)} X + P_{ii} \\label{eq:jac_dot}\n\\end{align}\n\nLet us investigate the scalar $X^\\top P^{(i)}X$. We observe that it is in fact a variance of a discrete distribution. Specifically:\n\\begin{equation} \\label{eq:cov}\n    X^\\top P^{(i)}X  = \\textstyle\\sum_k P_{ik} x_k^2 - \\left(\\textstyle\\sum_k P_{ik}  x_k\\right)^2 = \\mathrm{Var}(\\mathbb{X}),\n\\end{equation}\nwhere $\\mathbb{X}$ is a discrete distribution with support at the inputs $\\{x_1,\\ldots,x_N \\}$ and probability mass function given by their softmax probabilities $\\mathbb{P}(\\mathbb{X}=x_j)=P_{ij}$. \nA consequence of this interpretation is that $P^{(i)}$ is \\textit{positive semi-definite} (PSD) since $X^\\top P^{(i)} X = \\mathrm{Var}(\\mathbb{X}) \\geq 0$, with equality if and only if the $x_j$ are all equal.\n\nWe use this observation to show that $J_{ii}$ is unbounded, and so $\\|J_f\\|_p$ is unbounded, hence \\verb!DP-MHA! is \\emph{not} Lipschitz.\nConsider the case $x_i=0$. Then \n\\begin{equation*}\n    P_{i:}^\\top = \\softmax{XAx_i} = \\frac{1}{N} \\mathds{1},\n\\end{equation*}\ni.e.\\ we have uniform attention regardless of $x_{ \\neq i}$. \nThe first term of $J_{ii}$ in Equation \\eqref{eq:jac_dot} disappears since $e_{ii} X = [0, \\ldots, x_i, \\ldots, 0] = \\mathbf{0}$, and the last term becomes $\\frac{1}{N} I$. Now consider the second term $a X^\\top P^{(i)}X = a \\mathrm{Var}(\\mathbb{X}_l)$. Note $\\mathbb{X}$ is uniformly distributed, since $\\mathbb{P}(\\mathbb{X}=x_j)=P_{ij}= 1/N$. Hence the second term is equal to $a$ times the sample variance of ${x_1,\\ldots,x_N}$, which can be arbitrarily large. Hence $J_{ii}$ can become arbitrarily large, so the full Jacobian $J_f$ is unbounded.\n\\end{proof}\n\\textit{High-level intuition for proof.}\nAt $x_i=0$, $f_i(X) = \\frac{1}{N} \\sum_{k} x_k$, the mean of the inputs. \nThe rate of change of $f_i$ is governed by how fast the softmax saturates when $x_i$ is perturbed, which is determined by how spread out the $x_{\\neq i}$ are. \nThe more spread out they are (the higher the sample variance), the greater the rate of saturation of the softmax, and the faster the rate of change of $f_i$.\nSince the sample variance of $x_{\\neq i}$ can be arbitrarily large, the rate of change of $f_i$ can also be arbitrarily large, i.e.~the entries of the Jacobian (and hence its $p$-norm) can become arbitrarily large. In Appendix \\ref{apd:bias}, we show that adding bias terms to $\\mathbf{x}_i^\\top W^Q$ and $\\mathbf{x}_j^\\top W^K$ does \\emph{not} resolve the issue.\n\nThe implications of this result are the following.\n\\begin{inparaenum}[(1)]\n\\item There can be undesirable behaviour (e.g.~training instabilities) for the Transformer when some inputs are close to zero and others have large magnitude.\n\\item Dot-product self-attention (and hence the standard Transformer) is not a suitable choice when we require a Lipschitz neural network, such as for formulating invertible residual networks \\citep{behrmann2018invertible}.\n\\end{inparaenum}\nTherefore, to use self-attention and Transformers in such applications, a Lipschitz formulation of self-attention is required, together with an explicit (ideally tight) upper bound to its Lipschitz constant, to quantify how much the output can change with respect to changes in the input.\n\nOne method to make dot-product self-attention Lipschitz is by ensuring its inputs are bounded. Indeed, if the input space is compact, e.g.\\ $[0,1]^{N \\times D}$, any continuously differentiable function is Lipschitz, including dot-product self-attention.\nHowever, as we further discuss in Section \\ref{sec:conclusion}, such an approach has its own challenges, since it makes the Lipschitz constant depend on the input range. Instead, in the next section we formulate a version of self-attention that is provably Lipschitz on all of $\\mathbb{R}^{N\\times D}$, allowing us to derive an upper bound that holds for any subset of $\\mathbb{R}^{N\\times D}$.\n\n\\subsection{L2 self-attention: a Lipschitz formulation of self-attention}\nThe pathology in dot-product self-attention arises because the softmax probabilities $P_{i:}$ are constant with respect to $\\mathbf{x}_{\\neq i}$ when $\\mathbf{x}_i=0$. \nThis behaviour can be undesirable as we want $P_{ij}$ to vary according to $\\mathbf{x}_j$, regardless of whether $\\mathbf{x}_i$ is zero or not.\nHence we propose an alternative form of self-attention based on L2 distance:\n\\begin{align}\\label{eq:L2_self_attention_definition}\nP_{ij} \\propto \\exp(L_{ij}) \\coloneqq \\exp\\left(-\\frac{\\norm{ \\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K}_2^2}{\\sqrt{D/H}}\\right),\n\\end{align}\nwith the normalisation constant ensuring that $\\sum_j P_{ij} = 1$. \nWe will refer to it as \\textit{L2 self-attention}. \nIt is reminiscent of the standard squared-exponential kernel, but with softmax normalisation that ensures that each row of the kernel matrix sums to $1$. \nNormalisation is usually necessary to deal with inputs of varying length $N$ \\citep{wang2018non}, hence we keep the softmax for L2 self-attention. Similarly to dot-product self-attention, L2 self-attention can be computed efficiently with matrix operations; see Appendix \\ref{apd:l2_att_computation} for details, with a comparison of wall-clock runtimes between different choices of attention. \n\nWe first state the mathematical formulation of L2 multihead self-attention (\\verb!L2-MHA!) before proving the main result --- the upper bound of its Lipschitz constant with respect to $\\|\\cdot\\|_p$ for $p=2, \\infty$. The full \\verb!L2-MHA! map $F: \\mathbb{R}^{N \\times D} \\rightarrow \\mathbb{R}^{N \\times D}$ is defined as\n\\begin{align*}\n    F(X) &\\coloneqq \\left[f^1(X)W^{V,1}, \\ldots, f^H(X)W^{V,H}\\right] W^O \\\\\n    & \\quad\\text{where}\\quad\n    f^h(X) \\coloneqq P^h X A_h.\n\\end{align*}\nIn the above, $W^{V,h} \\in \\mathbb{R}^{D \\times D/H}$, $W^O \\in \\mathbb{R}^{D \\times D}$, $P^h$ is defined as in Equation \\eqref{eq:L2_self_attention_definition} with $W^{Q,h}=W^{K,h} \\in \\mathbb{R}^{D \\times D/H}$, and $A_h \\coloneqq W^{Q,h} W^{{Q,h}^\\top} / \\sqrt{D/H} \\in \\mathbb{R}^{D \\times D}$. \nThere are two changes from the usual form of multihead self-attention:\n\\begin{enumerate}[label=(\\arabic*), leftmargin=*]\n    \\item We require $W^{Q,h} = W^{K,h}$ for each head $f^h(X)$ to be Lipschitz. In Lemma \\ref{lemma:tie_weights} of Appendix \\ref{apd:proof} we show that \\verb!L2-MHA! is \\emph{not} Lipschitz for arbitrary $W^{Q,h}$, $W^{K,h}$, and that tying $W^{Q,h} = W^{K,h}$ is sufficient for \\verb!L2-MHA! to be Lipschitz, with intuition for why tying is sufficient.\n    \\item In each head of the self-attention $f^h(X)$, right multiplication by $A_h$ has been included for the theorem below to hold (details are in the proof). In practice, there is little harm done by this extra linear transformation, since when the heads are combined together in $F$, each $f^h(X)$ is additionally transformed by $W^{V,h}$, a free parameter.\n\\end{enumerate}\n\nThe second main result of the paper is the following:\n\\begin{theorem} \\label{thm:main}\n\\verb!L2-MHA! is Lipschitz, with the following bound on $\\lip_{\\infty}(F)$:\n\\begin{align*}\n    \\lip_{\\infty}(F)  \\leq &\\left(4 \\phi^{-1}(N-1) + \\frac{1}{\\sqrt{D/H}}\\right) \\|W^{O^\\top}\\|_{\\infty} \\\\\n    &\\max_h \\|W^{Q,h}\\|_{\\infty} \\|W^{{Q,h}^\\top}\\|_{\\infty} \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty} \n\\end{align*}\nand the following bound on $\\lip_{2}(F)$:\n\\begin{align*}\n    \\lip_2(F) \\leq & \\frac{\\sqrt{N}}{\\sqrt{D/H}}\n    \\left(4 \\phi^{-1}(N-1) + 1 \\right) \\\\ \n    & \\left(\\sqrt{\\textstyle\\sum_h \\|W^{Q,h}\\|_2^2\\, \\|W^{V,h}\\|_2^2}\\right) \\|W^O\\|_2 \n\\end{align*}\nwhere $\\phi(x) \\coloneqq x\\exp(x+1)$ is an invertible univariate function on $x > 0$, and $N$ is the input sequence length.\n\nSpecifically, $\\phi^{-1}(N-1) = W_0(\\frac{N}{e})$ where $W_0$ is the Lambert $W$-function, which grows sub-logarithmically as $O(\\log N - \\log \\log N)$ \\citep{corless1996lambertw}. Hence the above bounds can be simplified to $O(\\log N)$ for $p=\\infty$ and $O(\\sqrt{N} \\log N)$ for $p=2$.\n\\end{theorem}\n\\begin{proof}\nSee Appendix \\ref{apd:proof}, which uses the key observation that $X^\\top P^{(i)}X$ is a covariance matrix (c.f.\\ Equation \\eqref{eq:cov}) to bound $\\|J_F\\|_p$, the norm of the Jacobian of $F$. Appendix \\ref{apd:masking} shows how the argument can be modified to prove the analogous result for the case with masking in the self-attention.\n\\end{proof}\n\nThese bounds are complemented by the concurrent work of \\citet{vuckovic2020attention}, which provides a $O(\\sqrt{D\\log N})$ bound on $\\lip_1(F)$ using measure-theoretic tools.\n\\section{Application: Invertible Self-Attention}\n\\subsection{Invertible residual network} \\label{sec:invertible_resnet}\n\n\nConsider the residual function $g(x) \\coloneqq \\mathbf{x} + f(\\mathbf{x})$. \\citet{behrmann2018invertible} give the following sufficient condition for its invertibility: \nif $f$ is a \\textit{contraction} with respect to some\nmetric, i.e.~if $\\lip(f) < 1$, and the metric space on which $f$ is defined is complete,\nthen $g$ is invertible. \n(A Euclidean space with a metric induced by a $p$-norm $\\|\\cdot\\|_p$ for $p \\in [1, \\infty]$ is always complete.)\nSpecifically, the inverse $g^{-1}(\\mathbf{y})$ is the unique fixed point of the recursion $\\mathbf{x}^{i+1} \\coloneqq \\mathbf{y} - f(\\mathbf{x}^i)$, since by the definition of the inverse we have $\\mathbf{y} = g^{-1}(\\mathbf{y}) + f(g^{-1}(\\mathbf{y}))$. \nBecause $f$ is a contraction, \\textit{Banach's Fixed Point Theorem} guarantees that this fixed point exists and is unique for all $\\mathbf{y}$, and that the recursion converges for all initial values $\\mathbf{x}^0$ (often set to $\\mathbf{y}$ in practice) exponentially fast. \nHence the inverse can be computed to arbitrary accuracy (up to numerical precision in practice) by the above fixed-point iteration.\n\nNote that a composition of such invertible residual blocks is also invertible. \n\\citet{behrmann2018invertible} use this observation to design invertible ResNets: they take $f$ to be a \\verb!CNN! normalised by an upper bound on $\\lip(f)$ given by Corollary \\ref{cor:lip_conv}, making the resulting function \\textit{contractive}. \nFor the $2$-norm $\\|\\cdot\\|_2$, a hyperparameter $c < 1$ is chosen and each linear map (convolution) $W$ in the \\verb!CNN! is multiplied by $c/\\|W\\|_2$ if $c < \\|W\\|_2$ where $\\|W\\|_2$ is estimated by power iteration (c.f.\\ Appendix \\ref{apd:power_iteration}).\nThis multiplicative factor determines the scale of the Lipschitz constant of the normalised function.\n\n\n\\subsection{Invertible self-attention}\n\n\\begin{wrapfigure}{r}{0.2\\textwidth}\n    \\centering\n    \\includegraphics[width=0.2\\textwidth]{transformer_block_single.pdf}\n    \\caption{Transformer block.}\n    \\label{fig:transformer_block}\n\\end{wrapfigure}\n\nThe standard use case of self-attention is with a skip connection inside the Transformer. A Transformer block is composed of residual blocks of multihead self-attention (\\verb!MHA!) and fully-connected (\\verb!FCN!) layers (Figure \\ref{fig:transformer_block}).\nHence similarly to invertible ResNets, we can normalise \\verb!L2-MHA! by the upper bounds given in Theorem \\ref{thm:main} to obtain \\verb!Contractive-L2-MHA! $f$, with which we can obtain invertible self-attention $g(\\mathbf{x}) = \\mathbf{x} + f(\\mathbf{x})$. \nSince \\verb!Dropout! is also part of the residual branch along with \\verb!Contractive-L2-MHA!, we should check that it is also contractive.\nAt test time, \\verb!Dropout! multiplies inputs by the dropout keep probability $p < 1$, so it is a contraction with Lipschitz constant $p$ at evaluation time.\nAt training time, \\verb!Dropout! amounts to setting some inputs to zero, while keeping other inputs constant. This can be expressed as right multiplication by a diagonal binary matrix $M$, and for such matrices we can verify $\\|M\\|_p \\coloneqq \\sup_{\\|x\\|_p=1} \\|Mx\\|_p  \\leq 1$.\nNotice that \\verb!LayerNorm! is not part of the residual branch, hence its Lipschitz continuity is not relevant for invertibility; rather, we can replace it with an invertible normalisation such as \\verb!ActNorm! \\cite{kingma2018glow}. \nHowever, architectures that place \\verb!LayerNorm! inside the residual branch (termed \\verb!pre-LN! as opposed to the traditional \\verb!post-LN! in Figure \\ref{fig:transformer_block}) have become more prevalent in the literature \\cite{wang2019learning, xiong2020layer}, and in this case it makes sense to investigate its Lipschitz continuity.\nWe show that \\verb!LayerNorm! is Lipschitz in Appendix \\ref{apd:layernorm}, with a bound on its Lipschitz constant.\n\nIn the next section, we investigate the properties of invertible self-attention and how it compares with the standard dot-product self-attention; we replace \\verb!DP-MHA! in the Transformer with \\verb!Contractive-L2-MHA!, hence replacing the residual self-attention module with invertible self-attention.\nWe are not interested in the modified Transformer per se, but rather in comparing the properties of invertible self-attention to standard self-attention --- we only use the Transformer as a testbed for this purpose, since self-attention is commonly used in a Transformer.\nGiven the theoretical focus of the paper, we believe that a more challenging application of invertible self-attention, such as normalising flow-based modelling, would be more suitable as a separate paper focused on that particular application.\n\n\\section{Experimental Results}\n\\subsection{Asymptotic tightness of the upper bound on $\\boldsymbol{\\lip_{\\infty}(F)}$} \\label{sec:asymptotic}\n\n\\begin{figure}[htb!]\n    \\centering\n    \\includegraphics[width=0.5\\textwidth]{jacobian_opt.pdf}\n    \\caption{Lower and upper bound on $\\lip_{\\infty}(f)$ for L2-MHA $f$, with $H=D=1$ and varying $N$.}\n    \\label{fig:jacobian_opt}\n\\end{figure}\n\nA tight bound on the Lipschitz constant of self-attention is desirable for all listed applications in Section \\ref{sec:intro}; it leads to tighter generalisation bounds, lighter constraints for provable robustness, and better expressiveness in residual flow models.\nHence we investigate the tightness of our bound on the Lipschitz constant of \\verb!L2-MHA!. \nThe Lipschitz constant is a supremum over the space of inputs $X \\in \\mathbb{R}^{N \\times D}$ (c.f.\\ Equation \\eqref{eq:lipschitz_supremum_definition}) and approximating it requires solving an intractable \noptimisation problem. \nHence it is infeasible to estimate accurately in general, especially when $X$ is high-dimensional. \nHowever, we may compute a lower bound on the Lipschitz constant by maximising the norm of the Jacobian $\\|J_f(X)\\|$ with respect to $X$ until convergence.\nThis local optimum will form a lower bound by Theorem \\ref{thm:jacobian}, and we can expect this lower bound to be fairly tight for the low-dimensional case, provided the optimisation is thorough.\n\nWe use this observation to provide empirical evidence for the asymptotic tightness of the upper bound on $\\lip_{\\infty}(f)$ in Theorem \\ref{thm:main}. \nIn Figure \\ref{fig:jacobian_opt}, we show the upper bound as well as the lower bound on $\\lip_{\\infty}(f)$ obtained by optimising $\\|J_f(X)\\|_{\\infty}$ with respect to $X$ for \\verb!L2-MHA! $f$ with 50 different random initialisations of $X$, with $H=D=1$ and $N$ varying between $100$ and $1000$. \nSee Appendix \\ref{apd:experimental_details} for further details.\nNote that we use a log-scale for the x-axis, and recall that the upper bound is $O(\\log N - \\log \\log N)$, dominated by the $O(\\log N)$ term for large $N$.\nHence the plot for the upper bound shows a linear trend.\nWe also observe that the slope of the lower bound is very similar, providing empirical evidence that the $O(\\log N - \\log \\log N)$ upper bound is asymptotically tight.\n\nThere are at least two possible explanations for the gap between the upper and lower bounds.\n\\begin{inparaenum}[(1)]\n\\item The lower bound is only a local optimum --- the true Lipschitz constant is a global optimum across inputs, which can be difficult to attain especially for high values of $N$.\n\\item The multiplicative constant of the upper bound may be loose.\n\\end{inparaenum}\nAssuming asymptotic tightness, it remains an open question whether the multiplicative constant can be tightened.\nWe show the analogous plot for $\\lip_2(F)$ and discuss the results in Appendix \\ref{apd:jacobian_opt2}.\nAdditionally in Appendix \\ref{apd:jacobian_opt_dp}, we show that optimising $\\|J_f(X)\\|_{\\infty}$ w.r.t.~$X$ for \\verb!DP-MHA! $f$ causes the norm to diverge, providing empirical verification of Theorem \\ref{thm:dp_not_lipschitz}, that \\verb!DP-MHA! is indeed \\emph{not} Lipschitz.\n\n\\subsection{Numerical invertibility of MHA residual map}\\label{sec:numerical-invertibility}\n\n\\begin{figure}[htb!]\n    \\centering\n    \\includegraphics[width=0.5\\textwidth]{mha_invertibility_small.pdf}\n    \\caption{Invertibility of $g(\\mathbf{x})= \\mathbf{x} + c f(\\mathbf{x})$ where $f$ is L2-MHA (left) and DP-MHA (right).}\n    \\label{fig:mha_invertibility_small}\n\\end{figure}\n\n\\begin{figure*}[t!] \n  \\includegraphics[width=\\textwidth]{ptb_test.pdf}\n  \\caption{Test NLL curves during training for various LSTM/Transformer models on PTB character level language modelling.} \\label{fig:ptb}\n\\end{figure*}\n\nRecall from Section \\ref{sec:invertible_resnet} that $g(\\mathbf{x}) = \\mathbf{x} + f(\\mathbf{x})$ is invertible if $f$ is contractive. \nHence if $f$ is \\verb!Contractive-L2-MHA!, $g$ is necessarily invertible. \nHowever, technically we do not disprove the invertibility of \\verb!DP-MHA!, since the converse does not hold in general i.e.~if $f$ is \\verb!DP-MHA!, which we have shown is \\emph{not} Lipschitz hence \\emph{not} contractive, it may still be the case that $g$ \\emph{is} invertible.\nTo verify that \\verb!DP-MHA! (with the skip connection) is \\emph{not} invertible in practice, we compare the numerical invertibility of the residual map  $g(\\mathbf{x})= \\mathbf{x} + c f(\\mathbf{x})$ between the cases where $f$ is \\verb!L2-MHA! and \\verb!DP-MHA! in Figure \\ref{fig:mha_invertibility_small}.\nFor each, we take \\verb!MHA! with $8$ heads and randomly initialised weights, and quantify the maximum reconstruction error across a batch of $128$ inputs whose outputs are inverted via the fixed-point iteration described in Section \\ref{sec:invertible_resnet}. We use $N=64$, \n$D=64$,\nand $c \\in \\{0.5,0.7,0.9\\}$\n(see Appendix \\ref{apd:numerical_invertibility} for analogous results for a wider range of $N$ and $D$ and for \\verb!DP-MHA! with trained weights).\nTo highlight the difference between the two types\nof self-attention, recall in the proof of Theorem \\ref{thm:dp_not_lipschitz} (showing that \\verb!DP-MHA! is not Lipschitz) that when one of the inputs $\\mathbf{x}_i$ is $0$, some terms of the Jacobian grow with the sample variance of $\\mathbf{x}_{\\neq i}$. \nHence we check numerical invertibility at a set of $N$ inputs where $\\mathbf{x}_i=0$ and $\\mathbf{x}_{\\neq i}$ are chosen uniformly at random. \n\nIn Figure \\ref{fig:mha_invertibility_small}, we see that \\verb!DP-MHA! is \\emph{not} invertible whereas \\verb!L2-MHA! \\emph{is} invertible for sufficiently small $c$.\nThis shows how not having the theoretical guarantee of $f$ being contractive can cost us invertibility in practice.\nWe note that the figure shows local invertibility at the sampled inputs, as opposed to global invertibility across the whole input space, yet this clearly highlights the difference between the two choices of self-attention.\nExperiments with the globally invertible self-attention obtained by normalising with the Lipschitz upper bound are provided in the next section.\n\n\n\\subsection{Expressiveness of L2-MHA and invertible self-attention}\n\\label{sec:invertible_self-attention}\n\n\nA natural question to ask is: how does the expressiveness of \\verb!L2-MHA! and \\verb!Contractive-L2-MHA! (that leads to invertible self-attention with the skip connection) compare with the original \\verb!DP-MHA!?\nWe expect that the Lipschitz constraint will limit the expressiveness of the Transformer, and would like to find out by how much.\nWe investigate this by comparing the performance of the original Transformer and the Transformer with invertible self-attention (c.f.\\ Figure \\ref{fig:transformer_block}) at character-level language modelling on the Penn Treebank dataset \\citep{marcus1993building}. \nWe compare the test negative log-likelihood (NLL) of a baseline LSTM, the original Transformer (\\verb!DP-MHA!), and a series of models between the original Transformer and the Transformer with invertible self-attention (\\verb!Contractive-L2-MHA!),\nmaking one change at a time and tuning the hyperparameters on a validation set.\nFor \\verb!Contractive-L2-MHA!, we normalise $F=$\\verb!L2-MHA! by the bound on $\\lip_{\\infty}(F)$ as it is tighter than the bound on $\\lip_{2}(F)$. \nDuring training we backpropagate through these contractive blocks $F/\\lip_{\\infty}(F)$ (including the denominator) to update the model parameters.\nWe found that only backpropagating through the numerator (i.e. applying \\verb!stop-gradient! to denominator) gave slightly worse performance.\nSee Appendix \\ref{apd:experimental_details} for experimental details.\n\nThe results are shown in Figure \\ref{fig:ptb}. \nThe first plot shows the best performing LSTM reaching a test NLL of around $1.0$, and the second plot shows the best performing Transformer reaching a slightly improved performance for $3$--$5$ layers of Transformer blocks. \nWe observe instabilities in training for a higher number of layers, requiring careful tuning of the learning rate schedule for stability at the cost of performance, a commonly observed phenomenon in the literature of deep Transformer architectures \\mbox{\\citep{bapna2018training, parisotto2019stabilizing}}.\nThe third plot shows results for the Transformer with \\verb!DP-MHA! replaced with \\verb!L2-MHA! but without tying $W^Q$ and $W^K$, and we observe a very similar test performance. \nThe fourth plot shows the change when we further tie the query and key weights (making $W^Q=W^K$); we see that there is a small degradation in performance.\nHere the number of trainable parameters has been reduced, but in Appendix \\ref{apd:wq_wk_experiment} we show that matching parameter count does not help performance, suggesting that the reduction in performance when tying queries and keys is not solely due to having fewer parameters.\nWe note that performance saturates at around $5$ layers for each Transformer model so far.\nOn the rightmost plot we show results when further dividing self-attention in each block by the upper bound on $\\lip_{\\infty}(F)$, to obtain invertible self-attention. \nThis does give reduced performance for the same number of layers, but we can attain similar performance with more layers, no longer saturating at $5$ layers.\n\n\\begin{table*}[!htb]\n\\centering\n \\begin{tabular}{|c||c|c|c|c|c|c|c|c|c|} \n \\hline\n  Number of Layers & 2 & 4 & 6 & 8 & 10 & 12 & 14 & 16 & 18 \\\\\n \\hline \\hline\n Transformer (\\textbf{DP}) & 1.061 & 1.032 & 1.021 & \\textbf{1.017}  & 1.025 & - & - & - & - \\\\ \n \\hline\nTransformer (\\textbf{L2}), $W^Q=W^K$  & 1.168 & 1.040 & 1.023 & 1.024 & 1.019 & \\textbf{1.008}  & 1.018 & 1.027 & 1.034 \\\\\n \\hline\nTransformer (\\textbf{Contractive-L2}) & 1.246 & 1.135 & 1.103 & 1.079 & 1.072 & 1.060 & 1.039 & \\textbf{1.029} & 1.031 \\\\\n \\hline\n\\end{tabular}\n\\caption{Test NLL for Transformer models trained with \\textbf{fixed learning rate} on PTB character level language modelling.} \\label{tab:ptb_fixed}\n\\end{table*}\n\nThus we conclude the following. \\begin{inparaenum}[(1)]\n\\item Replacing the dot-product with the L2 distance incurs hardly any loss in expressiveness.\n\\item Tying the query and key weights to obtain Lipschitz self-attention incurs a small loss in expressiveness.\n\\item Dividing by the upper bound on $\\lip_{\\infty}(F)$ to obtain invertible self-attention incurs a noticeable loss in expressiveness, but also has a stabilising effect on the optimisation of the Transformer, thus allowing one to compensate for the apparent loss in expressiveness by increasing the number of layers. \n\\end{inparaenum}\n\n\\subsection{Training Stability of DP-MHA vs L2-MHA}\n\\label{sec:stability}\n\nIn Figure \\ref{fig:mha_output_variance}, we compare the output variance of trained \\verb!L2-MHA! against trained \\verb!DP-MHA!, with weights from the one-layer Transformer (L2), $W^Q=W^K$ model and (DP) model used for Figure \\ref{fig:ptb} respectively. We take the same distribution of inputs as used for the numerical invertibility experiment in Section \\ref{sec:numerical-invertibility}, and show the histogram of inputs and outputs after flattening the input/output tensors. We see that the range of outputs remains similar to the range of inputs for Lipschitz \\verb!L2-MHA!, whereas for \\verb!DP-MHA! the outputs have a much wider range, because the Jacobian norm is large for \\verb!DP-MHA! at these inputs. \n\n\\begin{figure}[!htb]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{mha_output_variance.pdf}\n    \\caption{Histogram showing distribution of inputs/outputs of trained L2-MHA and DP-MHA}\n    \\label{fig:mha_output_variance}\n\\end{figure}\n\nIn practice, this leads to instabilities in training for \\verb!DP-MHA!, hence requiring careful tuning of the learning rate schedule for training deeper Transformer models: linear warmup and square root decay, as detailed in Appendix \\ref{apd:experimental_details}. \nWe investigate the behaviour of the different Transformer models on the above PTB task when using a \\textbf{fixed learning rate}.\nWe observe that \\verb!DP-MHA! fails to train at all beyond 10 layers, whereas both \\verb!L2-MHA! ($W^Q=W^K$) (i.e. Lipschitz L2-MHA but not contractive) and \\verb!Contractive-L2-MHA! shows stable training for up to 18 layers (see Appendix \\ref{apd:stability} for the training curves). \nThis was the deepest model we could fit on a single GPU, and we expect to be able to train even deeper models with these two. \nIn Table \\ref{tab:ptb_fixed} we show the best Test NLL across training for each of the Transformer models. Note that for \\verb!DP-MHA! training becomes unstable beyond 10 layers, so we are only able to provide results up to 10 layers. The generalisation performance of the best model for each setting of self-attention is similar.\n\n\\section{Conclusion and Discussion}\n\\label{sec:conclusion}\nWe have shown that the widely used dot-product self-attention is \\emph{not} Lipschitz, and that the proposed L2 self-attention \\emph{is} Lipschitz, by deriving an $O(\\log N - \\log \\log N)$ Lipschitz bound for $p=\\infty$ and an $O(\\sqrt{N} (\\log N - \\log \\log N))$ bound for $p=2$, where $N$ is the input sequence length. \nWe also provided empirical evidence of the asymptotic tightness of the bound for $p=\\infty$.\nWe demonstrated that Lipschitz-constrained self-attention can be used to formulate invertible self-attention, which we experimentally evaluated on a character-level language modelling task.\nAnd finally, we also showed that L2-MHA is more stable during training, allowing the use of fixed learning rate for stable training of deep architectures.\n\nOur approach to Lipschitz self-attention has been to replace the dot-product kernel with an L2 kernel. An alternative would be to constrain the inputs of self-attention to be bounded; if the input space is compact, e.g.\\ $[0,1]^{N \\times D}$, \\emph{any} continuously differentiable function is Lipschitz, including dot-product self-attention. However, while being simple to implement, this solution has its own difficulties.\nFirst, it makes the Lipschitz constant depend on the range of the input, and thus obtaining a tight bound would require non-trivial mathematical work.\nWe stress that a guarantee that the function is Lipschitz does not tell us anything about its Lipschitz constant; without a tight Lipschitz bound, the true Lipschitz constant can be very large, at which point it is unhelpful that the function is Lipschitz.\nSecond, since self-attention is typically applied at multiple layers within a model (e.g.\\ Transformer), the input to each self-attention will live in a different compact set that depends on the parameters of the previous layers, complicating the analysis for subsequent layers. \nA solution is to constrain the inputs of each layer to be in the same compact set, e.g.\\ by passing them through a sigmoid non-linearity. \nThis however can have undesirable side effects such as vanishing gradients when the sigmoids are saturated.\nDespite these difficulties, this could be a worthwhile alternative route for obtaining Lipschitz self-attention to explore in the future.\n\nHaving a provably Lipschitz self-attention module at our disposal makes it possible to use Transformer-based architectures in applications requiring Lipschitz constraints, while enjoying theoretical guarantees.\nA natural application of Lipschitz self-attention is for residual flows \\mbox{\\citep{behrmann2018invertible}}, and for parameterising Neural ODEs \\citep{chen2018neural} where a Lipschitz vector field guarantees the existence of a unique solution to the ODE for all times. \nThese models can be used for density estimation and generative modelling of sets.\nAnother interesting direction for future work would be to analyse different variants of self-attention based on kernels other than dot-product and L2, as  \\cite{tsai2019transformer} do from an experimental perspective, for which we believe the mathematical tools developed in this paper may aid the analysis.\n\n\n\\section*{Acknowledgements}\nWe would like to thank Adam Kosiorek, Arnaud Doucet, Yee Whye Teh, Michalis Titsias, Emilien Dupont and Theophane Weber for helpful discussion and feedback.\n\n\\bibliographystyle{icml2021}\n\\bibliography{refs}\n\n\n\\newpage\n{\n\\appendix\n\\onecolumn\n{\\Large \\bf Appendix}\n\\section{Useful Identities for deriving Jacobian expressions}\n\\label{apd:identities}\nIn this section, we list some useful identities for deriving the Jacobians of the expressions in the paper.\n\nSuppose $\\lambda$ is a scalar, $\\mathbf{u},\\mathbf{v},\\mathbf{x} \\in \\mathbb{R}^{n \\times 1}$ are column vectors, and $f(\\mathbf{u})$ is a vector valued function. We use the standard convention that for $\\mathbf{a} \\in \\mathbb{R}^m$, $\\mathbf{b} \\in \\mathbb{R}^n$, we have $\\frac{\\partial{\\mathbf{a}}}{\\partial{\\mathbf{b}}} \\in \\mathbb{R}^{m \\times n}$. Then we have the following chain rule identities:\n\\begin{itemize}\n    \\item $\\frac{\\partial}{\\partial \\mathbf{x}}[\\lambda \\mathbf{u}] = \\lambda \\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}} + \\mathbf{x} \\frac{\\partial \\lambda}{\\partial \\mathbf{x}}$\n    \\item $\\frac{\\partial f(\\mathbf{u})}{\\partial \\mathbf{x}} = \\frac{\\partial f(\\mathbf{u})}{\\partial \\mathbf{u}} \\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}}$\n    \\item $\\frac{\\partial}{\\partial \\mathbf{x}}[\\mathbf{u}^\\top \\mathbf{v}] = \\mathbf{u}^\\top \\frac{\\partial \\mathbf{v}}{\\partial \\mathbf{x}} + \\mathbf{v}^\\top \\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}}$\n\\end{itemize}\nNote $\\frac{\\partial \\lambda}{\\partial \\mathbf{x}}$ is a row vector, so $\\mathbf{u}\\frac{\\partial \\lambda}{\\partial \\mathbf{x}}$ is a matrix.\n\nThe Jacobian of the softmax is also well-known. Suppose $\\mathbf{v} = \\softmax{\\mathbf{u}} \\in \\mathbb{R}^{n \\times 1}$. Then\n\\begin{equation*}\n    \\frac{\\partial \\mathbf{v}}{\\partial \\mathbf{u}} = \\diag(\\mathbf{v}) - \\mathbf{v} \\mathbf{v}^\\top \n    = \\begin{bmatrix}\n    v_1 (1-v_1) & - v_1 v_2 & \\ldots & - v_1 v_n \\\\\n    - v_2 v_1  & v_2 (1-v_2) & \\ldots & -v_2 v_n \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    -v_n v_1 & -v_n v_2 & \\ldots & v_n(1-v_n)\n    \\end{bmatrix}.\n\\end{equation*}\n\n\\section{Power Iteration} \\label{apd:power_iteration}\nAlthough $\\|W\\|_{\\infty}$ can be computed efficiently in $O(nm)$ time for $W \\in \\mathbb{R}^{m \\times n}$, na{\\\"i}vely computing $\\|W\\|_2= \\sigma_{\\text{max}}(W) \\coloneqq \\sqrt{\\lambda_{\\text{max}}(W^\\top W)}$ requires $O(n^3)$ operations. (By $\\lambda_{\\text{max}}(A)$ we denote the greatest eigenvalue of a symmetric matrix $A$.) We can however obtain an underestimate $\\tilde{\\sigma}(W)$ via \\textit{power iteration}:\n\\begin{equation} \\label{eq:sigma_tilde}\n    b_{k+1} = \\frac{W^\\top W b_k}{\\|W^\\top W b_k\\|_2},\n    \\quad \\tilde{\\sigma}_k(W) = \\sqrt{\\frac{b_k^\\top W^\\top W  b_k}{b_k^\\top b_k}}, \n\\end{equation}\nwith each iteration taking $O(n^2)$ time. Then using $K\\ll n$ iterations gives us an underestimate $\\tilde{\\sigma}_K$ in $O(Kn^2)$ time.\nSince this is an underestimate, the resulting approximation to the Lipschitz constant of the linear map will not be an upper bound. \nHowever the number of power iterations is usually chosen so that $\\tilde{\\sigma}$ is accurate enough --- $K=5$ is shown to be sufficient in the context of fully connected networks or convolutions considered by \\citet{behrmann2018invertible}.\n\nThe iteration will converge if $W^\\top W$ has an eigenvalue that is strictly greater in magnitude than its other eigenvalues, and the starting vector $b_0$ has a nonzero component in the direction of an eigenvector associated with the dominant eigenvalue. \nThis happens with probability $1$ if $b_0$ is chosen at random, and the convergence is geometric with ratio $|\\lambda_2/\\lambda_{\\max}|$ where $\\lambda_2$ is the eigenvalue with second largest magnitude \\citep{mises1929praktische}.\n\n\\newtheorem{innercustomthm}{Theorem}\n\\newenvironment{customthm}[1]\n  {\\renewcommand\\theinnercustomthm{#1}\\innercustomthm}\n  {\\endinnercustomthm}\n\n\\section{Proof of Theorem 3.1 for General $D$} \\label{apd:general_d}\n\\begin{customthm}{3.1}\n\\verb!DP-MHA! is not Lipschitz for any vector $p$-norm $\\|\\cdot\\|_p$ with $p \\in [1, \\infty]$.\n\\end{customthm}\n\\vspace{-4mm}\n\\begin{proof}\nThe mapping $f$ can be written as\n\\vspace{-5mm}\n\\begin{equation}\nf(X) = PX = \\softmax{X A^\\top X^\\top} X = \\begin{bmatrix}\n    f_1(X)^\\top \\\\\n    \\vdots \\\\\n    f_N(X)^\\top\n\\end{bmatrix} \\in \\mathbb{R}^{N \\times D},\n\\end{equation}\nwhere $A = W^K W^{Q^\\top} / \\sqrt{D/H} \\in \\mathbb{R}^{D \\times D}$ and\n$f_i(X) = \\sum_{j=1}^N P_{ij}\\mathbf{x}_j$ with $P_{i:}^\\top = \\softmax{XA\\mathbf{x}_i}$.\nHence $f$ can be interpreted as a map of each $\\mathbf{x}_i$ to a point in the convex hull of ${\\mathbf{x}_1,...,\\mathbf{x}_N}$.\nSince $f$ is a map from $\\mathbb{R}^{N \\times D}$ to $\\mathbb{R}^{N \\times D}$, its Jacobian is\n\\begin{equation}\n    J_f = \\begin{bmatrix}\n    J_{11} & \\dots & J_{1N} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    J_{N1} & \\dots & J_{NN} \\\\\n    \\end{bmatrix}\\in \\mathbb{R}^{ND \\times ND},\n\\end{equation}\nwhere $J_{ij} = \\frac{\\partial f_i(X)}{\\partial \\mathbf{x}_j} \\in \\mathbb{R}^{D \\times D}$. \nBy taking partial derivatives we can show that $J_{ij} = X^\\top P^{(i)} \\left[E_{ji}XA^\\top + XA\\delta_{ij}\\right] + P_{ij}I$\nwhere $E_{ij} \\in \\mathbb{R}^{N \\times N}$ is a binary matrix with zeros everywhere except the $(i,j)$th entry, $\\delta_{ij}$ is the Kronecker delta, and $P^{(i)} \\coloneqq \\diag(P_{i:}) - P_{i:}^\\top P_{i:}$.\nSo for $i=j$:\n\\begin{align}\nJ_{ii} &=X^\\top P^{(i)}E_{ii}XA^\\top + X^\\top P^{(i)}XA + P_{ii}I \\nonumber \\\\\n&= P_{ii}\\left(\\mathbf{x}_i - \\textstyle\\sum_k P_{ik} \\mathbf{x}_k\\right)\\mathbf{x}_i^\\top A^\\top + X^\\top P^{(i)}XA + P_{ii}I. \\label{eq:jac_dot_general}\n\\end{align}\nFor the last equality, note $E_{ii}X$ has all rows equal to zero except for the $i$th row given by $\\mathbf{x}_i^\\top$. We can then verify that $X^\\top P^{(i)}E_{ii}X$ simplifies to $P_{ii}(\\mathbf{x}_i - \\sum_k P_{ik} \\mathbf{x}_k)\\mathbf{x}_i^\\top$.\n\nFor vector $p$-norms, $\\|J_f\\|_p$ is bounded if and only if its entries are bounded, by definition of the operator norm. \nThe entries of $X^\\top P^{(i)}XA$ are bounded for arbitrary $A$ only if the entries of $X^\\top P^{(i)}X$ are bounded.\nSo let us investigate the entries of this $D\\times D$ matrix. \nWriting out each term of the matrix, we observe that it is in fact a covariance matrix of a discrete distribution. Specifically:\n\\begin{equation} \\label{eq:cov_general}\n    [X^\\top P^{(i)}X]_{lm}  = \\textstyle\\sum_k P_{ik} x_{kl} x_{km} - \\left(\\textstyle\\sum_k P_{ik}  x_{kl}\\right)\\left(\\textstyle\\sum_k P_{ik} x_{km}\\right) = \\mathrm{Cov}(\\mathbb{X}_l,\\mathbb{X}_m),\n\\end{equation}\nwhere $\\mathbb{X}$ is a discrete distribution with support at the inputs $\\{\\mathbf{x}_1,\\ldots,\\mathbf{x}_N \\}$ and probability mass function given by their softmax probabilities $\\mathbb{P}(\\mathbb{X}=\\mathbf{x}_j)=P_{ij}$. \nA consequence of this interpretation is that $P^{(i)}$ is \\textit{positive semi-definite} (PSD) since for $D=1$, Equation \\eqref{eq:cov_general} becomes $X^\\top P^{(i)} X = \\mathrm{Var}(\\mathbb{X}) \\geq 0$, with equality if and only if the $\\mathbf{x}_j$ are all equal.\n\nWe use this observation to show that the terms of $J_{ii}$ are unbounded, and so \\verb!DP-MHA! is \\emph{not} Lipschitz.\nConsider the case $\\mathbf{x}_i=0$. Then $P_{i:}^\\top = \\softmax{XA\\mathbf{x}_i} = \\frac{1}{N} \\mathds{1}$, i.e.\\ we have uniform attention regardless of $\\mathbf{x}_{ \\neq i}$. \nThe first term of $J_{ii}$ in Equation \\eqref{eq:jac_dot_general} disappears since $\\mathbf{x}_i=\\mathbf{0}$, and the last term becomes $\\frac{1}{N} I$. For the second term, the entries $[X^\\top P^{(i)}X]_{ll} = \\mathrm{Var}(\\mathbb{X}_l)$ are unbounded since the latter is equal to the sample variance of ${x_{1l},\\ldots,x_{Nl}}$, which can be arbitrarily large.\n\nNote that we have shown that single head dot-product self-atttention ($H=1$) is not Lipschitz, but it is clear that this implies multihead self-attention \\verb!DP-MHA! is also not Lipschitz, since the output of multihead attention is a linear combination of the outputs of each head.\n\\end{proof}\n\n\\section{Bias term in DP Self-Attention} \n\\label{apd:bias}\nA natural question to ask is whether we can add bias terms $b^Q$ to $\\mathbf{x}_i^\\top W^Q$ and $\\mathbf{b}^K$ to $\\mathbf{x}_j^\\top W^K$ to resolve the issue of attention weights $P_{i:}$ becoming uniform when $\\mathbf{x}_i=0$. \nThe answer is \\emph{no} in general. \nIt can again be shown that $J_{ii}$ is unbounded when $\\mathbf{x}_i$ is chosen such that \\smash{$\\mathbf{x}_i^\\top W^Q + \\mathbf{b}^Q=0$} (such a choice is possible assuming $W^Q$ is full rank, a dense set in \\smash{$\\mathbb{R}^{D \\times D/H}$}). Then \\smash{$P_{i:}^\\top=\\frac{1}{N}\\mathds{1}$} again, and the diagonal entries of \\smash{$X^\\top P^{(i)}X$} are unbounded.\n\n\\section{Efficient Computation of L2 Self-Attention} \\label{apd:l2_att_computation}\nDot-product self-attention only requires a few matrix multiplications to compute the logits (i.e.~the inputs to the softmax) between all pairs of inputs, without having to loop over pairs, hence it can be computed efficiently. \nSimilarly, we can show that L2 self-attention can also be computed in an efficient manner. \nUsing the identity $\\|a-b\\|_2^2 = \\|a\\|_2^2 - 2a^\\top b + \\|b\\|_2^2$ we can compute the logits of L2 attention between all pairs via matrix multiplications and computation of row-wise L2 norms, with negligible overhead compared to dot-product self-attention.\nSpecifically, for L2 self-attention we can show that\n\\begin{gather}\nP = \\softmax{-\\frac{\\|XW^Q\\|_{\\text{row}}^2\\mathds{1}^\\top - 2XW^Q (XW^K)^\\top  + \\mathds{1} \\|XW^K\\|_{\\text{row}}^{2\\top}}{\\sqrt{D/H}}}, \\label{eq:L2_W}\n\\end{gather}\nwhere $\\|A\\|_{\\text{row}}^2$ applies the squared L2 norm to each row of $A$, \nso if $A \\in \\mathbb{R}^{m \\times n}$ then $\\|A\\|_{\\text{row}}^2 \\in \\mathbb{R}^m$.\n\nIn Table \\ref{tab:time} we show the wall-clock training times for the Transformer models with different attention functions and a varying number of layers. It is evident that the differences between the models are rather small.\n\\begin{table}[!htb] \n\\centering\n \\begin{tabular}{|c||c|c|c|c|c|} \n \\hline\n  & 1 Layer & 2 Layers & 3 Layers & 4 Layers & 5 Layers \\\\\n \\hline \\hline\n Transformer \\textbf{(DP)} & 37 & 56 & 77 & 92 & 110 \\\\ \n \\hline\nTransformer \\textbf{(L2)} & 35 & 56 & 73 & 99 & 115 \\\\\n \\hline\nTransformer, $W^Q=W^K$ \\textbf{(L2)} & 39 & 58 & 79 & 91 & 108 \\\\\n \\hline\nTransformer,  \\textbf{(Contractive-L2)} & 37 & 60 & 81 & 102 & 127 \\\\\n \\hline\n\\end{tabular}\n\\caption{Wall clock training times for one epoch of training (seconds)} \\label{tab:time}\n\\end{table}\n\n\n\\section{Proof of Theorem \\ref{thm:main}} \\label{apd:proof}\nRecall the formulation of \\verb!L2-MHA!:\n\\begin{align*}\n    F&: \\mathbb{R}^{N \\times D} \\rightarrow \\mathbb{R}^{N \\times D} \\\\\n    F(X) &= \\left[f^1(X)W^{V,1}, \\ldots, f^H(X)W^{V,H}\\right] W^O \\\\\n    f^h(X) &= P^h X A_h \\\\\n    P^h_{ij} \\propto \\exp(L_{ij}) &\\coloneqq \\exp\\left(-\\frac{\\| \\mathbf{x}_i^\\top W^{Q,h} - \\mathbf{x}_j^\\top W^{K,h}\\|_2^2}{\\sqrt{D/H}}\\right), \\spaces \\sum_j P^h_{ij} = 1\n\\end{align*}\nwhere we have that $W^{Q,h}, W^{K,h}, W^{V,h} \\in \\mathbb{R}^{D \\times D/H}$, $W^O \\in \\mathbb{R}^{D \\times D}$, $P^h \\in \\mathbb{R}^{N \\times N}$ and $A_h \\coloneqq W^{Q,h} W^{{Q,h}^\\top} / \\sqrt{D/H} \\in \\mathbb{R}^{D \\times D}$, and the softmax is applied to each row of the input matrix. \nRecall Equation \\eqref{eq:L2_W}:\n\\begin{equation*}\nP^h = \\softmax{-\\frac{\\|XW^{Q,h}\\|_{\\text{row}}^2\\mathds{1}^\\top - 2XW^{Q,h} (XW^{K,h})^\\top  + \\mathds{1} \\|XW^{K,h}\\|_{\\text{row}}^{2^\\top}}{\\sqrt{D/H}}}.\n\\end{equation*}\n\n\\subsection{L2 self-attention is \\emph{not} Lipschitz for general $\\boldsymbol{W^Q, W^K}$}\nLet us first look at the case of $H=1$ and suppress the index $h$ to reduce clutter. \nConsider the map $\\tilde{f}(X) \\coloneqq PX$, so $f(X)=\\tilde{f}(X)A$.\nWe need $\\tilde{f}$ to be Lipschitz for $f$ and hence $F$ to be Lipschitz.\nNote that $P$ is defined as:\n\\begin{equation*}\nP_{ij} \\propto \\exp(L_{ij}) \\coloneqq \\exp\\left(-\\frac{\\| \\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K\\|_2^2}{\\sqrt{D/H}}\\right)\n\\end{equation*}\nand the normalisation constant satisfies $\\sum_j P_{ij} = 1$, for $P \\in \\mathbb{R}^{N \\times N}$, $X \\in \\mathbb{R}^{N \\times D}$.\n\nFor L2 self-attention, we may take partial derivatives and use the chain rule to show that the Jacobian of $\\tilde{f}$ is:\n\\begin{equation}\n    J_{\\tilde{f}} = \\begin{bmatrix}\n    \\tilde{J}_{11} & \\dots & \\tilde{J}_{1N} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    \\tilde{J}_{N1} & \\dots & \\tilde{J}_{NN} \\\\\n    \\end{bmatrix} \\in \\mathbb{R}^{ND \\times ND}\n\\end{equation}\nwith\n\\begin{equation}\n    \\tilde{J}_{ij} = X^\\top P^{(i)} \\frac{\\partial L_{i:}}{\\partial x_j} + P_{ij} I \\in \\mathbb{R}^{D \\times D}\n\\end{equation}\nwhere\n\\begin{equation}\n    \\frac{\\partial L_{i:}}{\\partial \\mathbf{x}_j} = \\frac{2}{\\sqrt{D/H}}\\left[\\left(XW^K - \\mathds{1} \\mathbf{x}_i^\\top W^Q \\right)W^{Q^\\top} \\delta_{ij} + \\left(E_{ji}XW^Q - E_{jj}XW^K\\right)W^{K^\\top}\\right]\n\\end{equation}\nand\n\\begin{equation*}\n    P^{(i)}  \\coloneqq \\diag(P_{i:}) - P_{i:}^\\top P_{i:} =\n    \\begin{bmatrix}\n    P_{i1}(1-P_{i1}) & -P_{i1} P_{i2} & \\dots  & - P_{i1} P_{iN}  \\\\\n    - P_{i2} P_{i1} & P_{i2}(1-P_{i2}) & \\dots  & - P_{i2} P_{iN} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    - P_{iN}P_{i1} & - P_{iN}P_{i2} & \\dots  & P_{iN}(1-P_{iN}) \n    \\end{bmatrix},\n\\end{equation*}\n\\begin{equation*}\n    P_{ij} = \\frac{\\exp\\left(-\\|\\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K\\|_2^2\\right)}{\\sum_k \\exp\\left(-\\|\\mathbf{x}_i^\\top W^Q - \\mathbf{x}_k^\\top W^K\\|_2^2\\right)}.\n\\end{equation*}\nRecall that $E_{ji} \\in \\mathbb{R}^{N \\times N}$ is a binary matrix with zeros everywhere except the $(j,i)$th entry. Hence $E_{ji}X$ has all rows equal to zero except for the $j$th row given by $\\mathbf{x}_i^\\top$. We can then verify:\n\\begin{equation}\n    X^\\top P^{(i)} E_{ji} X = P_{ij}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k) \\mathbf{x}_i^\\top.\n\\end{equation}\nAlso note $P^{(i)}$ is symmetric, and each row/colum sums to $0$, i.e.~$P^{(i)} \\mathds{1} = \\mathds{1}^\\top P^{(i)} = 0$.\nHence we may simplify the Jacobian terms as follows:\n\\begin{align}\n    \\tilde{J}_{ii} &= \\frac{2}{\\sqrt{D/H}}\\left[X^\\top P^{(i)}(XW^K - \\mathds{1}\\mathbf{x}_i^TW^Q)W^{Q^\\top} + X^\\top P^{(i)}E_{ii}X(W^Q-W^K)W^{K^\\top}\\right] + P_{ii} I \\nonumber \\\\\n    &= \\frac{2}{\\sqrt{D/H}}\\left[X^\\top P^{(i)}(XW^K - \\mathds{1}\\mathbf{x}_i^TW^Q)W^{Q^\\top} + P_{ii}(\\mathbf{x}_i - \\sum_k P_{ik} \\mathbf{x}_k)\\mathbf{x}_i^\\top(W^Q-W^K)W^{K^\\top}\\right] + P_{ii} I  \\nonumber \\\\\n    &= \\frac{2}{\\sqrt{D/H}}\\left[X^\\top P^{(i)}XW^K W^{Q^\\top} + P_{ii}(\\mathbf{x}_i - \\sum_k P_{ik} \\mathbf{x}_k)\\mathbf{x}_i^\\top(W^Q-W^K)W^{K^\\top}\\right] + P_{ii} I, \\label{eq:jii}\n\\end{align}\nand for $i\\neq j$:\n\\begin{align}\n    \\tilde{J}_{ij} &= \\frac{2}{\\sqrt{D/H}}X^\\top P^{(i)}(E_{ij}XW^Q - E_{jj}XW^K)W^{K^\\top} + P_{ij} I \\nonumber \\\\\n    &= \\frac{2}{\\sqrt{D/H}} P_{ij}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k)(\\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K)W^{K^\\top} + P_{ij} I. \\label{eq:jij}\n\\end{align}\n\nWe are now ready to show that $\\tilde{f}$ is \\emph{not} Lipschitz for general $W^Q, W^K$:\n\\begin{lemma} \\label{lemma:tie_weights}\nIf $W^K \\in \\mathbb{R}^{D \\times D/H}$ is full rank (i.e.~full column rank), and $W^K \\neq W^Q$, then $J_{ij}$ has terms that are unbounded for $i \\neq j$, hence $\\tilde{f}$ is \\emph{not} Lipschitz. \n\\end{lemma}\n\\begin{proof}\n\nLet us investigate the expression $\\tilde{K}_{ij} \\coloneqq P_{ij} W^{K^\\top}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k)(\\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K) \\in \\mathbb{R}^{\\frac{D}{H} \\times \\frac{D}{H}}$ for $i\\neq j$, which is related to $\\tilde{J}_{ij}$ as follows by Equation \\eqref{eq:jij}:\n\n\\begin{equation*}\n    W^{K^\\top} \\tilde{J}_{ij} = \\left(\\frac{2}{\\sqrt{D/H}} \\tilde{K}_{ij} + P_{ij}I \\right)\n    W^{K^\\top}.\n\\end{equation*}\n\nIt suffices to show that $\\tilde{K}_{ij}$ is unbounded to show that $\\tilde{J}_{ij}$ is unbounded, since $W^K$ is full rank and $P_{ij} \\in [0,1]$. \n\nLet $\\mathbf{y}_j^\\top = \\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K$. \nThen we have:\n\\begin{align*}\n    \\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k \n    &= W^{Q^\\top}\\mathbf{x}_i - W^{K^\\top}\\mathbf{x}_j - \\sum_k P_{ik} (W^{Q^\\top}\\mathbf{x}_i - W^{K^\\top}\\mathbf{x}_k)\\\\\n    &= W^{Q^\\top}\\mathbf{x}_i - W^{K^\\top}\\mathbf{x}_j - (W^{Q^\\top}\\mathbf{x}_i - \\sum_k P_{ik} W^{K^\\top}\\mathbf{x}_k) \\\\\n    &= - W^{K^\\top}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k).\n\\end{align*}\nHence $\\tilde{K}_{ij} = - P_{ij} (\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k) \\mathbf{y}_j^\\top$.\nNote $\\mathbf{y}_i$ can take an arbitrary value in $\\mathbb{R}^{D/H}$, since $W^K \\neq W^Q$ and $W^K$ is full-rank.\n\nFor all $j \\neq i$, let us choose $\\mathbf{x}_j$ such that $\\mathbf{y}_j = -\\mathbf{y}_i$. This is possible for any value of $\\mathbf{y}_i$ since $W^K$ is full-rank.\nNote $\\mathbf{y}_j = - \\mathbf{y}_i$ and not $\\mathbf{y}_i$.\nWe then have that $\\|\\mathbf{y}_j\\|_2^2$ is equal for all $j$, hence $P_{ij} \\coloneqq \\frac{\\exp(-\\|\\mathbf{y}_j\\|_2^2)}{\\sum_k \\exp(-\\|\\mathbf{y}_k\\|_2^2)} = \\frac{1}{N}$ for all $j$. \nThen for $i \\neq j$, $\\tilde{K}_{ij}$ simplifies to\n\\begin{equation*}\n\\tilde{K}_{ij} = - \\frac{1}{N} \\left(-\\mathbf{y}_i - \\frac{1}{N} (N-2) (-\\mathbf{y}_i)\\right) (-\\mathbf{y}_i)^\\top  = - \\frac{2N-2}{N^2} \\mathbf{y}_i \\mathbf{y}_i^\\top \n\\end{equation*}\nwhose entries are unbounded since $\\mathbf{y}_i$ can be any vector in $\\mathbb{R}^{D/H}$ (note we assume $N \\geq 2$ for self-attention to be well-defined, hence $2N-2 \\neq 0$).\n\\end{proof}\n\nThe intuition for this result is as follows: a reason for \\verb!DP-MHA! not being Lipschitz is that for $\\mathbf{x}_i=0$,, the attention weights $P_{ij}$ become uniform regardless of the values of $\\mathbf{x}_j$ for $j \\neq i$. A similar issue arises for \\verb!L2-MHA! with $W^Q \\neq W^K$ and full-rank $W^K$, as shown above: given any $\\mathbf{x}_i$, we can choose $\\mathbf{x}_j$ such that the $P_{ij}$ become uniform. \n\n\\subsection{L2 self-attention is Lipschitz for $\\boldsymbol{W^Q=W^K}$}\n\nHence we impose the restriction that $W^K=W^Q$. With this assumption we have\n\\begin{equation}\nP_{ij} \\propto \\exp\\left(-\\|(\\mathbf{x}_i - \\mathbf{x}_j)^\\top \\sqrt{A}\\|_2^2\\right)\n\\end{equation}\nwhere $A=W^Q W^{Q^\\top} / \\sqrt{D/H} \\in \\mathbb{R}^{D \\times D}$ and $\\sqrt{A}$ is chosen such that $A = \\sqrt{A}\\sqrt{A}^\\top $,\nin particular $\\sqrt{A} \\coloneqq W^Q / (D/H)^{\\frac{1}{4}}$. The terms in the Jacobian of $\\tilde{f}$ simplify to:\n\\begin{align}\n    \\tilde{J}_{ii} &= 2 X^\\top P^{(i)}XA + P_{ii} I \\hspace{2mm}  \\text{(note $P^{(i)} \\mathds{1} = 0$)},\\\\\n    \\tilde{J}_{ij} &= 2 P_{ij}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k)(\\mathbf{x}_i - \\mathbf{x}_j)^\\top A + P_{ij} I  \\hspace{2mm} \\text{for $i \\neq j$}.\n\\end{align}\nLet the Jacobian of $f(X)$ be:\n\\begin{equation}\n    J_{f} = \\begin{bmatrix}\n    J_{11} & \\dots & J_{1N} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    J_{N1} & \\dots & J_{NN} \\\\\n    \\end{bmatrix} \\in \\mathbb{R}^{ND \\times ND}.\n\\end{equation}\nSince $f(X) = \\tilde{f}(X)A$, and by the chain rule $\\frac{\\partial}{\\partial \\mathbf{x}_j}[\\tilde{f}_i(X)A]=A^\\top \\frac{\\partial \\tilde{f}_i(X)}{\\partial \\mathbf{x}_j}=A \\frac{\\partial \\tilde{f}_i(X)}{\\partial \\mathbf{x}_j}$ (by symmetry of $A$), we have that $J_{ij} = A \\tilde{J}_{ij}$. \nHence\n\\begin{align}\n    J_{ii} &= 2 AX^\\top P^{(i)}XA + P_{ii} A \\hspace{2mm}  \\text{(note $P^{(i)} \\mathds{1} = \\mathbf{0}$)},\\\\\n    J_{ij} &= 2 P_{ij} A (\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k)(\\mathbf{x}_i - \\mathbf{x}_j)^\\top A + P_{ij} A  \\hspace{2mm} \\text{for $i \\neq j$}.\n\\end{align}\nNoting $\\lip_p(f) = \\sup_X \\|J_f(X)\\|_p$, we would like to upper bound $\\|J_f\\|_p$.\n\n\\subsubsection{Upper bound on $\\boldsymbol{\\lip_{\\infty}(F)}$ for L2-MHA}\n\nConsider the choice $p=\\infty$, where $\\|J_f\\|_\\infty$ is the maximum absolute row sum of $J_f$. \nA key observation is that if we can bound the $\\infty$-norm of the Jacobian of $f_i$, a single output of $f$, (i.e.~a single block row $\\|[J_{i1},...,J_{iN}]\\|_\\infty$ of $J_f$) then this is also a bound on $\\|J_f\\|_{\\infty}$ due to permutation equivariance of self-attention; all block rows have the same maximal $\\|\\cdot\\|_\\infty$ when each is optimised over the input $X$. \nUsing this, we can prove that $\\|J_f\\|_\\infty$ admits an upper bound that is $O(\\log N - \\log \\log N)$. Below we state and prove lemmas that lead to the proof of this upper bound.\n\nFirst we analyse the term $\\sqrt{A}^\\top X^\\top P^{(i)}X \\sqrt{A}$, that appears in the first term of $J_{ii}$. \nNote that for $Y \\coloneqq X \\sqrt{A}$, so that the rows of $Y$ are $\\mathbf{y}_i^\\top \\coloneqq \\mathbf{x}_i^\\top \\sqrt{A}$, we have \n\\begin{equation}\n    \\sqrt{A}^\\top X^\\top P^{(i)}X \\sqrt{A}= Y^\\top P^{(i)} Y =  \\mathrm{Cov}(\\mathbb{Y})\n\\end{equation}\nwhere $\\mathbb{P}(\\mathbb{Y}=\\mathbf{y}_j)=P_{ij} = \\exp(-\\|\\mathbf{y}_j - \\mathbf{y}_i\\|^2_2)/\\sum_k \\exp(-\\|\\mathbf{y}_k - \\mathbf{y}_i\\|^2_2)$.\nThe last equality uses the observation in Equation \\eqref{eq:cov}.\n\nThe central inequality used throughout the proof of the main theorem is the following:\n\\begin{lemma} \\label{lemma:key}\n$\\Tr(\\mathrm{Cov}(\\mathbb{Y})) = \\sum_j P_{ij}\\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_2^2 \\leq \\sum_j P_{ij}\\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2 \\leq \\phi^{-1}(N-1)$ where $\\phi(c) = c \\exp(c+1)$ is a one-dimensional invertible function on $\\mathbb{R}_{\\geq 0}$.\n\\end{lemma}\n\\begin{proof}\nThe first equality holds since $\\Tr(\\mathrm{Cov}(\\mathbb{Y})) = \\sum_j \\mathrm{Cov}(\\mathbb{Y})_{jj} = \\sum_j \\mathrm{Var}(\\mathbb{Y}_j) = \\sum_j \\mathbb{E}[(\\mathbb{Y}_j -\\mathbb{E}[\\mathbb{Y}_j])^2]$. \nThe next inequality holds since $\\mathrm{Var}(\\mathbb{Y}_j) = \\mathrm{Var}(\\overline{\\mathbb{Y}}_j) = \\mathbb{E}[\\overline{\\mathbb{Y}}_j^2] -\\mathbb{E}[\\overline{\\mathbb{Y}}_j]^2 \\leq \\mathbb{E}[\\overline{\\mathbb{Y}}_j^2]$ where $\\overline{\\mathbb{Y}}= \\mathbb{Y} - y_i$. \nThe final inequality can be proved as follows.\n\nWe would like to bound \n\\begin{equation}\n    \\sum_j P_{ij}\\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2 = \\frac{\\sum_j \\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2 \\exp(-\\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2)}{\\sum_k \\exp(-\\|\\mathbf{y}_k-\\mathbf{y}_i\\|_2^2)}  = \\frac{\\sum_j z_j^2 \\exp(-z_j^2)}{\\sum_k \\exp(-z_k^2)}\n\\end{equation}\nwhere $z_j \\coloneqq \\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2$ (hence $z_i=0$). \nDefine:\n\\begin{equation}\n    g(\\mathbf{z}) \\coloneqq \\frac{\\sum_j z_j^2 \\exp(-z_j^2)}{\\sum_k \\exp(-z_k^2)} = \\frac{\\sum_{j \\neq i} z_j^2 \\exp(-z_j^2)}{1 + \\sum_{k \\neq i} \\exp(-z_k^2)}.\n\\end{equation}\nFirst note that as $z_j \\rightarrow \\infty$, $\\exp(-z_j^2) \\rightarrow 0$ exponentially fast, causing the product $z_j^2 \\exp(-z_j^2) \\rightarrow 0$.\nHence we expect the above quantity to be bounded and attain its maximum.\n\nLet $h(z_j) \\coloneqq \\exp(-z_j^2)$ for notational conciseness, and note $h(z_j) > 0$. By taking partial derivatives with the chain rule, we have that for $j \\neq i$\n\\begin{equation}\n\\frac{\\partial g(\\mathbf{z})}{\\partial z_j} = \\frac{2z_j h(z_j)}{(\\sum_k h(z_k))^2}\\left[(1-z_j^2)\\sum_k h(z_k) + \\sum_k h(z_k)z_k^2\\right].\n\\end{equation}\nHence the derivative is $0$ if and only if $z_j = 0$ or $(1-z_j^2)\\sum_k h(z_k) + \\sum_k h(z_k)z_k^2 = 0$, the latter being equivalent to $z_j^2 = 1 + \\frac{\\sum_k h(z_k)z_k^2}{\\sum_k h(z_k)} = 1 + g(\\mathbf{z})$. \nHence at the maximum, the non-zero values among $\\{z_j\\}_{j=1}^N$ must be equal to one another.\nIt is clear now that the maximum value $c$ is attained when $z_j^2 = 1 + c$ for $j \\neq i$ (and recall $z_i = 0$). \nSo $h(z_j) = \\exp(-1-c)$ for $j \\neq i$.\nSubstituting this into $g(z)$, and rearranging, we obtain $c \\exp(c+1) = N - 1$. Note $\\phi(x) \\coloneqq x \\exp(x+1)$ is increasing for $x > 0$ hence $c = \\phi^{-1}(N-1)$.\n\\end{proof}\nNote $\\phi(\\log N) = (\\log N) \\exp(\\log N + 1) \\geq N \\log N \\geq N -1$ for $N \\geq 3$. Since $\\phi$ is increasing, we have $\\phi^{-1}(N-1) \\leq \\log(N)$ for $N \\geq 3$. In fact, it is known that $\\phi^{-1}(N-1) = O(\\log N - \\log \\log N)$ \\citep{corless1996lambertw}.\n\nNote the $A$ term in $f(X) = \\tilde{f}(X) A$ allows us to use the above inequality, since $Y^\\top P^{(i)}Y = \\mathrm{Cov}(\\mathbb{Y})$ now appears in the terms of $J_f$:\n\\begin{align}\n    J_{ii}\n    &= 2 \\sqrt{A} [Y^\\top P^{(i)}Y]\\sqrt{A}^\\top + P_{ii} A,  \\\\ \n    J_{ij},\n    &= 2 \\sqrt{A} P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top \\sqrt{A}^\\top + P_{ij} A  \\hspace{2mm} \\text{for $i \\neq j$}.\n\\end{align}\n\nUsing the inequalities $\\|BC\\| \\leq \\|B\\| \\|C\\|$, $\\|B + C\\| \\leq \\|B\\| + \\|C\\|$ and $\\|[A_1, \\ldots, A_N]\\| \\leq \\sum_i \\|A_i\\|$, we have: \n\\begin{align*}\n\\|[J_{i1} &, \\ldots, J_{iN}]\\|_{\\infty}  \\\\\n\\leq & \\|J_{ii}\\|_{\\infty} + \\sum_{j \\neq i} \\|J_{ij}\\|_{\\infty} \\\\\n  \\leq & 2 \\|\\sqrt{A}\\|_{\\infty} \\|Y^\\top P^{(i)}Y\\|_{\\infty} \\|\\sqrt{A}^\\top\\|_{\\infty} + P_{ii} \\|A\\|_{\\infty} \\\\\n & + 2 \\sum_{j \\neq i} \\|\\sqrt{A}\\|_{\\infty} \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty \\|\\sqrt{A}^\\top\\|_{\\infty} + P_{ij} \\|A\\|_{\\infty}\\\\\n  = & 2  \\|\\sqrt{A}\\|_{\\infty}\\|\\sqrt{A}^\\top\\|_{\\infty} \n\\bigg(\\|Y^\\top P^{(i)}Y\\|_\\infty \n + \\sum_{j\\neq i} \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty\\bigg) + \\|A\\|_{\\infty} \\\\\n = & 2  \\frac{\\|W^{Q}\\|_{\\infty}\\|W^{Q^\\top}\\|_{\\infty}}{\\sqrt{D/H}} \n\\bigg(\\|Y^\\top P^{(i)}Y\\|_\\infty \n + \\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty\\bigg) + \\frac{\\|W^Q W^{Q^\\top}\\|_{\\infty}}{\\sqrt{D/H}}.\n\\end{align*}\nFor the first equality, note that $\\sum_j P_{ij}=1$. For the second equality, note that the summand for $j=i$ is $0$ because the term $\\mathbf{y}_i - \\mathbf{y}_j=\\mathbf{0}$. \nEach of the terms in the brackets are bounded by the following lemmas:\n\\begin{lemma}\n$\\|Y^\\top P^{(i)}Y\\|_\\infty \\leq  \\phi^{-1}(N-1) \\sqrt{D/H} $ ($\\phi$ defined as in Lemma \\ref{lemma:key}).\n\\end{lemma}\n\\begin{proof}\nRecall that $Y^\\top P^{(i)}Y = \\mathrm{Cov}(\\mathbb{Y})$. Let $\\sigma(\\mathbb{Y}_m)$ denote the standard deviation of $\\mathbb{Y}_m$. Then $[\\mathrm{Cov}(\\mathbb{Y})]_{lm} \\leq \\sigma(\\mathbb{Y}_l)\\sigma(\\mathbb{Y}_m)$.\nHence \n\\begin{align*}\n\\|\\mathrm{Cov}(\\mathbb{Y})\\|_{\\infty} = \\max_l \\sum_m \\left|[\\mathrm{Cov}(\\mathbb{Y})]_{lm}\\right| \n& \\leq  \\max_l \\sigma(\\mathbb{Y}_l) \\sum_m \\sigma(\\mathbb{Y}_m) \\\\\n& \\leq \\sqrt{\\frac{D}{H}} \\sum_m \\sigma^2(\\mathbb{Y}_m)  = \\sqrt{\\frac{D}{H}} \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\\\\n& \\leq \\sqrt{\\frac{D}{H}} \\phi^{-1}(N-1),\n\\end{align*}\nsince $\\sum_m \\sigma(\\mathbb{Y}_m) \\leq \\sqrt{\\frac{D}{H}} \\sqrt{\\sum_m \\sigma^2(\\mathbb{Y}_m)}$ (by e.g.~using the Cauchy--Schwartz inequality on $[\\sigma(\\mathbb{Y}_1), \\ldots, \\sigma(\\mathbb{Y}_{D/H})]$ and $\\mathds{1}$) and $\\max_l \\sigma(\\mathbb{Y}_l) \\leq \\sqrt{\\sum_m \\sigma^2(\\mathbb{Y}_m)}$, and the last inequality is from Lemma \\ref{lemma:key}. \n\\end{proof}\n\n\\begin{lemma} \\label{lemma:low_rank}\n$\\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty \\leq  \\phi^{-1}(N-1) \\sqrt{D/H}$.\n\\end{lemma}\n\\begin{proof}\nNote $\\|\\mathbf{u}\\mathbf{v}^\\top\\|_{\\infty} = \\|\\mathbf{u}\\|_{\\infty} \\|\\mathbf{v}\\|_1$ for real vectors $\\mathbf{u},\\mathbf{v}$. Hence\n\\begin{align*}\n    \\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty & = \\sum_j P_{ij} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_\\infty \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_1 \\\\\n    & = \\mathbf{a}^\\top \\mathbf{b} \\leq \\|\\mathbf{a}\\|_2 \\|\\mathbf{b}\\|_2,\n\\end{align*}\nwhere $a_j = \\sqrt{P_{ij}} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_\\infty$, $b_j = \\sqrt{P_{ij}} \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_1$.\n\nNote $a_j \\leq c_j \\coloneqq  \\sqrt{P_{ij}} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_2$ since $\\|\\mathbf{u}\\|_\\infty \\leq \\|\\mathbf{u}\\|_2$ for vector $\\mathbf{u}$. Hence $\\|\\mathbf{a}\\|_2 \\leq \\|\\mathbf{c}\\|_2$.\n\nAlso $b_j \\leq \\sqrt{\\frac{D}{H}} d_j \\coloneqq  \\sqrt{\\frac{D}{H}} \\sqrt{P_{ij}} \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_2$ since $\\|\\mathbf{u}\\|_1 \\leq \\sqrt{\\frac{D}{H}}\\|\\mathbf{u}\\|_2$ for $\\mathbf{u} \\in \\mathbb{R}^{D/H}$ (e.g.~by the Cauchy--Schwartz inequality on $[|\\mathbf{u}_1|, \\ldots, |\\mathbf{u}_{D/H}|]$ and $\\mathds{1}$). Hence $\\|b\\|_2 \\leq \\sqrt{\\frac{D}{H}}\\|d\\|_2$.\n\nNote $\\|c\\|_2^2 = \\sum_j P_{ij} \\|y_j - \\sum_k P_{ik} y_k\\|_2^2 = \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\phi^{-1}(N-1)$ from Lemma \\ref{lemma:key},\nand $\\|d\\|_2^2 =  \\sum_j P_{ij} \\|y_i - y_j\\|_2^2 \\leq  \\phi^{-1}(N-1)$ also from Lemma \\ref{lemma:key}.\nHence $\\|a\\|_2 \\|b\\|_2 \\leq \\sqrt{\\frac{D}{H}} \\|c\\|_2 \\|d\\|_2 \\leq \\sqrt{\\frac{D}{H}} \\phi^{-1}(N-1)$.\n\\end{proof}\n\n\nPutting the above lemmas altogether, with the observation $\\sup_X \\|J_f(X)\\|_\\infty = \\sup_X \\|[J_{i1}(X), \\ldots, J_{iN}(X)]\\|_\\infty$ by permutation invariance of $\\|J_f\\|_\\infty$ (since $f$ is permutation equivariant and $\\|\\cdot\\|_\\infty$ is the maximum absolute row sum), we have\n\\begin{align}\n\\|J_f\\|_{\\infty}\n& \\leq 4\\|W^Q\\|_{\\infty}\\|W^{Q^\\top}\\|_{\\infty} \\phi^{-1}(N-1)\n+ \\frac{\\|W^Q W^{Q^\\top}\\|_{\\infty}}{\\sqrt{D/H}} \\nonumber\\\\\n& \\leq \\|W^Q\\|_{\\infty}\\|W^{Q^\\top}\\|_{\\infty} \\left(4\\phi^{-1}(N-1) + \\frac{1}{\\sqrt{D/H}}\\right) \\label{ineq:infty}\\\\\n& \\leq \\|W^Q\\|_{\\infty} \\|W^{Q^\\top}\\|_{\\infty} \\left(4\\log N + \\frac{1}{\\sqrt{D/H}}\\right), \\nonumber\n\\end{align}\nwhere the last inequality holds for $N \\geq 3$.\n\nThe full multihead attention map that combines the heads $f^h(X)$ is:\n\\begin{equation*}\nF: X \\mapsto \\left[f^1(X)W^{V,1}, \\ldots f^H(X)W^{V,H}\\right] W^O = g(X) W^V W^O\n\\end{equation*}\nwhere $g:X \\mapsto [f^1(X),\\ldots,f^H(X)]$, $W^O \\in \\mathbb{R}^{D \\times D}$ and\n\\begin{equation*}\n    W^V = \\begin{bmatrix}\n    W^{V,1} & \\dots & 0 \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & W^{V,H} \\\\\n    \\end{bmatrix} \\in \\mathbb{R}^{DH \\times D}.\n\\end{equation*}\nNote the Jacobian $J_g$ is a block matrix whose rows are $J_{f^h}$, hence $\\|J_g\\|_{\\infty} = \\max_h \\|J_{f^h}\\|_{\\infty}$, and similarly $\\|W^{V^\\top}\\|_{\\infty} = \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty}$. Hence we have\n\\begin{equation*}\n    \\lip_{\\infty}(F) \\leq \\max_h \\|J_{f^h}\\|_{\\infty} \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty} \\|W^{O^\\top}\\|_{\\infty}.\n\\end{equation*}\n\nCombining this with Inequality (\\ref{ineq:infty}), we have:\n\\begin{equation*}\n    \\lip_{\\infty}(F)  \\leq \\left(4 \\phi^{-1}(N-1) + \\frac{1}{\\sqrt{D/H}}\\right) \\max_h \\|W^{Q,h}\\|_{\\infty} \\|W^{{Q,h}^\\top}\\|_{\\infty} \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty} \\ \\|W^{O^\\top}\\|_{\\infty}.\n\\end{equation*}\n\n\\subsubsection{Upper bound on $\\boldsymbol{\\lip_2(F)}$ for L2-MHA}\nFor $p=2$, we use the following lemma:\n\\begin{lemma} \\label{lemma:block_rows}\nLet A be a block matrix with block rows $A_1, \\ldots, A_N$. Then $\\|A\\|_2 \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2}$, and equality holds if and only if the first right singular vectors of the $A_i$ align.\n\\end{lemma}\n\\begin{proof}\n\\begin{equation*}\n\\|A\\|_2^2 = \\left\\Vert \\begin{bmatrix} A_1 \\\\ \\vdots \\\\ A_N \\\\ \\end{bmatrix}\\right\\Vert_2^2 = \\sup_{\\|\\mathbf{x}\\|_2=1} \\left\\Vert\\begin{bmatrix} A_1 \\\\ \\vdots \\\\ A_N \\\\ \\end{bmatrix} \\mathbf{x}\\right\\Vert_2^2 = \\sup_{\\|\\mathbf{x}\\|_2=1} \\sum_i \\|A_i \\mathbf{x}\\|_2^2 \\leq \\sum_i \\sup_{\\|\\mathbf{x}\\|_2=1} \\|A_i \\mathbf{x}\\|_2^2 = \\sum_i \\|A_i\\|_2^2.\n\\end{equation*}\nNote that equality holds if and only if the first right singular vectors of the $A_i$ align.\n\\end{proof}\nHence a bound on the spectral norm of each block row of $J_f$ can give us an $O(\\sqrt{N})$ bound on $\\|J_f\\|_2$, which may be loose, and it remains an open question as to whether this bound can be tightened.\n\nTo bound the $\\|\\cdot\\|_2$ norm of each row of $J_f$, we use the following lemmas:\n\\begin{lemma}\n$\\|Y^\\top P^{(i)}Y\\|_2 \\leq \\phi^{-1}(N-1)$\n\\end{lemma}\n\\begin{proof}\n$\\|Y^\\top P^{(i)}Y\\|_2=\\|\\mathrm{Cov}(\\mathbb{Y})\\|_2 = \\lambda_{\\max}(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\phi^{-1}(N-1)$, where the first equality holds by symmetry of $\\mathrm{Cov}(\\mathbb{Y})$ and the next holds by $\\mathrm{Cov}(\\mathbb{Y})$ being positive semi-definite, so all its eigenvalues are non-negative, and hence the maximal eigenvalue is bounded by the sum of the eigenvalues, equal to its trace. The final inequality is from Lemma \\ref{lemma:key}.\n\\end{proof}\n\n\\begin{lemma}\n$\\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_2 \\leq  \\phi^{-1}(N-1)$\n\\end{lemma}\n\\begin{proof}\nDirectly use Cauchy--Schwartz on $c$ and $d$ in the proof of Lemma \\ref{lemma:low_rank}. \n\\end{proof}\nAgain using the inequalities $\\|BC\\| \\leq \\|B\\| \\|C\\|$, $\\|B + C\\| \\leq \\|B\\| + \\|C\\|$ and $\\|[A_1, \\ldots, A_N]\\| \\leq \\sum_i \\|A_i\\|$, with the additional equality $\\|B^\\top\\|_2 = \\|B\\|_2$, we have the bound: \n\\begin{align*}\n&\\|[J_{i1}, \\ldots, J_{iN}]\\|_2 \\\\\n & \\leq  2  \\frac{\\|W^Q\\|_2\\|W^{Q^\\top}\\|_2}{\\sqrt{D/H}} \n\\bigg(\\|Y^\\top P^{(i)}Y\\|_2\n + \\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_2 \\bigg) + \\frac{\\|W^Q W^{Q^\\top}\\|_2}{\\sqrt{D/H}} \\\\\n & \\leq  4\\phi^{-1}(N-1) \\frac{\\|W^Q\\|_2^2}{\\sqrt{D/H}}  + \\frac{\\|W^Q W^{Q^\\top}\\|_2}{\\sqrt{D/H}} \\\\\n & \\leq  \\frac{\\|W^Q\\|_2^2}{\\sqrt{D/H}} \\bigg(4\\phi^{-1}(N-1)+1 \\bigg).\n\\end{align*}\nUsing Lemma \\ref{lemma:block_rows}, we have that\n\\begin{align}\n    \\|J_f\\|_2 & \\leq \\frac{\\sqrt{N}\\|W^Q\\|_2^2}{\\sqrt{D/H}} \\bigg(4\\phi^{-1}(N-1)+1 \\bigg) \\label{ineq:2} \\\\\n    & \\leq \\frac{\\sqrt{N}\\|W^Q\\|_2^2}{\\sqrt{D/H}}(4\\log N+1). \\nonumber\n\\end{align}\nTo obtain the final result for the full multihead self-attention $F$, we need a final lemma:\n\\begin{lemma} \\label{lemma:block_cols}\nLet A be a block matrix with block columns $A_1, \\ldots, A_N$. Then $\\|A\\|_2 \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2}$.\n\\end{lemma}\n\\begin{proof}\n\\begin{align*}\n\\|A\\|_2 &= \\|[A_1, \\ldots, A_N]\\|_2 = \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1}  \\left\\Vert [A_1, \\ldots, A_N] \\begin{bmatrix} \\mathbf{x}_1\\\\ \\vdots \\\\ \\mathbf{x}_N \\\\ \\end{bmatrix} \\right\\Vert_2^2 = \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1} \\|\\sum_i A_i \\mathbf{x}_i\\|_2 \\\\ \n& \\leq \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1} \\sum_i \\|A_i \\mathbf{x}_i\\|_2 = \\sup_{\\|\\mathbf{e}_i\\|_2=1, \\sum_i \\lambda_i^2 =1} \\sum_i \\lambda_i \\|A_i \\mathbf{e}_i\\|_2 = \\sup_{\\sum_i \\lambda_i^2 =1} \\sum_i \\lambda_i \\|A_i\\|_2 \\\\\n& \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2},\n\\end{align*}\nwhere we are using the substitution $\\mathbf{x}_i = \\lambda_i \\mathbf{e}_i$, and the last inequality holds by e.g.~Cauchy--Schwartz inequality on $[\\lambda_1, \\ldots, \\lambda_N]$ and $[\\|A_1\\|_2, \\ldots, \\|A_N\\|_2]$.\n\\end{proof}\nRecall that \n\\begin{equation*}\nF: X \\mapsto \\left[f^1(X)W^{V,1}, \\ldots, f^H(X)W^{V,H}\\right] W^O.\n\\end{equation*}\nSince $\\|f^h(X)W^{V,h}\\|_2 \\leq \\|J_{f^h}\\|_2 \\|W^{V,h}\\|_2$, by Lemma \\ref{lemma:block_cols} we have that\n\\begin{equation*}\n    \\left\\|[f^1(X)W^{V,1}, \\ldots, f^H(X)W^{V,H}]\\right\\|_2 \\leq \\sqrt{\\sum_h \\|J_{f^h}\\|_2^2 \\|W^{V,h}\\|_2^2}\n\\end{equation*} and hence\n\\begin{equation}\n    \\lip_2(F) \n    \\leq \\left(\\sqrt{\\sum_h \\|J_{f^h}\\|_2^2 \\|W^{V,h}\\|_2^2}\\right) \\|W^O\\|_2.\n\\end{equation}\nCombining this with Inequality (\\ref{ineq:2}), we have:\n\\begin{equation*}\n    \\lip_2(F) \\leq \\frac{\\sqrt{N}}{\\sqrt{D/H}}\n    \\left(4 \\phi^{-1}(N-1) + 1 \\right) \\left(\\sqrt{\\textstyle\\sum_h \\|W^{Q,h}\\|_2^2\\, \\|W^{V,h}\\|_2^2}\\right) \\|W^O\\|_2.\n\\end{equation*}\n\n\\section{The Case with Masking} \\label{apd:masking}\nSince self-attention is often used with \\textit{masking}, a natural question is how masking affects the derived bounds. In self-attention (for any choice of attention function), masking is implemented as follows: given a set of mask indices $\\mathcal{M}  \n\\subset \\{1, \\ldots, N\\} \\times \\{1, \\ldots, N\\}$, the logits (i.e.~the inputs to the softmax) are set to $-\\infty$ at the mask indices. That is,\n\\begin{equation*}\n    L_{ij}= \n\\begin{cases}\n    \\tilde{L}_{ij} & \\text{if } (i,j) \\notin \\mathcal{M}\\\\\n    -\\infty        & \\text{if } (i,j) \\in \\mathcal{M}\n\\end{cases}\n\\end{equation*}\nwhere $\\tilde{L}_{ij}$ is the original logit (e.g.~for L2 self-attention, $\\tilde{L}_{ij} = -(\\mathbf{x}_i - \\mathbf{x}_j)^\\top A (\\mathbf{x}_i - \\mathbf{x}_j)$). \n\nMasking implies $f_i(X)$ is not a function of $\\mathbf{x}_j$ for $(i,j) \\in \\mathcal{M}$, hence  $J_{ij} = 0$ for $(i,j) \\in \\mathcal{M}$.\nThus $f_i(X)$ is equal to the $i$th output for self-attention with inputs restricted to $\\{\\mathbf{x}_j: (i,j) \\notin \\mathcal{M}\\}$, the unmasked inputs with respect to the $i$th output. \nHence $J_{ij}$ will no longer contribute to the bound on $\\|[J_{i1}, \\ldots, J_{iN}]\\|$, and hence the bound for the unmasked case will continue to hold as long as $(i,i) \\in \\mathcal{M}$ i.e.~$\\mathbf{x}_i$ attends to itself (this is necessary for the proof of Lemma \\ref{lemma:key} to hold). \nThe bound can in fact be tightened by replacing $N$ with $|\\{\\mathbf{x}_j: (i,j) \\notin \\mathcal{M}\\}|$, the number of unmasked inputs with respect to the $i$th output.\n\n\\section{Experimental Details} \\label{apd:experimental_details}\nFor the experiment in Section \\ref{sec:asymptotic}, showing the asymptotic tightness of the upper bound on $\\lip_{\\infty}(F)$ where $F$ is \\verb!L2-MHA!, we fix all free parameters of $F$ (namely $W^Q, W^V$) to be the identity, and only optimise the input $X$. \nWe use $50$ random initialisations of $X$ for each $N$, where $X_{ij} \\sim \\unif [-c, c]$ for $c \\sim \\unif [0,10]$ (we observed that having $c$ itself be random improves optimisation). We display the top $5$ results for each value of $N$ after optimising each random initialisation till convergence using Adam \\citep{kingma2014adam} with a learning rate of $0.1$. \n\nFor the experiments in Section \\ref{sec:invertible_self-attention}, we comparing the performance of the original Transformer and the Transformer with Lipschitz/invertible self-attention at character-level language modelling on the Penn Treebank dataset \\citep{marcus1993building}.\\footnote{We use the standard training-validation-test split, and the dataset can be found at e.g.\\ \\url{https://github.com/harvardnlp/TextFlow/tree/master/data/ptb}.}\nEach training example is a sentence represented as a variable-length sequence of characters, and examples are batched according to length such that padding is minimised, with the maximum sequence length set to $288$. \nAll models are autoregressive, outputting the logits for the categorical likelihood predicting the next character, and are trained using maximum likelihood (cross-entropy loss) with a batch size of $64$. \nThe LSTM models have the dimensionality of the hidden state equal to the dimensionality $D$ of the cell state (the usual default implementation). \nThe Transformer models are trained with a varying number of blocks (number of layers) with $H=8$ heads and $D=512$, tuning hyperparameters for dropout rate in $\\{0,0.1,0.2\\}$ and base learning rate $\\gamma \\in \\{0.2,0.4,0.6,0.8,1.0,1.5,2.0\\}$ with number of warmup iterations $w \\in \\{1000,2000,4000,8000\\}$ for the standard custom learning rate schedule in \\citet{vaswani2017attention}:\n\\begin{equation*}\n  \\epsilon_t = \\frac{\\gamma}{\\sqrt{D}} \\min(t^{-1/2}, t w^{-3/2}),\n\\end{equation*}\nwhere $\\epsilon_t$ is the learning rate at training iteration $t$. Hence the learning rate linearly increases from $0$ to $(Dw)^{-1/2}$ over $w$ iterations, then decays proportionally to $t^{-1/2}$.\nWe use Glorot Uniform initialisation \\citep{glorot2010understanding} for all weights ($U\\left[-\\sqrt{\\frac{1}{d_{in} + d_{out}}}, \\sqrt{\\frac{1}{d_{in} + d_{out}}}\\right]$), except for weights in \\verb!L2-MHA! that are initialised from $U\\left[-\\frac{s}{\\sqrt{D}},\\frac{s}{\\sqrt{D}}\\right]$, and $s$ is a hyperparameter. For $D=512$, we used $s=\\frac{1}{2^4}$. All experiments were done in Tensorflow 1.14 \\citep{abadi2016tensorflow} on single Nvidia Tesla V100 GPUs.\n\n\\section{Numerical Invertibility of MHA Residual Map} \\label{apd:numerical_invertibility}\nFollowing Section \\ref{sec:numerical-invertibility}, Figure \\ref{fig:trained_dp_mha_invertibility} confirms that numerical invertibility does not hold for trained weights for dot-product multihead self-attention (DP-MHA) (obtained from one-layer Transformer (DP) model used for Figure \\ref{fig:ptb}), similar to the randomly initialised weight case.\nFigure \\ref{fig:mha_invertibility} shows additional results for different values of $N$ and $D$. \n\n\\begin{figure}[!htb]\n    \\centering\n    \\includegraphics[width=0.35\\textwidth]{trained_dp_mha_invertibility.pdf}\n    \\caption{Invertibility of $g(\\mathbf{x})=\\mathbf{x} + cf(\\mathbf{x})$ for trained DP-MHA $f$.}\n    \\label{fig:trained_dp_mha_invertibility}\n\\end{figure}\n\n\\begin{figure}[!htb]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{mha_invertibility.pdf}\n    \\caption{Numerical invertibility of $g(\\mathbf{x})= \\mathbf{x} + c f(\\mathbf{x})$ where $f$ is L2-MHA(left) or DP-MHA  (right), for different values of $N$ and $D$.}\n    \\label{fig:mha_invertibility}\n\\end{figure}\n\n\n\n\\newpage\n\\section{Behaviour of Lower Bound on $\\boldsymbol{\\lip_2(F)}$}\n\\label{apd:jacobian_opt2}\n\\begin{figure}[!htb]\n    \\centering\n    \\includegraphics[width=0.6\\columnwidth]{jacobian_opt2.pdf}\n    \\caption{Lower bound on $\\lip_{2}(F)$ where $F$ is L2-MHA, with $D=1$ and varying $N$, obtained by optimising $\\|J_F(X)\\|_{2}$ with respect to $X$, with $50$ random initialisations of $X$ for each $N$.}\n    \\label{fig:jacobian_opt2}\n\\end{figure}\nIn Figure \\ref{fig:jacobian_opt2}, we show the lower bound on $\\lip_{2}(F)$ obtained by optimising $\\|J_F(X)\\|_{2}$ using the same optimisation procedure as for Figure \\ref{fig:jacobian_opt} of Section \\ref{sec:asymptotic}. Here the optimisation is more difficult, evident in the variance of the top $5$ values, and the trend is less clear, but it appears that $\\lip_{2}(f)$ grows at a rate of $O(\\log N)$. The message is less clear here, and there are at least two possibilities: \n\\begin{enumerate}[label=(\\arabic*), leftmargin=*]\n    \\item The optimisation is difficult even for small values of $N$, hence Figure \\ref{fig:jacobian_opt2} shows a loose lower bound.\n    \\item If the lower bound is tight, this suggests that the $O(\\sqrt{N} \\log N)$ bound in Theorem \\ref{thm:main} is not asymptotically tight, and could be improved to $O(\\log N)$ (or $O(\\log N - \\log \\log N)$ as for $p=\\infty$).\n\\end{enumerate}\n\n\\section{Optimising the norm of the Jacobian of DP-MHA}\n\\label{apd:jacobian_opt_dp}\nIn Figure \\ref{fig:trained_dp_mha_lower_bound}, we show how the norm of the Jacobian $\\|J_f(X)\\|_{\\infty}$ for \\verb!DP-MHA! $f$ keeps increasing when being optimised with respect to $X$. This is a useful sanity check validating our theoretical result of Theorem \\ref{thm:dp_not_lipschitz}, that \\verb!DP-MHA! is \\emph{not} Lipshchitz. The oscillations are likely due to momentum term of Adam optimizer that was used to optimise the norm. \n\n\\begin{figure}[!htb]\n    \\centering\n    \\includegraphics[width=0.4\\columnwidth]{trained_dp_mha_lower_bound.pdf}\n    \\caption{Optimise $\\|J_f(X)\\|_{\\infty}$ w.r.t.~$X$ for trained DP-MHA $f$.}\n    \\label{fig:trained_dp_mha_lower_bound}\n\\end{figure}\n\n\\section{Experiment tying keys and queries of L2-MHA but preserving parameter count}\n\\label{apd:wq_wk_experiment}\nIn Figure \\ref{fig:ptb} of Section \\ref{sec:invertible_self-attention}, we have shown that there is a clear reduction in performance when tying the keys and queries. To test whether this can be attributed to the reduction in parameter count, we tried doubling the number of columns of $W^Q$ when the keys and queries are shared (i.e. from $D/H$ to $2D/H$) so that the shared model has the same number of parameters as the unshared model. In Figure \\ref{fig:ptb_wq_wk}, the third column shows results for shared \\verb!L2-MHA!, but with the same number of parameters as the unshared \\verb!L2-MHA! i.e.~without tying the keys and queries. The performance is similar to the second column (tying with a reduced number of parameters), suggesting that there is an inherent limitation in expressiveness to tying the keys and queries, and that the reduction in number of parameters is an insufficient explanation this phenomenon.\n\n\\begin{figure}[!htb]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{ptb_wq_wk_experiment.pdf}\n    \\caption{Experiment tying keys/queries but preserving parameter count.}\n    \\label{fig:ptb_wq_wk}\n\\end{figure}\n\n\n\\section{Training curves for fixed learning rate DP-MHA vs L2-MHA} \\label{apd:stability}\n\n\\begin{figure}[!ht]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{stability_experiment.pdf}\n    \\caption{Train NLL for Transformer (DP), Transformer (L2) and Transformer (Contractive-L2)}\n    \\label{fig:stability}\n\\end{figure}\n\n\\section{The Lipschitz constant of LayerNorm}\n\\label{apd:layernorm}\nIn this section, we show that \\verb!LayerNorm! is Lipschitz, with a loose bound on its Lipschitz constant w.r.t. to the $\\infty$-norm.\n\\verb!LayerNorm! is defined as follows:\n\\begin{align*}\n    \\text{LN}(\\mathbf{x}) &= \\frac{\\mathbf{x}-\\mu(\\mathbf{x})}{\\sqrt{\\sigma^2(\\mathbf{x}) + \\epsilon}} \\odot \\boldsymbol\\gamma + \\boldsymbol\\beta \\\\\n    \\mu(\\mathbf{x}) &= \\frac{1}{D} \\sum_{d=1}^D x_d \\\\\n    \\sigma^2(\\mathbf{x}) &= \\frac{1}{D}\\sum_{d=1}^D (x_d - \\mu(\\mathbf{x}))^2 \n\\end{align*}\nwhere $\\mathbf{x}, \\boldsymbol\\beta, \\boldsymbol\\gamma \\in \\mathbb{R}^D$. We will omit dependence on $x$ to write $\\mu, \\sigma^2$ in cases when there is no ambiguity to reduce clutter.\n\nIn the trivial case where $x_d$ are all equal or when $D=1$, $\\mathbf{x} = \\mu$ hence $LN(\\mathbf{x})=\\boldsymbol\\beta$, so its Lipschitz constant is 0. Thus let us assume $D > 2$ and not all $x_d$ are equal.\n\nFirst let us compute the derivative of $\\mu$ and $\\sigma^2$ w.r.t $x$:\n\\begin{align*}\n    \\frac{\\partial \\mu}{\\partial \\mathbf{x}} &= \\frac{1}{D} \\mathds{1}^\\top \\\\\n    \\frac{\\partial \\sigma^2}{\\partial \\mathbf{x}} &= \\frac{1}{D} \\sum_d 2(x_d - \\mu) \\frac{\\partial}{\\partial \\mathbf{x}}(x_d - \\mu) \\\\\n    &= \\frac{2}{D} \\sum_d (x_d - \\mu)(\\mathbf{e}_d - \\frac{1}{D}\\mathds{1})^\\top \\\\\n    &= \\frac{2}{D} \\bigg[\\sum_d (x_d - \\mu) \\mathbf{e}_d - \\frac{1}{D} \\mathds{1} \\sum_d (x_d - \\mu)\\bigg]^\\top \\\\\n    &= \\frac{2}{D}\\sum_d (x_d - \\mu) \\mathbf{e}_d^\\top \\\\\n    &= \\frac{2}{D}(\\mathbf{x} - \\mu)^\\top\n\\end{align*}\nwhere $\\mathbf{e}_d \\in \\mathbb{R}^D$ is a one-hot vector with $1$ at the $d$th element. Note the penultimate equality holds because $\\sum_d (x_d - \\mu) = 0$.\n\nNow the derivative of $\\text{LN}(\\mathbf{x})_d$, the $d$th element of $\\text{LN}(\\mathbf{x})$, w.r.t.$\\mathbf{x}$ is\n\\begin{align*}\n    \\frac{\\partial \\text{LN}(\\mathbf{x})_d}{\\partial \\mathbf{x}} &= \\gamma_d \\bigg[\\frac{\\partial}{\\partial \\mathbf{x}}(x_d - \\mu)(\\sigma^2 + \\epsilon)^{-\\frac{1}{2}} + (x_d - \\mu)\\Big(-\\frac{1}{2}(\\sigma^2 + \\epsilon)^{-\\frac{3}{2}}\\Big)\\frac{\\partial \\sigma^2}{\\partial \\mathbf{x}}\\bigg] \\\\\n    &= \\gamma_d (\\sigma^2 + \\epsilon)^{-\\frac{1}{2}} \\bigg[(\\mathbf{e}_d - \\frac{1}{D}\\mathds{1})^\\top - \\frac{1}{2} (x_d - \\mu)(\\sigma^2 + \\epsilon)^{-1} \\frac{2}{D}(\\mathbf{x} - \\mu)^\\top \\bigg] \\\\\n    &= \\gamma_d (\\sigma^2 + \\epsilon)^{-\\frac{1}{2}} \\bigg[(\\mathbf{e}_d - \\frac{1}{D}\\mathds{1})^\\top - \\frac{1}{D} (\\sigma^2 + \\epsilon)^{-1} (x_d - \\mu)(\\mathbf{x} - \\mu)^\\top \\bigg].\n\\end{align*}\n\nHence\n\\begin{align*}\n    \\frac{\\partial \\text{LN}(\\mathbf{x})}{\\partial \\mathbf{x}} &= (\\sigma^2 + \\epsilon)^{-\\frac{1}{2}} \\bigg[ \\text{diag}(\\boldsymbol\\gamma) - \\frac{1}{D}\\boldsymbol\\gamma \\mathds{1}^\\top - \\frac{1}{D} (\\sigma^2 + \\epsilon)^{-1}\\text{diag}(\\boldsymbol\\gamma)(\\mathbf{x} - \\mu)(\\mathbf{x} - \\mu)^\\top \\bigg].\n\\end{align*}\n\nNote\n\\begin{equation*}\n    \\text{diag}(\\boldsymbol\\gamma) - \\frac{1}{D}\\boldsymbol\\gamma \\mathds{1}^\\top = \\begin{bmatrix}\n    \\gamma_1 (D-1)/D & -\\gamma_1 / D & \\dots & -\\gamma_1 / D \\\\\n    - \\gamma_2 / D & \\gamma_2 (D-1)/D & \\dots & -\\gamma_2 / D \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    -\\gamma_D / D & -\\gamma_D / D & \\dots & \\gamma_D (D-1)/D\n    \\end{bmatrix},\n\\end{equation*}\n\nhence\n\\begin{equation} \\label{eq:first_terms_inf_norm}\n    \\left\\Vert \\text{diag}(\\boldsymbol\\gamma) - \\frac{1}{D}\\boldsymbol\\gamma \\mathds{1}^\\top \\right\\Vert_{\\infty} = \\frac{2(D-1)}{D}\\max_d |\\gamma_d|,\n\\end{equation}\nrecalling that $\\left\\Vert \\cdot \\right\\Vert_{\\infty}$ is the maximum absolute row sum.\n\nLet $z_d \\coloneqq x_d - \\mu$. Hence $\\sum_d z_d = 0$, $\\sigma^2 = \\frac{1}{D} \\sum_d z_d^2$ and\n\\begin{equation*}\n    \\mathrm{Cov}(\\mathbf{x}) = \n    (\\mathbf{x} - \\mu)(\\mathbf{x} - \\mu)^\\top =\n    \\begin{bmatrix}\n    z_1^2 & \\dots & z_1 z_D \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    z_D z_1 & \\dots & z_D^2 \\\\\n    \\end{bmatrix}.\n\\end{equation*}\n\nHence\n\\begin{equation*}\n\\frac{\\left\\Vert \\mathrm{Cov}(\\mathbf{x})\\right\\Vert_{\\infty}}{\\sigma^2} = \\frac{\\max_d |z_d| \\sum_{d'} |z_{d'}|}{\\frac{1}{D}\\sum_d z_d^2}.    \n\\end{equation*}\nNoting that this expression is scale-invariant in $\\mathbf{z}$, we may assume WLOG $\\max_d |z_d| = z_D = 1$, since we are assuming not all $x_d$ are equal and hence at least one $z_d$ is non-zero.\n\nThe expression now becomes\n\\begin{equation} \\label{eq:cov_expression}\n\\frac{\\left\\Vert \\mathrm{Cov}(\\mathbf{x})\\right\\Vert_{\\infty}}{\\sigma^2} =  D\\bigg(\\frac{1 + \\sum_{d < D} |z_d|}{1 + \\sum_{d<D} z_d^2}\\bigg). \n\\end{equation}\nSince all terms $|z_d| \\leq 1$ are bounded, this continuous expression reaches a global maximum for some value of $\\mathbf{z}$ with $z_D = 1$.\n\nIt is easy to see that at the global maximum, $z_d \\neq 0$ $\\forall d$:\nsuppose this were to be true, WLOG $z_1 = 0$.\nThen let us see how the quantity \\eqref{eq:cov_expression} changes when $z_1=0$ is increased by $0< \\delta < 1$ and $z_D=1$ is decreased by $\\delta$, keeping the sum constant.\nIt is easy to see that the numerator $\\sum_d |z_d|$ stays constant, but the denominator $\\sum_d z_d^2$ changes by $2\\delta^2 - 2\\delta < 0$.\nSince for small $\\delta$, the numerator of \\eqref{eq:cov_expression} stays constant but the denominator decreases, the quantity \\eqref{eq:cov_expression} increases, contradicting that the global max is obtained for $z_1 = 0$.\nHence we may assume that $z_d \\neq 0$ $\\forall d$.\n\nHence the quantity \\eqref{eq:cov_expression} (in particular, $\\sum_d {|z_d|}$) is differentiable at the global maximum, at which the partial derivatives of the following Lagrangian are zero:\n\\begin{equation*}\n    \\mathcal{L}(z_1,\\ldots,z_{D-1}, \\lambda) = \\frac{1 + \\sum_{d < D} |z_d|}{1 + \\sum_{d<D} z_d^2} - \\lambda(\\sum_{d<D} z_d + 1).\n\\end{equation*}\nFrom now on let us write $\\sum$ for $\\sum_{d < D}$ below to reduce clutter. Setting $\\frac{\\partial \\mathcal{L}}{\\partial z_k} = 0$ and noting $\\frac{d|z_k|}{dz_k} = \\text{sgn}(z_k)$, we obtain\n\\begin{align*}\n    & \\frac{\\text{sgn}(z_k)(1 + \\sum z_d^2) - 2z_k(1 + \\sum |z_d|)}{(1+\\sum z_d^2)^2} - \\lambda = 0 \\\\\n    \\iff & \\text{sgn}(z_k)(1 + \\sum z_d^2) - 2z_k(1 + \\sum |z_d|) = \\lambda (1+\\sum z_d^2)^2 \\\\\n    \\iff & z_k = \\frac{\\text{sgn}(z_k)(1 + \\sum z_d^2) - \\lambda (1+\\sum z_d^2)^2}{2(1 + \\sum |z_d|)} \\\\\n    \\iff & z_k = \\frac{(\\text{sgn}(z_k) - \\lambda (1+\\sum z_d^2))(1 + \\sum z_d^2)}{2(1 + \\sum |z_d|)}\n\\end{align*}\nHence at the global maximum, $z_k$ takes one of two values $a > 0$ and $b < 0$.\nFurther we have that\n\\begin{align} \\label{eq:max_expression}\n    \\frac{1 + \\sum|z_d|}{1 + \\sum z_d^2} = \\frac{\\text{sgn}(z_k) - \\lambda (1+\\sum z_d^2)}{2z_k}\n\\end{align}\nIf both $a$ and $b$ are among the $z_k$, we have that $\\frac{1 - \\lambda (1+\\sum z_d^2)}{2a} = \\frac{- 1 - \\lambda (1+\\sum z_d^2)}{2b}$.\nSolving for $\\lambda(1+\\sum z_d^2)$ and plugging it in back to Equation \\eqref{eq:max_expression}, we get:\n\\begin{align*}\n    \\frac{1 + \\sum|z_d|}{1 + \\sum z_d^2} = \\frac{1}{a-b}\n\\end{align*}\nSince $a > 0$, $b < 0$ and $\\sum z_d = -1$, $a-b$ is minimised when only one of the $z_d$ is $a$ and the rest are $b$. Hence a crude lower bound on $a-b$ is $\\frac{1}{D-2}$, giving a bound:\n\\begin{equation} \\label{eq:second_term_inf_norm}\n    \\frac{\\left\\Vert \\mathrm{Cov}(\\mathbf{x})\\right\\Vert_{\\infty}}{\\sigma^2}\n    \\leq D(D-2)\n\\end{equation}\n\nHowever we conjecture that the true global maximum is attained when $z_d = - \\frac{1}{D-1}$ $\\forall d < D$ (i.e. all the $z_d$ for $d < D$ are equal to $b < 0$), for which it is easy to show that $\\frac{1 + \\sum_{d < D} |z_d|}{1 + \\sum_{d<D} z_d^2} = 2(D-1)/D$.\n\nPutting together the above, we have:\n\\begin{align*}\n    \\left\\Vert \\frac{\\partial \\text{LN}(\\mathbf{x})}{\\partial \\mathbf{x}} \\right\\Vert_{\\infty} &= (\\sigma^2 + \\epsilon)^{-\\frac{1}{2}} \\left\\Vert   \\text{diag}(\\boldsymbol\\gamma) - \\frac{1}{D}\\boldsymbol\\gamma \\mathds{1}^\\top - \\frac{1}{D} (\\sigma^2 + \\epsilon)^{-1}\\text{diag}(\\boldsymbol\\gamma)(\\mathbf{x} - \\mu)(\\mathbf{x} - \\mu)^\\top \\right\\Vert_{\\infty} \\\\\n    &\\leq \\epsilon^{-\\frac{1}{2}} \\bigg( \\left\\Vert \\text{diag}(\\boldsymbol\\gamma) - \\frac{1}{D}\\boldsymbol\\gamma \\mathds{1}^\\top \\right\\Vert_{\\infty} + \\frac{1}{D}\\left\\Vert  \\text{diag}(\\boldsymbol\\gamma) \\right\\Vert_{\\infty} \\left\\Vert  (\\sigma^2 + \\epsilon)^{-1}(\\mathbf{x} - \\mu)(\\mathbf{x} - \\mu)^\\top \\right\\Vert_{\\infty} \\bigg) \\\\\n    &\\leq \\epsilon^{-\\frac{1}{2}} \\bigg( \\left\\Vert \\text{diag}(\\boldsymbol\\gamma) - \\frac{1}{D}\\boldsymbol\\gamma \\mathds{1}^\\top \\right\\Vert_{\\infty} + \\frac{1}{D}\\left\\Vert  \\text{diag}(\\boldsymbol\\gamma) \\right\\Vert_{\\infty} \\left\\Vert  \\mathrm{Cov}(\\mathbf{x}) / \\sigma^2 \\right\\Vert_{\\infty} \\bigg) \\\\\n    &\\leq \\epsilon^{-\\frac{1}{2}} \\bigg( \\frac{2(D-1)}{D}\\max_d |\\gamma_d| + \\frac{1}{D} \\max_d |\\gamma_d| D(D-2) \\bigg) \\\\\n    &= \\epsilon^{-\\frac{1}{2}} \\max_d |\\gamma_d| \\bigg(\\frac{2(D-1)}{D} + D-2 \\bigg) \\\\\n    &= \\epsilon^{-\\frac{1}{2}} \\max_d |\\gamma_d| \\bigg(\\frac{D^2-2}{D}\\bigg).\n\\end{align*}\n\n\n}\n\\end{document}\n\n==== END OF /2006.04710/main.tex ====",
            "processed_original_tex": "==== BEGINNING OF /2006.04710/main.tex ====\n\\documentclass{article}\n\\usepackage{times}\n\\usepackage[utf8]{inputenc} \n\\usepackage[T1]{fontenc}    \n\\usepackage{hyperref}       \n\\usepackage{url}            \n\\usepackage{booktabs}       \n\\usepackage{amsfonts}       \n\\usepackage{nicefrac}       \n\\usepackage{microtype}      \n\\usepackage{graphicx}\n\\usepackage{amsmath}\n\\usepackage{mathtools} \n\\usepackage{amsthm}\n\\usepackage{verbatim}\n\\usepackage{dsfont}  \n\\usepackage{wrapfig}  \n\\usepackage{caption}\n\\usepackage{subcaption}\n\\usepackage{xcolor}\n\\usepackage{amsbsy}\n\\usepackage{paralist}\n\\usepackage{enumitem}\n\n\\newtheorem{theorem}{Theorem}[section]\n\\newtheorem{lemma}{Lemma}[section]\n\\theoremstyle{definition}\n\\newtheorem{definition}{Definition}[section]\n\\newtheorem{corollary}{Corollary}[section]\n\n\\DeclareMathOperator{\\Tr}{Tr}\n\\newcommand{\\spaces}{\\hspace{2mm}}\n\\DeclareMathOperator{\\softmaxOp}{softmax}\n\\newcommand{\\softmax}[1]{\\softmaxOp\\left(#1\\right)}\n\\DeclareMathOperator{\\lip}{Lip}\n\\DeclareMathAlphabet{\\pazocal}{OMS}{zplm}{m}{n}\n\\newcommand{\\unif}{\\pazocal{U}}\n\\DeclareMathOperator{\\diag}{diag}\n\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n\n\\newcommand{\\andriy}[1]{\\textcolor{blue}{[AM: #1]}}\n\\newcommand{\\hyunjik}[1]{\\textcolor{red}{[HK: #1]}}\n\\newcommand{\\george}[1]{\\textcolor{magenta}{[George: #1]}}\n\n\n\\usepackage[accepted]{icml2021}\n\n\n\n\n\n\n\\icmltitlerunning{The Lipschitz Constant of Self-Attention}\n\n\\begin{document}\n\n\\twocolumn[\n\\icmltitle{The Lipschitz Constant of Self-Attention}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\icmlsetsymbol{equal}{*}\n\n\\begin{icmlauthorlist}\n\\icmlauthor{Hyunjik Kim}{dm}\n\\icmlauthor{George Papamakarios}{dm}\n\\icmlauthor{Andriy Mnih}{dm}\n\\end{icmlauthorlist}\n\n\\icmlaffiliation{dm}{DeepMind, UK}\n\n\\icmlcorrespondingauthor{Hyunjik Kim}{hyunjikk@google.com}\n\n\n\n\n\\icmlkeywords{Machine Learning, ICML}\n\n\\vskip 0.3in\n]\n\n\n\n\n\n\n\n\n\n\n\\printAffiliationsAndNotice{} \n\n\n\\begin{abstract}\nLipschitz constants of neural networks have been explored in various contexts in deep learning, such as provable adversarial robustness, estimating Wasserstein distance, stabilising training of GANs, and formulating invertible neural networks.\nSuch works have focused on bounding the Lipschitz constant of fully connected\nor convolutional networks, composed of linear maps and pointwise non-linearities. \nIn this paper, we investigate the Lipschitz constant of\nself-attention, a non-linear neural network module widely used in sequence modelling.\nWe prove that the standard dot-product self-attention is \\emph{not} Lipschitz for unbounded input domain, and propose an alternative L2 self-attention that \\emph{is} Lipschitz. \nWe derive an upper bound on the Lipschitz constant of L2 self-attention and provide empirical evidence for its asymptotic tightness. \nTo demonstrate the practical relevance of our theoretical work, we formulate invertible self-attention and use it in a Transformer-based architecture for a character-level language modelling task.\n\\end{abstract}\n\n\\section{Introduction}\n\\label{sec:intro}\nLipschitz continuity is a strong form of continuity for functions. \nLoosely speaking, a function is \\textit{Lipschitz continuous} if changing its input by a certain amount cannot change its output by more than $K$ times that amount. \nThe constant $K$ is a hard constraint on how rapidly the function's output can vary, and the smallest such $K$ is known as the function's \\textit{Lipschitz constant}.\nFor example, \\smash{$f_1(x) = \\sqrt{|x|}$} and $f_2(x) = \\exp(x)$ for $x\\in\\mathbb{R}$ are not Lipschitz continuous, because their output can change arbitrarily fast as $x$ approaches $0$ and $+\\infty$ respectively. \nOn the other hand, $g_1(x) = \\tanh(x)$ and $g_2(x) = \\alpha x$ are Lipschitz continuous, because their rate of change (derivative) is bounded.\n\nIn deep learning, we often use Lipschitz continuity as a constraint for neural networks, to control how much a network's output can change relative to its input. \nSuch Lipschitz constraints are useful in several contexts. \nFor example, Lipschitz constraints can endow models with provable robustness against adversarial pertubations \\citep{cisse2017parseval, tsuzuku2018lipschitz, anil2019sorting}, and guaranteed generalisation bounds \\citep{sokolic2017robust}.\nMoreover, the dual form of the Wasserstein distance is defined as a supremum over Lipschitz functions with a given Lipschitz constant, hence Lipschitz-constrained networks are used for estimating Wasserstein distances \\citep{peyre2019computational}. \nFurther, Lipschitz-constrained networks can stabilise training for GANs, an example being spectral normalisation \\citep{miyato2018spectral}.\nFinally, Lipschitz-constrained networks are also used to construct invertible models and normalising flows. \nFor example, Lipschitz-constrained networks can be used as a building block for invertible residual networks and hence flow-based generative models \\citep{behrmann2018invertible, chen2019residual}.\nAdditionally, Neural ODEs \\citep{chen2018neural, grathwohl2018ffjord} are typically defined using vector fields parameterized via Lipschitz networks, so that the flow generated by the vector field is guaranteed to exist for all times.\n\nNonetheless, designing Lipschitz-continuous neural networks and computing (or even upper-bounding) their Lipschitz constant is a hard problem. \nPrevious work mostly focused on fully-connected and convolutional networks, not only because they are common in deep learning, but also because they are relatively simple to analyze, as compositions of linear maps and pointwise non-linearities. \nEven in this case however, exact evaluation of the Lipschitz constant\nof fully-connected and convolutional networks is NP-hard \\citep{virmaux2018lipschitz} and obtaining a tight upper bound remains a challenging task \\citep{virmaux2018lipschitz, fazlyab2019efficient, Latorre2020Lipschitz}.\n\nFully-connected and convolutional networks are not the only neural networks worthy of interest.\nRecently, \\textit{self-attention} \\citep{vaswani2017attention} has become a popular alternative to recurrent neural networks. Self-attention is a key component of the Transformer \\citep{vaswani2017attention}, that has found success as a building block in models of various data modalities, starting with natural-language processing \\citep{vaswani2017attention, devlin2018bert, brown2020language} and extending to computer vision \\mbox{\\citep{zhang2018self, ramachandran2019stand}}, audio generation \\citep{huang2018music}, and reinforcement learning \\citep{parisotto2019stabilizing}. However, so far no previous work has analysed the Lipschitz properties of self-attention, and thus it has been unclear whether self-attention is a viable option in applications that require Lipschitz constraints.\nIn this work, we address this gap in the theory of self-attention by providing a thorough analysis of its Lipschitz properties. In particular, we make the following contributions: \n\\begin{itemize}[leftmargin=*]\n    \\item We prove that the widely used \\textit{dot-product self-attention} is \\emph{not} Lipschitz, and therefore not suitable to use in applications requiring Lipschitz constraints.\n    \\item We formulate \\textit{L2 self-attention} as an alternative, and show that it \\emph{is} Lipschitz.\n    \\item We derive a theoretical upper bound on the Lipschitz constant of L2 self-attention, and provide empirical evidence of the asymptotic tightness of the bound.\n    \\item As a practical demonstration of the theory, we use this bound to formulate invertible self-attention, and explore its use in a Transformer architecture for character-level language modelling. We compare its test log-likelihood and stability to dot-product self-attention.\n\\end{itemize}\n\n\\section{Lipschitz Constant of Fully-Connected/Convolutional Layers}\nWe first define the notion of Lipschitz continuity, and proceed to define the Lipschitz constant.\n\\begin{definition}\\label{Lipschitz_definition}\nGiven two metric spaces $(\\mathcal{X}, d_{\\mathcal{X}})$ and $(\\mathcal{Y}, d_{\\mathcal{Y}})$, a function $f:\\mathcal{X} \\rightarrow \\mathcal{Y}$ is called \\textit{Lipschitz continuous} (or $K$-\\textit{Lipschitz}) if there exists a constant $K\\geq 0$ such that \n\\begin{equation}\nd_{\\mathcal{Y}}(f(\\mathbf{x}),f(\\mathbf{x'})) \\leq K d_{\\mathcal{X}}(\\mathbf{x},\\mathbf{x'}) \\spaces\\spaces \\text{for all } \\mathbf{x},\\mathbf{x'} \\in \\mathcal{X}.\n\\end{equation}\nThe smallest such $K$ is the \\textit{Lipschitz constant} of $f$, denoted $\\lip(f)$.\n\\end{definition}\nIn this paper, we focus on the common case where $\\mathcal{X} = \\mathbb{R}^n$, $\\mathcal{Y} = \\mathbb{R}^m$, and $d_{\\mathcal{X}}, d_{\\mathcal{Y}}$ are induced by a $p$-norm \\smash{$\\|\\mathbf{x}\\|_p \\coloneqq (\\sum_{i} |x_i|^p)^{{1}/{p}}$}. \nWe will primarily consider the cases $p=2$ and $p=\\infty$, where $\\|\\mathbf{x}\\|_{\\infty} \\coloneqq \\max_i |x_i|$.\nTo emphasise the dependence of the Lipschitz constant on the choice of $p$-norm, we will often denote it by $\\lip_p(f)$. \nIn this case, it follows directly from Definition \\ref{Lipschitz_definition} that the Lipschitz constant is given by\n\\begin{equation}\\label{eq:lipschitz_supremum_definition}\n    \\lip_p(f) = \\sup_{\\mathbf{x}\\neq \\mathbf{x'} \\in \\mathbb{R}^n} \\frac{\\|f(\\mathbf{x})-f(\\mathbf{x'})\\|_p}{\\|\\mathbf{x}-\\mathbf{x'}\\|_p}.\n\\end{equation}\nNext, we outline some basic results that are useful for estimating Lipschitz constants, also covered in related works \\citep{virmaux2018lipschitz, behrmann2018invertible}. \nWe describe how these results are used to provide bounds on the Lipschitz constant of fully-connected networks (\\verb!FCN!) and convolutional neural networks (\\verb!CNN!), using the fact that both are compositions of linear maps and pointwise non-linearities.\nTo begin with, the following theorem suggests a way to bound $\\lip_p(f)$ for a differentiable Lipschitz function $f$:\n\\begin{theorem}[\\citealp{federer1969geometric}] \\label{thm:jacobian} Let $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ be differentiable and Lipschitz continuous under a choice of $p$-norm $\\|\\cdot\\|_p$. \nLet $J_f(x)$ denote its total derivative (Jacobian) at $x$. Then $\\lip_p(f) = \\sup_{\\mathbf{x}\\in \\mathbb{R}^n} \\|J_f(\\mathbf{x})\\|_p$ where $\\|J_f(\\mathbf{x})\\|_p$ is the induced operator norm on $J_f(\\mathbf{x})$. \n\\end{theorem}\nHence if $f$ is a linear map represented by a matrix $W$ then\n\\begin{align*}\n    \\lip_p(f)&= \\|W\\|_p \\coloneqq \\sup_{\\|\\mathbf{x}\\|_p=1} \\|W\\mathbf{x}\\|_p \\\\\n    &=\n    \\begin{cases}\n        \\sigma_{\\max}(W), & \\text{if } p=2\\\\\n        \\max_i \\sum_j |W_{ij}|      & \\text{if } p = \\infty\n    \\end{cases}\n\\end{align*}\nwhere $\\|W\\|_p$ is the operator norm on matrices induced by the vector $p$-norm, and $\\sigma_{\\max}(W)$ is the largest singular value of $W$. \nUnder this choice of norm, many common non-linearities (including \\verb!relu!, \\verb!sigmoid!, \\verb!tanh!, \\verb!elu!) are $1$-Lipschitz. \n$\\|W\\|_2= \\sigma_{\\text{max}}(W)$ is usually estimated via \\textit{power iteration}; we provide details on how this is done in Appendix \\ref{apd:power_iteration}.\n\nSince we now know the Lipschitz constants of the components of both \\verb!FCN! and \\verb!CNN!, we can bound their Lipschitz constants by applying the following lemma:\n\\begin{lemma}[\\citealp{federer1969geometric}]\nLet $g,h$ be two composable Lipschitz functions. Then $g \\circ h$ is also Lipschitz with $\\lip(g \\circ h) \\leq \\lip(g) \\lip(h)$.\n\\end{lemma}\n\\begin{corollary} \\label{cor:lip_conv}\nFor a fully-connected network (\\verb!FCN!) or a convolutional neural network (\\verb!CNN!) $f=W_K \\circ \\rho_{K-1} \\circ W_{K-1} \\circ \\ldots \\circ \\rho_1 \\circ W_1$, we have $\\lip_p(f) \\leq \\prod_k \\|W_k\\|_p$ under a choice of $p$-norm with $1$-Lipschitz non-linearities $\\rho_k$.\n\\end{corollary}\nThe above bound is not necessarily tight; there are various works that compute tighter bounds for \\verb!FCN! and \\verb!CNN! \\citep[e.g.][]{virmaux2018lipschitz, fazlyab2019efficient, Latorre2020Lipschitz}.\n\n\\section{Lipschitz Constant of Self-Attention}\n\n\\subsection{Dot-product self-attention is \\emph{not} Lipschitz}\n\nMoving on, we investigate whether self-attention is Lipschitz. We first consider the widely used \\textit{(scaled) dot-product multihead self-attention}  as formulated by \\citet{vaswani2017attention}.\nLet $\\mathbf{x}_1, \\ldots, \\mathbf{x}_N$ be a sequence of $N$ elements, where $\\mathbf{x}_i \\in \\mathbb{R}^D$ for $i=1,\\ldots,N$.\nWe represent this sequence as a matrix $X$:\n\\begin{equation}\n    X = \\begin{bmatrix}\n    \\text{---} & \\mathbf{x}_1^\\top & \\text{---}\\\\\n    & \\vdots & \\\\\n    \\text{---} & \\mathbf{x}_N^\\top & \\text{---} \\\\\n    \\end{bmatrix}\\in \\mathbb{R}^{N \\times D},\n\\end{equation}\n\nDot-product multihead self-attention (\\verb!DP-MHA!) is a map from $\\mathbb{R}^{N \\times D}$ to $\\mathbb{R}^{N \\times D}$ consisting of $H$ `heads', where $H$ is chosen to divide $D$. Each head is a map from $\\mathbb{R}^{N \\times D}$ to $\\mathbb{R}^{N \\times D/H}$ defined by\n\\begin{align*} \\label{eq:dot_product_self_attention_definition}\n    \\mathit{DP}(X) & \\coloneqq \\softmax{\\frac{X W^Q (X W^K)^\\top}{\\sqrt{D/H}}} X W^V \\\\\n    &= P X W^V, \n\\end{align*}\nwhere $W^Q, W^K, W^V \\in \\mathbb{R}^{D \\times D/H}$ are learnable parameters specific to each head, and $P \\in \\mathbb{R}^{N \\times N}$ is the output of the softmax (we suppress the dependence of $P$ on $X$ to reduce clutter below). The input to the softmax is an $N\\times N$ matrix of pairwise dot products (hence \\textit{dot-product} self-attention), and the softmax is applied to each row of this matrix. Finally, the outputs of all heads are concatenated into an $N\\times D$ matrix and are right multiplied by $W^O\\in \\mathbb{R}^{D \\times D}$, thus \\verb!DP-MHA! is defined by\n\\begin{equation}\n    \\mathit{MHA}_{DP}(X) \\coloneqq \\left[\\mathit{DP}^1(X), \\ldots, \\mathit{DP}^H(X)\\right] W^O.\n\\end{equation}\nIn what follows, we will prove that $\\mathit{MHA}$ as defined above is  \\emph{not} Lipschitz, assuming that the $\\mathit{MHA}$ map is non-trivial, i.e.~$W^Q, W^K, W^V, W^O \\neq 0$. It is sufficient to show that a single head $\\mathit{DP}$ is not Lipschitz, since $\\mathit{MHA}$ is a linear combination of the outputs of each head. Also note that $P$ is a stochastic matrix, i.e.~its entries are non-negative and its rows sum to $1$.\nSince the rows of $X$ are the $\\mathbf{x}_i$'s, a linear transformation of each $\\mathbf{x}_i$ by some matrix $A$ is equivalent to right multiplication of $X$ by $A^\\top$. \nSo right multiplication of $X$ by $W^V$ is a linear map and thus Lipschitz.\nTherefore, we are interested in the mapping $f(X) = PX$; this is \\emph{not} a linear mapping because $P$ itself is a non-linear function of $X$. \nIn fact, we show that $f$ \nis \\emph{not} Lipschitz, thus proving the first main result of the paper:\n\\begin{theorem} \\label{thm:dp_not_lipschitz}\n\\verb!DP-MHA! is not Lipschitz for any vector $p$-norm $\\|\\cdot\\|_p$ with $p \\in [1, \\infty]$.\n\\end{theorem}\n\\textit{Summary of Proof}. We use Theorem \\ref{thm:jacobian}, noting that if the supremum of the norm of the Jacobian is infinite, then the mapping is not Lipschitz.\nIn particular, we show that when $\\mathbf{x}_i=\\mathbf{0}$ for some $i$, some elements of the Jacobian of $f$ grow proportionally to the sample variance of $\\mathbf{x}_{\\neq i}$, which is unbounded.\n\\begin{proof}\nWe show the proof for the case $D=H=1$ (i.e.~$X \\in \\mathbb{R}^{N \\times 1}$, a column vector, and $x_i \\in \\mathbb{R}$) for readability. See Appendix \\ref{apd:general_d} for the general case, which follows the same logic. \n\nThe mapping $f$ can be written as\n\\begin{align*}\nf(X) = PX = & \\softmax{a X X^\\top} X = \\begin{bmatrix}\n    f_1(X) \\\\\n    \\vdots \\\\\n    f_N(X)\n\\end{bmatrix} \\in \\mathbb{R}^{N \\times 1}, \\\\\n\\text{where} \\quad & f_i(X) = \\sum_{j=1}^N P_{ij}x_j \\in \\mathbb{R}\n\\end{align*}\nand $a = W^K W^Q \\in \\mathbb{R}$ (we assume $a \\neq 0$ such that self-attention is non-trivial).\nHence $f$ can be interpreted as a map of each $x_i$ to a point in the convex hull of ${x_1,...,x_N}$.\nSince $f$ is a map from $\\mathbb{R}^{N \\times 1}$ to $\\mathbb{R}^{N \\times 1}$, its Jacobian is\n\\begin{equation}\n    J_f = \\begin{bmatrix}\n    J_{11} & \\dots & J_{1N} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    J_{N1} & \\dots & J_{NN} \\\\\n    \\end{bmatrix}\\in \\mathbb{R}^{N \\times N},\n\\end{equation}\nwhere $\\smash{J_{ij} = \\frac{\\partial f_i(X)}{\\partial x_j} \\in \\mathbb{R}}$. \nBy taking partial derivatives we can show that\n\\begin{equation*}\n    J_{ij} = a X^\\top P^{(i)} \\left[E_{ji}X + \\delta_{ij}X \\right] + P_{ij}I\n\\end{equation*}\nwhere \n\\begin{itemize}\n    \\item $E_{ij} \\in \\mathbb{R}^{N \\times N}$ is a binary matrix with zeros everywhere except the $(i,j)$th entry\n    \\item $\\delta_{ij} \\in \\{0,1\\}$ is the Kronecker delta\n    \\item $P^{(i)} \\coloneqq \\diag(P_{i:}) - P_{i:}^\\top P_{i:} \\in \\mathbb{R}^{N \\times N}$.\n\\end{itemize}\nSee Appendix \\ref{apd:identities} for useful identities in deriving the above Jacobian.\n\nSo for $i=j$:\n\\begin{align}\nJ_{ii} =\n a X^\\top P^{(i)} e_{ii} X + a X^\\top P^{(i)} X + P_{ii} \\label{eq:jac_dot}\n\\end{align}\n\nLet us investigate the scalar $X^\\top P^{(i)}X$. We observe that it is in fact a variance of a discrete distribution. Specifically:\n\\begin{equation} \\label{eq:cov}\n    X^\\top P^{(i)}X  = \\textstyle\\sum_k P_{ik} x_k^2 - \\left(\\textstyle\\sum_k P_{ik}  x_k\\right)^2 = \\mathrm{Var}(\\mathbb{X}),\n\\end{equation}\nwhere $\\mathbb{X}$ is a discrete distribution with support at the inputs $\\{x_1,\\ldots,x_N \\}$ and probability mass function given by their softmax probabilities $\\mathbb{P}(\\mathbb{X}=x_j)=P_{ij}$. \nA consequence of this interpretation is that $P^{(i)}$ is \\textit{positive semi-definite} (PSD) since $X^\\top P^{(i)} X = \\mathrm{Var}(\\mathbb{X}) \\geq 0$, with equality if and only if the $x_j$ are all equal.\n\nWe use this observation to show that $J_{ii}$ is unbounded, and so $\\|J_f\\|_p$ is unbounded, hence \\verb!DP-MHA! is \\emph{not} Lipschitz.\nConsider the case $x_i=0$. Then \n\\begin{equation*}\n    P_{i:}^\\top = \\softmax{XAx_i} = \\frac{1}{N} \\mathds{1},\n\\end{equation*}\ni.e.\\ we have uniform attention regardless of $x_{ \\neq i}$. \nThe first term of $J_{ii}$ in Equation \\eqref{eq:jac_dot} disappears since $e_{ii} X = [0, \\ldots, x_i, \\ldots, 0] = \\mathbf{0}$, and the last term becomes $\\frac{1}{N} I$. Now consider the second term $a X^\\top P^{(i)}X = a \\mathrm{Var}(\\mathbb{X}_l)$. Note $\\mathbb{X}$ is uniformly distributed, since $\\mathbb{P}(\\mathbb{X}=x_j)=P_{ij}= 1/N$. Hence the second term is equal to $a$ times the sample variance of ${x_1,\\ldots,x_N}$, which can be arbitrarily large. Hence $J_{ii}$ can become arbitrarily large, so the full Jacobian $J_f$ is unbounded.\n\\end{proof}\n\\textit{High-level intuition for proof.}\nAt $x_i=0$, $f_i(X) = \\frac{1}{N} \\sum_{k} x_k$, the mean of the inputs. \nThe rate of change of $f_i$ is governed by how fast the softmax saturates when $x_i$ is perturbed, which is determined by how spread out the $x_{\\neq i}$ are. \nThe more spread out they are (the higher the sample variance), the greater the rate of saturation of the softmax, and the faster the rate of change of $f_i$.\nSince the sample variance of $x_{\\neq i}$ can be arbitrarily large, the rate of change of $f_i$ can also be arbitrarily large, i.e.~the entries of the Jacobian (and hence its $p$-norm) can become arbitrarily large. In Appendix \\ref{apd:bias}, we show that adding bias terms to $\\mathbf{x}_i^\\top W^Q$ and $\\mathbf{x}_j^\\top W^K$ does \\emph{not} resolve the issue.\n\nThe implications of this result are the following.\n\\begin{inparaenum}[(1)]\n\\item There can be undesirable behaviour (e.g.~training instabilities) for the Transformer when some inputs are close to zero and others have large magnitude.\n\\item Dot-product self-attention (and hence the standard Transformer) is not a suitable choice when we require a Lipschitz neural network, such as for formulating invertible residual networks \\citep{behrmann2018invertible}.\n\\end{inparaenum}\nTherefore, to use self-attention and Transformers in such applications, a Lipschitz formulation of self-attention is required, together with an explicit (ideally tight) upper bound to its Lipschitz constant, to quantify how much the output can change with respect to changes in the input.\n\nOne method to make dot-product self-attention Lipschitz is by ensuring its inputs are bounded. Indeed, if the input space is compact, e.g.\\ $[0,1]^{N \\times D}$, any continuously differentiable function is Lipschitz, including dot-product self-attention.\nHowever, as we further discuss in Section \\ref{sec:conclusion}, such an approach has its own challenges, since it makes the Lipschitz constant depend on the input range. Instead, in the next section we formulate a version of self-attention that is provably Lipschitz on all of $\\mathbb{R}^{N\\times D}$, allowing us to derive an upper bound that holds for any subset of $\\mathbb{R}^{N\\times D}$.\n\n\\subsection{L2 self-attention: a Lipschitz formulation of self-attention}\nThe pathology in dot-product self-attention arises because the softmax probabilities $P_{i:}$ are constant with respect to $\\mathbf{x}_{\\neq i}$ when $\\mathbf{x}_i=0$. \nThis behaviour can be undesirable as we want $P_{ij}$ to vary according to $\\mathbf{x}_j$, regardless of whether $\\mathbf{x}_i$ is zero or not.\nHence we propose an alternative form of self-attention based on L2 distance:\n\\begin{align}\\label{eq:L2_self_attention_definition}\nP_{ij} \\propto \\exp(L_{ij}) \\coloneqq \\exp\\left(-\\frac{\\norm{ \\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K}_2^2}{\\sqrt{D/H}}\\right),\n\\end{align}\nwith the normalisation constant ensuring that $\\sum_j P_{ij} = 1$. \nWe will refer to it as \\textit{L2 self-attention}. \nIt is reminiscent of the standard squared-exponential kernel, but with softmax normalisation that ensures that each row of the kernel matrix sums to $1$. \nNormalisation is usually necessary to deal with inputs of varying length $N$ \\citep{wang2018non}, hence we keep the softmax for L2 self-attention. Similarly to dot-product self-attention, L2 self-attention can be computed efficiently with matrix operations; see Appendix \\ref{apd:l2_att_computation} for details, with a comparison of wall-clock runtimes between different choices of attention. \n\nWe first state the mathematical formulation of L2 multihead self-attention (\\verb!L2-MHA!) before proving the main result --- the upper bound of its Lipschitz constant with respect to $\\|\\cdot\\|_p$ for $p=2, \\infty$. The full \\verb!L2-MHA! map $F: \\mathbb{R}^{N \\times D} \\rightarrow \\mathbb{R}^{N \\times D}$ is defined as\n\\begin{align*}\n    F(X) &\\coloneqq \\left[f^1(X)W^{V,1}, \\ldots, f^H(X)W^{V,H}\\right] W^O \\\\\n    & \\quad\\text{where}\\quad\n    f^h(X) \\coloneqq P^h X A_h.\n\\end{align*}\nIn the above, $W^{V,h} \\in \\mathbb{R}^{D \\times D/H}$, $W^O \\in \\mathbb{R}^{D \\times D}$, $P^h$ is defined as in Equation \\eqref{eq:L2_self_attention_definition} with $W^{Q,h}=W^{K,h} \\in \\mathbb{R}^{D \\times D/H}$, and $A_h \\coloneqq W^{Q,h} W^{{Q,h}^\\top} / \\sqrt{D/H} \\in \\mathbb{R}^{D \\times D}$. \nThere are two changes from the usual form of multihead self-attention:\n\\begin{enumerate}[label=(\\arabic*), leftmargin=*]\n    \\item We require $W^{Q,h} = W^{K,h}$ for each head $f^h(X)$ to be Lipschitz. In Lemma \\ref{lemma:tie_weights} of Appendix \\ref{apd:proof} we show that \\verb!L2-MHA! is \\emph{not} Lipschitz for arbitrary $W^{Q,h}$, $W^{K,h}$, and that tying $W^{Q,h} = W^{K,h}$ is sufficient for \\verb!L2-MHA! to be Lipschitz, with intuition for why tying is sufficient.\n    \\item In each head of the self-attention $f^h(X)$, right multiplication by $A_h$ has been included for the theorem below to hold (details are in the proof). In practice, there is little harm done by this extra linear transformation, since when the heads are combined together in $F$, each $f^h(X)$ is additionally transformed by $W^{V,h}$, a free parameter.\n\\end{enumerate}\n\nThe second main result of the paper is the following:\n\\begin{theorem} \\label{thm:main}\n\\verb!L2-MHA! is Lipschitz, with the following bound on $\\lip_{\\infty}(F)$:\n\\begin{align*}\n    \\lip_{\\infty}(F)  \\leq &\\left(4 \\phi^{-1}(N-1) + \\frac{1}{\\sqrt{D/H}}\\right) \\|W^{O^\\top}\\|_{\\infty} \\\\\n    &\\max_h \\|W^{Q,h}\\|_{\\infty} \\|W^{{Q,h}^\\top}\\|_{\\infty} \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty} \n\\end{align*}\nand the following bound on $\\lip_{2}(F)$:\n\\begin{align*}\n    \\lip_2(F) \\leq & \\frac{\\sqrt{N}}{\\sqrt{D/H}}\n    \\left(4 \\phi^{-1}(N-1) + 1 \\right) \\\\ \n    & \\left(\\sqrt{\\textstyle\\sum_h \\|W^{Q,h}\\|_2^2\\, \\|W^{V,h}\\|_2^2}\\right) \\|W^O\\|_2 \n\\end{align*}\nwhere $\\phi(x) \\coloneqq x\\exp(x+1)$ is an invertible univariate function on $x > 0$, and $N$ is the input sequence length.\n\nSpecifically, $\\phi^{-1}(N-1) = W_0(\\frac{N}{e})$ where $W_0$ is the Lambert $W$-function, which grows sub-logarithmically as $O(\\log N - \\log \\log N)$ \\citep{corless1996lambertw}. Hence the above bounds can be simplified to $O(\\log N)$ for $p=\\infty$ and $O(\\sqrt{N} \\log N)$ for $p=2$.\n\\end{theorem}\n\\begin{proof}\nSee Appendix \\ref{apd:proof}, which uses the key observation that $X^\\top P^{(i)}X$ is a covariance matrix (c.f.\\ Equation \\eqref{eq:cov}) to bound $\\|J_F\\|_p$, the norm of the Jacobian of $F$. Appendix \\ref{apd:masking} shows how the argument can be modified to prove the analogous result for the case with masking in the self-attention.\n\\end{proof}\n\nThese bounds are complemented by the concurrent work of \\citet{vuckovic2020attention}, which provides a $O(\\sqrt{D\\log N})$ bound on $\\lip_1(F)$ using measure-theoretic tools.\n\\section{Application: Invertible Self-Attention}\n\\subsection{Invertible residual network} \\label{sec:invertible_resnet}\n\n\nConsider the residual function $g(x) \\coloneqq \\mathbf{x} + f(\\mathbf{x})$. \\citet{behrmann2018invertible} give the following sufficient condition for its invertibility: \nif $f$ is a \\textit{contraction} with respect to some\nmetric, i.e.~if $\\lip(f) < 1$, and the metric space on which $f$ is defined is complete,\nthen $g$ is invertible. \n(A Euclidean space with a metric induced by a $p$-norm $\\|\\cdot\\|_p$ for $p \\in [1, \\infty]$ is always complete.)\nSpecifically, the inverse $g^{-1}(\\mathbf{y})$ is the unique fixed point of the recursion $\\mathbf{x}^{i+1} \\coloneqq \\mathbf{y} - f(\\mathbf{x}^i)$, since by the definition of the inverse we have $\\mathbf{y} = g^{-1}(\\mathbf{y}) + f(g^{-1}(\\mathbf{y}))$. \nBecause $f$ is a contraction, \\textit{Banach's Fixed Point Theorem} guarantees that this fixed point exists and is unique for all $\\mathbf{y}$, and that the recursion converges for all initial values $\\mathbf{x}^0$ (often set to $\\mathbf{y}$ in practice) exponentially fast. \nHence the inverse can be computed to arbitrary accuracy (up to numerical precision in practice) by the above fixed-point iteration.\n\nNote that a composition of such invertible residual blocks is also invertible. \n\\citet{behrmann2018invertible} use this observation to design invertible ResNets: they take $f$ to be a \\verb!CNN! normalised by an upper bound on $\\lip(f)$ given by Corollary \\ref{cor:lip_conv}, making the resulting function \\textit{contractive}. \nFor the $2$-norm $\\|\\cdot\\|_2$, a hyperparameter $c < 1$ is chosen and each linear map (convolution) $W$ in the \\verb!CNN! is multiplied by $c/\\|W\\|_2$ if $c < \\|W\\|_2$ where $\\|W\\|_2$ is estimated by power iteration (c.f.\\ Appendix \\ref{apd:power_iteration}).\nThis multiplicative factor determines the scale of the Lipschitz constant of the normalised function.\n\n\n\\subsection{Invertible self-attention}\n\n\\begin{wrapfigure}{r}{0.2\\textwidth}\n    \\centering\n    \\includegraphics[width=0.2\\textwidth]{transformer_block_single.pdf}\n    \\caption{Transformer block.}\n    \\label{fig:transformer_block}\n\\end{wrapfigure}\n\nThe standard use case of self-attention is with a skip connection inside the Transformer. A Transformer block is composed of residual blocks of multihead self-attention (\\verb!MHA!) and fully-connected (\\verb!FCN!) layers (Figure \\ref{fig:transformer_block}).\nHence similarly to invertible ResNets, we can normalise \\verb!L2-MHA! by the upper bounds given in Theorem \\ref{thm:main} to obtain \\verb!Contractive-L2-MHA! $f$, with which we can obtain invertible self-attention $g(\\mathbf{x}) = \\mathbf{x} + f(\\mathbf{x})$. \nSince \\verb!Dropout! is also part of the residual branch along with \\verb!Contractive-L2-MHA!, we should check that it is also contractive.\nAt test time, \\verb!Dropout! multiplies inputs by the dropout keep probability $p < 1$, so it is a contraction with Lipschitz constant $p$ at evaluation time.\nAt training time, \\verb!Dropout! amounts to setting some inputs to zero, while keeping other inputs constant. This can be expressed as right multiplication by a diagonal binary matrix $M$, and for such matrices we can verify $\\|M\\|_p \\coloneqq \\sup_{\\|x\\|_p=1} \\|Mx\\|_p  \\leq 1$.\nNotice that \\verb!LayerNorm! is not part of the residual branch, hence its Lipschitz continuity is not relevant for invertibility; rather, we can replace it with an invertible normalisation such as \\verb!ActNorm! \\cite{kingma2018glow}. \nHowever, architectures that place \\verb!LayerNorm! inside the residual branch (termed \\verb!pre-LN! as opposed to the traditional \\verb!post-LN! in Figure \\ref{fig:transformer_block}) have become more prevalent in the literature \\cite{wang2019learning, xiong2020layer}, and in this case it makes sense to investigate its Lipschitz continuity.\nWe show that \\verb!LayerNorm! is Lipschitz in Appendix \\ref{apd:layernorm}, with a bound on its Lipschitz constant.\n\nIn the next section, we investigate the properties of invertible self-attention and how it compares with the standard dot-product self-attention; we replace \\verb!DP-MHA! in the Transformer with \\verb!Contractive-L2-MHA!, hence replacing the residual self-attention module with invertible self-attention.\nWe are not interested in the modified Transformer per se, but rather in comparing the properties of invertible self-attention to standard self-attention --- we only use the Transformer as a testbed for this purpose, since self-attention is commonly used in a Transformer.\nGiven the theoretical focus of the paper, we believe that a more challenging application of invertible self-attention, such as normalising flow-based modelling, would be more suitable as a separate paper focused on that particular application.\n\n\\section{Experimental Results}\n\\subsection{Asymptotic tightness of the upper bound on $\\boldsymbol{\\lip_{\\infty}(F)}$} \\label{sec:asymptotic}\n\n\\begin{figure}[htb!]\n    \\centering\n    \\includegraphics[width=0.5\\textwidth]{jacobian_opt.pdf}\n    \\caption{Lower and upper bound on $\\lip_{\\infty}(f)$ for L2-MHA $f$, with $H=D=1$ and varying $N$.}\n    \\label{fig:jacobian_opt}\n\\end{figure}\n\nA tight bound on the Lipschitz constant of self-attention is desirable for all listed applications in Section \\ref{sec:intro}; it leads to tighter generalisation bounds, lighter constraints for provable robustness, and better expressiveness in residual flow models.\nHence we investigate the tightness of our bound on the Lipschitz constant of \\verb!L2-MHA!. \nThe Lipschitz constant is a supremum over the space of inputs $X \\in \\mathbb{R}^{N \\times D}$ (c.f.\\ Equation \\eqref{eq:lipschitz_supremum_definition}) and approximating it requires solving an intractable \noptimisation problem. \nHence it is infeasible to estimate accurately in general, especially when $X$ is high-dimensional. \nHowever, we may compute a lower bound on the Lipschitz constant by maximising the norm of the Jacobian $\\|J_f(X)\\|$ with respect to $X$ until convergence.\nThis local optimum will form a lower bound by Theorem \\ref{thm:jacobian}, and we can expect this lower bound to be fairly tight for the low-dimensional case, provided the optimisation is thorough.\n\nWe use this observation to provide empirical evidence for the asymptotic tightness of the upper bound on $\\lip_{\\infty}(f)$ in Theorem \\ref{thm:main}. \nIn Figure \\ref{fig:jacobian_opt}, we show the upper bound as well as the lower bound on $\\lip_{\\infty}(f)$ obtained by optimising $\\|J_f(X)\\|_{\\infty}$ with respect to $X$ for \\verb!L2-MHA! $f$ with 50 different random initialisations of $X$, with $H=D=1$ and $N$ varying between $100$ and $1000$. \nSee Appendix \\ref{apd:experimental_details} for further details.\nNote that we use a log-scale for the x-axis, and recall that the upper bound is $O(\\log N - \\log \\log N)$, dominated by the $O(\\log N)$ term for large $N$.\nHence the plot for the upper bound shows a linear trend.\nWe also observe that the slope of the lower bound is very similar, providing empirical evidence that the $O(\\log N - \\log \\log N)$ upper bound is asymptotically tight.\n\nThere are at least two possible explanations for the gap between the upper and lower bounds.\n\\begin{inparaenum}[(1)]\n\\item The lower bound is only a local optimum --- the true Lipschitz constant is a global optimum across inputs, which can be difficult to attain especially for high values of $N$.\n\\item The multiplicative constant of the upper bound may be loose.\n\\end{inparaenum}\nAssuming asymptotic tightness, it remains an open question whether the multiplicative constant can be tightened.\nWe show the analogous plot for $\\lip_2(F)$ and discuss the results in Appendix \\ref{apd:jacobian_opt2}.\nAdditionally in Appendix \\ref{apd:jacobian_opt_dp}, we show that optimising $\\|J_f(X)\\|_{\\infty}$ w.r.t.~$X$ for \\verb!DP-MHA! $f$ causes the norm to diverge, providing empirical verification of Theorem \\ref{thm:dp_not_lipschitz}, that \\verb!DP-MHA! is indeed \\emph{not} Lipschitz.\n\n\\subsection{Numerical invertibility of MHA residual map}\\label{sec:numerical-invertibility}\n\n\\begin{figure}[htb!]\n    \\centering\n    \\includegraphics[width=0.5\\textwidth]{mha_invertibility_small.pdf}\n    \\caption{Invertibility of $g(\\mathbf{x})= \\mathbf{x} + c f(\\mathbf{x})$ where $f$ is L2-MHA (left) and DP-MHA (right).}\n    \\label{fig:mha_invertibility_small}\n\\end{figure}\n\n\\begin{figure*}[t!] \n  \\includegraphics[width=\\textwidth]{ptb_test.pdf}\n  \\caption{Test NLL curves during training for various LSTM/Transformer models on PTB character level language modelling.} \\label{fig:ptb}\n\\end{figure*}\n\nRecall from Section \\ref{sec:invertible_resnet} that $g(\\mathbf{x}) = \\mathbf{x} + f(\\mathbf{x})$ is invertible if $f$ is contractive. \nHence if $f$ is \\verb!Contractive-L2-MHA!, $g$ is necessarily invertible. \nHowever, technically we do not disprove the invertibility of \\verb!DP-MHA!, since the converse does not hold in general i.e.~if $f$ is \\verb!DP-MHA!, which we have shown is \\emph{not} Lipschitz hence \\emph{not} contractive, it may still be the case that $g$ \\emph{is} invertible.\nTo verify that \\verb!DP-MHA! (with the skip connection) is \\emph{not} invertible in practice, we compare the numerical invertibility of the residual map  $g(\\mathbf{x})= \\mathbf{x} + c f(\\mathbf{x})$ between the cases where $f$ is \\verb!L2-MHA! and \\verb!DP-MHA! in Figure \\ref{fig:mha_invertibility_small}.\nFor each, we take \\verb!MHA! with $8$ heads and randomly initialised weights, and quantify the maximum reconstruction error across a batch of $128$ inputs whose outputs are inverted via the fixed-point iteration described in Section \\ref{sec:invertible_resnet}. We use $N=64$, \n$D=64$,\nand $c \\in \\{0.5,0.7,0.9\\}$\n(see Appendix \\ref{apd:numerical_invertibility} for analogous results for a wider range of $N$ and $D$ and for \\verb!DP-MHA! with trained weights).\nTo highlight the difference between the two types\nof self-attention, recall in the proof of Theorem \\ref{thm:dp_not_lipschitz} (showing that \\verb!DP-MHA! is not Lipschitz) that when one of the inputs $\\mathbf{x}_i$ is $0$, some terms of the Jacobian grow with the sample variance of $\\mathbf{x}_{\\neq i}$. \nHence we check numerical invertibility at a set of $N$ inputs where $\\mathbf{x}_i=0$ and $\\mathbf{x}_{\\neq i}$ are chosen uniformly at random. \n\nIn Figure \\ref{fig:mha_invertibility_small}, we see that \\verb!DP-MHA! is \\emph{not} invertible whereas \\verb!L2-MHA! \\emph{is} invertible for sufficiently small $c$.\nThis shows how not having the theoretical guarantee of $f$ being contractive can cost us invertibility in practice.\nWe note that the figure shows local invertibility at the sampled inputs, as opposed to global invertibility across the whole input space, yet this clearly highlights the difference between the two choices of self-attention.\nExperiments with the globally invertible self-attention obtained by normalising with the Lipschitz upper bound are provided in the next section.\n\n\n\\subsection{Expressiveness of L2-MHA and invertible self-attention}\n\\label{sec:invertible_self-attention}\n\n\nA natural question to ask is: how does the expressiveness of \\verb!L2-MHA! and \\verb!Contractive-L2-MHA! (that leads to invertible self-attention with the skip connection) compare with the original \\verb!DP-MHA!?\nWe expect that the Lipschitz constraint will limit the expressiveness of the Transformer, and would like to find out by how much.\nWe investigate this by comparing the performance of the original Transformer and the Transformer with invertible self-attention (c.f.\\ Figure \\ref{fig:transformer_block}) at character-level language modelling on the Penn Treebank dataset \\citep{marcus1993building}. \nWe compare the test negative log-likelihood (NLL) of a baseline LSTM, the original Transformer (\\verb!DP-MHA!), and a series of models between the original Transformer and the Transformer with invertible self-attention (\\verb!Contractive-L2-MHA!),\nmaking one change at a time and tuning the hyperparameters on a validation set.\nFor \\verb!Contractive-L2-MHA!, we normalise $F=$\\verb!L2-MHA! by the bound on $\\lip_{\\infty}(F)$ as it is tighter than the bound on $\\lip_{2}(F)$. \nDuring training we backpropagate through these contractive blocks $F/\\lip_{\\infty}(F)$ (including the denominator) to update the model parameters.\nWe found that only backpropagating through the numerator (i.e. applying \\verb!stop-gradient! to denominator) gave slightly worse performance.\nSee Appendix \\ref{apd:experimental_details} for experimental details.\n\nThe results are shown in Figure \\ref{fig:ptb}. \nThe first plot shows the best performing LSTM reaching a test NLL of around $1.0$, and the second plot shows the best performing Transformer reaching a slightly improved performance for $3$--$5$ layers of Transformer blocks. \nWe observe instabilities in training for a higher number of layers, requiring careful tuning of the learning rate schedule for stability at the cost of performance, a commonly observed phenomenon in the literature of deep Transformer architectures \\mbox{\\citep{bapna2018training, parisotto2019stabilizing}}.\nThe third plot shows results for the Transformer with \\verb!DP-MHA! replaced with \\verb!L2-MHA! but without tying $W^Q$ and $W^K$, and we observe a very similar test performance. \nThe fourth plot shows the change when we further tie the query and key weights (making $W^Q=W^K$); we see that there is a small degradation in performance.\nHere the number of trainable parameters has been reduced, but in Appendix \\ref{apd:wq_wk_experiment} we show that matching parameter count does not help performance, suggesting that the reduction in performance when tying queries and keys is not solely due to having fewer parameters.\nWe note that performance saturates at around $5$ layers for each Transformer model so far.\nOn the rightmost plot we show results when further dividing self-attention in each block by the upper bound on $\\lip_{\\infty}(F)$, to obtain invertible self-attention. \nThis does give reduced performance for the same number of layers, but we can attain similar performance with more layers, no longer saturating at $5$ layers.\n\n\\begin{table*}[!htb]\n\\centering\n \\begin{tabular}{|c||c|c|c|c|c|c|c|c|c|} \n \\hline\n  Number of Layers & 2 & 4 & 6 & 8 & 10 & 12 & 14 & 16 & 18 \\\\\n \\hline \\hline\n Transformer (\\textbf{DP}) & 1.061 & 1.032 & 1.021 & \\textbf{1.017}  & 1.025 & - & - & - & - \\\\ \n \\hline\nTransformer (\\textbf{L2}), $W^Q=W^K$  & 1.168 & 1.040 & 1.023 & 1.024 & 1.019 & \\textbf{1.008}  & 1.018 & 1.027 & 1.034 \\\\\n \\hline\nTransformer (\\textbf{Contractive-L2}) & 1.246 & 1.135 & 1.103 & 1.079 & 1.072 & 1.060 & 1.039 & \\textbf{1.029} & 1.031 \\\\\n \\hline\n\\end{tabular}\n\\caption{Test NLL for Transformer models trained with \\textbf{fixed learning rate} on PTB character level language modelling.} \\label{tab:ptb_fixed}\n\\end{table*}\n\nThus we conclude the following. \\begin{inparaenum}[(1)]\n\\item Replacing the dot-product with the L2 distance incurs hardly any loss in expressiveness.\n\\item Tying the query and key weights to obtain Lipschitz self-attention incurs a small loss in expressiveness.\n\\item Dividing by the upper bound on $\\lip_{\\infty}(F)$ to obtain invertible self-attention incurs a noticeable loss in expressiveness, but also has a stabilising effect on the optimisation of the Transformer, thus allowing one to compensate for the apparent loss in expressiveness by increasing the number of layers. \n\\end{inparaenum}\n\n\\subsection{Training Stability of DP-MHA vs L2-MHA}\n\\label{sec:stability}\n\nIn Figure \\ref{fig:mha_output_variance}, we compare the output variance of trained \\verb!L2-MHA! against trained \\verb!DP-MHA!, with weights from the one-layer Transformer (L2), $W^Q=W^K$ model and (DP) model used for Figure \\ref{fig:ptb} respectively. We take the same distribution of inputs as used for the numerical invertibility experiment in Section \\ref{sec:numerical-invertibility}, and show the histogram of inputs and outputs after flattening the input/output tensors. We see that the range of outputs remains similar to the range of inputs for Lipschitz \\verb!L2-MHA!, whereas for \\verb!DP-MHA! the outputs have a much wider range, because the Jacobian norm is large for \\verb!DP-MHA! at these inputs. \n\n\\begin{figure}[!htb]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{mha_output_variance.pdf}\n    \\caption{Histogram showing distribution of inputs/outputs of trained L2-MHA and DP-MHA}\n    \\label{fig:mha_output_variance}\n\\end{figure}\n\nIn practice, this leads to instabilities in training for \\verb!DP-MHA!, hence requiring careful tuning of the learning rate schedule for training deeper Transformer models: linear warmup and square root decay, as detailed in Appendix \\ref{apd:experimental_details}. \nWe investigate the behaviour of the different Transformer models on the above PTB task when using a \\textbf{fixed learning rate}.\nWe observe that \\verb!DP-MHA! fails to train at all beyond 10 layers, whereas both \\verb!L2-MHA! ($W^Q=W^K$) (i.e. Lipschitz L2-MHA but not contractive) and \\verb!Contractive-L2-MHA! shows stable training for up to 18 layers (see Appendix \\ref{apd:stability} for the training curves). \nThis was the deepest model we could fit on a single GPU, and we expect to be able to train even deeper models with these two. \nIn Table \\ref{tab:ptb_fixed} we show the best Test NLL across training for each of the Transformer models. Note that for \\verb!DP-MHA! training becomes unstable beyond 10 layers, so we are only able to provide results up to 10 layers. The generalisation performance of the best model for each setting of self-attention is similar.\n\n\\section{Conclusion and Discussion}\n\\label{sec:conclusion}\nWe have shown that the widely used dot-product self-attention is \\emph{not} Lipschitz, and that the proposed L2 self-attention \\emph{is} Lipschitz, by deriving an $O(\\log N - \\log \\log N)$ Lipschitz bound for $p=\\infty$ and an $O(\\sqrt{N} (\\log N - \\log \\log N))$ bound for $p=2$, where $N$ is the input sequence length. \nWe also provided empirical evidence of the asymptotic tightness of the bound for $p=\\infty$.\nWe demonstrated that Lipschitz-constrained self-attention can be used to formulate invertible self-attention, which we experimentally evaluated on a character-level language modelling task.\nAnd finally, we also showed that L2-MHA is more stable during training, allowing the use of fixed learning rate for stable training of deep architectures.\n\nOur approach to Lipschitz self-attention has been to replace the dot-product kernel with an L2 kernel. An alternative would be to constrain the inputs of self-attention to be bounded; if the input space is compact, e.g.\\ $[0,1]^{N \\times D}$, \\emph{any} continuously differentiable function is Lipschitz, including dot-product self-attention. However, while being simple to implement, this solution has its own difficulties.\nFirst, it makes the Lipschitz constant depend on the range of the input, and thus obtaining a tight bound would require non-trivial mathematical work.\nWe stress that a guarantee that the function is Lipschitz does not tell us anything about its Lipschitz constant; without a tight Lipschitz bound, the true Lipschitz constant can be very large, at which point it is unhelpful that the function is Lipschitz.\nSecond, since self-attention is typically applied at multiple layers within a model (e.g.\\ Transformer), the input to each self-attention will live in a different compact set that depends on the parameters of the previous layers, complicating the analysis for subsequent layers. \nA solution is to constrain the inputs of each layer to be in the same compact set, e.g.\\ by passing them through a sigmoid non-linearity. \nThis however can have undesirable side effects such as vanishing gradients when the sigmoids are saturated.\nDespite these difficulties, this could be a worthwhile alternative route for obtaining Lipschitz self-attention to explore in the future.\n\nHaving a provably Lipschitz self-attention module at our disposal makes it possible to use Transformer-based architectures in applications requiring Lipschitz constraints, while enjoying theoretical guarantees.\nA natural application of Lipschitz self-attention is for residual flows \\mbox{\\citep{behrmann2018invertible}}, and for parameterising Neural ODEs \\citep{chen2018neural} where a Lipschitz vector field guarantees the existence of a unique solution to the ODE for all times. \nThese models can be used for density estimation and generative modelling of sets.\nAnother interesting direction for future work would be to analyse different variants of self-attention based on kernels other than dot-product and L2, as  \\cite{tsai2019transformer} do from an experimental perspective, for which we believe the mathematical tools developed in this paper may aid the analysis.\n\n\n\\section*{Acknowledgements}\nWe would like to thank Adam Kosiorek, Arnaud Doucet, Yee Whye Teh, Michalis Titsias, Emilien Dupont and Theophane Weber for helpful discussion and feedback.\n\n\\bibliographystyle{icml2021}\n\\bibliography{refs}\n\n\n\\newpage\n{\n\\appendix\n\\onecolumn\n{\\Large \\bf Appendix}\n\\section{Useful Identities for deriving Jacobian expressions}\n\\label{apd:identities}\nIn this section, we list some useful identities for deriving the Jacobians of the expressions in the paper.\n\nSuppose $\\lambda$ is a scalar, $\\mathbf{u},\\mathbf{v},\\mathbf{x} \\in \\mathbb{R}^{n \\times 1}$ are column vectors, and $f(\\mathbf{u})$ is a vector valued function. We use the standard convention that for $\\mathbf{a} \\in \\mathbb{R}^m$, $\\mathbf{b} \\in \\mathbb{R}^n$, we have $\\frac{\\partial{\\mathbf{a}}}{\\partial{\\mathbf{b}}} \\in \\mathbb{R}^{m \\times n}$. Then we have the following chain rule identities:\n\\begin{itemize}\n    \\item $\\frac{\\partial}{\\partial \\mathbf{x}}[\\lambda \\mathbf{u}] = \\lambda \\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}} + \\mathbf{x} \\frac{\\partial \\lambda}{\\partial \\mathbf{x}}$\n    \\item $\\frac{\\partial f(\\mathbf{u})}{\\partial \\mathbf{x}} = \\frac{\\partial f(\\mathbf{u})}{\\partial \\mathbf{u}} \\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}}$\n    \\item $\\frac{\\partial}{\\partial \\mathbf{x}}[\\mathbf{u}^\\top \\mathbf{v}] = \\mathbf{u}^\\top \\frac{\\partial \\mathbf{v}}{\\partial \\mathbf{x}} + \\mathbf{v}^\\top \\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}}$\n\\end{itemize}\nNote $\\frac{\\partial \\lambda}{\\partial \\mathbf{x}}$ is a row vector, so $\\mathbf{u}\\frac{\\partial \\lambda}{\\partial \\mathbf{x}}$ is a matrix.\n\nThe Jacobian of the softmax is also well-known. Suppose $\\mathbf{v} = \\softmax{\\mathbf{u}} \\in \\mathbb{R}^{n \\times 1}$. Then\n\\begin{equation*}\n    \\frac{\\partial \\mathbf{v}}{\\partial \\mathbf{u}} = \\diag(\\mathbf{v}) - \\mathbf{v} \\mathbf{v}^\\top \n    = \\begin{bmatrix}\n    v_1 (1-v_1) & - v_1 v_2 & \\ldots & - v_1 v_n \\\\\n    - v_2 v_1  & v_2 (1-v_2) & \\ldots & -v_2 v_n \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    -v_n v_1 & -v_n v_2 & \\ldots & v_n(1-v_n)\n    \\end{bmatrix}.\n\\end{equation*}\n\n\\section{Power Iteration} \\label{apd:power_iteration}\nAlthough $\\|W\\|_{\\infty}$ can be computed efficiently in $O(nm)$ time for $W \\in \\mathbb{R}^{m \\times n}$, na{\\\"i}vely computing $\\|W\\|_2= \\sigma_{\\text{max}}(W) \\coloneqq \\sqrt{\\lambda_{\\text{max}}(W^\\top W)}$ requires $O(n^3)$ operations. (By $\\lambda_{\\text{max}}(A)$ we denote the greatest eigenvalue of a symmetric matrix $A$.) We can however obtain an underestimate $\\tilde{\\sigma}(W)$ via \\textit{power iteration}:\n\\begin{equation} \\label{eq:sigma_tilde}\n    b_{k+1} = \\frac{W^\\top W b_k}{\\|W^\\top W b_k\\|_2},\n    \\quad \\tilde{\\sigma}_k(W) = \\sqrt{\\frac{b_k^\\top W^\\top W  b_k}{b_k^\\top b_k}}, \n\\end{equation}\nwith each iteration taking $O(n^2)$ time. Then using $K\\ll n$ iterations gives us an underestimate $\\tilde{\\sigma}_K$ in $O(Kn^2)$ time.\nSince this is an underestimate, the resulting approximation to the Lipschitz constant of the linear map will not be an upper bound. \nHowever the number of power iterations is usually chosen so that $\\tilde{\\sigma}$ is accurate enough --- $K=5$ is shown to be sufficient in the context of fully connected networks or convolutions considered by \\citet{behrmann2018invertible}.\n\nThe iteration will converge if $W^\\top W$ has an eigenvalue that is strictly greater in magnitude than its other eigenvalues, and the starting vector $b_0$ has a nonzero component in the direction of an eigenvector associated with the dominant eigenvalue. \nThis happens with probability $1$ if $b_0$ is chosen at random, and the convergence is geometric with ratio $|\\lambda_2/\\lambda_{\\max}|$ where $\\lambda_2$ is the eigenvalue with second largest magnitude \\citep{mises1929praktische}.\n\n\\newtheorem{innercustomthm}{Theorem}\n\\newenvironment{customthm}[1]\n  {\\renewcommand\\theinnercustomthm{#1}\\innercustomthm}\n  {\\endinnercustomthm}\n\n\\section{Proof of Theorem 3.1 for General $D$} \\label{apd:general_d}\n\\begin{customthm}{3.1}\n\\verb!DP-MHA! is not Lipschitz for any vector $p$-norm $\\|\\cdot\\|_p$ with $p \\in [1, \\infty]$.\n\\end{customthm}\n\\vspace{-4mm}\n\\begin{proof}\nThe mapping $f$ can be written as\n\\vspace{-5mm}\n\\begin{equation}\nf(X) = PX = \\softmax{X A^\\top X^\\top} X = \\begin{bmatrix}\n    f_1(X)^\\top \\\\\n    \\vdots \\\\\n    f_N(X)^\\top\n\\end{bmatrix} \\in \\mathbb{R}^{N \\times D},\n\\end{equation}\nwhere $A = W^K W^{Q^\\top} / \\sqrt{D/H} \\in \\mathbb{R}^{D \\times D}$ and\n$f_i(X) = \\sum_{j=1}^N P_{ij}\\mathbf{x}_j$ with $P_{i:}^\\top = \\softmax{XA\\mathbf{x}_i}$.\nHence $f$ can be interpreted as a map of each $\\mathbf{x}_i$ to a point in the convex hull of ${\\mathbf{x}_1,...,\\mathbf{x}_N}$.\nSince $f$ is a map from $\\mathbb{R}^{N \\times D}$ to $\\mathbb{R}^{N \\times D}$, its Jacobian is\n\\begin{equation}\n    J_f = \\begin{bmatrix}\n    J_{11} & \\dots & J_{1N} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    J_{N1} & \\dots & J_{NN} \\\\\n    \\end{bmatrix}\\in \\mathbb{R}^{ND \\times ND},\n\\end{equation}\nwhere $J_{ij} = \\frac{\\partial f_i(X)}{\\partial \\mathbf{x}_j} \\in \\mathbb{R}^{D \\times D}$. \nBy taking partial derivatives we can show that $J_{ij} = X^\\top P^{(i)} \\left[E_{ji}XA^\\top + XA\\delta_{ij}\\right] + P_{ij}I$\nwhere $E_{ij} \\in \\mathbb{R}^{N \\times N}$ is a binary matrix with zeros everywhere except the $(i,j)$th entry, $\\delta_{ij}$ is the Kronecker delta, and $P^{(i)} \\coloneqq \\diag(P_{i:}) - P_{i:}^\\top P_{i:}$.\nSo for $i=j$:\n\\begin{align}\nJ_{ii} &=X^\\top P^{(i)}E_{ii}XA^\\top + X^\\top P^{(i)}XA + P_{ii}I \\nonumber \\\\\n&= P_{ii}\\left(\\mathbf{x}_i - \\textstyle\\sum_k P_{ik} \\mathbf{x}_k\\right)\\mathbf{x}_i^\\top A^\\top + X^\\top P^{(i)}XA + P_{ii}I. \\label{eq:jac_dot_general}\n\\end{align}\nFor the last equality, note $E_{ii}X$ has all rows equal to zero except for the $i$th row given by $\\mathbf{x}_i^\\top$. We can then verify that $X^\\top P^{(i)}E_{ii}X$ simplifies to $P_{ii}(\\mathbf{x}_i - \\sum_k P_{ik} \\mathbf{x}_k)\\mathbf{x}_i^\\top$.\n\nFor vector $p$-norms, $\\|J_f\\|_p$ is bounded if and only if its entries are bounded, by definition of the operator norm. \nThe entries of $X^\\top P^{(i)}XA$ are bounded for arbitrary $A$ only if the entries of $X^\\top P^{(i)}X$ are bounded.\nSo let us investigate the entries of this $D\\times D$ matrix. \nWriting out each term of the matrix, we observe that it is in fact a covariance matrix of a discrete distribution. Specifically:\n\\begin{equation} \\label{eq:cov_general}\n    [X^\\top P^{(i)}X]_{lm}  = \\textstyle\\sum_k P_{ik} x_{kl} x_{km} - \\left(\\textstyle\\sum_k P_{ik}  x_{kl}\\right)\\left(\\textstyle\\sum_k P_{ik} x_{km}\\right) = \\mathrm{Cov}(\\mathbb{X}_l,\\mathbb{X}_m),\n\\end{equation}\nwhere $\\mathbb{X}$ is a discrete distribution with support at the inputs $\\{\\mathbf{x}_1,\\ldots,\\mathbf{x}_N \\}$ and probability mass function given by their softmax probabilities $\\mathbb{P}(\\mathbb{X}=\\mathbf{x}_j)=P_{ij}$. \nA consequence of this interpretation is that $P^{(i)}$ is \\textit{positive semi-definite} (PSD) since for $D=1$, Equation \\eqref{eq:cov_general} becomes $X^\\top P^{(i)} X = \\mathrm{Var}(\\mathbb{X}) \\geq 0$, with equality if and only if the $\\mathbf{x}_j$ are all equal.\n\nWe use this observation to show that the terms of $J_{ii}$ are unbounded, and so \\verb!DP-MHA! is \\emph{not} Lipschitz.\nConsider the case $\\mathbf{x}_i=0$. Then $P_{i:}^\\top = \\softmax{XA\\mathbf{x}_i} = \\frac{1}{N} \\mathds{1}$, i.e.\\ we have uniform attention regardless of $\\mathbf{x}_{ \\neq i}$. \nThe first term of $J_{ii}$ in Equation \\eqref{eq:jac_dot_general} disappears since $\\mathbf{x}_i=\\mathbf{0}$, and the last term becomes $\\frac{1}{N} I$. For the second term, the entries $[X^\\top P^{(i)}X]_{ll} = \\mathrm{Var}(\\mathbb{X}_l)$ are unbounded since the latter is equal to the sample variance of ${x_{1l},\\ldots,x_{Nl}}$, which can be arbitrarily large.\n\nNote that we have shown that single head dot-product self-atttention ($H=1$) is not Lipschitz, but it is clear that this implies multihead self-attention \\verb!DP-MHA! is also not Lipschitz, since the output of multihead attention is a linear combination of the outputs of each head.\n\\end{proof}\n\n\\section{Bias term in DP Self-Attention} \n\\label{apd:bias}\nA natural question to ask is whether we can add bias terms $b^Q$ to $\\mathbf{x}_i^\\top W^Q$ and $\\mathbf{b}^K$ to $\\mathbf{x}_j^\\top W^K$ to resolve the issue of attention weights $P_{i:}$ becoming uniform when $\\mathbf{x}_i=0$. \nThe answer is \\emph{no} in general. \nIt can again be shown that $J_{ii}$ is unbounded when $\\mathbf{x}_i$ is chosen such that \\smash{$\\mathbf{x}_i^\\top W^Q + \\mathbf{b}^Q=0$} (such a choice is possible assuming $W^Q$ is full rank, a dense set in \\smash{$\\mathbb{R}^{D \\times D/H}$}). Then \\smash{$P_{i:}^\\top=\\frac{1}{N}\\mathds{1}$} again, and the diagonal entries of \\smash{$X^\\top P^{(i)}X$} are unbounded.\n\n\\section{Efficient Computation of L2 Self-Attention} \\label{apd:l2_att_computation}\nDot-product self-attention only requires a few matrix multiplications to compute the logits (i.e.~the inputs to the softmax) between all pairs of inputs, without having to loop over pairs, hence it can be computed efficiently. \nSimilarly, we can show that L2 self-attention can also be computed in an efficient manner. \nUsing the identity $\\|a-b\\|_2^2 = \\|a\\|_2^2 - 2a^\\top b + \\|b\\|_2^2$ we can compute the logits of L2 attention between all pairs via matrix multiplications and computation of row-wise L2 norms, with negligible overhead compared to dot-product self-attention.\nSpecifically, for L2 self-attention we can show that\n\\begin{gather}\nP = \\softmax{-\\frac{\\|XW^Q\\|_{\\text{row}}^2\\mathds{1}^\\top - 2XW^Q (XW^K)^\\top  + \\mathds{1} \\|XW^K\\|_{\\text{row}}^{2\\top}}{\\sqrt{D/H}}}, \\label{eq:L2_W}\n\\end{gather}\nwhere $\\|A\\|_{\\text{row}}^2$ applies the squared L2 norm to each row of $A$, \nso if $A \\in \\mathbb{R}^{m \\times n}$ then $\\|A\\|_{\\text{row}}^2 \\in \\mathbb{R}^m$.\n\nIn Table \\ref{tab:time} we show the wall-clock training times for the Transformer models with different attention functions and a varying number of layers. It is evident that the differences between the models are rather small.\n\\begin{table}[!htb] \n\\centering\n \\begin{tabular}{|c||c|c|c|c|c|} \n \\hline\n  & 1 Layer & 2 Layers & 3 Layers & 4 Layers & 5 Layers \\\\\n \\hline \\hline\n Transformer \\textbf{(DP)} & 37 & 56 & 77 & 92 & 110 \\\\ \n \\hline\nTransformer \\textbf{(L2)} & 35 & 56 & 73 & 99 & 115 \\\\\n \\hline\nTransformer, $W^Q=W^K$ \\textbf{(L2)} & 39 & 58 & 79 & 91 & 108 \\\\\n \\hline\nTransformer,  \\textbf{(Contractive-L2)} & 37 & 60 & 81 & 102 & 127 \\\\\n \\hline\n\\end{tabular}\n\\caption{Wall clock training times for one epoch of training (seconds)} \\label{tab:time}\n\\end{table}\n\n\n\\section{Proof of Theorem \\ref{thm:main}} \\label{apd:proof}\nRecall the formulation of \\verb!L2-MHA!:\n\\begin{align*}\n    F&: \\mathbb{R}^{N \\times D} \\rightarrow \\mathbb{R}^{N \\times D} \\\\\n    F(X) &= \\left[f^1(X)W^{V,1}, \\ldots, f^H(X)W^{V,H}\\right] W^O \\\\\n    f^h(X) &= P^h X A_h \\\\\n    P^h_{ij} \\propto \\exp(L_{ij}) &\\coloneqq \\exp\\left(-\\frac{\\| \\mathbf{x}_i^\\top W^{Q,h} - \\mathbf{x}_j^\\top W^{K,h}\\|_2^2}{\\sqrt{D/H}}\\right), \\spaces \\sum_j P^h_{ij} = 1\n\\end{align*}\nwhere we have that $W^{Q,h}, W^{K,h}, W^{V,h} \\in \\mathbb{R}^{D \\times D/H}$, $W^O \\in \\mathbb{R}^{D \\times D}$, $P^h \\in \\mathbb{R}^{N \\times N}$ and $A_h \\coloneqq W^{Q,h} W^{{Q,h}^\\top} / \\sqrt{D/H} \\in \\mathbb{R}^{D \\times D}$, and the softmax is applied to each row of the input matrix. \nRecall Equation \\eqref{eq:L2_W}:\n\\begin{equation*}\nP^h = \\softmax{-\\frac{\\|XW^{Q,h}\\|_{\\text{row}}^2\\mathds{1}^\\top - 2XW^{Q,h} (XW^{K,h})^\\top  + \\mathds{1} \\|XW^{K,h}\\|_{\\text{row}}^{2^\\top}}{\\sqrt{D/H}}}.\n\\end{equation*}\n\n\\subsection{L2 self-attention is \\emph{not} Lipschitz for general $\\boldsymbol{W^Q, W^K}$}\nLet us first look at the case of $H=1$ and suppress the index $h$ to reduce clutter. \nConsider the map $\\tilde{f}(X) \\coloneqq PX$, so $f(X)=\\tilde{f}(X)A$.\nWe need $\\tilde{f}$ to be Lipschitz for $f$ and hence $F$ to be Lipschitz.\nNote that $P$ is defined as:\n\\begin{equation*}\nP_{ij} \\propto \\exp(L_{ij}) \\coloneqq \\exp\\left(-\\frac{\\| \\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K\\|_2^2}{\\sqrt{D/H}}\\right)\n\\end{equation*}\nand the normalisation constant satisfies $\\sum_j P_{ij} = 1$, for $P \\in \\mathbb{R}^{N \\times N}$, $X \\in \\mathbb{R}^{N \\times D}$.\n\nFor L2 self-attention, we may take partial derivatives and use the chain rule to show that the Jacobian of $\\tilde{f}$ is:\n\\begin{equation}\n    J_{\\tilde{f}} = \\begin{bmatrix}\n    \\tilde{J}_{11} & \\dots & \\tilde{J}_{1N} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    \\tilde{J}_{N1} & \\dots & \\tilde{J}_{NN} \\\\\n    \\end{bmatrix} \\in \\mathbb{R}^{ND \\times ND}\n\\end{equation}\nwith\n\\begin{equation}\n    \\tilde{J}_{ij} = X^\\top P^{(i)} \\frac{\\partial L_{i:}}{\\partial x_j} + P_{ij} I \\in \\mathbb{R}^{D \\times D}\n\\end{equation}\nwhere\n\\begin{equation}\n    \\frac{\\partial L_{i:}}{\\partial \\mathbf{x}_j} = \\frac{2}{\\sqrt{D/H}}\\left[\\left(XW^K - \\mathds{1} \\mathbf{x}_i^\\top W^Q \\right)W^{Q^\\top} \\delta_{ij} + \\left(E_{ji}XW^Q - E_{jj}XW^K\\right)W^{K^\\top}\\right]\n\\end{equation}\nand\n\\begin{equation*}\n    P^{(i)}  \\coloneqq \\diag(P_{i:}) - P_{i:}^\\top P_{i:} =\n    \\begin{bmatrix}\n    P_{i1}(1-P_{i1}) & -P_{i1} P_{i2} & \\dots  & - P_{i1} P_{iN}  \\\\\n    - P_{i2} P_{i1} & P_{i2}(1-P_{i2}) & \\dots  & - P_{i2} P_{iN} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    - P_{iN}P_{i1} & - P_{iN}P_{i2} & \\dots  & P_{iN}(1-P_{iN}) \n    \\end{bmatrix},\n\\end{equation*}\n\\begin{equation*}\n    P_{ij} = \\frac{\\exp\\left(-\\|\\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K\\|_2^2\\right)}{\\sum_k \\exp\\left(-\\|\\mathbf{x}_i^\\top W^Q - \\mathbf{x}_k^\\top W^K\\|_2^2\\right)}.\n\\end{equation*}\nRecall that $E_{ji} \\in \\mathbb{R}^{N \\times N}$ is a binary matrix with zeros everywhere except the $(j,i)$th entry. Hence $E_{ji}X$ has all rows equal to zero except for the $j$th row given by $\\mathbf{x}_i^\\top$. We can then verify:\n\\begin{equation}\n    X^\\top P^{(i)} E_{ji} X = P_{ij}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k) \\mathbf{x}_i^\\top.\n\\end{equation}\nAlso note $P^{(i)}$ is symmetric, and each row/colum sums to $0$, i.e.~$P^{(i)} \\mathds{1} = \\mathds{1}^\\top P^{(i)} = 0$.\nHence we may simplify the Jacobian terms as follows:\n\\begin{align}\n    \\tilde{J}_{ii} &= \\frac{2}{\\sqrt{D/H}}\\left[X^\\top P^{(i)}(XW^K - \\mathds{1}\\mathbf{x}_i^TW^Q)W^{Q^\\top} + X^\\top P^{(i)}E_{ii}X(W^Q-W^K)W^{K^\\top}\\right] + P_{ii} I \\nonumber \\\\\n    &= \\frac{2}{\\sqrt{D/H}}\\left[X^\\top P^{(i)}(XW^K - \\mathds{1}\\mathbf{x}_i^TW^Q)W^{Q^\\top} + P_{ii}(\\mathbf{x}_i - \\sum_k P_{ik} \\mathbf{x}_k)\\mathbf{x}_i^\\top(W^Q-W^K)W^{K^\\top}\\right] + P_{ii} I  \\nonumber \\\\\n    &= \\frac{2}{\\sqrt{D/H}}\\left[X^\\top P^{(i)}XW^K W^{Q^\\top} + P_{ii}(\\mathbf{x}_i - \\sum_k P_{ik} \\mathbf{x}_k)\\mathbf{x}_i^\\top(W^Q-W^K)W^{K^\\top}\\right] + P_{ii} I, \\label{eq:jii}\n\\end{align}\nand for $i\\neq j$:\n\\begin{align}\n    \\tilde{J}_{ij} &= \\frac{2}{\\sqrt{D/H}}X^\\top P^{(i)}(E_{ij}XW^Q - E_{jj}XW^K)W^{K^\\top} + P_{ij} I \\nonumber \\\\\n    &= \\frac{2}{\\sqrt{D/H}} P_{ij}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k)(\\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K)W^{K^\\top} + P_{ij} I. \\label{eq:jij}\n\\end{align}\n\nWe are now ready to show that $\\tilde{f}$ is \\emph{not} Lipschitz for general $W^Q, W^K$:\n\\begin{lemma} \\label{lemma:tie_weights}\nIf $W^K \\in \\mathbb{R}^{D \\times D/H}$ is full rank (i.e.~full column rank), and $W^K \\neq W^Q$, then $J_{ij}$ has terms that are unbounded for $i \\neq j$, hence $\\tilde{f}$ is \\emph{not} Lipschitz. \n\\end{lemma}\n\\begin{proof}\n\nLet us investigate the expression $\\tilde{K}_{ij} \\coloneqq P_{ij} W^{K^\\top}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k)(\\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K) \\in \\mathbb{R}^{\\frac{D}{H} \\times \\frac{D}{H}}$ for $i\\neq j$, which is related to $\\tilde{J}_{ij}$ as follows by Equation \\eqref{eq:jij}:\n\n\\begin{equation*}\n    W^{K^\\top} \\tilde{J}_{ij} = \\left(\\frac{2}{\\sqrt{D/H}} \\tilde{K}_{ij} + P_{ij}I \\right)\n    W^{K^\\top}.\n\\end{equation*}\n\nIt suffices to show that $\\tilde{K}_{ij}$ is unbounded to show that $\\tilde{J}_{ij}$ is unbounded, since $W^K$ is full rank and $P_{ij} \\in [0,1]$. \n\nLet $\\mathbf{y}_j^\\top = \\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K$. \nThen we have:\n\\begin{align*}\n    \\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k \n    &= W^{Q^\\top}\\mathbf{x}_i - W^{K^\\top}\\mathbf{x}_j - \\sum_k P_{ik} (W^{Q^\\top}\\mathbf{x}_i - W^{K^\\top}\\mathbf{x}_k)\\\\\n    &= W^{Q^\\top}\\mathbf{x}_i - W^{K^\\top}\\mathbf{x}_j - (W^{Q^\\top}\\mathbf{x}_i - \\sum_k P_{ik} W^{K^\\top}\\mathbf{x}_k) \\\\\n    &= - W^{K^\\top}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k).\n\\end{align*}\nHence $\\tilde{K}_{ij} = - P_{ij} (\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k) \\mathbf{y}_j^\\top$.\nNote $\\mathbf{y}_i$ can take an arbitrary value in $\\mathbb{R}^{D/H}$, since $W^K \\neq W^Q$ and $W^K$ is full-rank.\n\nFor all $j \\neq i$, let us choose $\\mathbf{x}_j$ such that $\\mathbf{y}_j = -\\mathbf{y}_i$. This is possible for any value of $\\mathbf{y}_i$ since $W^K$ is full-rank.\nNote $\\mathbf{y}_j = - \\mathbf{y}_i$ and not $\\mathbf{y}_i$.\nWe then have that $\\|\\mathbf{y}_j\\|_2^2$ is equal for all $j$, hence $P_{ij} \\coloneqq \\frac{\\exp(-\\|\\mathbf{y}_j\\|_2^2)}{\\sum_k \\exp(-\\|\\mathbf{y}_k\\|_2^2)} = \\frac{1}{N}$ for all $j$. \nThen for $i \\neq j$, $\\tilde{K}_{ij}$ simplifies to\n\\begin{equation*}\n\\tilde{K}_{ij} = - \\frac{1}{N} \\left(-\\mathbf{y}_i - \\frac{1}{N} (N-2) (-\\mathbf{y}_i)\\right) (-\\mathbf{y}_i)^\\top  = - \\frac{2N-2}{N^2} \\mathbf{y}_i \\mathbf{y}_i^\\top \n\\end{equation*}\nwhose entries are unbounded since $\\mathbf{y}_i$ can be any vector in $\\mathbb{R}^{D/H}$ (note we assume $N \\geq 2$ for self-attention to be well-defined, hence $2N-2 \\neq 0$).\n\\end{proof}\n\nThe intuition for this result is as follows: a reason for \\verb!DP-MHA! not being Lipschitz is that for $\\mathbf{x}_i=0$,, the attention weights $P_{ij}$ become uniform regardless of the values of $\\mathbf{x}_j$ for $j \\neq i$. A similar issue arises for \\verb!L2-MHA! with $W^Q \\neq W^K$ and full-rank $W^K$, as shown above: given any $\\mathbf{x}_i$, we can choose $\\mathbf{x}_j$ such that the $P_{ij}$ become uniform. \n\n\\subsection{L2 self-attention is Lipschitz for $\\boldsymbol{W^Q=W^K}$}\n\nHence we impose the restriction that $W^K=W^Q$. With this assumption we have\n\\begin{equation}\nP_{ij} \\propto \\exp\\left(-\\|(\\mathbf{x}_i - \\mathbf{x}_j)^\\top \\sqrt{A}\\|_2^2\\right)\n\\end{equation}\nwhere $A=W^Q W^{Q^\\top} / \\sqrt{D/H} \\in \\mathbb{R}^{D \\times D}$ and $\\sqrt{A}$ is chosen such that $A = \\sqrt{A}\\sqrt{A}^\\top $,\nin particular $\\sqrt{A} \\coloneqq W^Q / (D/H)^{\\frac{1}{4}}$. The terms in the Jacobian of $\\tilde{f}$ simplify to:\n\\begin{align}\n    \\tilde{J}_{ii} &= 2 X^\\top P^{(i)}XA + P_{ii} I \\hspace{2mm}  \\text{(note $P^{(i)} \\mathds{1} = 0$)},\\\\\n    \\tilde{J}_{ij} &= 2 P_{ij}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k)(\\mathbf{x}_i - \\mathbf{x}_j)^\\top A + P_{ij} I  \\hspace{2mm} \\text{for $i \\neq j$}.\n\\end{align}\nLet the Jacobian of $f(X)$ be:\n\\begin{equation}\n    J_{f} = \\begin{bmatrix}\n    J_{11} & \\dots & J_{1N} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    J_{N1} & \\dots & J_{NN} \\\\\n    \\end{bmatrix} \\in \\mathbb{R}^{ND \\times ND}.\n\\end{equation}\nSince $f(X) = \\tilde{f}(X)A$, and by the chain rule $\\frac{\\partial}{\\partial \\mathbf{x}_j}[\\tilde{f}_i(X)A]=A^\\top \\frac{\\partial \\tilde{f}_i(X)}{\\partial \\mathbf{x}_j}=A \\frac{\\partial \\tilde{f}_i(X)}{\\partial \\mathbf{x}_j}$ (by symmetry of $A$), we have that $J_{ij} = A \\tilde{J}_{ij}$. \nHence\n\\begin{align}\n    J_{ii} &= 2 AX^\\top P^{(i)}XA + P_{ii} A \\hspace{2mm}  \\text{(note $P^{(i)} \\mathds{1} = \\mathbf{0}$)},\\\\\n    J_{ij} &= 2 P_{ij} A (\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k)(\\mathbf{x}_i - \\mathbf{x}_j)^\\top A + P_{ij} A  \\hspace{2mm} \\text{for $i \\neq j$}.\n\\end{align}\nNoting $\\lip_p(f) = \\sup_X \\|J_f(X)\\|_p$, we would like to upper bound $\\|J_f\\|_p$.\n\n\\subsubsection{Upper bound on $\\boldsymbol{\\lip_{\\infty}(F)}$ for L2-MHA}\n\nConsider the choice $p=\\infty$, where $\\|J_f\\|_\\infty$ is the maximum absolute row sum of $J_f$. \nA key observation is that if we can bound the $\\infty$-norm of the Jacobian of $f_i$, a single output of $f$, (i.e.~a single block row $\\|[J_{i1},...,J_{iN}]\\|_\\infty$ of $J_f$) then this is also a bound on $\\|J_f\\|_{\\infty}$ due to permutation equivariance of self-attention; all block rows have the same maximal $\\|\\cdot\\|_\\infty$ when each is optimised over the input $X$. \nUsing this, we can prove that $\\|J_f\\|_\\infty$ admits an upper bound that is $O(\\log N - \\log \\log N)$. Below we state and prove lemmas that lead to the proof of this upper bound.\n\nFirst we analyse the term $\\sqrt{A}^\\top X^\\top P^{(i)}X \\sqrt{A}$, that appears in the first term of $J_{ii}$. \nNote that for $Y \\coloneqq X \\sqrt{A}$, so that the rows of $Y$ are $\\mathbf{y}_i^\\top \\coloneqq \\mathbf{x}_i^\\top \\sqrt{A}$, we have \n\\begin{equation}\n    \\sqrt{A}^\\top X^\\top P^{(i)}X \\sqrt{A}= Y^\\top P^{(i)} Y =  \\mathrm{Cov}(\\mathbb{Y})\n\\end{equation}\nwhere $\\mathbb{P}(\\mathbb{Y}=\\mathbf{y}_j)=P_{ij} = \\exp(-\\|\\mathbf{y}_j - \\mathbf{y}_i\\|^2_2)/\\sum_k \\exp(-\\|\\mathbf{y}_k - \\mathbf{y}_i\\|^2_2)$.\nThe last equality uses the observation in Equation \\eqref{eq:cov}.\n\nThe central inequality used throughout the proof of the main theorem is the following:\n\\begin{lemma} \\label{lemma:key}\n$\\Tr(\\mathrm{Cov}(\\mathbb{Y})) = \\sum_j P_{ij}\\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_2^2 \\leq \\sum_j P_{ij}\\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2 \\leq \\phi^{-1}(N-1)$ where $\\phi(c) = c \\exp(c+1)$ is a one-dimensional invertible function on $\\mathbb{R}_{\\geq 0}$.\n\\end{lemma}\n\\begin{proof}\nThe first equality holds since $\\Tr(\\mathrm{Cov}(\\mathbb{Y})) = \\sum_j \\mathrm{Cov}(\\mathbb{Y})_{jj} = \\sum_j \\mathrm{Var}(\\mathbb{Y}_j) = \\sum_j \\mathbb{E}[(\\mathbb{Y}_j -\\mathbb{E}[\\mathbb{Y}_j])^2]$. \nThe next inequality holds since $\\mathrm{Var}(\\mathbb{Y}_j) = \\mathrm{Var}(\\overline{\\mathbb{Y}}_j) = \\mathbb{E}[\\overline{\\mathbb{Y}}_j^2] -\\mathbb{E}[\\overline{\\mathbb{Y}}_j]^2 \\leq \\mathbb{E}[\\overline{\\mathbb{Y}}_j^2]$ where $\\overline{\\mathbb{Y}}= \\mathbb{Y} - y_i$. \nThe final inequality can be proved as follows.\n\nWe would like to bound \n\\begin{equation}\n    \\sum_j P_{ij}\\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2 = \\frac{\\sum_j \\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2 \\exp(-\\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2)}{\\sum_k \\exp(-\\|\\mathbf{y}_k-\\mathbf{y}_i\\|_2^2)}  = \\frac{\\sum_j z_j^2 \\exp(-z_j^2)}{\\sum_k \\exp(-z_k^2)}\n\\end{equation}\nwhere $z_j \\coloneqq \\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2$ (hence $z_i=0$). \nDefine:\n\\begin{equation}\n    g(\\mathbf{z}) \\coloneqq \\frac{\\sum_j z_j^2 \\exp(-z_j^2)}{\\sum_k \\exp(-z_k^2)} = \\frac{\\sum_{j \\neq i} z_j^2 \\exp(-z_j^2)}{1 + \\sum_{k \\neq i} \\exp(-z_k^2)}.\n\\end{equation}\nFirst note that as $z_j \\rightarrow \\infty$, $\\exp(-z_j^2) \\rightarrow 0$ exponentially fast, causing the product $z_j^2 \\exp(-z_j^2) \\rightarrow 0$.\nHence we expect the above quantity to be bounded and attain its maximum.\n\nLet $h(z_j) \\coloneqq \\exp(-z_j^2)$ for notational conciseness, and note $h(z_j) > 0$. By taking partial derivatives with the chain rule, we have that for $j \\neq i$\n\\begin{equation}\n\\frac{\\partial g(\\mathbf{z})}{\\partial z_j} = \\frac{2z_j h(z_j)}{(\\sum_k h(z_k))^2}\\left[(1-z_j^2)\\sum_k h(z_k) + \\sum_k h(z_k)z_k^2\\right].\n\\end{equation}\nHence the derivative is $0$ if and only if $z_j = 0$ or $(1-z_j^2)\\sum_k h(z_k) + \\sum_k h(z_k)z_k^2 = 0$, the latter being equivalent to $z_j^2 = 1 + \\frac{\\sum_k h(z_k)z_k^2}{\\sum_k h(z_k)} = 1 + g(\\mathbf{z})$. \nHence at the maximum, the non-zero values among $\\{z_j\\}_{j=1}^N$ must be equal to one another.\nIt is clear now that the maximum value $c$ is attained when $z_j^2 = 1 + c$ for $j \\neq i$ (and recall $z_i = 0$). \nSo $h(z_j) = \\exp(-1-c)$ for $j \\neq i$.\nSubstituting this into $g(z)$, and rearranging, we obtain $c \\exp(c+1) = N - 1$. Note $\\phi(x) \\coloneqq x \\exp(x+1)$ is increasing for $x > 0$ hence $c = \\phi^{-1}(N-1)$.\n\\end{proof}\nNote $\\phi(\\log N) = (\\log N) \\exp(\\log N + 1) \\geq N \\log N \\geq N -1$ for $N \\geq 3$. Since $\\phi$ is increasing, we have $\\phi^{-1}(N-1) \\leq \\log(N)$ for $N \\geq 3$. In fact, it is known that $\\phi^{-1}(N-1) = O(\\log N - \\log \\log N)$ \\citep{corless1996lambertw}.\n\nNote the $A$ term in $f(X) = \\tilde{f}(X) A$ allows us to use the above inequality, since $Y^\\top P^{(i)}Y = \\mathrm{Cov}(\\mathbb{Y})$ now appears in the terms of $J_f$:\n\\begin{align}\n    J_{ii}\n    &= 2 \\sqrt{A} [Y^\\top P^{(i)}Y]\\sqrt{A}^\\top + P_{ii} A,  \\\\ \n    J_{ij},\n    &= 2 \\sqrt{A} P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top \\sqrt{A}^\\top + P_{ij} A  \\hspace{2mm} \\text{for $i \\neq j$}.\n\\end{align}\n\nUsing the inequalities $\\|BC\\| \\leq \\|B\\| \\|C\\|$, $\\|B + C\\| \\leq \\|B\\| + \\|C\\|$ and $\\|[A_1, \\ldots, A_N]\\| \\leq \\sum_i \\|A_i\\|$, we have: \n\\begin{align*}\n\\|[J_{i1} &, \\ldots, J_{iN}]\\|_{\\infty}  \\\\\n\\leq & \\|J_{ii}\\|_{\\infty} + \\sum_{j \\neq i} \\|J_{ij}\\|_{\\infty} \\\\\n  \\leq & 2 \\|\\sqrt{A}\\|_{\\infty} \\|Y^\\top P^{(i)}Y\\|_{\\infty} \\|\\sqrt{A}^\\top\\|_{\\infty} + P_{ii} \\|A\\|_{\\infty} \\\\\n & + 2 \\sum_{j \\neq i} \\|\\sqrt{A}\\|_{\\infty} \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty \\|\\sqrt{A}^\\top\\|_{\\infty} + P_{ij} \\|A\\|_{\\infty}\\\\\n  = & 2  \\|\\sqrt{A}\\|_{\\infty}\\|\\sqrt{A}^\\top\\|_{\\infty} \n\\bigg(\\|Y^\\top P^{(i)}Y\\|_\\infty \n + \\sum_{j\\neq i} \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty\\bigg) + \\|A\\|_{\\infty} \\\\\n = & 2  \\frac{\\|W^{Q}\\|_{\\infty}\\|W^{Q^\\top}\\|_{\\infty}}{\\sqrt{D/H}} \n\\bigg(\\|Y^\\top P^{(i)}Y\\|_\\infty \n + \\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty\\bigg) + \\frac{\\|W^Q W^{Q^\\top}\\|_{\\infty}}{\\sqrt{D/H}}.\n\\end{align*}\nFor the first equality, note that $\\sum_j P_{ij}=1$. For the second equality, note that the summand for $j=i$ is $0$ because the term $\\mathbf{y}_i - \\mathbf{y}_j=\\mathbf{0}$. \nEach of the terms in the brackets are bounded by the following lemmas:\n\\begin{lemma}\n$\\|Y^\\top P^{(i)}Y\\|_\\infty \\leq  \\phi^{-1}(N-1) \\sqrt{D/H} $ ($\\phi$ defined as in Lemma \\ref{lemma:key}).\n\\end{lemma}\n\\begin{proof}\nRecall that $Y^\\top P^{(i)}Y = \\mathrm{Cov}(\\mathbb{Y})$. Let $\\sigma(\\mathbb{Y}_m)$ denote the standard deviation of $\\mathbb{Y}_m$. Then $[\\mathrm{Cov}(\\mathbb{Y})]_{lm} \\leq \\sigma(\\mathbb{Y}_l)\\sigma(\\mathbb{Y}_m)$.\nHence \n\\begin{align*}\n\\|\\mathrm{Cov}(\\mathbb{Y})\\|_{\\infty} = \\max_l \\sum_m \\left|[\\mathrm{Cov}(\\mathbb{Y})]_{lm}\\right| \n& \\leq  \\max_l \\sigma(\\mathbb{Y}_l) \\sum_m \\sigma(\\mathbb{Y}_m) \\\\\n& \\leq \\sqrt{\\frac{D}{H}} \\sum_m \\sigma^2(\\mathbb{Y}_m)  = \\sqrt{\\frac{D}{H}} \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\\\\n& \\leq \\sqrt{\\frac{D}{H}} \\phi^{-1}(N-1),\n\\end{align*}\nsince $\\sum_m \\sigma(\\mathbb{Y}_m) \\leq \\sqrt{\\frac{D}{H}} \\sqrt{\\sum_m \\sigma^2(\\mathbb{Y}_m)}$ (by e.g.~using the Cauchy--Schwartz inequality on $[\\sigma(\\mathbb{Y}_1), \\ldots, \\sigma(\\mathbb{Y}_{D/H})]$ and $\\mathds{1}$) and $\\max_l \\sigma(\\mathbb{Y}_l) \\leq \\sqrt{\\sum_m \\sigma^2(\\mathbb{Y}_m)}$, and the last inequality is from Lemma \\ref{lemma:key}. \n\\end{proof}\n\n\\begin{lemma} \\label{lemma:low_rank}\n$\\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty \\leq  \\phi^{-1}(N-1) \\sqrt{D/H}$.\n\\end{lemma}\n\\begin{proof}\nNote $\\|\\mathbf{u}\\mathbf{v}^\\top\\|_{\\infty} = \\|\\mathbf{u}\\|_{\\infty} \\|\\mathbf{v}\\|_1$ for real vectors $\\mathbf{u},\\mathbf{v}$. Hence\n\\begin{align*}\n    \\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty & = \\sum_j P_{ij} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_\\infty \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_1 \\\\\n    & = \\mathbf{a}^\\top \\mathbf{b} \\leq \\|\\mathbf{a}\\|_2 \\|\\mathbf{b}\\|_2,\n\\end{align*}\nwhere $a_j = \\sqrt{P_{ij}} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_\\infty$, $b_j = \\sqrt{P_{ij}} \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_1$.\n\nNote $a_j \\leq c_j \\coloneqq  \\sqrt{P_{ij}} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_2$ since $\\|\\mathbf{u}\\|_\\infty \\leq \\|\\mathbf{u}\\|_2$ for vector $\\mathbf{u}$. Hence $\\|\\mathbf{a}\\|_2 \\leq \\|\\mathbf{c}\\|_2$.\n\nAlso $b_j \\leq \\sqrt{\\frac{D}{H}} d_j \\coloneqq  \\sqrt{\\frac{D}{H}} \\sqrt{P_{ij}} \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_2$ since $\\|\\mathbf{u}\\|_1 \\leq \\sqrt{\\frac{D}{H}}\\|\\mathbf{u}\\|_2$ for $\\mathbf{u} \\in \\mathbb{R}^{D/H}$ (e.g.~by the Cauchy--Schwartz inequality on $[|\\mathbf{u}_1|, \\ldots, |\\mathbf{u}_{D/H}|]$ and $\\mathds{1}$). Hence $\\|b\\|_2 \\leq \\sqrt{\\frac{D}{H}}\\|d\\|_2$.\n\nNote $\\|c\\|_2^2 = \\sum_j P_{ij} \\|y_j - \\sum_k P_{ik} y_k\\|_2^2 = \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\phi^{-1}(N-1)$ from Lemma \\ref{lemma:key},\nand $\\|d\\|_2^2 =  \\sum_j P_{ij} \\|y_i - y_j\\|_2^2 \\leq  \\phi^{-1}(N-1)$ also from Lemma \\ref{lemma:key}.\nHence $\\|a\\|_2 \\|b\\|_2 \\leq \\sqrt{\\frac{D}{H}} \\|c\\|_2 \\|d\\|_2 \\leq \\sqrt{\\frac{D}{H}} \\phi^{-1}(N-1)$.\n\\end{proof}\n\n\nPutting the above lemmas altogether, with the observation $\\sup_X \\|J_f(X)\\|_\\infty = \\sup_X \\|[J_{i1}(X), \\ldots, J_{iN}(X)]\\|_\\infty$ by permutation invariance of $\\|J_f\\|_\\infty$ (since $f$ is permutation equivariant and $\\|\\cdot\\|_\\infty$ is the maximum absolute row sum), we have\n\\begin{align}\n\\|J_f\\|_{\\infty}\n& \\leq 4\\|W^Q\\|_{\\infty}\\|W^{Q^\\top}\\|_{\\infty} \\phi^{-1}(N-1)\n+ \\frac{\\|W^Q W^{Q^\\top}\\|_{\\infty}}{\\sqrt{D/H}} \\nonumber\\\\\n& \\leq \\|W^Q\\|_{\\infty}\\|W^{Q^\\top}\\|_{\\infty} \\left(4\\phi^{-1}(N-1) + \\frac{1}{\\sqrt{D/H}}\\right) \\label{ineq:infty}\\\\\n& \\leq \\|W^Q\\|_{\\infty} \\|W^{Q^\\top}\\|_{\\infty} \\left(4\\log N + \\frac{1}{\\sqrt{D/H}}\\right), \\nonumber\n\\end{align}\nwhere the last inequality holds for $N \\geq 3$.\n\nThe full multihead attention map that combines the heads $f^h(X)$ is:\n\\begin{equation*}\nF: X \\mapsto \\left[f^1(X)W^{V,1}, \\ldots f^H(X)W^{V,H}\\right] W^O = g(X) W^V W^O\n\\end{equation*}\nwhere $g:X \\mapsto [f^1(X),\\ldots,f^H(X)]$, $W^O \\in \\mathbb{R}^{D \\times D}$ and\n\\begin{equation*}\n    W^V = \\begin{bmatrix}\n    W^{V,1} & \\dots & 0 \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & W^{V,H} \\\\\n    \\end{bmatrix} \\in \\mathbb{R}^{DH \\times D}.\n\\end{equation*}\nNote the Jacobian $J_g$ is a block matrix whose rows are $J_{f^h}$, hence $\\|J_g\\|_{\\infty} = \\max_h \\|J_{f^h}\\|_{\\infty}$, and similarly $\\|W^{V^\\top}\\|_{\\infty} = \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty}$. Hence we have\n\\begin{equation*}\n    \\lip_{\\infty}(F) \\leq \\max_h \\|J_{f^h}\\|_{\\infty} \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty} \\|W^{O^\\top}\\|_{\\infty}.\n\\end{equation*}\n\nCombining this with Inequality (\\ref{ineq:infty}), we have:\n\\begin{equation*}\n    \\lip_{\\infty}(F)  \\leq \\left(4 \\phi^{-1}(N-1) + \\frac{1}{\\sqrt{D/H}}\\right) \\max_h \\|W^{Q,h}\\|_{\\infty} \\|W^{{Q,h}^\\top}\\|_{\\infty} \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty} \\ \\|W^{O^\\top}\\|_{\\infty}.\n\\end{equation*}\n\n\\subsubsection{Upper bound on $\\boldsymbol{\\lip_2(F)}$ for L2-MHA}\nFor $p=2$, we use the following lemma:\n\\begin{lemma} \\label{lemma:block_rows}\nLet A be a block matrix with block rows $A_1, \\ldots, A_N$. Then $\\|A\\|_2 \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2}$, and equality holds if and only if the first right singular vectors of the $A_i$ align.\n\\end{lemma}\n\\begin{proof}\n\\begin{equation*}\n\\|A\\|_2^2 = \\left\\Vert \\begin{bmatrix} A_1 \\\\ \\vdots \\\\ A_N \\\\ \\end{bmatrix}\\right\\Vert_2^2 = \\sup_{\\|\\mathbf{x}\\|_2=1} \\left\\Vert\\begin{bmatrix} A_1 \\\\ \\vdots \\\\ A_N \\\\ \\end{bmatrix} \\mathbf{x}\\right\\Vert_2^2 = \\sup_{\\|\\mathbf{x}\\|_2=1} \\sum_i \\|A_i \\mathbf{x}\\|_2^2 \\leq \\sum_i \\sup_{\\|\\mathbf{x}\\|_2=1} \\|A_i \\mathbf{x}\\|_2^2 = \\sum_i \\|A_i\\|_2^2.\n\\end{equation*}\nNote that equality holds if and only if the first right singular vectors of the $A_i$ align.\n\\end{proof}\nHence a bound on the spectral norm of each block row of $J_f$ can give us an $O(\\sqrt{N})$ bound on $\\|J_f\\|_2$, which may be loose, and it remains an open question as to whether this bound can be tightened.\n\nTo bound the $\\|\\cdot\\|_2$ norm of each row of $J_f$, we use the following lemmas:\n\\begin{lemma}\n$\\|Y^\\top P^{(i)}Y\\|_2 \\leq \\phi^{-1}(N-1)$\n\\end{lemma}\n\\begin{proof}\n$\\|Y^\\top P^{(i)}Y\\|_2=\\|\\mathrm{Cov}(\\mathbb{Y})\\|_2 = \\lambda_{\\max}(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\phi^{-1}(N-1)$, where the first equality holds by symmetry of $\\mathrm{Cov}(\\mathbb{Y})$ and the next holds by $\\mathrm{Cov}(\\mathbb{Y})$ being positive semi-definite, so all its eigenvalues are non-negative, and hence the maximal eigenvalue is bounded by the sum of the eigenvalues, equal to its trace. The final inequality is from Lemma \\ref{lemma:key}.\n\\end{proof}\n\n\\begin{lemma}\n$\\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_2 \\leq  \\phi^{-1}(N-1)$\n\\end{lemma}\n\\begin{proof}\nDirectly use Cauchy--Schwartz on $c$ and $d$ in the proof of Lemma \\ref{lemma:low_rank}. \n\\end{proof}\nAgain using the inequalities $\\|BC\\| \\leq \\|B\\| \\|C\\|$, $\\|B + C\\| \\leq \\|B\\| + \\|C\\|$ and $\\|[A_1, \\ldots, A_N]\\| \\leq \\sum_i \\|A_i\\|$, with the additional equality $\\|B^\\top\\|_2 = \\|B\\|_2$, we have the bound: \n\\begin{align*}\n&\\|[J_{i1}, \\ldots, J_{iN}]\\|_2 \\\\\n & \\leq  2  \\frac{\\|W^Q\\|_2\\|W^{Q^\\top}\\|_2}{\\sqrt{D/H}} \n\\bigg(\\|Y^\\top P^{(i)}Y\\|_2\n + \\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_2 \\bigg) + \\frac{\\|W^Q W^{Q^\\top}\\|_2}{\\sqrt{D/H}} \\\\\n & \\leq  4\\phi^{-1}(N-1) \\frac{\\|W^Q\\|_2^2}{\\sqrt{D/H}}  + \\frac{\\|W^Q W^{Q^\\top}\\|_2}{\\sqrt{D/H}} \\\\\n & \\leq  \\frac{\\|W^Q\\|_2^2}{\\sqrt{D/H}} \\bigg(4\\phi^{-1}(N-1)+1 \\bigg).\n\\end{align*}\nUsing Lemma \\ref{lemma:block_rows}, we have that\n\\begin{align}\n    \\|J_f\\|_2 & \\leq \\frac{\\sqrt{N}\\|W^Q\\|_2^2}{\\sqrt{D/H}} \\bigg(4\\phi^{-1}(N-1)+1 \\bigg) \\label{ineq:2} \\\\\n    & \\leq \\frac{\\sqrt{N}\\|W^Q\\|_2^2}{\\sqrt{D/H}}(4\\log N+1). \\nonumber\n\\end{align}\nTo obtain the final result for the full multihead self-attention $F$, we need a final lemma:\n\\begin{lemma} \\label{lemma:block_cols}\nLet A be a block matrix with block columns $A_1, \\ldots, A_N$. Then $\\|A\\|_2 \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2}$.\n\\end{lemma}\n\\begin{proof}\n\\begin{align*}\n\\|A\\|_2 &= \\|[A_1, \\ldots, A_N]\\|_2 = \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1}  \\left\\Vert [A_1, \\ldots, A_N] \\begin{bmatrix} \\mathbf{x}_1\\\\ \\vdots \\\\ \\mathbf{x}_N \\\\ \\end{bmatrix} \\right\\Vert_2^2 = \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1} \\|\\sum_i A_i \\mathbf{x}_i\\|_2 \\\\ \n& \\leq \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1} \\sum_i \\|A_i \\mathbf{x}_i\\|_2 = \\sup_{\\|\\mathbf{e}_i\\|_2=1, \\sum_i \\lambda_i^2 =1} \\sum_i \\lambda_i \\|A_i \\mathbf{e}_i\\|_2 = \\sup_{\\sum_i \\lambda_i^2 =1} \\sum_i \\lambda_i \\|A_i\\|_2 \\\\\n& \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2},\n\\end{align*}\nwhere we are using the substitution $\\mathbf{x}_i = \\lambda_i \\mathbf{e}_i$, and the last inequality holds by e.g.~Cauchy--Schwartz inequality on $[\\lambda_1, \\ldots, \\lambda_N]$ and $[\\|A_1\\|_2, \\ldots, \\|A_N\\|_2]$.\n\\end{proof}\nRecall that \n\\begin{equation*}\nF: X \\mapsto \\left[f^1(X)W^{V,1}, \\ldots, f^H(X)W^{V,H}\\right] W^O.\n\\end{equation*}\nSince $\\|f^h(X)W^{V,h}\\|_2 \\leq \\|J_{f^h}\\|_2 \\|W^{V,h}\\|_2$, by Lemma \\ref{lemma:block_cols} we have that\n\\begin{equation*}\n    \\left\\|[f^1(X)W^{V,1}, \\ldots, f^H(X)W^{V,H}]\\right\\|_2 \\leq \\sqrt{\\sum_h \\|J_{f^h}\\|_2^2 \\|W^{V,h}\\|_2^2}\n\\end{equation*} and hence\n\\begin{equation}\n    \\lip_2(F) \n    \\leq \\left(\\sqrt{\\sum_h \\|J_{f^h}\\|_2^2 \\|W^{V,h}\\|_2^2}\\right) \\|W^O\\|_2.\n\\end{equation}\nCombining this with Inequality (\\ref{ineq:2}), we have:\n\\begin{equation*}\n    \\lip_2(F) \\leq \\frac{\\sqrt{N}}{\\sqrt{D/H}}\n    \\left(4 \\phi^{-1}(N-1) + 1 \\right) \\left(\\sqrt{\\textstyle\\sum_h \\|W^{Q,h}\\|_2^2\\, \\|W^{V,h}\\|_2^2}\\right) \\|W^O\\|_2.\n\\end{equation*}\n\n\\section{The Case with Masking} \\label{apd:masking}\nSince self-attention is often used with \\textit{masking}, a natural question is how masking affects the derived bounds. In self-attention (for any choice of attention function), masking is implemented as follows: given a set of mask indices $\\mathcal{M}  \n\\subset \\{1, \\ldots, N\\} \\times \\{1, \\ldots, N\\}$, the logits (i.e.~the inputs to the softmax) are set to $-\\infty$ at the mask indices. That is,\n\\begin{equation*}\n    L_{ij}= \n\\begin{cases}\n    \\tilde{L}_{ij} & \\text{if } (i,j) \\notin \\mathcal{M}\\\\\n    -\\infty        & \\text{if } (i,j) \\in \\mathcal{M}\n\\end{cases}\n\\end{equation*}\nwhere $\\tilde{L}_{ij}$ is the original logit (e.g.~for L2 self-attention, $\\tilde{L}_{ij} = -(\\mathbf{x}_i - \\mathbf{x}_j)^\\top A (\\mathbf{x}_i - \\mathbf{x}_j)$). \n\nMasking implies $f_i(X)$ is not a function of $\\mathbf{x}_j$ for $(i,j) \\in \\mathcal{M}$, hence  $J_{ij} = 0$ for $(i,j) \\in \\mathcal{M}$.\nThus $f_i(X)$ is equal to the $i$th output for self-attention with inputs restricted to $\\{\\mathbf{x}_j: (i,j) \\notin \\mathcal{M}\\}$, the unmasked inputs with respect to the $i$th output. \nHence $J_{ij}$ will no longer contribute to the bound on $\\|[J_{i1}, \\ldots, J_{iN}]\\|$, and hence the bound for the unmasked case will continue to hold as long as $(i,i) \\in \\mathcal{M}$ i.e.~$\\mathbf{x}_i$ attends to itself (this is necessary for the proof of Lemma \\ref{lemma:key} to hold). \nThe bound can in fact be tightened by replacing $N$ with $|\\{\\mathbf{x}_j: (i,j) \\notin \\mathcal{M}\\}|$, the number of unmasked inputs with respect to the $i$th output.\n\n\\section{Experimental Details} \\label{apd:experimental_details}\nFor the experiment in Section \\ref{sec:asymptotic}, showing the asymptotic tightness of the upper bound on $\\lip_{\\infty}(F)$ where $F$ is \\verb!L2-MHA!, we fix all free parameters of $F$ (namely $W^Q, W^V$) to be the identity, and only optimise the input $X$. \nWe use $50$ random initialisations of $X$ for each $N$, where $X_{ij} \\sim \\unif [-c, c]$ for $c \\sim \\unif [0,10]$ (we observed that having $c$ itself be random improves optimisation). We display the top $5$ results for each value of $N$ after optimising each random initialisation till convergence using Adam \\citep{kingma2014adam} with a learning rate of $0.1$. \n\nFor the experiments in Section \\ref{sec:invertible_self-attention}, we comparing the performance of the original Transformer and the Transformer with Lipschitz/invertible self-attention at character-level language modelling on the Penn Treebank dataset \\citep{marcus1993building}.\\footnote{We use the standard training-validation-test split, and the dataset can be found at e.g.\\ \\url{https://github.com/harvardnlp/TextFlow/tree/master/data/ptb}.}\nEach training example is a sentence represented as a variable-length sequence of characters, and examples are batched according to length such that padding is minimised, with the maximum sequence length set to $288$. \nAll models are autoregressive, outputting the logits for the categorical likelihood predicting the next character, and are trained using maximum likelihood (cross-entropy loss) with a batch size of $64$. \nThe LSTM models have the dimensionality of the hidden state equal to the dimensionality $D$ of the cell state (the usual default implementation). \nThe Transformer models are trained with a varying number of blocks (number of layers) with $H=8$ heads and $D=512$, tuning hyperparameters for dropout rate in $\\{0,0.1,0.2\\}$ and base learning rate $\\gamma \\in \\{0.2,0.4,0.6,0.8,1.0,1.5,2.0\\}$ with number of warmup iterations $w \\in \\{1000,2000,4000,8000\\}$ for the standard custom learning rate schedule in \\citet{vaswani2017attention}:\n\\begin{equation*}\n  \\epsilon_t = \\frac{\\gamma}{\\sqrt{D}} \\min(t^{-1/2}, t w^{-3/2}),\n\\end{equation*}\nwhere $\\epsilon_t$ is the learning rate at training iteration $t$. Hence the learning rate linearly increases from $0$ to $(Dw)^{-1/2}$ over $w$ iterations, then decays proportionally to $t^{-1/2}$.\nWe use Glorot Uniform initialisation \\citep{glorot2010understanding} for all weights ($U\\left[-\\sqrt{\\frac{1}{d_{in} + d_{out}}}, \\sqrt{\\frac{1}{d_{in} + d_{out}}}\\right]$), except for weights in \\verb!L2-MHA! that are initialised from $U\\left[-\\frac{s}{\\sqrt{D}},\\frac{s}{\\sqrt{D}}\\right]$, and $s$ is a hyperparameter. For $D=512$, we used $s=\\frac{1}{2^4}$. All experiments were done in Tensorflow 1.14 \\citep{abadi2016tensorflow} on single Nvidia Tesla V100 GPUs.\n\n\\section{Numerical Invertibility of MHA Residual Map} \\label{apd:numerical_invertibility}\nFollowing Section \\ref{sec:numerical-invertibility}, Figure \\ref{fig:trained_dp_mha_invertibility} confirms that numerical invertibility does not hold for trained weights for dot-product multihead self-attention (DP-MHA) (obtained from one-layer Transformer (DP) model used for Figure \\ref{fig:ptb}), similar to the randomly initialised weight case.\nFigure \\ref{fig:mha_invertibility} shows additional results for different values of $N$ and $D$. \n\n\\begin{figure}[!htb]\n    \\centering\n    \\includegraphics[width=0.35\\textwidth]{trained_dp_mha_invertibility.pdf}\n    \\caption{Invertibility of $g(\\mathbf{x})=\\mathbf{x} + cf(\\mathbf{x})$ for trained DP-MHA $f$.}\n    \\label{fig:trained_dp_mha_invertibility}\n\\end{figure}\n\n\\begin{figure}[!htb]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{mha_invertibility.pdf}\n    \\caption{Numerical invertibility of $g(\\mathbf{x})= \\mathbf{x} + c f(\\mathbf{x})$ where $f$ is L2-MHA(left) or DP-MHA  (right), for different values of $N$ and $D$.}\n    \\label{fig:mha_invertibility}\n\\end{figure}\n\n\n\n\\newpage\n\\section{Behaviour of Lower Bound on $\\boldsymbol{\\lip_2(F)}$}\n\\label{apd:jacobian_opt2}\n\\begin{figure}[!htb]\n    \\centering\n    \\includegraphics[width=0.6\\columnwidth]{jacobian_opt2.pdf}\n    \\caption{Lower bound on $\\lip_{2}(F)$ where $F$ is L2-MHA, with $D=1$ and varying $N$, obtained by optimising $\\|J_F(X)\\|_{2}$ with respect to $X$, with $50$ random initialisations of $X$ for each $N$.}\n    \\label{fig:jacobian_opt2}\n\\end{figure}\nIn Figure \\ref{fig:jacobian_opt2}, we show the lower bound on $\\lip_{2}(F)$ obtained by optimising $\\|J_F(X)\\|_{2}$ using the same optimisation procedure as for Figure \\ref{fig:jacobian_opt} of Section \\ref{sec:asymptotic}. Here the optimisation is more difficult, evident in the variance of the top $5$ values, and the trend is less clear, but it appears that $\\lip_{2}(f)$ grows at a rate of $O(\\log N)$. The message is less clear here, and there are at least two possibilities: \n\\begin{enumerate}[label=(\\arabic*), leftmargin=*]\n    \\item The optimisation is difficult even for small values of $N$, hence Figure \\ref{fig:jacobian_opt2} shows a loose lower bound.\n    \\item If the lower bound is tight, this suggests that the $O(\\sqrt{N} \\log N)$ bound in Theorem \\ref{thm:main} is not asymptotically tight, and could be improved to $O(\\log N)$ (or $O(\\log N - \\log \\log N)$ as for $p=\\infty$).\n\\end{enumerate}\n\n\\section{Optimising the norm of the Jacobian of DP-MHA}\n\\label{apd:jacobian_opt_dp}\nIn Figure \\ref{fig:trained_dp_mha_lower_bound}, we show how the norm of the Jacobian $\\|J_f(X)\\|_{\\infty}$ for \\verb!DP-MHA! $f$ keeps increasing when being optimised with respect to $X$. This is a useful sanity check validating our theoretical result of Theorem \\ref{thm:dp_not_lipschitz}, that \\verb!DP-MHA! is \\emph{not} Lipshchitz. The oscillations are likely due to momentum term of Adam optimizer that was used to optimise the norm. \n\n\\begin{figure}[!htb]\n    \\centering\n    \\includegraphics[width=0.4\\columnwidth]{trained_dp_mha_lower_bound.pdf}\n    \\caption{Optimise $\\|J_f(X)\\|_{\\infty}$ w.r.t.~$X$ for trained DP-MHA $f$.}\n    \\label{fig:trained_dp_mha_lower_bound}\n\\end{figure}\n\n\\section{Experiment tying keys and queries of L2-MHA but preserving parameter count}\n\\label{apd:wq_wk_experiment}\nIn Figure \\ref{fig:ptb} of Section \\ref{sec:invertible_self-attention}, we have shown that there is a clear reduction in performance when tying the keys and queries. To test whether this can be attributed to the reduction in parameter count, we tried doubling the number of columns of $W^Q$ when the keys and queries are shared (i.e. from $D/H$ to $2D/H$) so that the shared model has the same number of parameters as the unshared model. In Figure \\ref{fig:ptb_wq_wk}, the third column shows results for shared \\verb!L2-MHA!, but with the same number of parameters as the unshared \\verb!L2-MHA! i.e.~without tying the keys and queries. The performance is similar to the second column (tying with a reduced number of parameters), suggesting that there is an inherent limitation in expressiveness to tying the keys and queries, and that the reduction in number of parameters is an insufficient explanation this phenomenon.\n\n\\begin{figure}[!htb]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{ptb_wq_wk_experiment.pdf}\n    \\caption{Experiment tying keys/queries but preserving parameter count.}\n    \\label{fig:ptb_wq_wk}\n\\end{figure}\n\n\n\\section{Training curves for fixed learning rate DP-MHA vs L2-MHA} \\label{apd:stability}\n\n\\begin{figure}[!ht]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{stability_experiment.pdf}\n    \\caption{Train NLL for Transformer (DP), Transformer (L2) and Transformer (Contractive-L2)}\n    \\label{fig:stability}\n\\end{figure}\n\n\\section{The Lipschitz constant of LayerNorm}\n\\label{apd:layernorm}\nIn this section, we show that \\verb!LayerNorm! is Lipschitz, with a loose bound on its Lipschitz constant w.r.t. to the $\\infty$-norm.\n\\verb!LayerNorm! is defined as follows:\n\\begin{align*}\n    \\text{LN}(\\mathbf{x}) &= \\frac{\\mathbf{x}-\\mu(\\mathbf{x})}{\\sqrt{\\sigma^2(\\mathbf{x}) + \\epsilon}} \\odot \\boldsymbol\\gamma + \\boldsymbol\\beta \\\\\n    \\mu(\\mathbf{x}) &= \\frac{1}{D} \\sum_{d=1}^D x_d \\\\\n    \\sigma^2(\\mathbf{x}) &= \\frac{1}{D}\\sum_{d=1}^D (x_d - \\mu(\\mathbf{x}))^2 \n\\end{align*}\nwhere $\\mathbf{x}, \\boldsymbol\\beta, \\boldsymbol\\gamma \\in \\mathbb{R}^D$. We will omit dependence on $x$ to write $\\mu, \\sigma^2$ in cases when there is no ambiguity to reduce clutter.\n\nIn the trivial case where $x_d$ are all equal or when $D=1$, $\\mathbf{x} = \\mu$ hence $LN(\\mathbf{x})=\\boldsymbol\\beta$, so its Lipschitz constant is 0. Thus let us assume $D > 2$ and not all $x_d$ are equal.\n\nFirst let us compute the derivative of $\\mu$ and $\\sigma^2$ w.r.t $x$:\n\\begin{align*}\n    \\frac{\\partial \\mu}{\\partial \\mathbf{x}} &= \\frac{1}{D} \\mathds{1}^\\top \\\\\n    \\frac{\\partial \\sigma^2}{\\partial \\mathbf{x}} &= \\frac{1}{D} \\sum_d 2(x_d - \\mu) \\frac{\\partial}{\\partial \\mathbf{x}}(x_d - \\mu) \\\\\n    &= \\frac{2}{D} \\sum_d (x_d - \\mu)(\\mathbf{e}_d - \\frac{1}{D}\\mathds{1})^\\top \\\\\n    &= \\frac{2}{D} \\bigg[\\sum_d (x_d - \\mu) \\mathbf{e}_d - \\frac{1}{D} \\mathds{1} \\sum_d (x_d - \\mu)\\bigg]^\\top \\\\\n    &= \\frac{2}{D}\\sum_d (x_d - \\mu) \\mathbf{e}_d^\\top \\\\\n    &= \\frac{2}{D}(\\mathbf{x} - \\mu)^\\top\n\\end{align*}\nwhere $\\mathbf{e}_d \\in \\mathbb{R}^D$ is a one-hot vector with $1$ at the $d$th element. Note the penultimate equality holds because $\\sum_d (x_d - \\mu) = 0$.\n\nNow the derivative of $\\text{LN}(\\mathbf{x})_d$, the $d$th element of $\\text{LN}(\\mathbf{x})$, w.r.t.$\\mathbf{x}$ is\n\\begin{align*}\n    \\frac{\\partial \\text{LN}(\\mathbf{x})_d}{\\partial \\mathbf{x}} &= \\gamma_d \\bigg[\\frac{\\partial}{\\partial \\mathbf{x}}(x_d - \\mu)(\\sigma^2 + \\epsilon)^{-\\frac{1}{2}} + (x_d - \\mu)\\Big(-\\frac{1}{2}(\\sigma^2 + \\epsilon)^{-\\frac{3}{2}}\\Big)\\frac{\\partial \\sigma^2}{\\partial \\mathbf{x}}\\bigg] \\\\\n    &= \\gamma_d (\\sigma^2 + \\epsilon)^{-\\frac{1}{2}} \\bigg[(\\mathbf{e}_d - \\frac{1}{D}\\mathds{1})^\\top - \\frac{1}{2} (x_d - \\mu)(\\sigma^2 + \\epsilon)^{-1} \\frac{2}{D}(\\mathbf{x} - \\mu)^\\top \\bigg] \\\\\n    &= \\gamma_d (\\sigma^2 + \\epsilon)^{-\\frac{1}{2}} \\bigg[(\\mathbf{e}_d - \\frac{1}{D}\\mathds{1})^\\top - \\frac{1}{D} (\\sigma^2 + \\epsilon)^{-1} (x_d - \\mu)(\\mathbf{x} - \\mu)^\\top \\bigg].\n\\end{align*}\n\nHence\n\\begin{align*}\n    \\frac{\\partial \\text{LN}(\\mathbf{x})}{\\partial \\mathbf{x}} &= (\\sigma^2 + \\epsilon)^{-\\frac{1}{2}} \\bigg[ \\text{diag}(\\boldsymbol\\gamma) - \\frac{1}{D}\\boldsymbol\\gamma \\mathds{1}^\\top - \\frac{1}{D} (\\sigma^2 + \\epsilon)^{-1}\\text{diag}(\\boldsymbol\\gamma)(\\mathbf{x} - \\mu)(\\mathbf{x} - \\mu)^\\top \\bigg].\n\\end{align*}\n\nNote\n\\begin{equation*}\n    \\text{diag}(\\boldsymbol\\gamma) - \\frac{1}{D}\\boldsymbol\\gamma \\mathds{1}^\\top = \\begin{bmatrix}\n    \\gamma_1 (D-1)/D & -\\gamma_1 / D & \\dots & -\\gamma_1 / D \\\\\n    - \\gamma_2 / D & \\gamma_2 (D-1)/D & \\dots & -\\gamma_2 / D \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    -\\gamma_D / D & -\\gamma_D / D & \\dots & \\gamma_D (D-1)/D\n    \\end{bmatrix},\n\\end{equation*}\n\nhence\n\\begin{equation} \\label{eq:first_terms_inf_norm}\n    \\left\\Vert \\text{diag}(\\boldsymbol\\gamma) - \\frac{1}{D}\\boldsymbol\\gamma \\mathds{1}^\\top \\right\\Vert_{\\infty} = \\frac{2(D-1)}{D}\\max_d |\\gamma_d|,\n\\end{equation}\nrecalling that $\\left\\Vert \\cdot \\right\\Vert_{\\infty}$ is the maximum absolute row sum.\n\nLet $z_d \\coloneqq x_d - \\mu$. Hence $\\sum_d z_d = 0$, $\\sigma^2 = \\frac{1}{D} \\sum_d z_d^2$ and\n\\begin{equation*}\n    \\mathrm{Cov}(\\mathbf{x}) = \n    (\\mathbf{x} - \\mu)(\\mathbf{x} - \\mu)^\\top =\n    \\begin{bmatrix}\n    z_1^2 & \\dots & z_1 z_D \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    z_D z_1 & \\dots & z_D^2 \\\\\n    \\end{bmatrix}.\n\\end{equation*}\n\nHence\n\\begin{equation*}\n\\frac{\\left\\Vert \\mathrm{Cov}(\\mathbf{x})\\right\\Vert_{\\infty}}{\\sigma^2} = \\frac{\\max_d |z_d| \\sum_{d'} |z_{d'}|}{\\frac{1}{D}\\sum_d z_d^2}.    \n\\end{equation*}\nNoting that this expression is scale-invariant in $\\mathbf{z}$, we may assume WLOG $\\max_d |z_d| = z_D = 1$, since we are assuming not all $x_d$ are equal and hence at least one $z_d$ is non-zero.\n\nThe expression now becomes\n\\begin{equation} \\label{eq:cov_expression}\n\\frac{\\left\\Vert \\mathrm{Cov}(\\mathbf{x})\\right\\Vert_{\\infty}}{\\sigma^2} =  D\\bigg(\\frac{1 + \\sum_{d < D} |z_d|}{1 + \\sum_{d<D} z_d^2}\\bigg). \n\\end{equation}\nSince all terms $|z_d| \\leq 1$ are bounded, this continuous expression reaches a global maximum for some value of $\\mathbf{z}$ with $z_D = 1$.\n\nIt is easy to see that at the global maximum, $z_d \\neq 0$ $\\forall d$:\nsuppose this were to be true, WLOG $z_1 = 0$.\nThen let us see how the quantity \\eqref{eq:cov_expression} changes when $z_1=0$ is increased by $0< \\delta < 1$ and $z_D=1$ is decreased by $\\delta$, keeping the sum constant.\nIt is easy to see that the numerator $\\sum_d |z_d|$ stays constant, but the denominator $\\sum_d z_d^2$ changes by $2\\delta^2 - 2\\delta < 0$.\nSince for small $\\delta$, the numerator of \\eqref{eq:cov_expression} stays constant but the denominator decreases, the quantity \\eqref{eq:cov_expression} increases, contradicting that the global max is obtained for $z_1 = 0$.\nHence we may assume that $z_d \\neq 0$ $\\forall d$.\n\nHence the quantity \\eqref{eq:cov_expression} (in particular, $\\sum_d {|z_d|}$) is differentiable at the global maximum, at which the partial derivatives of the following Lagrangian are zero:\n\\begin{equation*}\n    \\mathcal{L}(z_1,\\ldots,z_{D-1}, \\lambda) = \\frac{1 + \\sum_{d < D} |z_d|}{1 + \\sum_{d<D} z_d^2} - \\lambda(\\sum_{d<D} z_d + 1).\n\\end{equation*}\nFrom now on let us write $\\sum$ for $\\sum_{d < D}$ below to reduce clutter. Setting $\\frac{\\partial \\mathcal{L}}{\\partial z_k} = 0$ and noting $\\frac{d|z_k|}{dz_k} = \\text{sgn}(z_k)$, we obtain\n\\begin{align*}\n    & \\frac{\\text{sgn}(z_k)(1 + \\sum z_d^2) - 2z_k(1 + \\sum |z_d|)}{(1+\\sum z_d^2)^2} - \\lambda = 0 \\\\\n    \\iff & \\text{sgn}(z_k)(1 + \\sum z_d^2) - 2z_k(1 + \\sum |z_d|) = \\lambda (1+\\sum z_d^2)^2 \\\\\n    \\iff & z_k = \\frac{\\text{sgn}(z_k)(1 + \\sum z_d^2) - \\lambda (1+\\sum z_d^2)^2}{2(1 + \\sum |z_d|)} \\\\\n    \\iff & z_k = \\frac{(\\text{sgn}(z_k) - \\lambda (1+\\sum z_d^2))(1 + \\sum z_d^2)}{2(1 + \\sum |z_d|)}\n\\end{align*}\nHence at the global maximum, $z_k$ takes one of two values $a > 0$ and $b < 0$.\nFurther we have that\n\\begin{align} \\label{eq:max_expression}\n    \\frac{1 + \\sum|z_d|}{1 + \\sum z_d^2} = \\frac{\\text{sgn}(z_k) - \\lambda (1+\\sum z_d^2)}{2z_k}\n\\end{align}\nIf both $a$ and $b$ are among the $z_k$, we have that $\\frac{1 - \\lambda (1+\\sum z_d^2)}{2a} = \\frac{- 1 - \\lambda (1+\\sum z_d^2)}{2b}$.\nSolving for $\\lambda(1+\\sum z_d^2)$ and plugging it in back to Equation \\eqref{eq:max_expression}, we get:\n\\begin{align*}\n    \\frac{1 + \\sum|z_d|}{1 + \\sum z_d^2} = \\frac{1}{a-b}\n\\end{align*}\nSince $a > 0$, $b < 0$ and $\\sum z_d = -1$, $a-b$ is minimised when only one of the $z_d$ is $a$ and the rest are $b$. Hence a crude lower bound on $a-b$ is $\\frac{1}{D-2}$, giving a bound:\n\\begin{equation} \\label{eq:second_term_inf_norm}\n    \\frac{\\left\\Vert \\mathrm{Cov}(\\mathbf{x})\\right\\Vert_{\\infty}}{\\sigma^2}\n    \\leq D(D-2)\n\\end{equation}\n\nHowever we conjecture that the true global maximum is attained when $z_d = - \\frac{1}{D-1}$ $\\forall d < D$ (i.e. all the $z_d$ for $d < D$ are equal to $b < 0$), for which it is easy to show that $\\frac{1 + \\sum_{d < D} |z_d|}{1 + \\sum_{d<D} z_d^2} = 2(D-1)/D$.\n\nPutting together the above, we have:\n\\begin{align*}\n    \\left\\Vert \\frac{\\partial \\text{LN}(\\mathbf{x})}{\\partial \\mathbf{x}} \\right\\Vert_{\\infty} &= (\\sigma^2 + \\epsilon)^{-\\frac{1}{2}} \\left\\Vert   \\text{diag}(\\boldsymbol\\gamma) - \\frac{1}{D}\\boldsymbol\\gamma \\mathds{1}^\\top - \\frac{1}{D} (\\sigma^2 + \\epsilon)^{-1}\\text{diag}(\\boldsymbol\\gamma)(\\mathbf{x} - \\mu)(\\mathbf{x} - \\mu)^\\top \\right\\Vert_{\\infty} \\\\\n    &\\leq \\epsilon^{-\\frac{1}{2}} \\bigg( \\left\\Vert \\text{diag}(\\boldsymbol\\gamma) - \\frac{1}{D}\\boldsymbol\\gamma \\mathds{1}^\\top \\right\\Vert_{\\infty} + \\frac{1}{D}\\left\\Vert  \\text{diag}(\\boldsymbol\\gamma) \\right\\Vert_{\\infty} \\left\\Vert  (\\sigma^2 + \\epsilon)^{-1}(\\mathbf{x} - \\mu)(\\mathbf{x} - \\mu)^\\top \\right\\Vert_{\\infty} \\bigg) \\\\\n    &\\leq \\epsilon^{-\\frac{1}{2}} \\bigg( \\left\\Vert \\text{diag}(\\boldsymbol\\gamma) - \\frac{1}{D}\\boldsymbol\\gamma \\mathds{1}^\\top \\right\\Vert_{\\infty} + \\frac{1}{D}\\left\\Vert  \\text{diag}(\\boldsymbol\\gamma) \\right\\Vert_{\\infty} \\left\\Vert  \\mathrm{Cov}(\\mathbf{x}) / \\sigma^2 \\right\\Vert_{\\infty} \\bigg) \\\\\n    &\\leq \\epsilon^{-\\frac{1}{2}} \\bigg( \\frac{2(D-1)}{D}\\max_d |\\gamma_d| + \\frac{1}{D} \\max_d |\\gamma_d| D(D-2) \\bigg) \\\\\n    &= \\epsilon^{-\\frac{1}{2}} \\max_d |\\gamma_d| \\bigg(\\frac{2(D-1)}{D} + D-2 \\bigg) \\\\\n    &= \\epsilon^{-\\frac{1}{2}} \\max_d |\\gamma_d| \\bigg(\\frac{D^2-2}{D}\\bigg).\n\\end{align*}\n\n\n}\n\\end{document}\n\n==== END OF /2006.04710/main.tex ====",
            "statements": {
                "definitions": [
                    {
                        "statement_id": "a0589e9a-e69c-4a1b-addc-72a7905f06eb",
                        "paper_id": "083fc279-e9c8-48f3-94e5-2b3a363832b3",
                        "library_nr": 1,
                        "library_name": "Definition 1",
                        "title": "Lipschitz Continuity",
                        "statement_original_tex": "\\begin{definition}\\label{Lipschitz_definition}\nGiven two metric spaces $(\\mathcal{X}, d_{\\mathcal{X}})$ and $(\\mathcal{Y}, d_{\\mathcal{Y}})$, a function $f:\\mathcal{X} \\rightarrow \\mathcal{Y}$ is called \\textit{Lipschitz continuous} (or $K$-\\textit{Lipschitz}) if there exists a constant $K\\geq 0$ such that \n\\begin{equation}\nd_{\\mathcal{Y}}(f(\\mathbf{x}),f(\\mathbf{x'})) \\leq K d_{\\mathcal{X}}(\\mathbf{x},\\mathbf{x'}) \\spaces\\spaces \\text{for all } \\mathbf{x},\\mathbf{x'} \\in \\mathcal{X}.\n\\end{equation}\nThe smallest such $K$ is the \\textit{Lipschitz constant} of $f$, denoted $\\lip(f)$.\n\\end{definition}",
                        "statement_html": "Given two metric spaces $(\\mathcal{X}, d_{\\mathcal{X}})$ and $(\\mathcal{Y}, d_{\\mathcal{Y}})$, a function $f:\\mathcal{X} \\rightarrow \\mathcal{Y}$ is called $\\textit{Lipschitz continuous}$ (or $K$-$\\textit{Lipschitz}$) if there exists a constant $K\\geq 0$ such that \n\\begin{equation}\nd_{\\mathcal{Y}}(f(\\mathbf{x}),f(\\mathbf{x'})) \\leq K d_{\\mathcal{X}}(\\mathbf{x},\\mathbf{x'}) \\spaces\\spaces \\text{for all } \\mathbf{x},\\mathbf{x'} \\in \\mathcal{X}.\n\\end{equation}\nThe smallest such $K$ is the $\\textit{Lipschitz constant}$ of $f$, denoted $\\lip(f)$.",
                        "statement_type": "definition",
                        "statement_motivation_html": "Lipschitz continuity is a crucial concept in analysis and geometry, providing a way to control how much a function can stretch distances. It is particularly useful in numerical analysis, optimization, and differential equations, where ensuring that small changes in input lead to proportionally small changes in output is essential. The Lipschitz constant $K$ gives a precise measure of this control, making it easier to analyze the stability and convergence of algorithms.",
                        "html_url": "library/definitions/definition_1/index.html"
                    }
                ],
                "axioms": [],
                "lemmas": [
                    {
                        "statement_id": "194c3400-773c-45af-8af4-613495fc57c5",
                        "paper_id": "083fc279-e9c8-48f3-94e5-2b3a363832b3",
                        "library_nr": 1,
                        "library_name": "Lemma 1",
                        "title": "Lipschitz Composition Lemma",
                        "statement_original_tex": "\\begin{lemma}[\\citealp{federer1969geometric}]\nLet $g,h$ be two composable Lipschitz functions. Then $g \\circ h$ is also Lipschitz with $\\lip(g \\circ h) \\leq \\lip(g) \\lip(h)$.\n\\end{lemma}",
                        "statement_html": "Let $g,h$ be two composable Lipschitz functions. Then $g \\circ h$ is also Lipschitz with $\\lip(g \\circ h) \\leq \\lip(g) \\lip(h)$.",
                        "statement_type": "lemma",
                        "statement_motivation_html": "The composition of Lipschitz functions theorem is useful in analysis and applied mathematics, particularly when dealing with functions that have bounded rates of change. If you know that two functions $g$ and $h$ are Lipschitz, this theorem allows you to conclude that their composition $g \\circ h$ is also Lipschitz. This is particularly valuable in scenarios where you need to ensure that the composed function does not exhibit wild fluctuations, which is crucial in stability analysis, numerical methods, and various applications in differential equations and optimization.",
                        "html_url": "library/lemmas/lemma_1/index.html",
                        "corollary_ids": [
                            "538626a5-cda2-4ab3-90d7-6dc79ff8a845"
                        ],
                        "proof": {
                            "statement_id": "d5ad0dac-bcef-43e4-88af-bd0fb4bf5d71",
                            "paper_id": null,
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "See proof at:\nFederer, \\textit{H. Geometric Measure Theory}. Classics in Mathematics. Springer Berlin Heidelberg, 1969. ISBN 9783642620102.",
                            "statement_html": "See proof at:\nFederer, $\\textit{H. Geometric Measure Theory}$. Classics in Mathematics. Springer Berlin Heidelberg, 1969. ISBN 9783642620102.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "The proof in Federer's \"Geometric Measure Theory\" involves several key steps:\n\n1. <i></i>Preliminaries and Definitions<i></i>: The proof begins by establishing the necessary definitions and preliminary results. This includes defining key concepts such as measures, measurable sets, and geometric properties relevant to the theorem.\n\n2. <i></i>Construction of Auxiliary Functions<i></i>: Auxiliary functions are constructed to facilitate the manipulation and analysis of the geometric objects under consideration. These functions often help in approximating or bounding the measures of sets.\n\n3. <i></i>Application of Geometric Inequalities<i></i>: The proof employs various geometric inequalities that relate different measures and dimensions. These inequalities are crucial for deriving bounds and relationships between the quantities involved.\n\n4. <i></i>Integration and Measure Theory Techniques<i></i>: Techniques from integration and measure theory are applied to handle the complexities of the geometric objects. This includes using integrals to sum over continuous distributions and applying measure-theoretic results to ensure the validity of the steps.\n\n5. <i></i>Limit Processes and Convergence<i></i>: The proof often involves taking limits and ensuring the convergence of sequences of functions or sets. This step is essential for transitioning from finite approximations to the actual infinite or continuous case.\n\n6. <i></i>Conclusion and Verification<i></i>: Finally, the proof concludes by verifying that all the conditions of the theorem are satisfied and that the desired result follows logically from the established steps.\n\nEach of these steps is meticulously detailed in Federer's text, providing a comprehensive and rigorous foundation for the theorem in question."
                        }
                    },
                    {
                        "statement_id": "6eef858c-f19b-4ecf-997c-94ebeaf4dd62",
                        "paper_id": "083fc279-e9c8-48f3-94e5-2b3a363832b3",
                        "library_nr": 2,
                        "library_name": "Lemma 2",
                        "title": "Non-Lipschitz Condition for Distinct Full Rank Matrices",
                        "statement_original_tex": "\\begin{lemma} \\label{lemma:tie_weights}\nIf $W^K \\in \\mathbb{R}^{D \\times D/H}$ is full rank (i.e.~full column rank), and $W^K \\neq W^Q$, then $J_{ij}$ has terms that are unbounded for $i \\neq j$, hence $\\tilde{f}$ is \\emph{not} Lipschitz. \n\\end{lemma}",
                        "statement_html": "If $W^K \\in \\mathbb{R}^{D \\times D/H}$ is full rank (i.e.~full column rank), and $W^K \\neq W^Q$, then $J_{ij}$ has terms that are unbounded for $i \\neq j$, hence $\\tilde{f}$ is $\\emph{not}$ Lipschitz.",
                        "statement_type": "lemma",
                        "statement_motivation_html": "This statement is useful in the context of analyzing the behavior of certain functions, particularly in machine learning and optimization. If $W^K$ is full rank and distinct from $W^Q$, it indicates that the function $\\tilde{f}$ can exhibit unbounded differences between its outputs for different inputs, meaning it is not Lipschitz continuous. This insight is crucial when assessing the stability and convergence properties of algorithms, as Lipschitz continuity often guarantees more predictable and controlled behavior.",
                        "html_url": "library/lemmas/lemma_2/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "6cd2e035-9280-43a4-8e9c-ea2ddb511653",
                            "paper_id": "083fc279-e9c8-48f3-94e5-2b3a363832b3",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\n\nLet us investigate the expression $\\tilde{K}_{ij} \\coloneqq P_{ij} W^{K^\\top}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k)(\\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K) \\in \\mathbb{R}^{\\frac{D}{H} \\times \\frac{D}{H}}$ for $i\\neq j$, which is related to $\\tilde{J}_{ij}$ as follows by Equation \\eqref{eq:jij}:\n\n\\begin{equation*}\n    W^{K^\\top} \\tilde{J}_{ij} = \\left(\\frac{2}{\\sqrt{D/H}} \\tilde{K}_{ij} + P_{ij}I \\right)\n    W^{K^\\top}.\n\\end{equation*}\n\nIt suffices to show that $\\tilde{K}_{ij}$ is unbounded to show that $\\tilde{J}_{ij}$ is unbounded, since $W^K$ is full rank and $P_{ij} \\in [0,1]$. \n\nLet $\\mathbf{y}_j^\\top = \\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K$. \nThen we have:\n\\begin{align*}\n    \\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k \n    &= W^{Q^\\top}\\mathbf{x}_i - W^{K^\\top}\\mathbf{x}_j - \\sum_k P_{ik} (W^{Q^\\top}\\mathbf{x}_i - W^{K^\\top}\\mathbf{x}_k)\\\\\n    &= W^{Q^\\top}\\mathbf{x}_i - W^{K^\\top}\\mathbf{x}_j - (W^{Q^\\top}\\mathbf{x}_i - \\sum_k P_{ik} W^{K^\\top}\\mathbf{x}_k) \\\\\n    &= - W^{K^\\top}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k).\n\\end{align*}\nHence $\\tilde{K}_{ij} = - P_{ij} (\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k) \\mathbf{y}_j^\\top$.\nNote $\\mathbf{y}_i$ can take an arbitrary value in $\\mathbb{R}^{D/H}$, since $W^K \\neq W^Q$ and $W^K$ is full-rank.\n\nFor all $j \\neq i$, let us choose $\\mathbf{x}_j$ such that $\\mathbf{y}_j = -\\mathbf{y}_i$. This is possible for any value of $\\mathbf{y}_i$ since $W^K$ is full-rank.\nNote $\\mathbf{y}_j = - \\mathbf{y}_i$ and not $\\mathbf{y}_i$.\nWe then have that $\\|\\mathbf{y}_j\\|_2^2$ is equal for all $j$, hence $P_{ij} \\coloneqq \\frac{\\exp(-\\|\\mathbf{y}_j\\|_2^2)}{\\sum_k \\exp(-\\|\\mathbf{y}_k\\|_2^2)} = \\frac{1}{N}$ for all $j$. \nThen for $i \\neq j$, $\\tilde{K}_{ij}$ simplifies to\n\\begin{equation*}\n\\tilde{K}_{ij} = - \\frac{1}{N} \\left(-\\mathbf{y}_i - \\frac{1}{N} (N-2) (-\\mathbf{y}_i)\\right) (-\\mathbf{y}_i)^\\top  = - \\frac{2N-2}{N^2} \\mathbf{y}_i \\mathbf{y}_i^\\top \n\\end{equation*}\nwhose entries are unbounded since $\\mathbf{y}_i$ can be any vector in $\\mathbb{R}^{D/H}$ (note we assume $N \\geq 2$ for self-attention to be well-defined, hence $2N-2 \\neq 0$).\n\\end{proof}",
                            "statement_html": "Let us investigate the expression $\\tilde{K}_{ij} \\coloneqq P_{ij} W^{K^\\top}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k)(\\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K) \\in \\mathbb{R}^{\\frac{D}{H} \\times \\frac{D}{H}}$ for $i\\neq j$, which is related to $\\tilde{J}_{ij}$ as follows by Equation (20) [see original paper]:\n\n\\begin{equation*}\n    W^{K^\\top} \\tilde{J}_{ij} = \\left(\\frac{2}{\\sqrt{D/H}} \\tilde{K}_{ij} + P_{ij}I \\right)\n    W^{K^\\top}.\n\\end{equation*}\n\nIt suffices to show that $\\tilde{K}_{ij}$ is unbounded to show that $\\tilde{J}_{ij}$ is unbounded, since $W^K$ is full rank and $P_{ij} \\in [0,1]$. \n\nLet $\\mathbf{y}_j^\\top = \\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K$. \nThen we have:\n\\begin{align*}\n    \\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k \n    &= W^{Q^\\top}\\mathbf{x}_i - W^{K^\\top}\\mathbf{x}_j - \\sum_k P_{ik} (W^{Q^\\top}\\mathbf{x}_i - W^{K^\\top}\\mathbf{x}_k)\\\\\n    &= W^{Q^\\top}\\mathbf{x}_i - W^{K^\\top}\\mathbf{x}_j - (W^{Q^\\top}\\mathbf{x}_i - \\sum_k P_{ik} W^{K^\\top}\\mathbf{x}_k) \\\\\n    &= - W^{K^\\top}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k).\n\\end{align*}\nHence $\\tilde{K}_{ij} = - P_{ij} (\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k) \\mathbf{y}_j^\\top$.\nNote $\\mathbf{y}_i$ can take an arbitrary value in $\\mathbb{R}^{D/H}$, since $W^K \\neq W^Q$ and $W^K$ is full-rank.\n\nFor all $j \\neq i$, let us choose $\\mathbf{x}_j$ such that $\\mathbf{y}_j = -\\mathbf{y}_i$. This is possible for any value of $\\mathbf{y}_i$ since $W^K$ is full-rank.\nNote $\\mathbf{y}_j = - \\mathbf{y}_i$ and not $\\mathbf{y}_i$.\nWe then have that $\\|\\mathbf{y}_j\\|_2^2$ is equal for all $j$, hence $P_{ij} \\coloneqq \\frac{\\exp(-\\|\\mathbf{y}_j\\|_2^2)}{\\sum_k \\exp(-\\|\\mathbf{y}_k\\|_2^2)} = \\frac{1}{N}$ for all $j$. \nThen for $i \\neq j$, $\\tilde{K}_{ij}$ simplifies to\n\\begin{equation*}\n\\tilde{K}_{ij} = - \\frac{1}{N} \\left(-\\mathbf{y}_i - \\frac{1}{N} (N-2) (-\\mathbf{y}_i)\\right) (-\\mathbf{y}_i)^\\top  = - \\frac{2N-2}{N^2} \\mathbf{y}_i \\mathbf{y}_i^\\top \n\\end{equation*}\nwhose entries are unbounded since $\\mathbf{y}_i$ can be any vector in $\\mathbb{R}^{D/H}$ (note we assume $N \\geq 2$ for self-attention to be well-defined, hence $2N-2 \\neq 0$).",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break down the steps involved:\n\n1. <i></i>Expression Definition<i></i>:\n   We start by defining the expression \\(\\tilde{K}_{ij}\\):\n   \\[\n   \\tilde{K}_{ij} \\coloneqq P_{ij} W^{K^\\top}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k)(\\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K) \\in \\mathbb{R}^{\\frac{D}{H} \\times \\frac{D}{H}}\n   \\]\n   for \\(i \\neq j\\). This expression is related to \\(\\tilde{J}_{ij}\\) by the equation:\n   \\[\n   W^{K^\\top} \\tilde{J}_{ij} = \\left(\\frac{2}{\\sqrt{D/H}} \\tilde{K}_{ij} + P_{ij}I \\right) W^{K^\\top}.\n   \\]\n\n2. <i></i>Objective<i></i>:\n   The goal is to show that \\(\\tilde{K}_{ij}\\) is unbounded, which would imply that \\(\\tilde{J}_{ij}\\) is also unbounded, given that \\(W^K\\) is full rank and \\(P_{ij} \\in [0,1]\\).\n\n3. <i></i>Substitution<i></i>:\n   Define \\(\\mathbf{y}_j^\\top = \\mathbf{x}_i^\\top W^Q - \\mathbf{x}_j^\\top W^K\\). Then:\n   \\[\n   \\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k = W^{Q^\\top}\\mathbf{x}_i - W^{K^\\top}\\mathbf{x}_j - \\sum_k P_{ik} (W^{Q^\\top}\\mathbf{x}_i - W^{K^\\top}\\mathbf{x}_k).\n   \\]\n\n4. <i></i>Simplification<i></i>:\n   Simplify the above expression:\n   \\[\n   \\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k = - W^{K^\\top}(\\mathbf{x}_j - \\sum_k P_{ik} \\mathbf{x}_k).\n   \\]\n   Hence:\n   \\[\n   \\tilde{K}_{ij} = - P_{ij} (\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k) \\mathbf{y}_j^\\top.\n   \\]\n\n5. <i></i>Arbitrary Value<i></i>:\n   Note that \\(\\mathbf{y}_i\\) can take any value in \\(\\mathbb{R}^{D/H}\\) since \\(W^K \\neq W^Q\\) and \\(W^K\\) is full-rank.\n\n6. <i></i>Choice of \\(\\mathbf{x}_j\\)<i></i>:\n   For all \\(j \\neq i\\), choose \\(\\mathbf{x}_j\\) such that \\(\\mathbf{y}_j = -\\mathbf{y}_i\\). This is possible for any value of \\(\\mathbf{y}_i\\) since \\(W^K\\) is full-rank.\n\n7. <i></i>Probability Simplification<i></i>:\n   Given \\(\\mathbf{y}_j = - \\mathbf{y}_i\\), the norm \\(\\|\\mathbf{y}_j\\|_2^2\\) is equal for all \\(j\\). Thus:\n   \\[\n   P_{ij} \\coloneqq \\frac{\\exp(-\\|\\mathbf{y}_j\\|_2^2)}{\\sum_k \\exp(-\\|\\mathbf{y}_k\\|_2^2)} = \\frac{1}{N}\n   \\]\n   for all \\(j\\).\n\n8. <i></i>Final Simplification<i></i>:\n   For \\(i \\neq j\\), \\(\\tilde{K}_{ij}\\) simplifies to:\n   \\[\n   \\tilde{K}_{ij} = - \\frac{1}{N} \\left(-\\mathbf{y}_i - \\frac{1}{N} (N-2) (-\\mathbf{y}_i)\\right) (-\\mathbf{y}_i)^\\top = - \\frac{2N-2}{N^2} \\mathbf{y}_i \\mathbf{y}_i^\\top.\n   \\]\n   The entries of \\(\\tilde{K}_{ij}\\) are unbounded since \\(\\mathbf{y}_i\\) can be any vector in \\(\\mathbb{R}^{D/H}\\) (assuming \\(N \\geq 2\\) for self-attention to be well-defined, hence \\(2N-2 \\neq 0\\))."
                        }
                    },
                    {
                        "statement_id": "ac15f238-f0d7-4541-bd58-18f3bd73c6d6",
                        "paper_id": "083fc279-e9c8-48f3-94e5-2b3a363832b3",
                        "library_nr": 3,
                        "library_name": "Lemma 3",
                        "title": "Covariance Trace Bound",
                        "statement_original_tex": "\\begin{lemma} \\label{lemma:key}\n$\\Tr(\\mathrm{Cov}(\\mathbb{Y})) = \\sum_j P_{ij}\\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_2^2 \\leq \\sum_j P_{ij}\\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2 \\leq \\phi^{-1}(N-1)$ where $\\phi(c) = c \\exp(c+1)$ is a one-dimensional invertible function on $\\mathbb{R}_{\\geq 0}$.\n\\end{lemma}",
                        "statement_html": "$\\Tr(\\mathrm{Cov}(\\mathbb{Y})) = \\sum_j P_{ij}\\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_2^2 \\leq \\sum_j P_{ij}\\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2 \\leq \\phi^{-1}(N-1)$ where $\\phi(c) = c \\exp(c+1)$ is a one-dimensional invertible function on $\\mathbb{R}_{\\geq 0}$.",
                        "statement_type": "lemma",
                        "statement_motivation_html": "This inequality is useful in the context of covariance analysis and clustering. It provides an upper bound on the trace of the covariance matrix, which can be used to assess the spread of data points around their mean. This is particularly helpful in clustering algorithms to ensure that clusters are well-formed and not overly dispersed.",
                        "html_url": "library/lemmas/lemma_3/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "45a0436c-7ffa-4166-830a-2cf8a483ea31",
                            "paper_id": "083fc279-e9c8-48f3-94e5-2b3a363832b3",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\nThe first equality holds since $\\Tr(\\mathrm{Cov}(\\mathbb{Y})) = \\sum_j \\mathrm{Cov}(\\mathbb{Y})_{jj} = \\sum_j \\mathrm{Var}(\\mathbb{Y}_j) = \\sum_j \\mathbb{E}[(\\mathbb{Y}_j -\\mathbb{E}[\\mathbb{Y}_j])^2]$. \nThe next inequality holds since $\\mathrm{Var}(\\mathbb{Y}_j) = \\mathrm{Var}(\\overline{\\mathbb{Y}}_j) = \\mathbb{E}[\\overline{\\mathbb{Y}}_j^2] -\\mathbb{E}[\\overline{\\mathbb{Y}}_j]^2 \\leq \\mathbb{E}[\\overline{\\mathbb{Y}}_j^2]$ where $\\overline{\\mathbb{Y}}= \\mathbb{Y} - y_i$. \nThe final inequality can be proved as follows.\n\nWe would like to bound \n\\begin{equation}\n    \\sum_j P_{ij}\\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2 = \\frac{\\sum_j \\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2 \\exp(-\\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2)}{\\sum_k \\exp(-\\|\\mathbf{y}_k-\\mathbf{y}_i\\|_2^2)}  = \\frac{\\sum_j z_j^2 \\exp(-z_j^2)}{\\sum_k \\exp(-z_k^2)}\n\\end{equation}\nwhere $z_j \\coloneqq \\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2$ (hence $z_i=0$). \nDefine:\n\\begin{equation}\n    g(\\mathbf{z}) \\coloneqq \\frac{\\sum_j z_j^2 \\exp(-z_j^2)}{\\sum_k \\exp(-z_k^2)} = \\frac{\\sum_{j \\neq i} z_j^2 \\exp(-z_j^2)}{1 + \\sum_{k \\neq i} \\exp(-z_k^2)}.\n\\end{equation}\nFirst note that as $z_j \\rightarrow \\infty$, $\\exp(-z_j^2) \\rightarrow 0$ exponentially fast, causing the product $z_j^2 \\exp(-z_j^2) \\rightarrow 0$.\nHence we expect the above quantity to be bounded and attain its maximum.\n\nLet $h(z_j) \\coloneqq \\exp(-z_j^2)$ for notational conciseness, and note $h(z_j) > 0$. By taking partial derivatives with the chain rule, we have that for $j \\neq i$\n\\begin{equation}\n\\frac{\\partial g(\\mathbf{z})}{\\partial z_j} = \\frac{2z_j h(z_j)}{(\\sum_k h(z_k))^2}\\left[(1-z_j^2)\\sum_k h(z_k) + \\sum_k h(z_k)z_k^2\\right].\n\\end{equation}\nHence the derivative is $0$ if and only if $z_j = 0$ or $(1-z_j^2)\\sum_k h(z_k) + \\sum_k h(z_k)z_k^2 = 0$, the latter being equivalent to $z_j^2 = 1 + \\frac{\\sum_k h(z_k)z_k^2}{\\sum_k h(z_k)} = 1 + g(\\mathbf{z})$. \nHence at the maximum, the non-zero values among $\\{z_j\\}_{j=1}^N$ must be equal to one another.\nIt is clear now that the maximum value $c$ is attained when $z_j^2 = 1 + c$ for $j \\neq i$ (and recall $z_i = 0$). \nSo $h(z_j) = \\exp(-1-c)$ for $j \\neq i$.\nSubstituting this into $g(z)$, and rearranging, we obtain $c \\exp(c+1) = N - 1$. Note $\\phi(x) \\coloneqq x \\exp(x+1)$ is increasing for $x > 0$ hence $c = \\phi^{-1}(N-1)$.\n\\end{proof}",
                            "statement_html": "The first equality holds since $\\Tr(\\mathrm{Cov}(\\mathbb{Y})) = \\sum_j \\mathrm{Cov}(\\mathbb{Y})_{jj} = \\sum_j \\mathrm{Var}(\\mathbb{Y}_j) = \\sum_j \\mathbb{E}[(\\mathbb{Y}_j -\\mathbb{E}[\\mathbb{Y}_j])^2]$. \nThe next inequality holds since $\\mathrm{Var}(\\mathbb{Y}_j) = \\mathrm{Var}(\\overline{\\mathbb{Y}}_j) = \\mathbb{E}[\\overline{\\mathbb{Y}}_j^2] -\\mathbb{E}[\\overline{\\mathbb{Y}}_j]^2 \\leq \\mathbb{E}[\\overline{\\mathbb{Y}}_j^2]$ where $\\overline{\\mathbb{Y}}= \\mathbb{Y} - y_i$. \nThe final inequality can be proved as follows.\n\nWe would like to bound \n\\begin{equation}\n    \\sum_j P_{ij}\\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2 = \\frac{\\sum_j \\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2 \\exp(-\\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2^2)}{\\sum_k \\exp(-\\|\\mathbf{y}_k-\\mathbf{y}_i\\|_2^2)}  = \\frac{\\sum_j z_j^2 \\exp(-z_j^2)}{\\sum_k \\exp(-z_k^2)}\n\\end{equation}\nwhere $z_j \\coloneqq \\|\\mathbf{y}_j-\\mathbf{y}_i\\|_2$ (hence $z_i=0$). \nDefine:\n\\begin{equation}\n    g(\\mathbf{z}) \\coloneqq \\frac{\\sum_j z_j^2 \\exp(-z_j^2)}{\\sum_k \\exp(-z_k^2)} = \\frac{\\sum_{j \\neq i} z_j^2 \\exp(-z_j^2)}{1 + \\sum_{k \\neq i} \\exp(-z_k^2)}.\n\\end{equation}\nFirst note that as $z_j \\rightarrow \\infty$, $\\exp(-z_j^2) \\rightarrow 0$ exponentially fast, causing the product $z_j^2 \\exp(-z_j^2) \\rightarrow 0$.\nHence we expect the above quantity to be bounded and attain its maximum.\n\nLet $h(z_j) \\coloneqq \\exp(-z_j^2)$ for notational conciseness, and note $h(z_j) > 0$. By taking partial derivatives with the chain rule, we have that for $j \\neq i$\n\\begin{equation}\n\\frac{\\partial g(\\mathbf{z})}{\\partial z_j} = \\frac{2z_j h(z_j)}{(\\sum_k h(z_k))^2}\\left[(1-z_j^2)\\sum_k h(z_k) + \\sum_k h(z_k)z_k^2\\right].\n\\end{equation}\nHence the derivative is $0$ if and only if $z_j = 0$ or $(1-z_j^2)\\sum_k h(z_k) + \\sum_k h(z_k)z_k^2 = 0$, the latter being equivalent to $z_j^2 = 1 + \\frac{\\sum_k h(z_k)z_k^2}{\\sum_k h(z_k)} = 1 + g(\\mathbf{z})$. \nHence at the maximum, the non-zero values among $\\{z_j\\}_{j=1}^N$ must be equal to one another.\nIt is clear now that the maximum value $c$ is attained when $z_j^2 = 1 + c$ for $j \\neq i$ (and recall $z_i = 0$). \nSo $h(z_j) = \\exp(-1-c)$ for $j \\neq i$.\nSubstituting this into $g(z)$, and rearranging, we obtain $c \\exp(c+1) = N - 1$. Note $\\phi(x) \\coloneqq x \\exp(x+1)$ is increasing for $x > 0$ hence $c = \\phi^{-1}(N-1)$.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "The proof involves several steps, each building on the previous one to establish the final result. Here is a breakdown of the different steps:\n\n1. <i></i>Trace of Covariance Matrix<i></i>:\n   The first equality is derived from the definition of the trace of the covariance matrix:\n   \\[\n   \\Tr(\\mathrm{Cov}(\\mathbb{Y})) = \\sum_j \\mathrm{Cov}(\\mathbb{Y})_{jj} = \\sum_j \\mathrm{Var}(\\\\mathbb{Y}_j) = \\sum_j \\mathbb{E}[(\\mathbb{Y}_j - \\mathbb{E}[\\mathbb{Y}_j])^2].\n   \\]\n\n2. <i></i>Variance Inequality<i></i>:\n   The next inequality uses the property of variance:\n   \\[\n   \\mathrm{Var}(\\mathbb{Y}_j) = \\mathrm{Var}(\\overline{\\mathbb{Y}}_j) = \\mathbb{E}[\\overline{\\mathbb{Y}}_j^2] - \\mathbb{E}[\\overline{\\mathbb{Y}}_j]^2 \\leq \\mathbb{E}[\\overline{\\mathbb{Y}}_j^2],\n   \\]\n   where \\(\\overline{\\mathbb{Y}} = \\mathbb{Y} - y_i\\).\n\n3. <i></i>Bounding the Sum<i></i>:\n   The goal is to bound the expression:\n   \\[\n   \\sum_j P_{ij}\\|\\mathbf{y}_j - \\mathbf{y}_i\\|_2^2 = \\frac{\\sum_j \\|\\mathbf{y}_j - \\mathbf{y}_i\\|_2^2 \\exp(-\\|\\mathbf{y}_j - \\mathbf{y}_i\\|_2^2)}{\\\\sum_k \\exp(-\\|\\mathbf{y}_k - \\mathbf{y}_i\\|_2^2)} = \\frac{\\sum_j z_j^2 \\exp(-z_j^2)}{\\sum_k \\exp(-z_k^2)},\n   \\]\n   where \\(z_j \\coloneqq \\|\\mathbf{y}_j - \\mathbf{y}_i\\|_2\\) and \\(z_i = 0\\).\n\n4. <i></i>Function Definition<i></i>:\n   Define the function:\n   \\[\n   g(\\mathbf{z}) \\coloneqq \\frac{\\sum_j z_j^2 \\exp(-z_j^2)}{\\sum_k \\exp(-z_k^2)} = \\frac{\\sum_{j \\neq i} z_j^2 \\exp(-z_j^2)}{1 + \\sum_{k \\neq i} \\exp(-z_k^2)}.\n   \\]\n\n5. <i></i>Behavior at Infinity<i></i>:\n   As \\(z_j \\rightarrow \\infty\\), \\(\\exp(-z_j^2) \\rightarrow 0\\) exponentially fast, causing \\(z_j^2 \\exp(-z_j^2) \\rightarrow 0\\). This suggests that the quantity is bounded and attains its maximum.\n\n6. <i></i>Partial Derivatives<i></i>:\n   Let \\(h(z_j) \\coloneqq \\exp(-z_j^2)\\). By taking partial derivatives using the chain rule, we get:\n   \\[\n   \\frac{\\partial g(\\mathbf{z})}{\\partial z_j} = \\frac{2z_j h(z_j)}{(\\sum_k h(z_k))^2}\\left[(1 - z_j^2)\\sum_k h(z_k) + \\sum_k h(z_k)z_k^2\\right].\n   \\]\n\n7. <i></i>Critical Points<i></i>:\n   The derivative is zero if and only if \\(z_j = 0\\) or \\((1 - z_j^2)\\sum_k h(z_k) + \\sum_k h(z_k)z_k^2 = 0\\), which simplifies to \\(z_j^2 = 1 + \\frac{\\sum_k h(z_k)z_k^2}{\\sum_k h(z_k)} = 1 + g(\\mathbf{z})\\).\n\n8. <i></i>Maximum Value<i></i>:\n   At the maximum, the non-zero values among \\(\\{z_j\\}_{j=1}^N\\) must be equal. The maximum value \\(c\\) is attained when \\(z_j^2 = 1 + c\\) for \\(j \\neq i\\) (and \\(z_i = 0\\)). Thus, \\(h(z_j) = \\exp(-1 - c)\\) for \\(j \\neq i\\).\n\n9. <i></i>Final Expression<i></i>:\n   Substituting into \\(g(z)\\) and rearranging, we get:\n   \\[\n   c \\exp(c + 1) = N - 1.\n   \\]\n   The function \\(\\phi(x) \\coloneqq x \\exp(x + 1)\\) is increasing for \\(x > 0\\), hence \\(c = \\phi^{-1}(N - 1)\\)."
                        }
                    },
                    {
                        "statement_id": "f47d629b-cbfa-4c6a-85ac-960378cc9ec4",
                        "paper_id": "083fc279-e9c8-48f3-94e5-2b3a363832b3",
                        "library_nr": 4,
                        "library_name": "Lemma 4",
                        "title": "Matrix Norm Bound Lemma",
                        "statement_original_tex": "\\begin{lemma}\\label{lemma:f3}\n$\\|Y^\\top P^{(i)}Y\\|_\\infty \\leq  \\phi^{-1}(N-1) \\sqrt{D/H} $ ($\\phi$ defined as in Lemma \\ref{lemma:key}).\n\\end{lemma}",
                        "statement_html": "$\\|Y^\\top P^{(i)}Y\\|_\\infty \\leq  \\phi^{-1}(N-1) \\sqrt{D/H} $ ($\\phi$ defined as in <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_3/index.html#lemma%3Akey\">Lemma 3</a>).",
                        "statement_type": "lemma",
                        "statement_motivation_html": "This inequality is useful in the context of matrix analysis and optimization. It provides an upper bound on the infinity norm of the product of a transposed matrix $Y^\\top$, a matrix $P^{(i)}$, and the matrix $Y$. This can be particularly helpful when assessing the stability and performance of algorithms that involve such matrix products, especially in high-dimensional spaces where direct computation might be infeasible.",
                        "html_url": "library/lemmas/lemma_4/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "9b81d604-abc7-48cf-b7b7-372ce82c5816",
                            "paper_id": "083fc279-e9c8-48f3-94e5-2b3a363832b3",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\nRecall that $Y^\\top P^{(i)}Y = \\mathrm{Cov}(\\mathbb{Y})$. Let $\\sigma(\\mathbb{Y}_m)$ denote the standard deviation of $\\mathbb{Y}_m$. Then $[\\mathrm{Cov}(\\mathbb{Y})]_{lm} \\leq \\sigma(\\mathbb{Y}_l)\\sigma(\\mathbb{Y}_m)$.\nHence \n\\begin{align*}\n\\|\\mathrm{Cov}(\\mathbb{Y})\\|_{\\infty} = \\max_l \\sum_m \\left|[\\mathrm{Cov}(\\mathbb{Y})]_{lm}\\right| \n& \\leq  \\max_l \\sigma(\\mathbb{Y}_l) \\sum_m \\sigma(\\mathbb{Y}_m) \\\\\n& \\leq \\sqrt{\\frac{D}{H}} \\sum_m \\sigma^2(\\mathbb{Y}_m)  = \\sqrt{\\frac{D}{H}} \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\\\\n& \\leq \\sqrt{\\frac{D}{H}} \\phi^{-1}(N-1),\n\\end{align*}\nsince $\\sum_m \\sigma(\\mathbb{Y}_m) \\leq \\sqrt{\\frac{D}{H}} \\sqrt{\\sum_m \\sigma^2(\\mathbb{Y}_m)}$ (by e.g.~using the Cauchy--Schwartz inequality on $[\\sigma(\\mathbb{Y}_1), \\ldots, \\sigma(\\mathbb{Y}_{D/H})]$ and $\\mathds{1}$) and $\\max_l \\sigma(\\mathbb{Y}_l) \\leq \\sqrt{\\sum_m \\sigma^2(\\mathbb{Y}_m)}$, and the last inequality is from Lemma \\ref{lemma:key}. \n\\end{proof}",
                            "statement_html": "Recall that $Y^\\top P^{(i)}Y = \\mathrm{Cov}(\\mathbb{Y})$. Let $\\sigma(\\mathbb{Y}_m)$ denote the standard deviation of $\\mathbb{Y}_m$. Then $[\\mathrm{Cov}(\\mathbb{Y})]_{lm} \\leq \\sigma(\\mathbb{Y}_l)\\sigma(\\mathbb{Y}_m)$. Hence \n\\begin{align*}\n\\|\\mathrm{Cov}(\\mathbb{Y})\\|_{\\infty} = \\max_l \\sum_m \\left|[\\mathrm{Cov}(\\mathbb{Y})]_{lm}\\right| \n& \\leq  \\max_l \\sigma(\\mathbb{Y}_l) \\sum_m \\sigma(\\mathbb{Y}_m) \\\\\n& \\leq \\sqrt{\\frac{D}{H}} \\sum_m \\sigma^2(\\mathbb{Y}_m)  = \\sqrt{\\frac{D}{H}} \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\\\\n& \\leq \\sqrt{\\frac{D}{H}} \\phi^{-1}(N-1),\n\\end{align*}\nsince $\\sum_m \\sigma(\\mathbb{Y}_m) \\leq \\sqrt{\\frac{D}{H}} \\sqrt{\\sum_m \\sigma^2(\\mathbb{Y}_m)}$ (by e.g.~using the Cauchy--Schwartz inequality on $[\\sigma(\\mathbb{Y}_1), \\ldots, \\sigma(\\mathbb{Y}_{D/H})]$ and $\\mathbf{1}$) and $\\max_l \\sigma(\\mathbb{Y}_l) \\leq \\sqrt{\\sum_m \\sigma^2(\\mathbb{Y}_m)}$, and the last inequality is from <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_3/index.html#lemma%3Akey\">Lemma 3</a>.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "Recall that \\( Y^\\top P^{(i)}Y = \\mathrm{Cov}(\\mathbb{Y}) \\). Let \\(\\sigma(\\mathbb{Y}_m)\\) denote the standard deviation of \\(\\mathbb{Y}_m\\). Then \\([\\mathrm{Cov}(\\mathbb{Y})]_{lm} \\leq \\sigma(\\mathbb{Y}_l)\\sigma(\\mathbb{Y}_m)\\). Hence \n\n\\[\n\\|\\mathrm{Cov}(\\mathbb{Y})\\|_{\\infty} = \\max_l \\sum_m \\left|[\\mathrm{Cov}(\\mathbb{Y})]_{lm}\\right| \n\\leq  \\max_l \\sigma(\\mathbb{Y}_l) \\sum_m \\sigma(\\mathbb{Y}_m)\n\\]\n\nWe then use the Cauchy-Schwarz inequality to get:\n\n\\[\n\\sum_m \\sigma(\\mathbb{Y}_m) \\leq \\sqrt{\\frac{D}{H}} \\sqrt{\\sum_m \\sigma^2(\\mathbb{Y}_m)}\n\\]\n\nand \n\n\\[\n\\max_l \\sigma(\\mathbb{Y}_l) \\leq \\sqrt{\\sum_m \\sigma^2(\\mathbb{Y}_m)}\n\\]\n\nCombining these results, we have:\n\n\\[\n\\|\\mathrm{Cov}(\\mathbb{Y})\\|_{\\infty} \\leq \\sqrt{\\frac{D}{H}} \\sum_m \\sigma^2(\\mathbb{Y}_m) = \\sqrt{\\frac{D}{H}} \\Tr(\\mathrm{Cov}(\\mathbb{Y}))\n\\]\n\nFinally, using <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_3/index.html#lemma%3Akey\">Lemma 3</a>, we get:\n\n\\[\n\\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\phi^{-1}(N-1)\n\\]\n\nThus,\n\n\\[\n\\|\\mathrm{Cov}(\\mathbb{Y})\\|_{\\infty} \\leq \\sqrt{\\frac{D}{H}} \\phi^{-1}(N-1)\n\\]"
                        }
                    },
                    {
                        "statement_id": "520091f9-1b15-4fc7-82e9-19ab260a6003",
                        "paper_id": "083fc279-e9c8-48f3-94e5-2b3a363832b3",
                        "library_nr": 5,
                        "library_name": "Lemma 5",
                        "title": "Low-Rank Approximation Bound",
                        "statement_original_tex": "\\begin{lemma} \\label{lemma:low_rank}\n$\\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty \\leq  \\phi^{-1}(N-1) \\sqrt{D/H}$.\n\\end{lemma}",
                        "statement_html": "$\\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty \\leq  \\phi^{-1}(N-1) \\sqrt{D/H}$.",
                        "statement_type": "lemma",
                        "statement_motivation_html": "This inequality is useful in the context of analyzing the stability and convergence of certain iterative algorithms, particularly in high-dimensional spaces. It provides a bound on the infinity norm of a sum involving projection matrices and vectors, which can be critical for ensuring that the algorithm behaves as expected. This type of result is often used in optimization and machine learning to guarantee that updates to parameters do not lead to instability or divergence.",
                        "html_url": "library/lemmas/lemma_5/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "9401f7ca-61e5-43d2-9885-f2eba6e543b2",
                            "paper_id": "083fc279-e9c8-48f3-94e5-2b3a363832b3",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof} \\label{lemma:f4}\nNote $\\|\\mathbf{u}\\mathbf{v}^\\top\\|_{\\infty} = \\|\\mathbf{u}\\|_{\\infty} \\|\\mathbf{v}\\|_1$ for real vectors $\\mathbf{u},\\mathbf{v}$. Hence\n\\begin{align*}\n    \\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty & = \\sum_j P_{ij} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_\\infty \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_1 \\\\\n    & = \\mathbf{a}^\\top \\mathbf{b} \\leq \\|\\mathbf{a}\\|_2 \\|\\mathbf{b}\\|_2,\n\\end{align*}\nwhere $a_j = \\sqrt{P_{ij}} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_\\infty$, $b_j = \\sqrt{P_{ij}} \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_1$.\n\nNote $a_j \\leq c_j \\coloneqq  \\sqrt{P_{ij}} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_2$ since $\\|\\mathbf{u}\\|_\\infty \\leq \\|\\mathbf{u}\\|_2$ for vector $\\mathbf{u}$. Hence $\\|\\mathbf{a}\\|_2 \\leq \\|\\mathbf{c}\\|_2$.\n\nAlso $b_j \\leq \\sqrt{\\frac{D}{H}} d_j \\coloneqq  \\sqrt{\\frac{D}{H}} \\sqrt{P_{ij}} \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_2$ since $\\|\\mathbf{u}\\|_1 \\leq \\sqrt{\\frac{D}{H}}\\|\\mathbf{u}\\|_2$ for $\\mathbf{u} \\in \\mathbb{R}^{D/H}$ (e.g.~by the Cauchy--Schwartz inequality on $[|\\mathbf{u}_1|, \\ldots, |\\mathbf{u}_{D/H}|]$ and $\\mathds{1}$). Hence $\\|b\\|_2 \\leq \\sqrt{\\frac{D}{H}}\\|d\\|_2$.\n\nNote $\\|c\\|_2^2 = \\sum_j P_{ij} \\|y_j - \\sum_k P_{ik} y_k\\|_2^2 = \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\phi^{-1}(N-1)$ from Lemma \\ref{lemma:key},\nand $\\|d\\|_2^2 =  \\sum_j P_{ij} \\|y_i - y_j\\|_2^2 \\leq  \\phi^{-1}(N-1)$ also from Lemma \\ref{lemma:key}.\nHence $\\|a\\|_2 \\|b\\|_2 \\leq \\sqrt{\\frac{D}{H}} \\|c\\|_2 \\|d\\|_2 \\leq \\sqrt{\\frac{D}{H}} \\phi^{-1}(N-1)$.\n\\end{proof}",
                            "statement_html": "Note $\\|\\mathbf{u}\\mathbf{v}^\\top\\|_{\\infty} = \\|\\mathbf{u}\\|_{\\infty} \\|\\mathbf{v}\\|_1$ for real vectors $\\mathbf{u},\\mathbf{v}$. Hence\n\\begin{align*}\n    \\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty & = \\sum_j P_{ij} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_\\infty \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_1 \\\\\n    & = \\mathbf{a}^\\top \\mathbf{b} \\leq \\|\\mathbf{a}\\|_2 \\|\\mathbf{b}\\|_2,\n\\end{align*}\nwhere $a_j = \\sqrt{P_{ij}} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_\\infty$, $b_j = \\sqrt{P_{ij}} \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_1$.\n\nNote $a_j \\leq c_j \\coloneqq  \\sqrt{P_{ij}} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_2$ since $\\|\\mathbf{u}\\|_\\infty \\leq \\|\\mathbf{u}\\|_2$ for vector $\\mathbf{u}$. Hence $\\|\\mathbf{a}\\|_2 \\leq \\|\\mathbf{c}\\|_2$.\n\nAlso $b_j \\leq \\sqrt{\\frac{D}{H}} d_j \\coloneqq  \\sqrt{\\frac{D}{H}} \\sqrt{P_{ij}} \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_2$ since $\\|\\mathbf{u}\\|_1 \\leq \\sqrt{\\frac{D}{H}}\\|\\mathbf{u}\\|_2$ for $\\mathbf{u} \\in \\mathbb{R}^{D/H}$ (e.g.~by the Cauchy--Schwartz inequality on $[|\\mathbf{u}_1|, \\ldots, |\\mathbf{u}_{D/H}|]$ and $\\mathbf{1}$). Hence $\\|b\\|_2 \\leq \\sqrt{\\frac{D}{H}}\\|d\\|_2$.\n\nNote $\\|c\\|_2^2 = \\sum_j P_{ij} \\|y_j - \\sum_k P_{ik} y_k\\|_2^2 = \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\phi^{-1}(N-1)$ from <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_3/index.html#lemma%3Akey\">Lemma 3</a>,\nand $\\|d\\|_2^2 =  \\sum_j P_{ij} \\|y_i - y_j\\|_2^2 \\leq  \\phi^{-1}(N-1)$ also from <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_3/index.html#lemma%3Akey\">Lemma 3</a>.\nHence $\\|a\\|_2 \\|b\\|_2 \\leq \\sqrt{\\frac{D}{H}} \\|c\\|_2 \\|d\\|_2 \\leq \\sqrt{\\frac{D}{H}} \\phi^{-1}(N-1)$.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break down the steps involved:\n\n1. <i></i>Initial Norm Equality<i></i>:\n   - The proof starts by noting the equality $\\|\\mathbf{u}\\mathbf{v}^\\top\\|_{\\infty} = \\|\\mathbf{u}\\|_{\\infty} \\|\\mathbf{v}\\|_1$ for real vectors $\\mathbf{u}$ and $\\mathbf{v}$.\n\n2. <i></i>Summation and Norms<i></i>:\n   - The expression $\\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty$ is rewritten using the initial norm equality:\n     \\[\n     \\sum_j P_{ij} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_\\infty \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_1\n     \\]\n   - This is then expressed as a dot product $\\mathbf{a}^\\top \\mathbf{b}$, where:\n     \\[\n     a_j = \\sqrt{P_{ij}} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_\\infty, \\quad b_j = \\sqrt{P_{ij}} \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_1\n     \\]\n\n3. <i></i>Cauchy-Schwarz Inequality<i></i>:\n   - The dot product $\\mathbf{a}^\\top \\mathbf{b}$ is bounded by the Cauchy-Schwarz inequality:\n     \\[\n     \\mathbf{a}^\\top \\mathbf{b} \\leq \\|\\mathbf{a}\\|_2 \\|\\mathbf{b}\\|_2\n     \\]\n\n4. <i></i>Bounding $\\mathbf{a}$<i></i>:\n   - The vector $\\mathbf{a}$ is bounded by another vector $\\mathbf{c}$:\n     \\[\n     a_j \\leq c_j \\coloneqq \\sqrt{P_{ij}} \\|\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k\\|_2\n     \\]\n   - This follows from the inequality $\\|\\mathbf{u}\\|_\\infty \\leq \\|\\mathbf{u}\\|_2$ for any vector $\\mathbf{u}$.\n   - Consequently, $\\|\\mathbf{a}\\|_2 \\leq \\|\\mathbf{c}\\|_2$.\n\n5. <i></i>Bounding $\\mathbf{b}$<i></i>:\n   - The vector $\\mathbf{b}$ is bounded by another vector $\\mathbf{d}$:\n     \\[\n     b_j \\leq \\sqrt{\\frac{D}{H}} d_j \\coloneqq \\sqrt{\\frac{D}{H}} \\sqrt{P_{ij}} \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_2\n     \\]\n   - This follows from the inequality $\\|\\mathbf{u}\\|_1 \\leq \\sqrt{\\frac{D}{H}}\\|\\mathbf{u}\\|_2$ for $\\mathbf{u} \\in \\mathbb{R}^{D/H}$, which can be derived using the Cauchy-Schwarz inequality.\n   - Consequently, $\\|\\mathbf{b}\\|_2 \\leq \\sqrt{\\frac{D}{H}}\\|\\mathbf{d}\\|_2$.\n\n6. <i></i>Norms of $\\mathbf{c}$ and $\\mathbf{d}$<i></i>:\n   - The norms $\\|\\mathbf{c}\\|_2^2$ and $\\|\\mathbf{d}\\|_2^2$ are evaluated using <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_3/index.html#lemma%3Akey\">Lemma 3</a>:\n     \\[\n     \\|c\\|_2^2 = \\sum_j P_{ij} \\|y_j - \\sum_k P_{ik} y_k\\|_2^2 = \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\phi^{-1}(N-1)\n     \\]\n     \\[\n     \\|d\\|_2^2 = \\sum_j P_{ij} \\|y_i - y_j\\|_2^2 \\leq \\phi^{-1}(N-1)\n     \\]\n\n7. <i></i>Final Bound<i></i>:\n   - Combining the bounds, we get:\n     \\[\n     \\|\\mathbf{a}\\|_2 \\|\\mathbf{b}\\|_2 \\leq \\sqrt{\\frac{D}{H}} \\|\\mathbf{c}\\|_2 \\|\\mathbf{d}\\|_2 \\leq \\sqrt{\\frac{D}{H}} \\phi^{-1}(N-1)\n     \\]\n\nThis completes the proof by showing the desired inequality."
                        }
                    },
                    {
                        "statement_id": "82d5ab2b-2f9b-4d59-94d0-0e0803c15217",
                        "paper_id": "083fc279-e9c8-48f3-94e5-2b3a363832b3",
                        "library_nr": 6,
                        "library_name": "Lemma 6",
                        "title": "Block Row Norm Inequality",
                        "statement_original_tex": "\\begin{lemma} \\label{lemma:block_rows}\nLet A be a block matrix with block rows $A_1, \\ldots, A_N$. Then $\\|A\\|_2 \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2}$, and equality holds if and only if the first right singular vectors of the $A_i$ align.\n\\end{lemma}",
                        "statement_html": "Let $A$ be a block matrix with block rows $A_1, \\ldots, A_N$. Then $\\|A\\|_2 \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2}$, and equality holds if and only if the first right singular vectors of the $A_i$ align.",
                        "statement_type": "lemma",
                        "statement_motivation_html": "The given inequality for block matrices is particularly useful in numerical linear algebra and matrix analysis. It provides an upper bound for the spectral norm of a block matrix in terms of the spectral norms of its block rows. This can be applied in scenarios where the matrix is too large to handle directly, but its block structure can be exploited to simplify computations. The condition for equality is also significant in understanding the alignment of singular vectors, which can be crucial in applications like signal processing and data compression.",
                        "html_url": "library/lemmas/lemma_6/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "b2f49832-a0ee-474b-af1d-eac1b127419e",
                            "paper_id": "083fc279-e9c8-48f3-94e5-2b3a363832b3",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\n\\begin{equation*}\n\\|A\\|_2^2 = \\left\\Vert \\begin{bmatrix} A_1 \\\\ \\vdots \\\\ A_N \\\\ \\end{bmatrix}\\right\\Vert_2^2 = \\sup_{\\|\\mathbf{x}\\|_2=1} \\left\\Vert\\begin{bmatrix} A_1 \\\\ \\vdots \\\\ A_N \\\\ \\end{bmatrix} \\mathbf{x}\\right\\Vert_2^2 = \\sup_{\\|\\mathbf{x}\\|_2=1} \\sum_i \\|A_i \\mathbf{x}\\|_2^2 \\leq \\sum_i \\sup_{\\|\\mathbf{x}\\|_2=1} \\|A_i \\mathbf{x}\\|_2^2 = \\sum_i \\|A_i\\|_2^2.\n\\end{equation*}\nNote that equality holds if and only if the first right singular vectors of the $A_i$ align.\n\\end{proof}",
                            "statement_html": "\\begin{equation*}\n\\|A\\|_2^2 = \\left\\Vert \\begin{bmatrix} A_1 \\\\ \\vdots \\\\ A_N \\\\ \\end{bmatrix}\\right\\Vert_2^2 = \\sup_{\\|\\mathbf{x}\\|_2=1} \\left\\Vert\\begin{bmatrix} A_1 \\\\ \\vdots \\\\ A_N \\\\ \\end{bmatrix} \\mathbf{x}\\right\\Vert_2^2 = \\sup_{\\|\\mathbf{x}\\|_2=1} \\sum_i \\|A_i \\mathbf{x}\\|_2^2 \\leq \\sum_i \\sup_{\\|\\mathbf{x}\\|_2=1} \\|A_i \\mathbf{x}\\|_2^2 = \\sum_i \\|A_i\\|_2^2.\n\\end{equation*}\nNote that equality holds if and only if the first right singular vectors of the $A_i$ align.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "The proof demonstrates the relationship between the squared 2-norm of a block matrix \\( A \\) and the sum of the squared 2-norms of its submatrices \\( A_i \\). Here are the steps explained:\n\n1. <i></i>Expression of the 2-norm squared of \\( A \\)<i></i>:\n   \\[\n   \\|A\\|_2^2 = \\left\\Vert \\begin{bmatrix} A_1 \\\\ \\vdots \\\\ A_N \\\\ \\end{bmatrix}\\right\\Vert_2^2\n   \\]\n   This step expresses the squared 2-norm of the matrix \\( A \\) in terms of its block components \\( A_i \\).\n\n2. <i></i>Supremum over unit vectors<i></i>:\n   \\[\n   \\|A\\|_2^2 = \\sup_{\\|\\mathbf{x}\\|_2=1} \\left\\Vert\\begin{bmatrix} A_1 \\\\ \\vdots \\\\ A_N \\\\ \\end{bmatrix} \\mathbf{x}\\right\\Vert_2^2\n   \\]\n   The squared 2-norm of \\( A \\) is defined as the supremum of the squared 2-norm of \\( A \\mathbf{x} \\) over all unit vectors \\( \\mathbf{x} \\).\n\n3. <i></i>Sum of squared norms<i></i>:\n   \\[\n   \\sup_{\\|\\mathbf{x}\\|_2=1} \\left\\Vert\\begin{bmatrix} A_1 \\\\ \\vdots \\\\ A_N \\\\ \\end{bmatrix} \\mathbf{x}\\right\\Vert_2^2 = \\sup_{\\|\\mathbf{x}\\|_2=1} \\sum_i \\|A_i \\mathbf{x}\\|_2^2\n   \\]\n   The squared 2-norm of the block matrix \\( A \\mathbf{x} \\) is the sum of the squared 2-norms of each block component \\( A_i \\mathbf{x} \\).\n\n4. <i></i>Inequality using supremum<i></i>:\n   \\[\n   \\sup_{\\|\\mathbf{x}\\|_2=1} \\sum_i \\|A_i \\mathbf{x}\\|_2^2 \\leq \\sum_i \\sup_{\\|\\mathbf{x}\\|_2=1} \\|A_i \\mathbf{x}\\|_2^2\n   \\]\n   The supremum of a sum is less than or equal to the sum of the suprema.\n\n5. <i></i>Sum of squared norms of \\( A_i \\)<i></i>:\n   \\[\n   \\sum_i \\sup_{\\|\\mathbf{x}\\|_2=1} \\|A_i \\mathbf{x}\\|_2^2 = \\sum_i \\|A_i\\|_2^2\n   \\]\n   The supremum of the squared 2-norm of \\( A_i \\mathbf{x} \\) over all unit vectors \\( \\mathbf{x} \\) is the squared 2-norm of \\( A_i \\).\n\n6. <i></i>Conclusion<i></i>:\n   \\[\n   \\|A\\|_2^2 \\leq \\sum_i \\|A_i\\|_2^2\n   \\]\n   Combining the steps, we conclude that the squared 2-norm of \\( A \\) is less than or equal to the sum of the squared 2-norms of its submatrices \\( A_i \\).\n\n7. <i></i>Condition for equality<i></i>:\n   Note that equality holds if and only if the first right singular vectors of the \\( A_i \\) align.\n   This condition specifies when the inequality becomes an equality, which occurs when the first right singular vectors of all \\( A_i \\) are aligned."
                        }
                    },
                    {
                        "statement_id": "86fe1d7b-9edf-4f09-88a6-ac07330ef7fb",
                        "paper_id": "083fc279-e9c8-48f3-94e5-2b3a363832b3",
                        "library_nr": 7,
                        "library_name": "Lemma 7",
                        "title": "Matrix Norm Bound",
                        "statement_original_tex": "\\begin{lemma} \\label{lemma:f6}\n$\\|Y^\\top P^{(i)}Y\\|_2 \\leq \\phi^{-1}(N-1)$\n\\end{lemma}",
                        "statement_html": "$\\|Y^\\top P^{(i)}Y\\|_2 \\leq \\phi^{-1}(N-1)$",
                        "statement_type": "lemma",
                        "statement_motivation_html": "The given inequality $\\|Y^\\top P^{(i)}Y\\|_2 \\leq \\phi^{-1}(N-1)$ is useful in the context of matrix analysis and optimization. It provides a bound on the spectral norm of a transformed matrix, which can be critical in ensuring stability and convergence in numerical algorithms. This statement is particularly valuable when dealing with iterative methods or when assessing the performance of algorithms that involve matrix decompositions or transformations.",
                        "html_url": "library/lemmas/lemma_7/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "cac9c75c-b08c-4e93-b5d8-9f03237897bc",
                            "paper_id": "083fc279-e9c8-48f3-94e5-2b3a363832b3",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\n$\\|Y^\\top P^{(i)}Y\\|_2=\\|\\mathrm{Cov}(\\mathbb{Y})\\|_2 = \\lambda_{\\max}(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\phi^{-1}(N-1)$, where the first equality holds by symmetry of $\\mathrm{Cov}(\\mathbb{Y})$ and the next holds by $\\mathrm{Cov}(\\mathbb{Y})$ being positive semi-definite, so all its eigenvalues are non-negative, and hence the maximal eigenvalue is bounded by the sum of the eigenvalues, equal to its trace. The final inequality is from Lemma \\ref{lemma:key}.\n\\end{proof}",
                            "statement_html": "$\\|Y^\\top P^{(i)}Y\\|_2=\\|\\mathrm{Cov}(\\mathbb{Y})\\|_2 = \\lambda_{\\max}(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\phi^{-1}(N-1)$, where the first equality holds by symmetry of $\\mathrm{Cov}(\\mathbb{Y})$ and the next holds by $\\mathrm{Cov}(\\mathbb{Y})$ being positive semi-definite, so all its eigenvalues are non-negative, and hence the maximal eigenvalue is bounded by the sum of the eigenvalues, equal to its trace. The final inequality is from <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_3/index.html#lemma%3Akey\">Lemma 3</a>.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break down each step:<br><br>1. <i></i>Equality by Symmetry<i></i>: \n   \\[\n   \\|Y^\\top P^{(i)}Y\\|_2 = \\|\\mathrm{Cov}(\\mathbb{Y})\\|_2\n   \\]\n   This equality holds because the covariance matrix $\\mathrm{Cov}(\\mathbb{Y})$ is symmetric. For symmetric matrices, the 2-norm (spectral norm) is equal to the largest singular value, which is the same as the largest eigenvalue.<br><br>2. <i></i>Equality by Positive Semi-Definiteness<i></i>:\n   \\[\n   \\|\\mathrm{Cov}(\\mathbb{Y})\\|_2 = \\lambda_{\\max}(\\mathrm{Cov}(\\mathbb{Y}))\n   \\]\n   Since $\\mathrm{Cov}(\\mathbb{Y})$ is positive semi-definite, all its eigenvalues are non-negative. Therefore, the 2-norm of the covariance matrix is equal to its largest eigenvalue, $\\lambda_{\\max}$.<br><br>3. <i></i>Inequality by Trace<i></i>:\n   \\[\n   \\lambda_{\\max}(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\Tr(\\mathrm{Cov}(\\mathbb{Y}))\n   \\]\n   The largest eigenvalue of a positive semi-definite matrix is bounded above by the sum of all its eigenvalues. The sum of the eigenvalues of a matrix is equal to its trace.<br><br>4. <i></i>Final Inequality from <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_3/index.html#lemma%3Akey\">Lemma 3</a><i></i>:\n   \\[\n   \\Tr(\\mathrm{Cov}(\\mathbb{Y})) \\leq \\phi^{-1}(N-1)\n   \\]\n   This inequality is derived from <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_3/index.html#lemma%3Akey\">Lemma 3</a>, which provides an upper bound for the trace of the covariance matrix in terms of $\\phi^{-1}$ and $N-1$.<br><br>By following these steps, we establish the desired inequality:\n\\[\n\\|Y^\\top P^{(i)}Y\\|_2 \\leq \\phi^{-1}(N-1)\n\\]"
                        }
                    },
                    {
                        "statement_id": "86d0562a-1d70-4acc-9fbb-34f5b10581e0",
                        "paper_id": "083fc279-e9c8-48f3-94e5-2b3a363832b3",
                        "library_nr": 8,
                        "library_name": "Lemma 8",
                        "title": "Bounding Sum of Projected Differences",
                        "statement_original_tex": "\\begin{lemma} \\label{lemma:f7}\n$\\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_2 \\leq  \\phi^{-1}(N-1)$\n\\end{lemma}",
                        "statement_html": "$\\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_2 \\leq  \\phi^{-1}(N-1)$",
                        "statement_type": "lemma",
                        "statement_motivation_html": "This inequality is useful in the context of analyzing the stability and convergence of iterative algorithms, particularly in optimization and machine learning. It provides a bound on the sum of certain matrix norms, which can be critical for ensuring that the updates in an iterative process do not diverge. This can be particularly important when dealing with high-dimensional data or complex models, where ensuring numerical stability is crucial for obtaining reliable results.",
                        "html_url": "library/lemmas/lemma_8/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "d65a5807-115e-48d8-a882-85806be5b4f4",
                            "paper_id": "083fc279-e9c8-48f3-94e5-2b3a363832b3",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\nDirectly use Cauchy--Schwartz on $c$ and $d$ in the proof of Lemma \\ref{lemma:low_rank}. \n\\end{proof}",
                            "statement_html": "Directly use Cauchy--Schwartz on $c$ and $d$ in the proof of <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_5/index.html#lemma%3Alow_rank\">Lemma 5</a>.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, we need to apply the Cauchy-Schwarz inequality to the vectors \\( c \\) and \\( d \\). The Cauchy-Schwarz inequality states that for any vectors \\( \\mathbf{u} \\) and \\( \\mathbf{v} \\) in an inner product space, the following inequality holds:\n\n\\[\n|\\langle \\mathbf{u}, \\mathbf{v} \\rangle| \\leq \\|\\mathbf{u}\\| \\|\\mathbf{v}\\|\n\\]\n\nIn this context, let \\( \\mathbf{u} = c \\) and \\( \\mathbf{v} = d \\). Applying the Cauchy-Schwarz inequality, we get:\n\n\\[\n|\\langle c, d \\rangle| \\leq \\|c\\| \\|d\\|\n\\]\n\nThis inequality is used directly in the proof of <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_5/index.html#lemma%3Alow_rank\">Lemma 5</a> to establish the necessary bounds and relationships between the vectors \\( c \\) and \\( d \\)."
                        }
                    },
                    {
                        "statement_id": "ab1936ea-086f-4fd2-93c8-f1642c81d056",
                        "paper_id": "083fc279-e9c8-48f3-94e5-2b3a363832b3",
                        "library_nr": 9,
                        "library_name": "Lemma 9",
                        "title": "Block Column Norm Inequality",
                        "statement_original_tex": "\\begin{lemma} \\label{lemma:block_cols}\nLet A be a block matrix with block columns $A_1, \\ldots, A_N$. Then $\\|A\\|_2 \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2}$.\n\\end{lemma}",
                        "statement_html": "Let $A$ be a block matrix with block columns $A_1, \\ldots, A_N$. Then $\\|A\\|_2 \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2}$.",
                        "statement_type": "lemma",
                        "statement_motivation_html": "The given inequality is useful in the context of block matrices, particularly when estimating the spectral norm of a matrix. It provides an upper bound for the spectral norm of a block matrix in terms of the spectral norms of its block columns. This can be particularly helpful in numerical linear algebra and matrix analysis, where such bounds are often needed to ensure stability and convergence of algorithms.",
                        "html_url": "library/lemmas/lemma_9/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "1fc8349a-f74d-4363-be22-3a7066088da1",
                            "paper_id": "083fc279-e9c8-48f3-94e5-2b3a363832b3",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\n\\begin{align*}\n\\|A\\|_2 &= \\|[A_1, \\ldots, A_N]\\|_2 = \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1}  \\left\\Vert [A_1, \\ldots, A_N] \\begin{bmatrix} \\mathbf{x}_1\\\\ \\vdots \\\\ \\mathbf{x}_N \\\\ \\end{bmatrix} \\right\\Vert_2^2 = \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1} \\|\\sum_i A_i \\mathbf{x}_i\\|_2 \\\\ \n& \\leq \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1} \\sum_i \\|A_i \\mathbf{x}_i\\|_2 = \\sup_{\\|\\mathbf{e}_i\\|_2=1, \\sum_i \\lambda_i^2 =1} \\sum_i \\lambda_i \\|A_i \\mathbf{e}_i\\|_2 = \\sup_{\\sum_i \\lambda_i^2 =1} \\sum_i \\lambda_i \\|A_i\\|_2 \\\\\n& \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2},\n\\end{align*}\nwhere we are using the substitution $\\mathbf{x}_i = \\lambda_i \\mathbf{e}_i$, and the last inequality holds by e.g.~Cauchy--Schwartz inequality on $[\\lambda_1, \\ldots, \\lambda_N]$ and $[\\|A_1\\|_2, \\ldots, \\|A_N\\|_2]$.\n\\end{proof}",
                            "statement_html": "\\begin{align*}\n\\|A\\|_2 &= \\|[A_1, \\ldots, A_N]\\|_2 = \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1}  \\left\\Vert [A_1, \\ldots, A_N] \\begin{bmatrix} \\mathbf{x}_1\\\\ \\vdots \\\\ \\mathbf{x}_N \\\\ \\end{bmatrix} \\right\\Vert_2^2 = \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1} \\|\\sum_i A_i \\mathbf{x}_i\\|_2 \\\\ \n& \\leq \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1} \\sum_i \\|A_i \\mathbf{x}_i\\|_2 = \\sup_{\\|\\mathbf{e}_i\\|_2=1, \\sum_i \\lambda_i^2 =1} \\sum_i \\lambda_i \\|A_i \\mathbf{e}_i\\|_2 = \\sup_{\\sum_i \\lambda_i^2 =1} \\sum_i \\lambda_i \\|A_i\\|_2 \\\\\n& \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2},\n\\end{align*}\nwhere we are using the substitution $\\mathbf{x}_i = \\lambda_i \\mathbf{e}_i$, and the last inequality holds by e.g.~Cauchy--Schwartz inequality on $[\\lambda_1, \\ldots, \\lambda_N]$ and $[\\|A_1\\|_2, \\ldots, \\|A_N\\|_2]$.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "The proof demonstrates an upper bound for the 2-norm of a matrix \\( A \\) composed of submatrices \\( A_1, \\ldots, A_N \\). Here are the steps explained:\n\n1. <i></i>Definition of the 2-norm<i></i>: The 2-norm of the matrix \\( A \\) is defined as:\n   \\[\n   \\|A\\|_2 = \\|[A_1, \\ldots, A_N]\\|_2\n   \\]\n   This is the supremum of the 2-norm of \\( A \\) applied to a vector, over all vectors with unit 2-norm.\n\n2. <i></i>Supremum over unit vectors<i></i>: The expression is rewritten to consider the supremum over vectors \\( \\mathbf{x}_i \\) such that the sum of their squared norms is 1:\n   \\[\n   \\|A\\|_2 = \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1}  \\left\\Vert [A_1, \\ldots, A_N] \\begin{bmatrix} \\mathbf{x}_1\\\\ \\vdots \\\\ \\mathbf{x}_N \\\\ \\end{bmatrix} \\right\\Vert_2^2\n   \\]\n\n3. <i></i>Matrix-vector multiplication<i></i>: This is simplified to:\n   \\[\n   \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1} \\|\\sum_i A_i \\mathbf{x}_i\\|_2\n   \\]\n\n4. <i></i>Triangle inequality<i></i>: Using the triangle inequality, we get:\n   \\[\n   \\leq \\sup_{\\sum_i\\|\\mathbf{x}_i\\|^2_2=1} \\sum_i \\|A_i \\mathbf{x}_i\\|_2\n   \\]\n\n5. <i></i>Substitution<i></i>: By substituting \\( \\mathbf{x}_i = \\lambda_i \\mathbf{e}_i \\) where \\( \\|\\mathbf{e}_i\\|_2 = 1 \\) and \\( \\sum_i \\lambda_i^2 = 1 \\), the expression becomes:\n   \\[\n   = \\sup_{\\|\\mathbf{e}_i\\|_2=1, \\sum_i \\lambda_i^2 =1} \\sum_i \\lambda_i \\|A_i \\mathbf{e}_i\\|_2\n   \\]\n\n6. <i></i>Norm property<i></i>: Recognizing that \\( \\|A_i \\mathbf{e}_i\\|_2 \\leq \\|A_i\\|_2 \\), we get:\n   \\[\n   = \\sup_{\\sum_i \\lambda_i^2 =1} \\sum_i \\lambda_i \\|A_i\\|_2\n   \\]\n\n7. <i></i>Cauchy-Schwarz inequality<i></i>: Applying the Cauchy-Schwarz inequality to the vectors \\( [\\lambda_1, \\ldots, \\lambda_N] \\) and \\( [\\|A_1\\|_2, \\ldots, \\|A_N\\|_2] \\), we obtain:\n   \\[\n   \\leq \\sqrt{\\sum_i \\|A_i\\|_2^2}\n   \\]\n\nThus, the 2-norm of the matrix \\( A \\) is bounded above by the square root of the sum of the squares of the 2-norms of its submatrices \\( A_i \\)."
                        }
                    }
                ],
                "theorems": [
                    {
                        "statement_id": "01076721-09d6-4ac8-81ee-3b37a722d45b",
                        "paper_id": "083fc279-e9c8-48f3-94e5-2b3a363832b3",
                        "library_nr": 1,
                        "library_name": "Theorem 1",
                        "title": "Jacobian Lipschitz Norm Theorem",
                        "statement_original_tex": "\\begin{theorem}[\\citealp{federer1969geometric}] \\label{thm:jacobian} Let $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ be differentiable and Lipschitz continuous under a choice of $p$-norm $\\|\\cdot\\|_p$. \nLet $J_f(x)$ denote its total derivative (Jacobian) at $x$. Then $\\lip_p(f) = \\sup_{\\mathbf{x}\\in \\mathbb{R}^n} \\|J_f(\\mathbf{x})\\|_p$ where $\\|J_f(\\mathbf{x})\\|_p$ is the induced operator norm on $J_f(\\mathbf{x})$. \n\\end{theorem}",
                        "statement_html": "Let $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ be differentiable and Lipschitz continuous under a choice of $p$-norm $\\|\\cdot\\|_p$. \nLet $J_f(x)$ denote its total derivative (Jacobian) at $x$. Then $\\lip_p(f) = \\sup_{\\mathbf{x}\\in \\mathbb{R}^n} \\|J_f(\\mathbf{x})\\|_p$ where $\\|J_f(\\mathbf{x})\\|_p$ is the induced operator norm on $J_f(\\mathbf{x})$.",
                        "statement_type": "theorem",
                        "statement_motivation_html": "Understanding the Lipschitz constant of a differentiable function $f$ is crucial in various fields such as optimization, numerical analysis, and machine learning. The given statement provides a way to compute the Lipschitz constant using the Jacobian matrix, which is particularly useful when dealing with high-dimensional functions. This approach allows for a more precise control over the function's behavior, ensuring stability and convergence in iterative algorithms.",
                        "html_url": "library/theorems/theorem_1/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "a35c5360-764c-43ee-ad59-eb8602fb0660",
                            "paper_id": null,
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "See proof at:\nFederer, \\textit{H. Geometric Measure Theory}. Classics in Mathematics. Springer Berlin Heidelberg, 1969. ISBN 9783642620102.",
                            "statement_html": "See proof at:\nFederer, $\\textit{H. Geometric Measure Theory}$. Classics in Mathematics. Springer Berlin Heidelberg, 1969. ISBN 9783642620102.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof presented in Federer's \"Geometric Measure Theory,\" we can break it down into several key steps:\n\n1. <i></i>Introduction of Concepts<i></i>: The proof begins by introducing the fundamental concepts and definitions necessary for understanding geometric measure theory. This includes measures, measurable sets, and the notion of rectifiability.\n\n2. <i></i>Preliminary Lemmas<i></i>: Several preliminary lemmas are established to build the foundation for the main proof. These lemmas address properties of measures, such as countable additivity and the behavior of measures under transformations.\n\n3. <i></i>Construction of Sets<i></i>: The proof involves constructing specific sets that exhibit the properties required for the theorem. This step often includes defining sets with particular geometric or measure-theoretic characteristics.\n\n4. <i></i>Application of Lemmas<i></i>: The previously established lemmas are applied to the constructed sets. This step is crucial as it links the abstract properties of measures to the specific sets under consideration.\n\n5. <i></i>Verification of Conditions<i></i>: The proof verifies that the constructed sets and measures satisfy the conditions of the theorem. This involves checking that all necessary criteria, such as measurability and rectifiability, are met.\n\n6. <i></i>Conclusion<i></i>: Finally, the proof concludes by summarizing how the established lemmas and verified conditions lead to the desired result. This step often includes a restatement of the theorem in the context of the constructed sets and measures.\n\nBy following these steps, the proof systematically builds from basic definitions to the final conclusion, ensuring that each part logically follows from the previous ones. For a detailed and rigorous presentation, refer to Federer's \"Geometric Measure Theory.\""
                        }
                    },
                    {
                        "statement_id": "44ba1450-0d9a-487f-b810-13a780878029",
                        "paper_id": "083fc279-e9c8-48f3-94e5-2b3a363832b3",
                        "library_nr": 2,
                        "library_name": "Theorem 2",
                        "title": "Non-Lipschitzness of DP-MHA",
                        "statement_original_tex": "\\begin{theorem} \\label{thm:dp_not_lipschitz}\n\\verb!DP-MHA! is not Lipschitz for any vector $p$-norm $\\|\\cdot\\|_p$ with $p \\in [1, \\infty]$.\n\\end{theorem}",
                        "statement_html": "$\\verb!DP-MHA!$ is not Lipschitz for any vector $p$-norm $\\|\\cdot\\|_p$ with $p \\in [1, \\infty]$.",
                        "statement_type": "theorem",
                        "statement_motivation_html": "The fact that $\\verb!DP-MHA!$ is not Lipschitz for any vector $p$-norm $\\|\\cdot\\|_p$ with $p \\in [1, \\infty]$ is crucial in understanding the limitations of certain algorithms in machine learning and optimization. This result highlights that $\\verb!DP-MHA!$ may exhibit sensitivity to small changes in input, which can affect the stability and robustness of the algorithm. It is particularly useful when analyzing the performance and reliability of algorithms in high-dimensional spaces, where ensuring Lipschitz continuity can be challenging.",
                        "html_url": "library/theorems/theorem_2/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "2f7de574-224b-42b1-845a-d3c515afe16c",
                            "paper_id": "083fc279-e9c8-48f3-94e5-2b3a363832b3",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\nThe mapping $f$ can be written as\n\\vspace{-5mm}\n\\begin{equation}\nf(X) = PX = \\softmax{X A^\\top X^\\top} X = \\begin{bmatrix}\n    f_1(X)^\\top \\\\\n    \\vdots \\\\\n    f_N(X)^\\top\n\\end{bmatrix} \\in \\mathbb{R}^{N \\times D},\n\\end{equation}\nwhere $A = W^K W^{Q^\\top} / \\sqrt{D/H} \\in \\mathbb{R}^{D \\times D}$ and\n$f_i(X) = \\sum_{j=1}^N P_{ij}\\mathbf{x}_j$ with $P_{i:}^\\top = \\softmax{XA\\mathbf{x}_i}$.\nHence $f$ can be interpreted as a map of each $\\mathbf{x}_i$ to a point in the convex hull of ${\\mathbf{x}_1,...,\\mathbf{x}_N}$.\nSince $f$ is a map from $\\mathbb{R}^{N \\times D}$ to $\\mathbb{R}^{N \\times D}$, its Jacobian is\n\\begin{equation}\n    J_f = \\begin{bmatrix}\n    J_{11} & \\dots & J_{1N} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    J_{N1} & \\dots & J_{NN} \\\\\n    \\end{bmatrix}\\in \\mathbb{R}^{ND \\times ND},\n\\end{equation}\nwhere $J_{ij} = \\frac{\\partial f_i(X)}{\\partial \\mathbf{x}_j} \\in \\mathbb{R}^{D \\times D}$. \nBy taking partial derivatives we can show that $J_{ij} = X^\\top P^{(i)} \\left[E_{ji}XA^\\top + XA\\delta_{ij}\\right] + P_{ij}I$\nwhere $E_{ij} \\in \\mathbb{R}^{N \\times N}$ is a binary matrix with zeros everywhere except the $(i,j)$th entry, $\\delta_{ij}$ is the Kronecker delta, and $P^{(i)} \\coloneqq \\diag(P_{i:}) - P_{i:}^\\top P_{i:}$.\nSo for $i=j$:\n\\begin{align}\nJ_{ii} &=X^\\top P^{(i)}E_{ii}XA^\\top + X^\\top P^{(i)}XA + P_{ii}I \\nonumber \\\\\n&= P_{ii}\\left(\\mathbf{x}_i - \\textstyle\\sum_k P_{ik} \\mathbf{x}_k\\right)\\mathbf{x}_i^\\top A^\\top + X^\\top P^{(i)}XA + P_{ii}I. \\label{eq:jac_dot_general}\n\\end{align}\nFor the last equality, note $E_{ii}X$ has all rows equal to zero except for the $i$th row given by $\\mathbf{x}_i^\\top$. We can then verify that $X^\\top P^{(i)}E_{ii}X$ simplifies to $P_{ii}(\\mathbf{x}_i - \\sum_k P_{ik} \\mathbf{x}_k)\\mathbf{x}_i^\\top$.\n\nFor vector $p$-norms, $\\|J_f\\|_p$ is bounded if and only if its entries are bounded, by definition of the operator norm. \nThe entries of $X^\\top P^{(i)}XA$ are bounded for arbitrary $A$ only if the entries of $X^\\top P^{(i)}X$ are bounded.\nSo let us investigate the entries of this $D\\times D$ matrix. \nWriting out each term of the matrix, we observe that it is in fact a covariance matrix of a discrete distribution. Specifically:\n\\begin{equation} \\label{eq:cov_general}\n    [X^\\top P^{(i)}X]_{lm}  = \\textstyle\\sum_k P_{ik} x_{kl} x_{km} - \\left(\\textstyle\\sum_k P_{ik}  x_{kl}\\right)\\left(\\textstyle\\sum_k P_{ik} x_{km}\\right) = \\mathrm{Cov}(\\mathbb{X}_l,\\mathbb{X}_m),\n\\end{equation}\nwhere $\\mathbb{X}$ is a discrete distribution with support at the inputs $\\{\\mathbf{x}_1,\\ldots,\\mathbf{x}_N \\}$ and probability mass function given by their softmax probabilities $\\mathbb{P}(\\mathbb{X}=\\mathbf{x}_j)=P_{ij}$. \nA consequence of this interpretation is that $P^{(i)}$ is \\textit{positive semi-definite} (PSD) since for $D=1$, Equation \\eqref{eq:cov_general} becomes $X^\\top P^{(i)} X = \\mathrm{Var}(\\mathbb{X}) \\geq 0$, with equality if and only if the $\\mathbf{x}_j$ are all equal.\n\nWe use this observation to show that the terms of $J_{ii}$ are unbounded, and so \\verb!DP-MHA! is \\emph{not} Lipschitz.\nConsider the case $\\mathbf{x}_i=0$. Then $P_{i:}^\\top = \\softmax{XA\\mathbf{x}_i} = \\frac{1}{N} \\mathds{1}$, i.e.\\ we have uniform attention regardless of $\\mathbf{x}_{ \\neq i}$. \nThe first term of $J_{ii}$ in Equation \\eqref{eq:jac_dot_general} disappears since $\\mathbf{x}_i=\\mathbf{0}$, and the last term becomes $\\frac{1}{N} I$. For the second term, the entries $[X^\\top P^{(i)}X]_{ll} = \\mathrm{Var}(\\mathbb{X}_l)$ are unbounded since the latter is equal to the sample variance of ${x_{1l},\\ldots,x_{Nl}}$, which can be arbitrarily large.\n\nNote that we have shown that single head dot-product self-atttention ($H=1$) is not Lipschitz, but it is clear that this implies multihead self-attention \\verb!DP-MHA! is also not Lipschitz, since the output of multihead attention is a linear combination of the outputs of each head.\n\\end{proof}",
                            "statement_html": "The mapping $f$ can be written as\n\n\\begin{equation}\nf(X) = PX = \\softmax{X A^\\top X^\\top} X = \\begin{bmatrix}\n    f_1(X)^\\top \\\\\n    \\vdots \\\\\n    f_N(X)^\\top\n\\end{bmatrix} \\in \\mathbb{R}^{N \\times D},\n\\end{equation}\nwhere $A = W^K W^{Q^\\top} / \\sqrt{D/H} \\in \\mathbb{R}^{D \\times D}$ and\n$f_i(X) = \\sum_{j=1}^N P_{ij}\\mathbf{x}_j$ with $P_{i:}^\\top = \\softmax{XA\\mathbf{x}_i}$.\nHence $f$ can be interpreted as a map of each $\\mathbf{x}_i$ to a point in the convex hull of ${\\mathbf{x}_1,...,\\mathbf{x}_N}$.\nSince $f$ is a map from $\\mathbb{R}^{N \\times D}$ to $\\mathbb{R}^{N \\times D}$, its Jacobian is\n\\begin{equation}\n    J_f = \\begin{bmatrix}\n    J_{11} & \\dots & J_{1N} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    J_{N1} & \\dots & J_{NN} \\\\\n    \\end{bmatrix}\\in \\mathbb{R}^{ND \\times ND},\n\\end{equation}\nwhere $J_{ij} = \\frac{\\partial f_i(X)}{\\partial \\mathbf{x}_j} \\in \\mathbb{R}^{D \\times D}$. \nBy taking partial derivatives we can show that $J_{ij} = X^\\top P^{(i)} \\left[E_{ji}XA^\\top + XA\\delta_{ij}\\right] + P_{ij}I$\nwhere $E_{ij} \\in \\mathbb{R}^{N \\times N}$ is a binary matrix with zeros everywhere except the $(i,j)$th entry, $\\delta_{ij}$ is the Kronecker delta, and $P^{(i)} \\coloneqq \\diag(P_{i:}) - P_{i:}^\\top P_{i:}$.\nSo for $i=j$:\n\\begin{align}\nJ_{ii} &=X^\\top P^{(i)}E_{ii}XA^\\top + X^\\top P^{(i)}XA + P_{ii}I \\nonumber \\\\\n&= P_{ii}\\left(\\mathbf{x}_i - \\textstyle\\sum_k P_{ik} \\mathbf{x}_k\\right)\\mathbf{x}_i^\\top A^\\top + X^\\top P^{(i)}XA + P_{ii}I. \\label{eq:jac_dot_general}\n\\end{align}\nFor the last equality, note $E_{ii}X$ has all rows equal to zero except for the $i$th row given by $\\mathbf{x}_i^\\top$. We can then verify that $X^\\top P^{(i)}E_{ii}X$ simplifies to $P_{ii}(\\mathbf{x}_i - \\sum_k P_{ik} \\mathbf{x}_k)\\mathbf{x}_i^\\top$.\n\nFor vector $p$-norms, $\\|J_f\\|_p$ is bounded if and only if its entries are bounded, by definition of the operator norm. \nThe entries of $X^\\top P^{(i)}XA$ are bounded for arbitrary $A$ only if the entries of $X^\\top P^{(i)}X$ are bounded.\nSo let us investigate the entries of this $D\\times D$ matrix. \nWriting out each term of the matrix, we observe that it is in fact a covariance matrix of a discrete distribution. Specifically:\n\\begin{equation} \\label{eq:cov_general}\n    [X^\\top P^{(i)}X]_{lm}  = \\textstyle\\sum_k P_{ik} x_{kl} x_{km} - \\left(\\textstyle\\sum_k P_{ik}  x_{kl}\\right)\\left(\\textstyle\\sum_k P_{ik} x_{km}\\right) = \\mathrm{Cov}(\\mathbb{X}_l,\\mathbb{X}_m),\n\\end{equation}\nwhere $\\mathbb{X}$ is a discrete distribution with support at the inputs $\\{\\mathbf{x}_1,\\ldots,\\mathbf{x}_N \\}$ and probability mass function given by their softmax probabilities $\\mathbb{P}(\\mathbb{X}=\\mathbf{x}_j)=P_{ij}$. \nA consequence of this interpretation is that $P^{(i)}$ is $\\textit{positive semi-definite}$ (PSD) since for $D=1$, Equation \\eqref{eq:cov_general} becomes $X^\\top P^{(i)} X = \\mathrm{Var}(\\mathbb{X}) \\geq 0$, with equality if and only if the $\\mathbf{x}_j$ are all equal.\n\nWe use this observation to show that the terms of $J_{ii}$ are unbounded, and so $\\verb!DP-MHA!$ is $\\emph{not}$ Lipschitz.\nConsider the case $\\mathbf{x}_i=0$. Then $P_{i:}^\\top = \\softmax{XA\\mathbf{x}_i} = \\frac{1}{N} \\mathbf{1}$, i.e.\\ we have uniform attention regardless of $\\mathbf{x}_{ \\neq i}$. \nThe first term of $J_{ii}$ in Equation \\eqref{eq:jac_dot_general} disappears since $\\mathbf{x}_i=\\mathbf{0}$, and the last term becomes $\\frac{1}{N} I$. For the second term, the entries $[X^\\top P^{(i)}X]_{ll} = \\mathrm{Var}(\\mathbb{X}_l)$ are unbounded since the latter is equal to the sample variance of ${x_{1l},\\ldots,x_{Nl}}$, which can be arbitrarily large.\n\nNote that we have shown that single head dot-product self-atttention ($H=1$) is not Lipschitz, but it is clear that this implies multihead self-attention $\\verb!DP-MHA!$ is also not Lipschitz, since the output of multihead attention is a linear combination of the outputs of each head.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "The proof involves several steps to demonstrate that the mapping \\( f \\) is not Lipschitz. Here is a breakdown of the steps:\n\n1. <i></i>Definition of the Mapping \\( f \\)<i></i>:\n   The mapping \\( f \\) is defined as:\n   \\[\n   f(X) = PX = \\softmax{X A^\\top X^\\top} X = \\begin{bmatrix}\n       f_1(X)^\\top \\\\\n       \\vdots \\\\\n       f_N(X)^\\top\n   \\end{bmatrix} \\in \\mathbb{R}^{N \\times D},\n   \\]\n   where \\( A = W^K W^{Q^\\top} / \\sqrt{D/H} \\in \\mathbb{R}^{D \\times D} \\) and \\( f_i(X) = \\sum_{j=1}^N P_{ij}\\mathbf{x}_j \\) with \\( P_{i:}^\\top = \\softmax{XA\\mathbf{x}_i} \\).\n\n2. <i></i>Interpretation of \\( f \\)<i></i>:\n   \\( f \\) maps each \\( \\mathbf{x}_i \\) to a point in the convex hull of \\( \\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_N\\} \\).\n\n3. <i></i>Jacobian of \\( f \\)<i></i>:\n   The Jacobian \\( J_f \\) of \\( f \\) is given by:\n   \\[\n   J_f = \\begin{bmatrix}\n       J_{11} & \\dots & J_{1N} \\\\\n       \\vdots & \\ddots & \\vdots \\\\\n       J_{N1} & \\dots & J_{NN} \\\\\n   \\end{bmatrix} \\in \\mathbb{R}^{ND \\times ND},\n   \\]\n   where \\( J_{ij} = \\frac{\\partial f_i(X)}{\\partial \\mathbf{x}_j} \\in \\mathbb{R}^{D \\times D} \\).\n\n4. <i></i>Expression for \\( J_{ij} \\)<i></i>:\n   By taking partial derivatives, it is shown that:\n   \\[\n   J_{ij} = X^\\top P^{(i)} \\left[E_{ji}XA^\\top + XA\\delta_{ij}\\right] + P_{ij}I,\n   \\]\n   where \\( E_{ij} \\in \\mathbb{R}^{N \\times N} \\) is a binary matrix, \\( \\delta_{ij} \\) is the Kronecker delta, and \\( P^{(i)} \\coloneqq \\diag(P_{i:}) - P_{i:}^\\top P_{i:} \\).\n\n5. <i></i>Simplification for \\( i = j \\)<i></i>:\n   For \\( i = j \\):\n   \\[\n   J_{ii} = P_{ii}\\left(\\mathbf{x}_i - \\textstyle\\sum_k P_{ik} \\mathbf{x}_k\\right)\\mathbf{x}_i^\\top A^\\top + X^\\top P^{(i)}XA + P_{ii}I.\n   \\]\n\n6. <i></i>Boundedness of \\( \\|J_f\\|_p \\)<i></i>:\n   The entries of \\( X^\\top P^{(i)}XA \\) are bounded if and only if the entries of \\( X^\\top P^{(i)}X \\) are bounded.\n\n7. <i></i>Covariance Matrix Interpretation<i></i>:\n   The entries of \\( X^\\top P^{(i)}X \\) form a covariance matrix:\n   \\[\n   [X^\\top P^{(i)}X]_{lm} = \\textstyle\\sum_k P_{ik} x_{kl} x_{km} - \\left(\\textstyle\\sum_k P_{ik}  x_{kl}\\right)\\left(\\textstyle\\sum_k P_{ik} x_{km}\\right) = \\mathrm{Cov}(\\mathbb{X}_l,\\mathbb{X}_m),\n   \\]\n   where \\( \\mathbb{X} \\) is a discrete distribution with support at the inputs \\( \\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\} \\) and probability mass function given by their softmax probabilities.\n\n8. <i></i>Positive Semi-Definiteness<i></i>:\n   \\( P^{(i)} \\) is positive semi-definite (PSD).\n\n9. <i></i>Unboundedness of \\( J_{ii} \\)<i></i>:\n   For \\( \\mathbf{x}_i = 0 \\), the entries of \\( J_{ii} \\) are unbounded, showing that \\( \\verb!DP-MHA! \\) is not Lipschitz.\n\n10. <i></i>Conclusion<i></i>:\n    Since single head dot-product self-attention (\\( H = 1 \\)) is not Lipschitz, multihead self-attention (\\( \\verb!DP-MHA! \\)) is also not Lipschitz, as the output of multihead attention is a linear combination of the outputs of each head."
                        }
                    },
                    {
                        "statement_id": "b539a1ff-cd1f-4dad-baf4-898790c647e8",
                        "paper_id": "083fc279-e9c8-48f3-94e5-2b3a363832b3",
                        "library_nr": 3,
                        "library_name": "Theorem 3",
                        "title": "Lipschitz Bound for L2-MHA",
                        "statement_original_tex": "\\begin{theorem} \\label{thm:main}\n\\verb!L2-MHA! is Lipschitz, with the following bound on $\\lip_{\\infty}(F)$:\n\\begin{align*}\n    \\lip_{\\infty}(F)  \\leq &\\left(4 \\phi^{-1}(N-1) + \\frac{1}{\\sqrt{D/H}}\\right) \\|W^{O^\\top}\\|_{\\infty} \\\\\n    &\\max_h \\|W^{Q,h}\\|_{\\infty} \\|W^{{Q,h}^\\top}\\|_{\\infty} \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty} \n\\end{align*}\nand the following bound on $\\lip_{2}(F)$:\n\\begin{align*}\n    \\lip_2(F) \\leq & \\frac{\\sqrt{N}}{\\sqrt{D/H}}\n    \\left(4 \\phi^{-1}(N-1) + 1 \\right) \\\\ \n    & \\left(\\sqrt{\\textstyle\\sum_h \\|W^{Q,h}\\|_2^2\\, \\|W^{V,h}\\|_2^2}\\right) \\|W^O\\|_2 \n\\end{align*}\nwhere $\\phi(x) \\coloneqq x\\exp(x+1)$ is an invertible univariate function on $x > 0$, and $N$ is the input sequence length.\n\nSpecifically, $\\phi^{-1}(N-1) = W_0(\\frac{N}{e})$ where $W_0$ is the Lambert $W$-function, which grows sub-logarithmically as $O(\\log N - \\log \\log N)$ \\citep{corless1996lambertw}. Hence the above bounds can be simplified to $O(\\log N)$ for $p=\\infty$ and $O(\\sqrt{N} \\log N)$ for $p=2$.\n\\end{theorem}",
                        "statement_html": "$\\verb!L2-MHA!$ is Lipschitz, with the following bound on $\\lip_{\\infty}(F)$:\n\\begin{align*}\n    \\lip_{\\infty}(F)  \\leq &\\left(4 \\phi^{-1}(N-1) + \\frac{1}{\\sqrt{D/H}}\\right) \\|W^{O^\\top}\\|_{\\infty} \\\\\n    &\\max_h \\|W^{Q,h}\\|_{\\infty} \\|W^{{Q,h}^\\top}\\|_{\\infty} \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty} \n\\end{align*}\nand the following bound on $\\lip_{2}(F)$:\n\\begin{align*}\n    \\lip_2(F) \\leq & \\frac{\\sqrt{N}}{\\sqrt{D/H}}\n    \\left(4 \\phi^{-1}(N-1) + 1 \\right) \\\\ \n    & \\left(\\sqrt{\\textstyle\\sum_h \\|W^{Q,h}\\|_2^2\\, \\|W^{V,h}\\|_2^2}\\right) \\|W^O\\|_2 \n\\end{align*}\nwhere $\\phi(x) \\coloneqq x\\exp(x+1)$ is an invertible univariate function on $x > 0$, and $N$ is the input sequence length.\n\nSpecifically, $\\phi^{-1}(N-1) = W_0(\\frac{N}{e})$ where $W_0$ is the Lambert $W$-function, which grows sub-logarithmically as $O(\\log N - \\log \\log N)$. Hence the above bounds can be simplified to $O(\\log N)$ for $p=\\infty$ and $O(\\sqrt{N} \\log N)$ for $p=2$.",
                        "statement_type": "theorem",
                        "statement_motivation_html": "Understanding the Lipschitz properties of $\\verb!L2-MHA!$ is crucial for analyzing the stability and robustness of neural networks, particularly in the context of multi-head attention mechanisms. These bounds provide insights into how the output of the network changes in response to small perturbations in the input, which is essential for ensuring reliable performance in various applications, such as natural language processing and computer vision. The bounds also help in designing networks that are less sensitive to noise and adversarial attacks, thereby improving their generalization capabilities.",
                        "html_url": "library/theorems/theorem_3/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "3be4b15f-0b3a-44a0-a76a-bdba6d359e77",
                            "paper_id": "083fc279-e9c8-48f3-94e5-2b3a363832b3",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\subsubsection{Upper bound on $\\boldsymbol{\\lip_{\\infty}(F)}$ for L2-MHA}\n\nConsider the choice $p=\\infty$, where $\\|J_f\\|_\\infty$ is the maximum absolute row sum of $J_f$. \nA key observation is that if we can bound the $\\infty$-norm of the Jacobian of $f_i$, a single output of $f$, (i.e.~a single block row $\\|[J_{i1},...,J_{iN}]\\|_\\infty$ of $J_f$) then this is also a bound on $\\|J_f\\|_{\\infty}$ due to permutation equivariance of self-attention; all block rows have the same maximal $\\|\\cdot\\|_\\infty$ when each is optimised over the input $X$. \nUsing this, we can prove that $\\|J_f\\|_\\infty$ admits an upper bound that is $O(\\log N - \\log \\log N)$. Below we state and prove lemmas that lead to the proof of this upper bound.\n\nFirst we analyse the term $\\sqrt{A}^\\top X^\\top P^{(i)}X \\sqrt{A}$, that appears in the first term of $J_{ii}$. \nNote that for $Y \\coloneqq X \\sqrt{A}$, so that the rows of $Y$ are $\\mathbf{y}_i^\\top \\coloneqq \\mathbf{x}_i^\\top \\sqrt{A}$, we have \n\\begin{equation}\n    \\sqrt{A}^\\top X^\\top P^{(i)}X \\sqrt{A}= Y^\\top P^{(i)} Y =  \\mathrm{Cov}(\\mathbb{Y})\n\\end{equation}\nwhere $\\mathbb{P}(\\mathbb{Y}=\\mathbf{y}_j)=P_{ij} = \\exp(-\\|\\mathbf{y}_j - \\mathbf{y}_i\\|^2_2)/\\sum_k \\exp(-\\|\\mathbf{y}_k - \\mathbf{y}_i\\|^2_2)$.\nThe last equality uses the observation in Equation \\eqref{eq:cov}.\n\nThe central inequality used throughout the proof of the main theorem is the following:\n[Lemma \\ref{lemma:key}]\n\nNote $\\phi(\\log N) = (\\log N) \\exp(\\log N + 1) \\geq N \\log N \\geq N -1$ for $N \\geq 3$. Since $\\phi$ is increasing, we have $\\phi^{-1}(N-1) \\leq \\log(N)$ for $N \\geq 3$. In fact, it is known that $\\phi^{-1}(N-1) = O(\\log N - \\log \\log N)$ \\citep{corless1996lambertw}.\n\nNote the $A$ term in $f(X) = \\tilde{f}(X) A$ allows us to use the above inequality, since $Y^\\top P^{(i)}Y = \\mathrm{Cov}(\\mathbb{Y})$ now appears in the terms of $J_f$:\n\\begin{align}\n    J_{ii}\n    &= 2 \\sqrt{A} [Y^\\top P^{(i)}Y]\\sqrt{A}^\\top + P_{ii} A,  \\\\ \n    J_{ij},\n    &= 2 \\sqrt{A} P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top \\sqrt{A}^\\top + P_{ij} A  \\hspace{2mm} \\text{for $i \\neq j$}.\n\\end{align}\n\nUsing the inequalities $\\|BC\\| \\leq \\|B\\| \\|C\\|$, $\\|B + C\\| \\leq \\|B\\| + \\|C\\|$ and $\\|[A_1, \\ldots, A_N]\\| \\leq \\sum_i \\|A_i\\|$, we have: \n\\begin{align*}\n\\|[J_{i1} &, \\ldots, J_{iN}]\\|_{\\infty}  \\\\\n\\leq & \\|J_{ii}\\|_{\\infty} + \\sum_{j \\neq i} \\|J_{ij}\\|_{\\infty} \\\\\n  \\leq & 2 \\|\\sqrt{A}\\|_{\\infty} \\|Y^\\top P^{(i)}Y\\|_{\\infty} \\|\\sqrt{A}^\\top\\|_{\\infty} + P_{ii} \\|A\\|_{\\infty} \\\\\n & + 2 \\sum_{j \\neq i} \\|\\sqrt{A}\\|_{\\infty} \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty \\|\\sqrt{A}^\\top\\|_{\\infty} + P_{ij} \\|A\\|_{\\infty}\\\\\n  = & 2  \\|\\sqrt{A}\\|_{\\infty}\\|\\sqrt{A}^\\top\\|_{\\infty} \n\\bigg(\\|Y^\\top P^{(i)}Y\\|_\\infty \n + \\sum_{j\\neq i} \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty\\bigg) + \\|A\\|_{\\infty} \\\\\n = & 2  \\frac{\\|W^{Q}\\|_{\\infty}\\|W^{Q^\\top}\\|_{\\infty}}{\\sqrt{D/H}} \n\\bigg(\\|Y^\\top P^{(i)}Y\\|_\\infty \n + \\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty\\bigg) + \\frac{\\|W^Q W^{Q^\\top}\\|_{\\infty}}{\\sqrt{D/H}}.\n\\end{align*}\nFor the first equality, note that $\\sum_j P_{ij}=1$. For the second equality, note that the summand for $j=i$ is $0$ because the term $\\mathbf{y}_i - \\mathbf{y}_j=\\mathbf{0}$. \nEach of the terms in the brackets are bounded by the following lemmas:\n[Lemma \\ref{lemma:f3}]\n[Lemma \\ref{lemma:f4}]\n\nPutting the above lemmas altogether, with the observation $\\sup_X \\|J_f(X)\\|_\\infty = \\sup_X \\|[J_{i1}(X), \\ldots, J_{iN}(X)]\\|_\\infty$ by permutation invariance of $\\|J_f\\|_\\infty$ (since $f$ is permutation equivariant and $\\|\\cdot\\|_\\infty$ is the maximum absolute row sum), we have\n\\begin{align}\n\\|J_f\\|_{\\infty}\n& \\leq 4\\|W^Q\\|_{\\infty}\\|W^{Q^\\top}\\|_{\\infty} \\phi^{-1}(N-1)\n+ \\frac{\\|W^Q W^{Q^\\top}\\|_{\\infty}}{\\sqrt{D/H}} \\nonumber\\\\\n& \\leq \\|W^Q\\|_{\\infty}\\|W^{Q^\\top}\\|_{\\infty} \\left(4\\phi^{-1}(N-1) + \\frac{1}{\\sqrt{D/H}}\\right) \\label{ineq:infty}\\\\\n& \\leq \\|W^Q\\|_{\\infty} \\|W^{Q^\\top}\\|_{\\infty} \\left(4\\log N + \\frac{1}{\\sqrt{D/H}}\\right), \\nonumber\n\\end{align}\nwhere the last inequality holds for $N \\geq 3$.\n\nThe full multihead attention map that combines the heads $f^h(X)$ is:\n\\begin{equation*}\nF: X \\mapsto \\left[f^1(X)W^{V,1}, \\ldots f^H(X)W^{V,H}\\right] W^O = g(X) W^V W^O\n\\end{equation*}\nwhere $g:X \\mapsto [f^1(X),\\ldots,f^H(X)]$, $W^O \\in \\mathbb{R}^{D \\times D}$ and\n\\begin{equation*}\n    W^V = \\begin{bmatrix}\n    W^{V,1} & \\dots & 0 \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & W^{V,H} \\\\\n    \\end{bmatrix} \\in \\mathbb{R}^{DH \\times D}.\n\\end{equation*}\nNote the Jacobian $J_g$ is a block matrix whose rows are $J_{f^h}$, hence $\\|J_g\\|_{\\infty} = \\max_h \\|J_{f^h}\\|_{\\infty}$, and similarly $\\|W^{V^\\top}\\|_{\\infty} = \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty}$. Hence we have\n\\begin{equation*}\n    \\lip_{\\infty}(F) \\leq \\max_h \\|J_{f^h}\\|_{\\infty} \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty} \\|W^{O^\\top}\\|_{\\infty}.\n\\end{equation*}\n\nCombining this with Inequality (\\ref{ineq:infty}), we have:\n\\begin{equation*}\n    \\lip_{\\infty}(F)  \\leq \\left(4 \\phi^{-1}(N-1) + \\frac{1}{\\sqrt{D/H}}\\right) \\max_h \\|W^{Q,h}\\|_{\\infty} \\|W^{{Q,h}^\\top}\\|_{\\infty} \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty} \\ \\|W^{O^\\top}\\|_{\\infty}.\n\\end{equation*}\n\n\n\\subsubsection{Upper bound on $\\boldsymbol{\\lip_2(F)}$ for L2-MHA}\nFor $p=2$, we use the following lemma:\n[\ref{lemma:block_rows}]\nHence a bound on the spectral norm of each block row of $J_f$ can give us an $O(\\sqrt{N})$ bound on $\\|J_f\\|_2$, which may be loose, and it remains an open question as to whether this bound can be tightened.\n\nTo bound the $\\|\\cdot\\|_2$ norm of each row of $J_f$, we use the following lemmas:\n[\ref{lemma:f6}]\n[\ref{lemma:f7}]\n\nAgain using the inequalities $\\|BC\\| \\leq \\|B\\| \\|C\\|$, $\\|B + C\\| \\leq \\|B\\| + \\|C\\|$ and $\\|[A_1, \\ldots, A_N]\\| \\leq \\sum_i \\|A_i\\|$, with the additional equality $\\|B^\\top\\|_2 = \\|B\\|_2$, we have the bound: \n\\begin{align*}\n&\\|[J_{i1}, \\ldots, J_{iN}]\\|_2 \\\\\n & \\leq  2  \\frac{\\|W^Q\\|_2\\|W^{Q^\\top}\\|_2}{\\sqrt{D/H}} \n\\bigg(\\|Y^\\top P^{(i)}Y\\|_2\n + \\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_2 \\bigg) + \\frac{\\|W^Q W^{Q^\\top}\\|_2}{\\sqrt{D/H}} \\\\\n & \\leq  4\\phi^{-1}(N-1) \\frac{\\|W^Q\\|_2^2}{\\sqrt{D/H}}  + \\frac{\\|W^Q W^{Q^\\top}\\|_2}{\\sqrt{D/H}} \\\\\n & \\leq  \\frac{\\|W^Q\\|_2^2}{\\sqrt{D/H}} \\bigg(4\\phi^{-1}(N-1)+1 \\bigg).\n\\end{align*}\nUsing Lemma \\ref{lemma:block_rows}, we have that\n\\begin{align}\n    \\|J_f\\|_2 & \\leq \\frac{\\sqrt{N}\\|W^Q\\|_2^2}{\\sqrt{D/H}} \\bigg(4\\phi^{-1}(N-1)+1 \\bigg) \\label{ineq:2} \\\\\n    & \\leq \\frac{\\sqrt{N}\\|W^Q\\|_2^2}{\\sqrt{D/H}}(4\\log N+1). \\nonumber\n\\end{align}\nTo obtain the final result for the full multihead self-attention $F$, we need a final lemma:\n[\ref{{lemma:block_cols}}]\nRecall that \n\\begin{equation*}\nF: X \\mapsto \\left[f^1(X)W^{V,1}, \\ldots, f^H(X)W^{V,H}\\right] W^O.\n\\end{equation*}\nSince $\\|f^h(X)W^{V,h}\\|_2 \\leq \\|J_{f^h}\\|_2 \\|W^{V,h}\\|_2$, by Lemma \\ref{lemma:block_cols} we have that\n\\begin{equation*}\n    \\left\\|[f^1(X)W^{V,1}, \\ldots, f^H(X)W^{V,H}]\\right\\|_2 \\leq \\sqrt{\\sum_h \\|J_{f^h}\\|_2^2 \\|W^{V,h}\\|_2^2}\n\\end{equation*} and hence\n\\begin{equation}\n    \\lip_2(F) \n    \\leq \\left(\\sqrt{\\sum_h \\|J_{f^h}\\|_2^2 \\|W^{V,h}\\|_2^2}\\right) \\|W^O\\|_2.\n\\end{equation}\nCombining this with Inequality (\\ref{ineq:2}), we have:\n\\begin{equation*}\n    \\lip_2(F) \\leq \\frac{\\sqrt{N}}{\\sqrt{D/H}}\n    \\left(4 \\phi^{-1}(N-1) + 1 \\right) \\left(\\sqrt{\\textstyle\\sum_h \\|W^{Q,h}\\|_2^2\\, \\|W^{V,h}\\|_2^2}\\right) \\|W^O\\|_2.\n\\end{equation*}\n",
                            "statement_html": "<h4>Upper bound on $\\boldsymbol{\\lip_{\\infty}(F)}$ for L2-MHA</h4>\n\nConsider the choice $p=\\infty$, where $\\|J_f\\|_\\infty$ is the maximum absolute row sum of $J_f$. A key observation is that if we can bound the $\\infty$-norm of the Jacobian of $f_i$, a single output of $f$, (i.e.~a single block row $\\|[J_{i1},...,J_{iN}]\\|_\\infty$ of $J_f$) then this is also a bound on $\\|J_f\\|_{\\infty}$ due to permutation equivariance of self-attention; all block rows have the same maximal $\\|\\cdot\\|_\\infty$ when each is optimised over the input $X$. Using this, we can prove that $\\|J_f\\|_\\infty$ admits an upper bound that is $O(\\log N - \\log \\log N)$. Below we state and prove lemmas that lead to the proof of this upper bound.\n\nFirst we analyse the term $\\sqrt{A}^\\top X^\\top P^{(i)}X \\sqrt{A}$, that appears in the first term of $J_{ii}$. Note that for $Y \\coloneqq X \\sqrt{A}$, so that the rows of $Y$ are $\\mathbf{y}_i^\\top \\coloneqq \\mathbf{x}_i^\\top \\sqrt{A}$, we have \n\\begin{equation}\n    \\sqrt{A}^\\top X^\\top P^{(i)}X \\sqrt{A}= Y^\\top P^{(i)} Y =  \\mathrm{Cov}(\\mathbb{Y})\n\\end{equation}\nwhere $\\mathbb{P}(\\mathbb{Y}=\\mathbf{y}_j)=P_{ij} = \\exp(-\\|\\mathbf{y}_j - \\mathbf{y}_i\\|^2_2)/\\sum_k \\exp(-\\|\\mathbf{y}_k - \\mathbf{y}_i\\|^2_2)$. The last equality uses the observation in Equation (7) [in <a href=\"https://arxiv.org/pdf/2006.04710#equation.3.7\">original paper</a>].\n\nThe central inequality used throughout the proof of the main theorem is the following:\n[<a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_3/index.html#lemma%3Akey\">Lemma 3</a>]<br><br>\n\nNote $\\phi(\\log N) = (\\log N) \\exp(\\log N + 1) \\geq N \\log N \\geq N -1$ for $N \\geq 3$. Since $\\phi$ is increasing, we have $\\phi^{-1}(N-1) \\leq \\log(N)$ for $N \\geq 3$. In fact, it is known that $\\phi^{-1}(N-1) = O(\\log N - \\log \\log N)$.\n\nNote the $A$ term in $f(X) = \\tilde{f}(X) A$ allows us to use the above inequality, since $Y^\\top P^{(i)}Y = \\mathrm{Cov}(\\mathbb{Y})$ now appears in the terms of $J_f$:\n\\begin{align}\n    J_{ii}\n    &= 2 \\sqrt{A} [Y^\\top P^{(i)}Y]\\sqrt{A}^\\top + P_{ii} A,  \\\\ \n    J_{ij},\n    &= 2 \\sqrt{A} P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top \\sqrt{A}^\\top + P_{ij} A  \\hspace{2mm} \\text{for $i \\neq j$}.\n\\end{align}\n\nUsing the inequalities $\\|BC\\| \\leq \\|B\\| \\|C\\|$, $\\|B + C\\| \\leq \\|B\\| + \\|C\\|$ and $\\|[A_1, \\ldots, A_N]\\| \\leq \\sum_i \\|A_i\\|$, we have: \n\\begin{align*}\n\\|[J_{i1} &, \\ldots, J_{iN}]\\|_{\\infty}  \\\\\n\\leq & \\|J_{ii}\\|_{\\infty} + \\sum_{j \\neq i} \\|J_{ij}\\|_{\\infty} \\\\\n  \\leq & 2 \\|\\sqrt{A}\\|_{\\infty} \\|Y^\\top P^{(i)}Y\\|_{\\infty} \\|\\sqrt{A}^\\top\\|_{\\infty} + P_{ii} \\|A\\|_{\\infty} \\\\\n & + 2 \\sum_{j \\neq i} \\|\\sqrt{A}\\|_{\\infty} \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty \\|\\sqrt{A}^\\top\\|_{\\infty} + P_{ij} \\|A\\|_{\\infty}\\\\\n  = & 2  \\|\\sqrt{A}\\|_{\\infty}\\|\\sqrt{A}^\\top\\|_{\\infty} \n\\bigg(\\|Y^\\top P^{(i)}Y\\|_\\infty \n + \\sum_{j\\neq i} \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty\\bigg) + \\|A\\|_{\\infty} \\\\\n = & 2  \\frac{\\|W^{Q}\\|_{\\infty}\\|W^{Q^\\top}\\|_{\\infty}}{\\sqrt{D/H}} \n\\bigg(\\|Y^\\top P^{(i)}Y\\|_\\infty \n + \\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty\\bigg) + \\frac{\\|W^Q W^{Q^\\top}\\|_{\\infty}}{\\sqrt{D/H}}.\n\\end{align*}\nFor the first equality, note that $\\sum_j P_{ij}=1$. For the second equality, note that the summand for $j=i$ is $0$ because the term $\\mathbf{y}_i - \\mathbf{y}_j=\\mathbf{0}$. Each of the terms in the brackets are bounded by the following lemmas:<br>[<a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_4/index.html#lemma%3Af3\">Lemma 4</a>]<br>\n[<a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_5/index.html#lemma%3Af4\">Lemma 5</a>]<br><br>\n\nPutting the above lemmas altogether, with the observation $\\sup_X \\|J_f(X)\\|_\\infty = \\sup_X \\|[J_{i1}(X), \\ldots, J_{iN}(X)]\\|_\\infty$ by permutation invariance of $\\|J_f\\|_\\infty$ (since $f$ is permutation equivariant and $\\|\\cdot\\|_\\infty$ is the maximum absolute row sum), we have\n\\begin{align}\n\\|J_f\\|_{\\infty}\n& \\leq 4\\|W^Q\\|_{\\infty}\\|W^{Q^\\top}\\|_{\\infty} \\phi^{-1}(N-1)\n+ \\frac{\\|W^Q W^{Q^\\top}\\|_{\\infty}}{\\sqrt{D/H}} \\nonumber\\\\\n& \\leq \\|W^Q\\|_{\\infty}\\|W^{Q^\\top}\\|_{\\infty} \\left(4\\phi^{-1}(N-1) + \\frac{1}{\\sqrt{D/H}}\\right) \\label{ineq:infty}\\\\\n& \\leq \\|W^Q\\|_{\\infty} \\|W^{Q^\\top}\\|_{\\infty} \\left(4\\log N + \\frac{1}{\\sqrt{D/H}}\\right), \\nonumber\n\\end{align}\nwhere the last inequality holds for $N \\geq 3$.\n\nThe full multihead attention map that combines the heads $f^h(X)$ is:\n\\begin{equation*}\nF: X \\mapsto \\left[f^1(X)W^{V,1}, \\ldots f^H(X)W^{V,H}\\right] W^O = g(X) W^V W^O\n\\end{equation*}\nwhere $g:X \\mapsto [f^1(X),\\ldots,f^H(X)]$, $W^O \\in \\mathbb{R}^{D \\times D}$ and\n\\begin{equation*}\n    W^V = \\begin{bmatrix}\n    W^{V,1} & \\dots & 0 \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    0 & \\dots & W^{V,H} \\\\\n    \\end{bmatrix} \\in \\mathbb{R}^{DH \\times D}.\n\\end{equation*}\nNote the Jacobian $J_g$ is a block matrix whose rows are $J_{f^h}$, hence $\\|J_g\\|_{\\infty} = \\max_h \\|J_{f^h}\\|_{\\infty}$, and similarly $\\|W^{V^\\top}\\|_{\\infty} = \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty}$. Hence we have\n\\begin{equation*}\n    \\lip_{\\infty}(F) \\leq \\max_h \\|J_{f^h}\\|_{\\infty} \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty} \\|W^{O^\\top}\\|_{\\infty}.\n\\end{equation*}\n\nCombining this with Inequality (\\ref{ineq:infty}), we have:\n\\begin{equation*}\n    \\lip_{\\infty}(F)  \\leq \\left(4 \\phi^{-1}(N-1) + \\frac{1}{\\sqrt{D/H}}\\right) \\max_h \\|W^{Q,h}\\|_{\\infty} \\|W^{{Q,h}^\\top}\\|_{\\infty} \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty} \\ \\|W^{O^\\top}\\|_{\\infty}.\n\\end{equation*}\n\n<h4>Upper bound on $\\boldsymbol{\\lip_2(F)}$ for L2-MHA</h4>\n\nFor $p=2$, we use the following lemma:<br>[<a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_6/index.html#lemma%3Ablock_rows\">Lemma 6</a>]<br><br>\nHence a bound on the spectral norm of each block row of $J_f$ can give us an $O(\\sqrt{N})$ bound on $\\|J_f\\|_2$, which may be loose, and it remains an open question as to whether this bound can be tightened.\n\nTo bound the $\\|\\cdot\\|_2$ norm of each row of $J_f$, we use the following lemmas:<br>[<a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_7/index.html#lemma%3Af6\">Lemma 7</a>]<br>\n[<a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_8/index.html#lemma%3Af7\">Lemma 8</a>]<br><br>\n\nAgain using the inequalities $\\|BC\\| \\leq \\|B\\| \\|C\\|$, $\\|B + C\\| \\leq \\|B\\| + \\|C\\|$ and $\\|[A_1, \\ldots, A_N]\\| \\leq \\sum_i \\|A_i\\|$, with the additional equality $\\|B^\\top\\|_2 = \\|B\\|_2$, we have the bound: \n\\begin{align*}\n&\\|[J_{i1}, \\ldots, J_{iN}]\\|_2 \\\\\n & \\leq  2  \\frac{\\|W^Q\\|_2\\|W^{Q^\\top}\\|_2}{\\sqrt{D/H}} \n\\bigg(\\|Y^\\top P^{(i)}Y\\|_2\n + \\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_2 \\bigg) + \\frac{\\|W^Q W^{Q^\\top}\\|_2}{\\sqrt{D/H}} \\\\\n & \\leq  4\\phi^{-1}(N-1) \\frac{\\|W^Q\\|_2^2}{\\sqrt{D/H}}  + \\frac{\\|W^Q W^{Q^\\top}\\|_2}{\\sqrt{D/H}} \\\\\n & \\leq  \\frac{\\|W^Q\\|_2^2}{\\sqrt{D/H}} \\bigg(4\\phi^{-1}(N-1)+1 \\bigg).\n\\end{align*}\nUsing <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_6/index.html#lemma%3Ablock_rows\">Lemma 6</a>, we have that\n\\begin{align}\n    \\|J_f\\|_2 & \\leq \\frac{\\sqrt{N}\\|W^Q\\|_2^2}{\\sqrt{D/H}} \\bigg(4\\phi^{-1}(N-1)+1 \\bigg) \\label{ineq:2} \\\\\n    & \\leq \\frac{\\sqrt{N}\\|W^Q\\|_2^2}{\\sqrt{D/H}}(4\\log N+1). \\nonumber\n\\end{align}\nTo obtain the final result for the full multihead self-attention $F$, we need a final lemma:<br>\n[<a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_9/index.html#lemma%3Ablock_cols\">Lemma 9</a>]<br><br>\nRecall that \n\\begin{equation*}\nF: X \\mapsto \\left[f^1(X)W^{V,1}, \\ldots, f^H(X)W^{V,H}\\right] W^O.\n\\end{equation*}\nSince $\\|f^h(X)W^{V,h}\\|_2 \\leq \\|J_{f^h}\\|_2 \\|W^{V,h}\\|_2$, by <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_9/index.html#lemma%3Ablock_cols\">Lemma 9</a> we have that\n\\begin{equation*}\n    \\left\\|[f^1(X)W^{V,1}, \\ldots, f^H(X)W^{V,H}]\\right\\|_2 \\leq \\sqrt{\\sum_h \\|J_{f^h}\\|_2^2 \\|W^{V,h}\\|_2^2}\n\\end{equation*} and hence\n\\begin{equation}\n    \\lip_2(F) \n    \\leq \\left(\\sqrt{\\sum_h \\|J_{f^h}\\|_2^2 \\|W^{V,h}\\|_2^2}\\right) \\|W^O\\|_2.\n\\end{equation}\nCombining this with Inequality (\\ref{ineq:2}), we have:\n\\begin{equation*}\n    \\lip_2(F) \\leq \\frac{\\sqrt{N}}{\\sqrt{D/H}}\n    \\left(4 \\phi^{-1}(N-1) + 1 \\right) \\left(\\sqrt{\\textstyle\\sum_h \\|W^{Q,h}\\|_2^2\\, \\|W^{V,h}\\|_2^2}\\right) \\|W^O\\|_2.\n\\end{equation*}",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "[missing]"
                        }
                    }
                ],
                "corollaries": [
                    {
                        "statement_id": "538626a5-cda2-4ab3-90d7-6dc79ff8a845",
                        "paper_id": "083fc279-e9c8-48f3-94e5-2b3a363832b3",
                        "library_nr": 1,
                        "library_name": "Corollary 1",
                        "title": "Lipschitz Bound for Neural Networks",
                        "statement_original_tex": "\\begin{corollary} \\label{cor:lip_conv}\nFor a fully-connected network (\\verb!FCN!) or a convolutional neural network (\\verb!CNN!) $f=W_K \\circ \\rho_{K-1} \\circ W_{K-1} \\circ \\ldots \\circ \\rho_1 \\circ W_1$, we have $\\lip_p(f) \\leq \\prod_k \\|W_k\\|_p$ under a choice of $p$-norm with $1$-Lipschitz non-linearities $\\rho_k$.\n\\end{corollary}",
                        "statement_html": "For a fully-connected network ($\\verb!FCN!$) or a convolutional neural network ($\\verb!CNN!$) $f=W_K \\circ \\rho_{K-1} \\circ W_{K-1} \\circ \\ldots \\circ \\rho_1 \\circ W_1$, we have $\\lip_p(f) \\leq \\prod_k \\|W_k\\|_p$ under a choice of $p$-norm with $1$-Lipschitz non-linearities $\\rho_k$.",
                        "statement_type": "corollary",
                        "statement_motivation_html": "Understanding the Lipschitz constant of a neural network is crucial for analyzing its robustness and generalization capabilities. The given inequality $\\lip_p(f) \\leq \\prod_k \\|W_k\\|_p$ provides an upper bound on the Lipschitz constant of the network $f$ in terms of the $p$-norms of its weight matrices $W_k$. This is particularly useful when designing or evaluating neural networks, as it helps in ensuring that small changes in the input do not lead to disproportionately large changes in the output, thereby promoting stability and reliability in the network's predictions.",
                        "html_url": "library/corollaries/corollary_1/index.html",
                        "proof": null,
                        "parent_id": "194c3400-773c-45af-8af4-613495fc57c5"
                    }
                ]
            },
            "mathjax_macros": [
                "emph: [\"\\\\textit{#1}\", 1]",
                "bm: [\"\\\\boldsymbol{\\\\mathbf{#1}}\", 1]",
                "mathds: [\"\\\\mathbf{#1}\", 1]",
                "textsl: [\"\\\\textit{#1}\", 1]",
                "vspace: [\"\", 1]",
                "xspace: \"\"",
                "Tr: \"\\\\operatorname{Tr}\"",
                "softmaxOp: \"\\\\operatorname{softmax}\"",
                "lip: \"\\\\operatorname{Lip}\"",
                "diag: \"\\\\operatorname{diag}\"",
                "spaces: \"\\\\hspace{2mm}\"",
                "unif: \"\\\\pazocal{U}\"",
                "softmax: [\"\\\\softmaxOp\\\\left(#1\\\\right)\", 1]",
                "norm: [\"\\\\left\\\\lVert#1\\\\right\\\\rVert\", 1]",
                "andriy: [\"\\\\textcolor{blue}{[AM: #1]}\", 1]",
                "hyunjik: [\"\\\\textcolor{red}{[HK: #1]}\", 1]",
                "george: [\"\\\\textcolor{magenta}{[George: #1]}\", 1]"
            ],
            "mathjax_environments": [
                "subequations: [\"{\", \"}\"]"
            ],
            "label2statementid": {
                "Lipschitz_definition": "a0589e9a-e69c-4a1b-addc-72a7905f06eb",
                "lemma:tie_weights": "6eef858c-f19b-4ecf-997c-94ebeaf4dd62",
                "lemma:key": "ac15f238-f0d7-4541-bd58-18f3bd73c6d6",
                "lemma:f3": "f47d629b-cbfa-4c6a-85ac-960378cc9ec4",
                "lemma:low_rank": "520091f9-1b15-4fc7-82e9-19ab260a6003",
                "lemma:f4": "520091f9-1b15-4fc7-82e9-19ab260a6003",
                "lemma:block_rows": "82d5ab2b-2f9b-4d59-94d0-0e0803c15217",
                "lemma:f6": "86fe1d7b-9edf-4f09-88a6-ac07330ef7fb",
                "lemma:f7": "86d0562a-1d70-4acc-9fbb-34f5b10581e0",
                "lemma:block_cols": "ab1936ea-086f-4fd2-93c8-f1642c81d056",
                "thm:jacobian": "01076721-09d6-4ac8-81ee-3b37a722d45b",
                "thm:dp_not_lipschitz": "44ba1450-0d9a-487f-b810-13a780878029",
                "eq:jac_dot_general": "44ba1450-0d9a-487f-b810-13a780878029",
                "eq:cov_general": "44ba1450-0d9a-487f-b810-13a780878029",
                "thm:main": "b539a1ff-cd1f-4dad-baf4-898790c647e8",
                "ineq:infty": "b539a1ff-cd1f-4dad-baf4-898790c647e8",
                "ineq:2": "b539a1ff-cd1f-4dad-baf4-898790c647e8",
                "cor:lip_conv": "538626a5-cda2-4ab3-90d7-6dc79ff8a845"
            }
        },
        {
            "paper_id": "fa48a779-dedb-4614-9ac9-e052c1896ac6",
            "title": "Transformer Normalisation Layers and the Independence of Semantic Subspaces",
            "authors": [
                "Stephen Menary",
                "Samuel Kaski",
                "Andre Freitas"
            ],
            "year": 2024,
            "source_url": "https://arxiv.org/abs/2406.17837",
            "html_url": "library/papers/transformer_normalisation_layers_and_the_independence_of_semantic_subspaces/index.html",
            "bibtex": "@misc{menary2024transformernormalisationlayersindependence,\n\ttitle={Transformer Normalisation Layers and the Independence of Semantic Subspaces}, \n\tauthor={Stephen Menary and Samuel Kaski and Andre Freitas},\n\tyear={2024},\n\teprint={2406.17837},\n\tarchivePrefix={arXiv},\n\tprimaryClass={cs.LG},\n\turl={https://arxiv.org/abs/2406.17837}\n}",
            "original_tex": "==== BEGINNING OF /2406.17837/main.tex ====\n\\documentclass{article}\n\n\\usepackage{amsmath}\n\\usepackage{amsthm}\n\\usepackage{amssymb}\n\\usepackage{multirow}\n\\usepackage{makecell}\n\\usepackage{pbox}\n\\usepackage{graphicx}\n\\usepackage[]{mdframed}\n\\usepackage{float}\n\\usepackage[dvipsnames]{xcolor}\n\\usepackage[numbers,sort&compress]{natbib}\n\n\\newtheorem{theorem}{Theorem}\n\\newtheorem{corollary}{Corollary}[theorem]\n\n\n\\renewcommand\\theadalign{bc}\n\\renewcommand\\theadfont{\\bfseries}\n\\renewcommand\\theadgape{\\Gape[4pt]}\n\\renewcommand\\cellgape{\\Gape[4pt]}\n\n\n% ready for submission\n\\usepackage[preprint]{neurips_2024}\n\n\n% to compile a preprint version, e.g., for submission to arXiv, add add the\n% [preprint] option:\n%     \\usepackage[preprint]{neurips_2024}\n\n\n% to compile a camera-ready version, add the [final] option, e.g.:\n%     \\usepackage[final]{neurips_2024}\n\n\n% to avoid loading the natbib package, add option nonatbib:\n%    \\usepackage[nonatbib]{neurips_2024}\n\n\n\\usepackage[utf8]{inputenc} % allow utf-8 input\n\\usepackage[T1]{fontenc}    % use 8-bit T1 fonts\n\\usepackage{hyperref}       % hyperlinks\n\\usepackage{url}            % simple URL typesetting\n\\usepackage{booktabs}       % professional-quality tables\n\\usepackage{amsfonts}       % blackboard math symbols\n\\usepackage{nicefrac}       % compact symbols for 1/2, etc.\n\\usepackage{microtype}      % microtypography\n\\usepackage{xcolor}         % colors\n\n\n%\\title{Attention Layer Normalisation is Important for the Separability of Independent Residual Subspaces}\n\\title{Transformer Normalisation Layers and the Independence of Semantic Subspaces}\n%\\title{Transformer Normalisation Layers and the Separation of Independent Subspaces of the Residual Stream}\n\n\n% The \\author macro works with any number of authors. There are two commands\n% used to separate the names and addresses of multiple authors: \\And and \\AND.\n%\n% Using \\And between authors leaves it to LaTeX to determine where to break the\n% lines. Using \\AND forces a line break at that point. So, if LaTeX puts 3 of 4\n% authors names on the first line, and the last on the second line, try using\n% \\AND instead of \\And before the third author name.\n\n\n\\author{%\n  Stephen Menary \\\\\n  University of Manchester, UK \\\\\n  \\texttt{stephen.menary@manchester.ac.uk} \\\\\n   \\And\n  Samuel Kaski \\\\\n  $^{1}$ University of Manchester, UK \\\\\n  $^{2}$ Aalto University, Finland \\\\\n  \\texttt{samuel.kaski@aalto.fi} \\\\\n   \\And\n  Andr{\\'e} Freitas \\\\\n    $^{1}$ Department of Computer Science, University of Manchester, UK \\\\\n    $^{2}$ Idiap Research Institute, Switzerland \\\\\n    $^{3}$ National Biomarker Centre, CRUK-MI, University of Manchester, UK\\\\\n  \\texttt{andre.freitas@manchester.ac.uk} \\\\\n}\n\n\n\\begin{document}\n\n\n%%%\n%%%  Title\n%%%\n\\maketitle\n\n%%%\n%%%  Abstract\n%%%\n\\input{sections/abstract}\n\n%%%\n%%%  Section: Introduction\n%%%\n\\input{sections/introduction}\n\n%%%\n%%%  Section: The idea\n%%%\n\\input{sections/the_idea}\n\n%%\n%%  Section:  Related Works\n%%\n\\input{sections/related_works}\n\n%%\n%%  Section:  Formulation\n%%\n\\input{sections/formulation}\n\n%%\n%%  Section:  Theory: residual structure\n%%\n%\\clearpage\n\\input{sections/theory_structure}\n\n%%\n%%  Section:  Theory: circuit stability\n%%\n%\\clearpage\n\\input{sections/theory_stability}\n\n%%\n%%  Section: Experimental results\n%%\n\\input{sections/experiments}\n\n%%\n%%  Section:  Limitations\n%%\n\\input{sections/limitations_outlook}\n\n%%\n%%  Acknowledgements\n%%\n\\begin{ack}\nThis work is supported by UKRI Turing AI World-Leading Researcher Fellowship (EP/W002973/1). This work was partially funded by the Swiss National Science Foundation (SNSF) project NeuMath (\\href{https://data.snf.ch/grants/grant/204617}{200021\\_204617}), by the EPSRC grant EP/T026995/1, ``EnnCore: End-to-End Conceptual Guarding of Neural Architectures'' under Security for all in an AI enabled society, by the CRUK National Biomarker Centre, and supported by the Manchester Experimental Cancer Medicine Centre and the NIHR Manchester Biomedical Research Centre.\n\\end{ack}\n\n%%\n%%  References\n%%\n%\\clearpage\n\\bibliographystyle{unsrt} % We choose the \"plain\" reference style\n\\bibliography{bibliography}\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n%\\clearpage\n%\\input{checklist}\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n\\appendix\n\n%%\n%%  Appendix:  Experimental setup\n%%\n\\clearpage\n\\input{appendices/experimental_setup}\n\n%%\n%%  Appendix:  Main experiments \n%%\n\\clearpage\n\\input{appendices/main_experiments_extended}\n\n%%\n%%  Appendix:  Reproducibility of results across model variations\n%%\n\\clearpage\n\\input{appendices/experiment_replications}\n\n%%\n%%  Appendix:  Pre-Norm vs QKV-Norm\n%%\n\\clearpage\n\\input{appendices/pre_vs_qkvnorm}\n\n%%\n%%  Appendix:  Supplementary theorems\n%%\n\\clearpage\n\\input{appendices/supplementary_theorems}\n\n%%\n%%  Appendix:  Theorem proofs\n%%\n\\clearpage\n\\input{appendices/proofs}\n\n\n\n\\end{document}\n==== END OF /2406.17837/main.tex ====\n==== BEGINNING OF /2406.17837/sections/theory_structure.tex ====\n\\section{Theory: representation structure required for independent subspaces}\n\\label{sec: theory: residual structure}\n\nLet $\\mathbb{S}^N \\equiv \\mathbb{R}^N$ be an $N$-dimensional latent representation of $\\mathcal{X}$ or $\\mathcal{Y}$.\n\n\\begin{mdframed}[backgroundcolor=red!5]\n\\textbf{[Definition] ~ Semantic subspace:} any independent $N_\\alpha$-dimensional subspace $\\mathbb{S}_\\alpha^{N_\\alpha} \\subset \\mathbb{S}^N$ for which every element may be uniquely identified by some parameters $\\theta_\\alpha$, such that it is possible for the attention scores $w_t$ to be fully specified by $\\theta_\\alpha$. \\textbf{Semantic separability:} ability for parallel heads to be fully specified by different semantic subspaces.\n%\\textbf{[Corollary]} ~ Using linear-attention, the residual representation may be written as the tensor sum $\\mathbb{S} = \\bigoplus_\\alpha \\mathbb{S}_\\alpha$ where $\\{\\alpha\\}$ includes the special subspaces \\texttt{other} (a subspace that carries degrees-of-freedom but is non-attended) and \\texttt{null} (the attention null-space), and $\\mathbb{S}_\\alpha$ has $N_\\alpha$-dimensions such that $N=\\sum_\\alpha N_\\alpha$.\n\\end{mdframed}\n\n\nLet $\\{\\alpha\\}$ be the set of indivisible semantic subspaces. This can be seen as a \\textit{co-ordinate system} for the attendable embedding space. Semantic separability requires that each co-ordinate $\\alpha$ be independently measurable by an attention head. Let $\\mathbb{S}^N$ contain $N_s$ indivisible semantic subspaces $1\\leq\\alpha\\leq N_s$. Then $\\mathbb{S}^N=\\prod_\\alpha \\mathbb{S}^{N_\\alpha}_\\alpha \\bigoplus\\mathbb{S}_\\mathrm{null}$ such that $\\sum_\\alpha N_\\alpha \\leq N$ satisfies semantic separability, where $\\prod_\\alpha,\\bigoplus$ are Cartesian products and $\\mathbb{S}_\\mathrm{null}$ is a separable space of non-attended information. \n\nThe following theorems derive the representation structure required for semantic separability:\n\n\\begin{mdframed}[backgroundcolor=blue!5]\n\\underline{\\textbf{Semantically separable representation structures}}\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\textit{[Proofs in appendix~\\ref{appendix: proofs}]}\n\n\\begin{theorem}\n    \\texttt{No-Norm}: If two heads with finite non-zero temperature attend to different semantic subspaces, the subspaces must be linearly independent $\\mathbb{S}^{N_\\alpha}_\\alpha \\equiv \\mathbb{R}^{N_\\alpha}$. Corollary: $W_{QK}$ is a low-rank matrix with (left and right) null-spaces that span all non-attended information.\n\\label{theorem: structure: no-norm}\n\\end{theorem}\n\n\\begin{theorem}\n    \\texttt{Pre-Norm}: Semantic subspaces must be represented as orthogonal spheres $\\mathbb{S}^{N_\\alpha}\\equiv\\mathcal{S}^{N_\\alpha-1}$ defined using the $L_2$-norm. Corollary: if either orthogonality or constant-norm are violated, semantic subspaces interfere through a multiplicative factor on $w_t$.\n\\label{theorem: structure: pre-norm}\n\\end{theorem}\n\n\\begin{theorem}\n    \\texttt{QKV-Norm}: Semantic subspaces must be linearly independent.\n\\label{theorem: structure: qkv-norm}\n\\end{theorem}\n\\end{mdframed}\n\nWe note that every linear subspace $\\mathbb{R}^{N_\\alpha}$ has $N_\\alpha$ continuous degrees of freedom, whilst $\\mathcal{S}^{N_\\alpha-1}$ has only $N_\\alpha-1$, the other being removed by the fixed-norm constraint. The subspace $\\mathcal{S}^0$ is allowed and may be seen as a binary variable with values $\\pm const_\\alpha$, and the total representation can store $N_s$ such variables. For \\texttt{QKV-Norm}, we note that the residual subspace $\\mathbb{R}^{N_\\alpha}$ only contributes $N_\\alpha-1$ continuous degrees of freedom to the attention calculation, because we apply the projection $\\mathbb{R}^{N_\\alpha} \\rightarrow \\mathcal{S}^{N_\\alpha-1}$ after extracting the subspace. Table~\\ref{table:residual structures main} provides a summary.\n\n\\vspace{0.2cm}\n\\noindent\n\\textbf{Structure of messages}\n\nWe note the special case of compositional annotation, in which a layer creates a semantic subspace that is extracted by a later layer. This is used by circuits including the \\textit{induction circuit} \\cite{elhage2021mathematical} described in section~\\ref{sec: idea}. By normalising the inputs, \\texttt{Pre-Norm} induces a \\textit{spheroid} message structure close to the \\textit{sphere} required for separability in later layers. This may facilitate compositional annotation, aiding in circuit-formation. Message structures are summarised in Table~\\ref{table:value structures main}.\n\n\\begin{table}[h]\n\\centering\n\\begin{tabular}{rlll}\n    Strategy                 &  $\\mathbb{S}^N$  &  Representation structure   &  Attendable d.o.f.  \\\\\n    \\hline\n    \\texttt{No-Norm}         &  $\\prod_\\alpha \\mathbb{R}^{N_\\alpha}$     &  Linearly independent subspaces  &  $N$            \\\\\n    \\texttt{Pre-LayerNorm}   &  $\\prod_\\alpha \\mathcal{S}^{N_\\alpha-1}$  &  Orthogonal spheres $\\perp 1^N$  &  $N - N_s - 1$  \\\\\n    \\texttt{Pre-RMSNorm}  &  $\\prod_\\alpha \\mathcal{S}^{N_\\alpha-1}$  &  Orthogonal spheres              &  $N - N_s$      \\\\\n    \\texttt{QKV-Norm}     &  $\\prod_\\alpha \\mathbb{R}^{N_\\alpha}$     &  Linearly independent subspaces  &  $N - N_s$      \\\\\n\\end{tabular}\n\\vspace{0.1cm}\n\\caption{Representation structure required for semantic separability; \\textit{d.o.f.} means \\textit{degrees of freedom}.}\n\\label{table:residual structures main}\n\\vspace{0.1cm}\n\\begin{tabular}{rlll}\n    Strategy                     &  $m_t$  &  Structure of $m_t$  &  Compositional annotation if  \\\\\n    \\hline\n    \\texttt{No-Norm}    &   $W_{OV} y_t$                       &  Linear      &   $m_t$ on independent subspace  \\\\\n    \\texttt{Pre-Norm}   &   $W_{OV} \\texttt{N}(y_t;\\alpha_v)$  &  Spheroid    &   $m_t$ on orthogonal sphere     \\\\\n    \\texttt{QKV-Norm}   &   $W_O\\texttt{N}(W_Vy_t;\\alpha_v)$   &  Spheroid    &   $m_t$ on independent subspace  \\\\\n\\end{tabular}\n\\vspace{0.1cm}\n\\caption{Summary of message structures induced by different placements of normalisation layer.}\n\\label{table:value structures main}\n\\end{table}\n==== END OF /2406.17837/sections/theory_structure.tex ====\n==== BEGINNING OF /2406.17837/sections/the_idea.tex ====\n\\section{The idea}\n\\label{sec: idea}\n\n\\noindent\n\\textbf{Independent subspaces are observed in real-world transformer circuits}\n\nBefore providing a formal definition in section~\\ref{sec: theory: residual structure}, we explain what we mean by a \\textit{semantic subspace} of the latent representation. To emphasise that this is observed in real-world models, we use a known example: the \\textit{induction circuit} \\cite{olsson2022context,elhage2021mathematical}. This two-layer circuit emerges in next-token-prediction models and implements a simple contextual reasoning algorithm called \\textit{prefix-matching}. %\\footnote{\\textit{In-context learning} is when the answer to the prediction task does not exist in training, and it can only be solved by parsing the observed provided at inference-time.}\n\nConsider text to be a sequence of tokens\\footnote{In this example, we tokenise per-word to help with visualisation.}, and our task is to predict the next token at every point. The induction circuit solves this by copying a previous example from the context window: e.g. if the input includes the phrase ``Harry Potter'' and the last observed word was ``Harry'', the induction circuit will predict that ``Potter'' comes next. This solves the task even if the combination ``Harry Potter'' never occurred in the training data.\n %Importantly, the model may have nave seen the combination ``Harry Potter'' at training time, and so \\textit{the only} way to solve this task is by consulting the context.\n\nTo achieve this, we initially create an embedding for each token, encoding it's \\textit{position} and \\textit{type}. Attention layers then \\textit{copy information between embeddings in a directed way}, using two components that determine (i) \\textit{which} embeddings to extract information from, and (ii) \\textit{what} to extract. Remarkably, the model learns to implement logical gates that we will call ``match\\&pass'', internally composing the algorithm:\n\n\\includegraphics[page=2, width=\\textwidth, clip=True, trim=0 17cm 0 0cm]{figures/Paper_diagrams.pdf}\n\nEach match\\&pass step operates only on an independent subspace of information, which we will call a \\textit{semantic subspace}. In this example, there are four semantic subspaces corresponding to \\textit{position}, \\textit{type}, \\textit{prev-type}, and \\textit{pred-suffix}. We observe that the latent embeddings can contain various information, and it is instructive to think of them as memory buffers rather than tokens. The principle of \\textit{composing logical operations that act on latent semantic subspaces} is also observed in the more complex example of indirect-object identification in GPT2-Small \\cite{wang2022interpretability}.\n\n%\\clearpage\n%\\textbf{Linear-attention requires linearly-independent semantic subspaces}\n\n%t $P_\\alpha x_\\alpha = x_\\alpha$ and $P_\\alpha x_{\\beta\\neq\\alpha} = 0$, then $P_\\alpha x = P_\\alpha \\sum_\\beta x_\\beta = x_\\alpha$.\n%\\begin{equation}\n%\tP_\\alpha x ~=~ P_\\alpha \\sum_\\beta x_\\beta ~=~ \\sum_\\beta P_\\alpha x_\\beta ~=~ P_\\alpha x_\\alpha ~+~ \\sum_{\\beta\\neq\\alpha} P_\\alpha x_\\beta ~=~ P_\\alpha x_\\alpha ~+~ 0 ~=~ x_\\alpha\n%\\end{equation}\n%can be used to extract an individual semantic for use in e.g. the match\\&pass operation.\n\n\n\\vspace{0.2cm}\n\\noindent\n\\textbf{The problem with Pre-Norm}\n\nWe express the latent embeddings as $x = \\sum_\\alpha x_\\alpha$ where $x_\\alpha$ encodes the value of concept $\\alpha$. This is important, because linear-attention layers extract information from $x$ using linear operators (section~\\ref{sec: formulation}), and can only isolate $x_\\alpha$ if each subspace $\\{x_\\alpha~|~\\alpha\\}$ is \\textit{linearly independent}. In other words, there must always exist a linear projection operator $P_\\alpha$ such that $P_\\alpha x = x_\\alpha$.\n\nMost transformers use either \\texttt{RMSNorm} \\cite{NEURIPS2019_1e8a1942} or \\texttt{LayerNorm} \\cite{DBLP:journals/corr/BaKH16} for their internal normalisation layers. Geometrically, \\texttt{RMSNorm} projects a vector $z \\in \\mathbb{R}^N$ onto the unit-sphere $S^{N-1}$ according to\n\\begin{equation}\n\tz ~\\rightarrow~ \\frac{z}{|z|} ~~~~~~~~~~~~~~~~~~~\\mathrm{where}~|z| ~\\triangleq~ \\sqrt[+]{\\sum_{i=1}^N z_i^2} ~~\\mathrm{is~the}~L_2\\mathrm{-norm.}\n\\end{equation}\n\\texttt{LayerNorm} is similar, projecting onto the sphere $S^{N-2}$ defined perpendicular to the direction $1^N$. This does not affect our analysis, and we focus on \\texttt{RMSNorm} for simplicity. Normalisation layers sometimes also include gain and/or bias parameters, applying a stretch-and-translate to the sphere. \\texttt{Pre-Norm} \\cite{DBLP:journals/corr/abs-2002-04745} normalises the latent embeddings at the \\textit{input} to every attention layer. Consider the example $x = x_\\mathrm{pos} + x_\\mathrm{type} + x_\\mathrm{prev-type}$. %This is transformed to\n%\\begin{equation}\n%\\frac{x}{|x|} ~=~ \\frac{x_\\mathrm{pos} ~+~ x_\\mathrm{type} ~+~ x_\\mathrm{prev-type}}{|x_\\mathrm{pos} ~+~ x_\\mathrm{type} ~+~ x_\\mathrm{prev-type}|}\n%\\end{equation}\nApplying \\texttt{Pre-Norm}, we find $P_\\mathrm{pos}  x = x_\\mathrm{pos}$ is replaced by\n\\begin{equation}\nP_\\mathrm{pos}  \\frac{x}{|x|} ~=~  \\frac{P_\\mathrm{pos} x}{|x|} ~=~ \\frac{x_\\mathrm{pos}}{|x_\\mathrm{pos} ~+~ x_\\mathrm{type} ~+~ x_\\mathrm{prev-type}|}\n\\end{equation}\n\\textbf{Therefore it is impossible for a linear-attention layer to extract $x_\\mathrm{pos}$ without interference from $x_\\mathrm{type}$ and $x_\\mathrm{prev-type}$, unless $|x_\\mathrm{pos} + x_\\mathrm{type} + x_\\mathrm{prev-type}|$ is a constant}. In general, we have $P_\\alpha \\frac{x}{|x|} = \\frac{x_\\alpha}{|\\sum_\\beta x_\\beta|}$, and semantic subspaces are entangled unless $|\\sum_\\alpha x_\\alpha|$ is constant. This is only possible if $|x_\\alpha|^2 = const_\\alpha ~\\forall~ \\alpha$, i.e. every subspace is a sphere, and $x_\\alpha^Tx_\\beta = 0 ~\\forall~ x_\\alpha,x_{\\beta \\neq \\alpha}$, i.e. all spheres are orthogonal (to maintain independence). This has several possible implications:\n\\begin{enumerate}\n    \\item It is a restrictive structure that must be learned during training, with unknown difficulty. Finite steps of gradient descent may separate the model from the manifold of acceptable representations, hindering the learning of circuit components that require semantic separation, like match\\&pass, especially when training with large learning rates.\n    \\item The constraint $|x_\\alpha|^2 = const_\\alpha$ removes a degree of freedom for every $\\alpha$, reducing the information capacity of the embedding space. For example, an embedding on $\\mathbb{R}^5$ could have the two-subspace structure $\\mathcal{S}^2 \\bigoplus \\mathcal{S}^1$ but not $\\mathcal{S}^2 \\bigoplus \\mathcal{S}^2$.\n\t\\item We hypothesise that the structure may be violated by (i) a tradeoff with other representational effects, (ii) imperfect model training, or (iii) encountering unexpected semantic combinations at inference-time when generalising out-of-distribution. These would cause semantic subspaces to interfere through their common normalisation factor, manifesting as noise on the $L_2$-norms of the \\{query,~key,~value\\} vectors.\n\t\\item It is a structure that we can search for empirically.\n\\end{enumerate}\n\n\n\\vspace{0.2cm}\n\\noindent\n\\textbf{A possible solution: QKV-Norm}\n\nA natural fix could be to apply the normalisation layer \\textit{after} the linear operators. In practice this means that we normalise the \\{query,~key,~value\\} vectors, called \\texttt{QKV-Norm} and defined in section~\\ref{sec: formulation}. \n\n\n\\vspace{0.2cm}\n\\noindent\n\\textbf{Paper strategy}\n\nOur work is based on three key observations: (i) semantic subspaces are observed in known circuits, (ii) they contribute to the model behaviour, and (iii) \\texttt{Pre-Norm} requires them to follow a strict latent embedding structure or else interfere through the $L_2$-norms of the \\{query,~key,~value\\} vectors.\n\nHowever, it is difficult to demonstrate specific examples of subspace interference. Firstly, a fully-converged model should learn to manage interference for in-distribution examples. Instead, we expect it to concern (i) training stability, (ii) model inductive bias, and (iii) out-of-distribution behaviour. Secondly, circuit explainability is difficult, only being achieved in simple cases. In general we expect circuits to become complicated, contain steps that are harder to interpret than match\\&pass, and exploit non-interpretable latent subspaces. Difficulty is further increased by polysemanticity \\cite{elhage2022superposition}, the ability for heads and features to change behaviour according to context.\n\nIn this work, we take an abstract approach instead. We formally define latent semantic separability, then investigate the theoretical consequences for \\texttt{Pre-Norm} architectures if this behaviour is important generally. This allows us to make testable predictions about representation structure and model stability without needing to fully reverse-engineer a network or explain subspaces in human terms. We then place some data-driven limits on the effect size. Nonetheless, direct observation remains important, and we hope that future works can confirm or falsify the importance of the proposed representation structure and interference effect.\n==== END OF /2406.17837/sections/the_idea.tex ====\n==== BEGINNING OF /2406.17837/sections/abstract.tex ====\n\n\\begin{abstract}\nRecent works have shown that transformers can solve contextual reasoning tasks by internally executing computational graphs called \\textit{circuits}. Circuits often use attention to logically match information from subspaces of the representation, e.g. using position-in-sequence to identify the previous token. In this work, we consider a \\textit{semantic subspace} to be any independent subspace of the latent representation that can fully determine an attention distribution. We show that \\texttt{Pre-Norm}, the placement of normalisation layer used by state-of-the-art transformers, violates this ability unless the model learns a strict representation structure of orthogonal spheres. This is because it causes linear subspaces to interfere through their common normalisation factor. Theoretically, we analyse circuit stability by modelling this interference as random noise on the $L_2$-norms of the query/key/value vectors, predicting a phenomenon of \\textit{circuit collapse} when sparse-attention shifts to a different token. Empirically, we investigate the sensitivity of real-world models trained for mathematical addition, observing a 1\\% rate of circuit collapse when the norms are artificially perturbed by $\\lesssim$10\\%. We contrast \\texttt{Pre-Norm} with \\texttt{QKV-Norm}, which places normalisation \\textit{after} the attention head's linear operators. Theoretically this relaxes the representational constraints. Empirically we observe comparable in-distribution but worse out-of-distribution performance. \n%This phenomenological study improves our understanding of model behaviour without requiring full reverse-engineering.\n%However, we find evidence that it changes the algorithmic behaviour of the model. %We finally study the emergent residual structures. %However, we show evidence that it also changes the inductive bias of circuit-formation. %We hypothesise that robustness to semantic entanglement is important when generalising out-of-distribution to applications with un-seen semantic combinations. %By contrast, most interpretability works ignore normalisation due to it's ``linear up to scale'' structure. \n%We propose \\textit{semantic separability at the attention-head level} as a target for transformer architecture design. This stabilises the  ``match\\&pass'' operation performed by transformers when implementing known contextual circuits. Theoretically, we define a semantic as a residual degree-of-freedom that may be measured by an attention-head without interference. We show that this may only be achieved by \\texttt{Pre-Norm} when semantic subspaces are orthogonal spheres - a structure that must be learned by the model - and study the stability of attention in the presence of semantic interference. By contrast, \\texttt{QKV-Norm} requires only linearly independent subspaces. Empirically, we provide evidence that \\texttt{QKV-Norm} is more stable when training with large learning rates and large latent dimensions. However, it often learns slower for small learning rates and narrow models, suggesting a trade-off of effects in practice. \\textcolor{red}{Mention tests for embedding structure if we can do it.}\n%We propose semantic separability at the attention-head level as a target for transformer architecture design, and show that it may be achieved by normalising over the \\{query, key, value\\} vectors instead of the full residual embeddings. Our motivation is to promote the formation of stable circuits, hypothesising this to influence both training stability and algorithmic inductive bias. This builds on recent works showing that transformers can implement simple contextual reasoning by composing a sequence of match\\&pass operational primitives acting on discrete semantic subspaces of the residual representation. An intriguing possibility is that sophisticated behaviours emerge using deep circuits built out of similar primitives. For attention heads to access distinct semantics using linear operators, semantic subspaces must be linearly separable at the point of operation - driving empirical observations of vector structure in word embeddings. In this work we (i) show that semantic separability is violated by the standard PreNorm architecture choice, unless a strict residual structure is learned in which every subspace is an orthogonal sphere; (ii) derive stability conditions for circuits with semantic interference induced by PreNorm; (iii) show that QKVNorm requires only linearly independent subspaces; (iv) supplement sparse existing studies of QKNorm with empirical evidence for training stability using QKVNorm.\n\\end{abstract}\n==== END OF /2406.17837/sections/abstract.tex ====\n==== BEGINNING OF /2406.17837/sections/limitations_outlook.tex ====\n% \\section{Limitations}\n% \\label{sec: limitations}\n\n%Motivated by known behaviour in simple circuits, this work formalises \\textit{semantic separation and interference} as a probe for studying transformers, studies the implied structure on embeddings and circuit stability, and estimates sensitivity in real-world models.\n\n%Our work contributes an abstract understanding of semantic interference and its implications for residual structure and circuit stability, provides data-driven constraints on the effect-size, and studies the \\texttt{QKV-Norm} architectural change. Further work is required to understand the importance of semantic separability for complex real-world models. We hope that these phenomenological studies can complement low-level interpretability to understand model behaviour at different levels of abstraction, enabling more predictable and intentionally designed architectures.\n\n\n\n\\section{Summary \\& Outlook}\n\\label{sec: summary}\n\nWe have presented the idea that transformer \\texttt{Pre-Norm} can cause interference between independent subspaces of the latent embeddings, a feature used by some real-world transformer circuits. Theoretically, we found this can only be avoided when using an embedding structure of \\textit{orthogonal spheres}. By contrast, the \\texttt{QKV-Norm} architecture requires only linearly independent subspaces. We predict that sparse attention is stable with respect to interference, until a certain threshold of noise is reached, at which point it undergoes a phase transition called \\textit{circuit collapse}. \n\nEmpirically, we observe that the $L_2$-norms of attended embeddings are contained within a spread of $\\pm20\\%$ for \\texttt{Pre-Norm} (with 90\\% coverage), whilst \\texttt{QKV-Norm} creates a wider spread. We simulate interference by introducing artificial noise onto the $L_2$-norms of $\\{q, k_t, v_t\\}$ in our trained \\texttt{Pre-Norm} model, observing that $1\\%$ of sparse distributions collapse at a noise level of $11\\%$. We observe that per-token accuracy degrades by $\\mathcal{O}(10\\%)$ when norms are simultaneously perturbed by noise of 1\\% in all layers, but is stable to \\%-scale noise in only sparse distributions. This may be attributed to either the predicted stability of sparse attention, or to a difference in the importance of sparse vs non-sparse heads induced by frequency and depth-dependence. More work is needed to disentangle these.\n\nThis work contributes a theoretical hypothesis of model behaviour, and empirically constrains the effect size without full model reverse-engineering. We have made predictions on representation structure, interference, and circuit collapse that practitioners may search for in their own models.\n\n\\section{Limitations}\n\\label{sec: limitations}\n\nWe have not directly observed subspace independence or interference, and further work is required to establish their importance in real-world models. Experimentally, we simulate interference as being independent and similar in amplitude across heads and layers, however it is possible that it is correlated and depth-dependent. Whilst our stability experiments demonstrate that the model is more stable with respect to noise in sparse than non-sparse distributions, we have not shown whether this is due to the inherent stability of the attention distribution predicted by our theory, or the relative importance of sparse vs non-sparse distributions to the model. We show experimental results for a small model on a targeted task (with model variations in Appendix~\\ref{app: model variations}); further work is needed to study the behaviour of larger models and different corpora.\n\n%It is not shown whether the drop in model performance is driven by perturbations in early layers, where we expect  \n%stability experiments do not disentangle the contributions of simulated interference and compounding errors, in which the perturbation of early layers also affects the inputs to later ones. \n==== END OF /2406.17837/sections/limitations_outlook.tex ====\n==== BEGINNING OF /2406.17837/sections/formulation.tex ====\n\\section{Formulation}\n\\label{sec: formulation}\n\n%We provide a concise summary of our formulation, with further details in appendix~\\ref{}.\n\n%We use similar terminology to \\cite{elhage2021mathematical}. \n\nConsider the \\texttt{No-Norm} case. Let $\\mathcal{X}$ be an unordered set of message receiving tokens, and $\\mathcal{Y}$ the message senders. Let $x \\in \\mathbb{R}^{N_x}$ be the $N_x$-dimensional representation of an element in $\\mathcal{X}$, and $y_t \\in \\mathbb{R}^{N_y}$ be the $t^\\mathrm{th}$ element in $\\mathcal{Y}$, with $1 \\leq t \\leq T$. For self-attention we have $\\mathcal{X}=\\mathcal{Y}$. Let $W_Q\\in\\mathbb{R}^{N_{qkv}\\times N_x}$ and $W_K\\in\\mathbb{R}^{N_{qkv}\\times N_y}$ be the query and key weight matrices, with associated vectors $q=W_Qx\\in\\mathbb{R}^{N_{qkv}}$ and $k_t=W_Ky_t\\in\\mathbb{R}^{N_{qkv}}$ on an $N_{qkv}$-dimensional latent space. We do not include biases in $\\{q,~k_t\\}$ because they contribute terms that are nullified by the \\texttt{softmax}, or are reproduced by constant directions in $x$ (Theorem~\\ref{theorem: decomposition}).  We define dot-product attention \\textit{scores} as:\n\\begin{equation}\n    w_t ~=~ q^Tk_t ~=~ x^T W_Q^T W_K y_t ~=~ x^T W_{QK} y_t\n\\end{equation}\nwhere $W_{QK}\\triangleq W_Q^TW_K \\in \\mathbb{R}^{N_x\\times N_y}$ is a matrix with $Rank(W_{QK}) \\leq \\min(N_x,N_y,N_{qkv})$. This is the maximum span of the attended subspace in $\\{x,y_t\\}$. The attention \\textit{weights} are\n\\begin{equation}\n    a_t ~=~ \\texttt{softmax}\\left(w_t\\right) ~=~ \\frac{e^{w_t}}{\\sum_{t'} e^{w_{t'}}} ~~~~~~.\n\\end{equation}\nLet $v_t=W_Vy_t\\in\\mathbb{R}^{N_x}$ be the value vectors with $W_V\\in\\mathbb{R}^{N_{qkv}\\times N_y}$. We do not include biases in $v_t$ because they carry no dependence on the attended token. Each token emits the message $m_t = W_Ov_t \\equiv W_OW_Vy_t \\triangleq W_{OV}y_t$ where $W_O = \\mathbb{R}^{N_x\\times N_{qkv}}$ is the output-matrix. Each attention-head updates $x$ by adding the attention-weighted convex combination of messages, $x \\rightarrow x + \\Delta x$ with $\\Delta x = \\sum_{t} a_t m_t$. We usually run $H$ attention-heads in parallel, giving the total update:\n\\begin{equation}\n     x ~\\rightarrow~ x ~ + ~\\sum_{h=1}^{H} \\sum_{t=1}^{T} a_t^{(h)} m_t^{(h)} ~~~~~~~~~~~~~~~~~~ \\text{Multi-head~attention}\n\\end{equation}\nwith unique weights $\\{W_Q^{(h)},W_K^{(h)},W_V^{(h)},W_O^{(h)}\\}$ for each head index $h$.\n\n%Each attention head modifies the token representation $x$ via the linear addition of information, called \\textit{annotation}. We often view $x$ as a buffer of information called the \\textit{residual stream}. Independent circuits can compose operations by channelling information through the residual streams of different tokens. \n\nWe now introduce normalisation layers. Let $z\\in\\mathbb{R}^{N_z}$ be any $N_z$-dimensional vector, then $\\texttt{N}(z;\\alpha_z):\\mathbb{R}^{N_z}\\rightarrow\\mathbb{R}^{N_z}$ is a normalisation function with parameters $\\alpha_z$. We consider two such functions:\n% \\begin{equation}\n% \\begin{split}\n%     \\texttt{LayerNorm}\\left(z;~\\alpha_z\\right)  \n%         ~&\\equiv~ \\mathrm{LN}\\left(z;~\\alpha_z\\right) \n%         ~=~  \\frac{1}{\\sigma(z)} \\mathrm{diag}\\left(\\alpha_z\\right)P_\\perp z ~=~ \\frac{\\sqrt{N_z}}{|z_\\perp|} \\mathrm{diag}\\left(\\alpha_z\\right) z_\\perp   \\\\\n%     \\texttt{RMSNorm}\\left(z;~\\alpha_z\\right) \n%         ~&\\equiv~ \\mathrm{VN}\\left(z;~\\alpha_z\\right) \n%         ~=~  \\frac{\\sqrt{N_z}}{|z|} \\mathrm{diag}\\left(\\alpha_z\\right) z \\\\\n% \\end{split}\n% \\end{equation}\n\\begin{equation}\n    \\texttt{RMSNorm}\\left(z;~\\alpha_z\\right) =  \\frac{\\sqrt{N_z}}{|z|} \\mathrm{diag}\\left(\\alpha_z\\right) z \n%~~\\cite{NEURIPS2019_1e8a1942}\n~~~~~~~~~~~~~\n    \\texttt{LayerNorm}\\left(z;~\\alpha_z\\right) = \\frac{\\sqrt{N_z}}{|z_\\perp|} \\mathrm{diag}\\left(\\alpha_z\\right) z_\\perp \n%~~\\cite{DBLP:journals/corr/BaKH16}\n\\end{equation}\n\\cite{NEURIPS2019_1e8a1942,DBLP:journals/corr/BaKH16} where $P_\\perp \\triangleq \\mathrm{diag}\\left(1^{N_z}\\right) - 1^{N_z}{1^{N_z}}^T$ is a linear operator that subtracts the mean of $z$ from every component, $1^{N_z}$ is vector of ones, and $z_\\perp \\triangleq P_\\perp z$ is the component of $z$ perpendicular to $1^{N_z}$.\n\nThe \\texttt{Pre-Norm} strategy means applying normalisation to the inputs $\\{x,y_t\\}$. The \\texttt{QKV-Norm} strategy means applying normalisation to the vectors $\\{q,k_t,v_t\\}$. We then have three cases:\n\n%\\vspace{0.3cm}\n\\noindent\n\\begin{tabular}{rr|c|c|c}\n  ~  &  ~  &  $w_t$  &  $v_t$  &  Norm params  \\\\\n\\hline\n                & \\texttt{No-Norm}    &  $x^T ~W_{QK}~ y_t$  &  $W_V~y_t$ & - \\\\\n     (baseline) & \\texttt{Pre-Norm}   &  $\\texttt{N}\\left(x;\\alpha_x\\right)^T ~W_{QK}~ \\texttt{N}\\left(y_t;\\alpha^K_y\\right)$  & $W_V~\\texttt{N}\\left(y_t;\\alpha^V_y\\right)$ & $\\{\\alpha_x,~\\alpha^K_y,~\\alpha^V_y\\}$ \\\\\n\n     (alternate)     & \\texttt{QKV-Norm}  &  $\\texttt{N}\\left(W_Q x;\\alpha_q\\right)^T \\texttt{N}\\left(W_K y_t;\\alpha_k\\right)$  & $\\texttt{N}\\left(W_V y_t;\\alpha_v\\right)$ & $\\{\\alpha_q,~\\alpha_k,~\\alpha_v\\}$ \\\\\n\\end{tabular}\n\nWe note that several of these degrees of freedom are redundant and could be combined, e.g. $\\alpha_q$ and $\\alpha_k$. We do not consider these variations (i) because they are not relevant for the results of this paper, and (ii) to standardise the number of training parameters.\n==== END OF /2406.17837/sections/formulation.tex ====\n==== BEGINNING OF /2406.17837/sections/limitations.tex ====\n\\section{Limitations}\n\\label{sec: limitations}\n\n\\textcolor{red}{Main limitation is lack of evidence for behaviour in complex models. Generality of logical-operations on discrete semantics no yet known, or whether there are mechanisms for error-correction. Summarise theory assumptions.}\n==== END OF /2406.17837/sections/limitations.tex ====\n==== BEGINNING OF /2406.17837/sections/related_works.tex ====\n\\section{Related Works}\n\\label{sec: related works}\n\nOur work is motivated by transformer circuit discovery \\cite{olsson2022context,elhage2021mathematical,wang2022interpretability,stolfo2023mechanistic,GoldowskyDill2023LocalizingMB,Ferrando2024InformationFR} and formation \\cite{singh2023the,singh2024needs}. See \\cite{ferrando2024primer} for a recent review of interpretability for language decoder models, with a list of known logical operations implemented by attention heads. This builds upon works in BERTology \\cite{DBLP:journals/corr/abs-2002-12327,Devlin2019BERTPO}. We study normalisation, for which several formulations have been proposed \\cite{DBLP:journals/corr/abs-2002-04745,nguyen-salazar-2019-transformers,nguyen-chiang-2018-improving,shleifer2021normformer,NEURIPS2019_2f4fe03d}. Our \\texttt{QKV-Norm} variant is similar to \\texttt{QK-Norm}, which is studied by \\cite{DBLP:journals/corr/abs-2010-04245,wortsman2023smallscale,dehghani2023scaling} for asymptotic performance and training stability at large learning rates. These are motivated by logit-regularisation, whereas we are motivated by representational inductive bias and stability to latent semantic interference. %Our work therefore contributes an additional motivation for studying the performance of these normalisation methods.\n\nWe highlight other works that study transformer normalisation through its geometric interpretation as a projection onto a sphere. \\cite{kobayashi-etal-2021-incorporating} investigated the role of normalisation in mixing the attention output with the residual stream in \\texttt{Post-Norm} models, but does not consider \\texttt{Pre-Norm}. \\cite{brody2023expressivity} studies the computational abilities of \\texttt{Pre-LayerNorm} architectures, in particular demonstrating that projection onto a sphere ensures that all keys reside on their own convex hull, preventing them from becoming ``unselectable''. \\cite{Molina2023TravelingWA} interprets the latent embeddings of \\texttt{Pre-Norm} models as a trajectory on a sphere. These works do not consider the interference of semantic subspaces. \\cite{Dong2021AttentionIN} and the contemporary work \\cite{wu2024role} study the role of \\texttt{LayerNorm} in the related phenomenon of embedding rank collapse.\n\nWe highlight the contemporary work of \\cite{wang2024understanding}, who also study multi-step contextual reasoning in transformers using matching operations over independent subspaces, for both \\texttt{Pre-Norm} and \\texttt{Post-Norm}. This builds upon \\cite{boixadsera2024when}, who study the learning of abstract symbolic reasoning in transformers, and works that manipulate the flow of information to promote algorithmic reasoning, e.g. \\cite{csordas2022the}.\n\nWe are not aware of previous works that study the impact of \\texttt{Pre-Norm}'s spherical geometry on the structure of latent subspaces. However, many works consider linear subspaces, described in the following paragraph. These results are directly applicable to the \\texttt{No-Norm} and \\texttt{QKV-Norm} methods in this work, although \\texttt{QKV-Norm} applies a subsequent spherical projection. \\cite{Lamb2021TransformersWC} design subspace separability into their model by decoupling the normalisation layers for different mechanisms.\n\nWorks on vector embeddings \\cite{word2vec1,NIPS2013_9aa42b31,Pennington2014GloVeGV} and the \\textit{linear representation hypothesis} \\cite{liguistic_regularities,Park2023TheLR,jiang2024origins} study the emergence of linear subspaces that encode separable concepts in embedding-unembedding models, using both interpretation and intervention techniques. Many works search for linear subspaces/directions in a transformer representation (e.g. linear probes \\cite{semantic_subspace_probing,belinkov-2022-probing}) or search for faithful causal abstractions (e.g. \\cite{Geiger2023FindingAB}), with a survey provided in \\cite{ferrando2024primer} sections 3-4. %Some works focus on human interpretability in the input and output embeddings, and whilst we emphasise non-human-interpretable latent concepts and circuit stability, both share the common feature of linear independence. \nWe also highlight works that study the use of features in linear superposition \\cite{superposition1,elhage2022superposition}. This allows a model to store more features than it has dimensions, at the cost of interference in their linear projections. %, which we note is a different type of interference to the one induced by \\texttt{Pre-Norm}.\n\nThe terminology of \\textit{semantic subspaces} is used more generally, e.g. \\cite{semantic_subspace_probing,5596640,Coenen2019VisualizingAM}. We consider a definition that does not require humans to define the separable concepts, only that abstract latent features remain independent in an attention layer. We also highlight works that study subspaces of static (model input) and contextual (latent or model output) embeddings in transformers, e.g. \\cite{Hewitt2019ASP,Coenen2019VisualizingAM,ethayarajh-2019-contextual,song2024uncovering,muppet2022,hernandez2024linearity,chi-etal-2020-finding,cai2021isotropy,Hernandez2021TheLL} (review in \\cite{DBLP:journals/corr/abs-2002-12327}). These are relevant because they also decompose embeddings into a combination of abstract subspaces, capturing different semantic and syntactic structures in a natural language setting. These may be used as semantic subspaces in our work. We highlight \\cite{song2024uncovering} which studies interference between positional and contextual components using a decomposition similar to ours, and also experiments using a next-token addition task.\n==== END OF /2406.17837/sections/related_works.tex ====\n==== BEGINNING OF /2406.17837/sections/introduction.tex ====\n\\section{Introduction}\n\\label{sec:intro}\n\n%- Very difficult to study structure empirically, strength of theoretical approach is that we can also study principles axiomatically\n%- Add theorems: double norm is ineffective; heads can perform binary match\\&pass operations (attention polysemanticity); heads can perform different operations on sparsely separated features; heads can fallback to doing nothing, creating a category of ``other'' without structural constraints\n%In contrast to other works, we do not seek human-interpretable explanations of circuits. Instead we identify the key structural components and seek to understand how these must behave. This allows us to decouple the understanding of logival struicture from human interpretability, which is a core difficulty of prior works\n\nTransformer-based models \\cite{DBLP:journals/corr/VaswaniSPUJGKP17} are commonplace in machine learning, providing state-of-the-art contextual reasoning in domains ranging from natural language \\cite{bubeck2023sparks,touvron2023llama} to protein-folding \\cite{AlphaFold3,doi:10.1126/science.abj8754,10.7554/eLife.82819} and theoretical physics \\cite{cai2024transforming}. Recent interpretability work investigates the internal mechanisms that lead to specific model behaviours \\cite{olsson2022context,elhage2021mathematical,wang2022interpretability,ferrando2024primer,meng2022locating,DBLP:journals/corr/abs-2002-12327,liu2023transformers,goldowskydill2023localizing}. This is important for predicting behaviour in new environments, enables practitioners to match the inductive bias of a model with the structure of its task, and informs the design of architectures that promote desirable behaviour.\n\nTwo such works discovered complete \\textit{circuits} \\cite{cammarata2020thread:} in trained transformers \\cite{olsson2022context,elhage2021mathematical,wang2022interpretability}. These are computational graphs that dominate the model prediction when activated in a specialised context. They perform a type of \\textit{algorithmic reasoning} by internally executing a sequence of logical operations, using attention to pass information between memory buffers that begin as token embeddings and become increasingly abstract. Furthermore, a number of attention heads have been identified as performing logical operations (see \\cite{ferrando2024primer} section 5). To understand transformer behaviour, an important goal is to understand how logical attention heads operate, and their generality beyond the simple cases that facilitate interpretability.\n\nOne key observation is that \\textbf{the attention distribution is sometimes fully-determined by an independent subspace of the representation} - for example, an attention layer can identify the previous token by accessing a subspace that encodes only position-in-sequence. Indeed, low-rank weight matrices can only access linear subspaces by construction. A second observation is that, like most deep architectures, transformers use normalisation layers to improve training stability. A leading choice is to place normalisation at the \\textit{input} to each attention layer, which we call \\texttt{Pre-Norm} \\cite{DBLP:journals/corr/abs-2002-04745}. Some interpretability works ignore this layer because it has a linear-up-to-scale structure, absorbing the linear part into adjacent weights. In this work we argue that the layer is important, because \\textbf{Pre-Norm causes independent linear subspaces to interfere through a common normalisation factor, preventing their separation by linear attention layers}. \n\nThe purpose of this work is to ask: if the use of independent subspaces is generally important, what are the expected consequences of \\texttt{Pre-Norm} for (i) the latent representation structure, and (ii) circuit stability? To answer this, we take an abstract approach that complements direct interpretability by considering general behaviour beyond the interpretable limit. Our contributions are:\n\\begin{enumerate}\n\t\\item \\textbf{Conceptual:} we identify interference between independent subspaces as a potential destabiliser of circuits caused by \\texttt{Pre-Norm}. We suggest \\textit{separability of latent subspaces} as a target for study, and show it is easily satisfied by the alternative \\texttt{QKV-Norm}. This differs from \\texttt{Pre-Norm} by placing the normalisation layer after the linear operators. It is similar to \\texttt{QK-Norm}, for which sparse evidence currently exists \\cite{DBLP:journals/corr/abs-2010-04245,wortsman2023smallscale,dehghani2023scaling}.\n\t\\item \\textbf{Theoretical:} we formalise a \\textit{semantic subspace} as any independent subspace of the latent representation that can fully determine the attention distribution. We show that \\texttt{Pre-Norm} can only achieve this when semantic subspaces are spherical and mutually orthogonal. By contrast, \\texttt{QKV-Norm} requires only that subspaces be linearly independent, matching the \\texttt{No-Norm} case in this sense. We study the stability of attention to subspace interference, predicting a potentially problematic phenomenon of \\textit{circuit collapse} when a sparse-attention distribution changes which embedding it attends to. \n\t%\\item \\textbf{Experimental:} we measure the sensitivity of trained models to simulated interference in a numerical addition task. We find that per-token accuracy degrades by $\\gtrsim 10\\%$ when vector norms are perturbed by $\\mathcal{O}(1\\%)$. Supporting our theory, we find that (i) \\texttt{Pre-Norm} models induce a narrower distribution of embedding norms than \\texttt{QKV-Norm}, and (ii) the circuit collapse phenomenon occurs in practice when norms are perturbed by $\\mathcal{O}(5-10\\%)$.\n\t\\item \\textbf{Experimental:} we measure the sensitivity of trained models to simulated interference in a numerical addition task. Constraining our predictions, we find that (i) \\texttt{Pre-Norm} models induce a narrower distribution of embedding $L_2$-norms than \\texttt{QKV-Norm}, (ii) we bound the spread of $L_2$-norms to $\\pm20\\%$ with 90\\% coverage, and (iii) the circuit collapse phenomenon occurs at a rate of 1\\% when norms are perturbed by $\\mathcal{O}(10\\%)$.\n\\end{enumerate}\n\n\n\n\n\n%\\begin{enumerate}\n%\t\\item \\textbf{Conceptual:} we identify a discrepancy between (i) the mechanism that underpins known contextual transformer circuits, which we call the ``match\\&pass'' operation, and (ii) \\texttt{Pre-Norm}, the prevailing architectural choice for applying layer normalisation. The discrepancy is that learned match\\&pass operations act only on distinct linear subspaces of the token embedding, which we call \\textit{semantics}. We show that \\texttt{Pre-Norm} inhibits the linear separation of semantics, whereas \\texttt{QKV-Norm} does not. \n%\t\\item \\textbf{Theoretical:} we formally define \\textit{semantic separability}; we show that  \\texttt{Pre-Norm} can only achieve this property if the token representations satisfy a strict structural requirement \\textit{at the input to every attention layer}: every semantic subspace must be an orthogonal sphere. This structure must be learned during training, which we expect to be difficult with large learning rates, and may disfavour circuit discovery, impacting algorithmic inductive bias. When the model does not satisfy this condition, we show how the resulting \\textit{multiplicative interference} destabilises circuits in the limits of sparse and isotropic attention.\n%\t\\item \\textbf{Experimental:} we train networks with a focus on circuit-formation rather than memorisation of shallow statistics, and:\n%\t\\begin{enumerate}\n%\t\t\\item Show hints that the residual structure is observed in practice. These searches are difficult because there exist a large number of trade-offs and confounders, therefore our results serve to \\textit{bound} any trade-off made by \\texttt{Pre-Norm} models.\n%\t\t\\item Compare the training stability of \\texttt{Pre-Norm} and \\texttt{QKV-Norm}, finding that \\texttt{QKV-Norm} is preferred for large learning rates and large latent dimentions.\n%\t\\end{enumerate}\n%\\end{enumerate}\n\t\n%This work is important because:\n%\\begin{itemize}\n%\t\\item It contributes a theoretical understanding of how transformers must internally represent information, if the principle of semantic separability by ``match\\&pass'' operations generalises from known circuits to a wider class.\n%\t\\item It promotes an architectural choice that enables higher learning rates, reducing the expense of training large models.\n%\t\\item Circuits can drive model behaviour when faced with novel inputs, and so govern out-of-distribution generalisation (e.g. one-shot [], few-shot [], in-context learning []). We hope that understanding the link between architecture and circuit formation/stability may enable design choices with more deliberate algorithmic inductive bias.\n%\\end{itemize}\n==== END OF /2406.17837/sections/introduction.tex ====\n==== BEGINNING OF /2406.17837/sections/theory_stability.tex ====\n\\section{Theory: stability to subspace interference}\n\\label{sec: theory: circuit stability}\n\nWe now investigate the impact of interfering subspaces. Consider the almost-separable limit, modelling interference as a random infinitesimal perturbation of the vectors $\\{q,k_t,m_t\\}$. Let $\\epsilon$-symbols denote perturbations such that $\\epsilon^{\\Delta x(q)} \\rightarrow \\frac{\\partial \\Delta x}{\\partial q} \\epsilon^q$ for $\\epsilon^q \\rightarrow 0$ is the change of $\\Delta x$ induced by $\\epsilon^q$. %We then study specific normalisation strategies by considering the structure of perturbation they induce. Specifically, we expect \\texttt{Pre-Normalisation} to entangle semantics via a multiplicative factor, therefore $\\epsilon^q \\propto q$ etc.\nWe consider (i) the sparse limit, in which the attention is concentrated entirely on a single embedding, and (ii) the isotropic limit, in which it is distributed evenly among embeddings. We are particularly interested in the sparse case, since this highly directed flow of information is used by match\\&pass, although semantic separation can also be used by non-sparse heads. %We also study the opposite limit of isotropic attention, which attends to all tokens equally.\n\n\\begin{mdframed}[backgroundcolor=red!5]\n\\textbf{[Definition] ~ Sparse attention:} the low-temperature limit $a_t \\approx \\delta_{tt^*}$ and $\\Delta x = m_{t^*}$, where $\\delta$ is the Kronecker delta. This occurs when there is a large difference between the top two scores: $t^* = \\mathrm{argmax}_t w_t$ and $w_{t^*} - \\max_{t\\neq t^*} w_t \\gg 1$. \\textbf{Isotropic attention:} the high-temperature limit $a_t = \\frac{1}{T}$ and $\\Delta x = \\langle m_t\\rangle_t$. This occurs when $w_t$ is constant, requiring $q=0$ or constant $k_t$.\n\\end{mdframed}\n\n\\begin{mdframed}[backgroundcolor=blue!5]\n\\underline{\\textbf{Stability of attention updates to perturbations on q, k, v}}\n~~~~~~~~~~~~~~~~~~~\\textit{[Proofs in appendix~\\ref{appendix: proofs}]}\n\n\\begin{theorem}\n    Consider independent infinitesimal perturbations on queries $\\epsilon^q \\in \\mathbb{R}^{N_{qkv}}$, keys $\\epsilon^k_t \\in \\mathbb{R}^{N_{qkv}}$, and messages $\\epsilon^m_t \\in \\mathbb{R}^{N_{qkv}}$. These propagate onto $\\Delta x = \\sum_{t}a_tm_t$ as\n    \\begin{align}\n        \\epsilon^{\\Delta x(q)} ~~&\\xrightarrow[\\epsilon^q\\rightarrow0]{\\mathrm{~~~~perturb~q~~~~}}~~ \\mathop{\\mathbb{E}}_{a_t} \\Big[ m_t {\\tilde k}_t^T \\Big] \\epsilon^q ~~~~~~~~~~~~~~~~~  {\\tilde k}_t ~\\triangleq~ k_t ~- \\mathop{\\mathbb{E}}_{a_t} \\Big[ k_t \\Big] \\\\\n        \\epsilon^{\\Delta x(k)} ~~&\\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{~~~~perturb~k~~~~}}~~ \\mathop{\\mathbb{E}}_{a_t} \\Big[ {\\tilde m}_t {\\epsilon^k_t}^T \\Big] q ~~~~~~~~~~~~~~~~~  {\\tilde m}_t ~\\triangleq~ m_t ~- \\mathop{\\mathbb{E}}_{a_t} \\Big[ m_t \\Big] \\\\\n        \\epsilon^{\\Delta x(m)} ~~&\\xrightarrow[\\epsilon^m_t\\rightarrow0]{\\mathrm{~~~~perturb~m~~~~}}~~ \\mathop{\\mathbb{E}}_{a_t} \\Big[ \\epsilon^m_t \\Big]\n    \\end{align}\n    where ${\\tilde z}_t$ is the value of $z_t$ measured from the attention-weighted centroid $\\mathbb{E}_{a_t}[z_t] = \\sum_t a_t z_t$.\n\\label{theorem: stability: general}\n\\end{theorem}\n\n\\begin{theorem}\n    For sparse attention:\n    \\begin{equation}\n        \\epsilon^{\\Delta x(q)} \\xrightarrow[\\epsilon^q\\rightarrow0]{\\mathrm{~~perturb~q~~}} 0   ~~~~~~~~~~\n        \\epsilon^{\\Delta x(k)} \\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{~~perturb~k~~}} 0   ~~~~~~~~~~\n        \\epsilon^{\\Delta x(m)} \\xrightarrow[\\epsilon^m_t\\rightarrow0]{\\mathrm{~~perturb~m~~}} \\epsilon^m_{t^*}\n    \\end{equation}\n    i.e. the message is stable with respect to small interference in the queries and keys. Interference in the selected value is linearly transferred onto the message.\n\\label{theorem: stability: sparse}\n\\end{theorem}\n\n\\begin{theorem}\n    For isotropic attention:\n    \\begin{equation}\n        \\epsilon^{\\Delta x(q)} \\xrightarrow[\\epsilon^q\\rightarrow0]{\\mathrm{perturb~q}} \\langle m_t {\\tilde k}_t^T \\rangle_t \\epsilon^q ~~~~~~~~\n        %\\epsilon^{\\Delta x(q)} \\xrightarrow[\\epsilon^q\\rightarrow0]{\\mathrm{~perturb~q~}} 0   ~~~~~~~~~\n        \\epsilon^{\\Delta x(k)} \\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{perturb~k}} \\langle {\\tilde m}_t {\\epsilon^k_t}^T \\rangle_t ~q   ~~~~~~~~\n        \\epsilon^{\\Delta x(m)} \\xrightarrow[\\epsilon^m_t\\rightarrow0]{\\mathrm{perturb~m}} \\langle \\epsilon^m_t \\rangle_t\n    \\end{equation}\n    % \\begin{align}\n    %     \\epsilon^{\\Delta x(q)} ~&\\xrightarrow[\\epsilon^q\\rightarrow0]{\\mathrm{~~perturb~q~~}}~ 0 \\\\\n    %     \\epsilon^{\\Delta x(k)} ~&\\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{~~perturb~k~~}}~ \\langle {\\tilde v}_t {\\epsilon^k_t}^T \\rangle_t ~q  \\\\\n    %     \\epsilon^{\\Delta x(v)} ~&\\xrightarrow[\\epsilon^v_t\\rightarrow0]{\\mathrm{~~perturb~v~~}}~ \\langle \\epsilon^v_t \\rangle_t\n    % \\end{alig\n    N.B. isotropy requires $k_t=const$ or $q=0$. Lemma 1: the update is stable to noisy $q$ when $k_t=const$, or when $m_t \\perp k_t$ (c.f. keys and messages from independent subspaces). Lemma 2: the update is stable to noisy $k_t$ when $q=0$, or when $m_t \\perp \\epsilon_t^k$. Lemma 3: the update is stable to noisy $m_t$ when $\\langle \\epsilon^m_t \\rangle_t=0$. Other cases propagate linearly.\n\\label{theorem: stability: isotropic}\n\\end{theorem}\n\\end{mdframed}\n\nThe stability of sparse attention is because \\texttt{softmax} becomes an \\texttt{argmax} for low-temperature heads, which is only sensitive to the order of $w_t$. However, this introduces a different vulnerability when perturbations cause the order of $w_t$ to change, as the attention distribution undergoes a phase transition to select a different token. We call this \\textit{circuit collapse}. For example, the induction circuit collapses when the operation \\textit{attend to the previous token} attends to any other token because of interference.\n\n\\begin{mdframed}[backgroundcolor=red!5]\n\\textbf{[Definition] ~ Circuit collapse:} spontaneous phase transition in which a sparse attention distribution selects a different token due to noise on $\\{q, k_t\\}$. Let $\\epsilon^w_t = k_t^T\\epsilon^q + q^T\\epsilon^k_t + \\mathcal{O}({\\epsilon^q}^T\\epsilon^k_t)$ be perturbations on $w_t$ that result from $\\epsilon^q$ and $\\epsilon^k_t$. Circuit collapse occurs when there exists a $t \\neq t^*$ for which $w_{t^*} - w_t < \\epsilon^w_t - \\epsilon^w_{t^*}$.\n\\end{mdframed}\n\nWe now study the $L_2$-norm interference that we expect to be induced by \\texttt{Pre-Norm} when semantic separability is violated. This is characterised by perturbations that are parallel to their corresponding vector. Theorem~\\ref{theorem: multiplicative stability: sparse} shows the conditions under which we expect circuit collapse to occur.\n\n\\begin{mdframed}[backgroundcolor=blue!5]\n\\underline{\\textbf{Stability of attention updates to scaling of q, k, v}}\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\textit{[Proofs in appendix~\\ref{appendix: proofs}]}\n\n\\begin{theorem}\n    Sensitivity of sparse attention to multiplicative perturbations $\\epsilon^q = \\kappa^q q$ and $\\epsilon^k = \\kappa^k_t k_t$ with $\\kappa^q,\\kappa^k_t\\ll1$. Circuit collapse occurs when $\\exists~ t \\neq t^*$ for which:\n    \\begin{equation}\n        \\frac{w_{t^*}}{w_t} ~\\begin{cases} ~<~ \\lambda_w & \\mathrm{if}~ w_t \\left(1 + \\kappa^q + \\kappa^k_{t^*}\\right) > 0 \\\\\n        ~>~ \\lambda_w & \\mathrm{otherwise} \\\\ \\end{cases}\n        ~~~~~~~~~~~~~ \\lambda_w ~\\triangleq~ \\frac{1 + \\kappa^q + \\kappa^k_t}{1 + \\kappa^q + \\kappa^k_{t^*}}\n    \\end{equation}\n    where temperature cancels in the fraction. \\textbf{Attention is fully stable above the critical transition point $\\lambda_w$} (c.f. $w_t \\left(1 + \\kappa^q + \\kappa^k_{t^*}\\right) > 0$). We see that query perturbations alone are insufficient, as they result in $\\lambda_w=1$. Lemma: consider the special case when all keys have similar length $k_t \\approx const$, the attended token has $\\theta_{t^*}\\approx0$, the keys are far-from-orthogonal s.t. $\\theta_t \\ll 1$, and $\\kappa^q\\approx0$. Using $w_t \\triangleq |q| |k_t| \\cos\\theta_t$, circuit collapse occurs when $\\exists~ t \\neq t^*$ for which:\n    \\begin{equation}\n            \\frac{1}{2}\\theta_t^2 ~\\lesssim~ \\kappa^k_t - \\kappa^k_{t^*}   ~~~~~~~~~~~ \\mathrm{if}~ w_t \\left(1  + \\kappa^k_{t^*}\\right) > 0 ~\\text{, otherwise reverse}\n    \\label{eq: sparse circuit collapse result}\n    \\end{equation}\n    i.e. stability requires either well-separated keys s.t. $\\theta_t \\gg 0$, or small perturbations $\\kappa_t-\\kappa^*_t \\ll 1$.\n\\label{theorem: multiplicative stability: sparse}\n\\end{theorem}\n\n\\begin{theorem}\n    Sensitivity of isotropic attention to multiplicative perturbations. Say $\\epsilon^k = \\kappa^k_t k_t$ with $\\kappa^k_t\\ll1$ where $\\{\\kappa_t\\}$ have comparable amplitudes. Then\n    \\begin{equation}\n        \\epsilon^{\\Delta x(k)} %~\\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{~~perturb~k~~}}%~ w~ \\langle {\\tilde v}_t \\kappa^k_t \\rangle_t \n        ~\\approx~\n        \\begin{cases}\n        0 ~&~ \\text{if~$\\kappa_t$~independent~of~${\\tilde m}_t$,~by~symmetry} \\\\\n        0 ~&~ \\text{if~$\\kappa_t\\equiv\\kappa$~for~constant~$\\kappa$} \\\\\n        0 ~&~ \\text{if~$q=0$} \\\\\n        w \\langle {\\tilde m}_t \\kappa^k_t \\rangle_t  ~&~ \\text{otherwise}\n        \\end{cases}\n    \\end{equation}\n\\label{theorem: multiplicative stability: isotropic}\n\\end{theorem}\n\\end{mdframed}\n\n%In summary, sparse attention is unstable with respect to a spontaneous phase transition induced when interference changes the ordering of $w_t$, particularly when keys are densely spaced. To protect against this, keys must either have a highly orthogonal substructure, or be highly separable from other semantics s.t. interference is negligible\n\n%In summary, isotropic attention distributions are stable with respect to small interference on query vectors, are sensitive to interference on the key vectors only when it correlates strongly with $v_t$, and linearly transport interference on value vectors - but with a stabilising effect in the mean-field limit.\n==== END OF /2406.17837/sections/theory_stability.tex ====\n==== BEGINNING OF /2406.17837/sections/experiments.tex ====\n\\section{Experimental results}\n\\label{sec: experimental results}\n\nWe now use experiments to empirically probe (i) the real-world embedding structure, and (ii) the sensitivity to artificial noise on the \\{query, key, value\\} $L_2$-norms. Whilst this does not directly observe real-world interference, it constrains the effect importance.\n\nWe consider a base-10 integer-addition task with a question-answer structure, and train for next-token prediction. We use a decoder architecture, common for state-of-the-art language models, with $10$ layers, per-character tokenisation, and begin \\texttt{[} and end \\texttt{]} tokens. In the output, we mask \\texttt{*} tokens that precede the answer. For example, the first training sequence has input \\textcolor{Maroon}{\\texttt{[453+16+17-N846=1332}} and output \\textcolor{Maroon}{\\texttt{***************1332]}}. We compare two models that use \\texttt{Pre-Norm} and \\texttt{QKV-Norm} respectively. Appendices~\\ref{appendix: experimental setup}-\\ref{app: model variations} provide a full experimental setup and supplementary plots. %Code and log files are provided as a \\texttt{zip} in supplementary materials. \nIn this section we make all plots using an in-distribution test set that is expected to have some overlap with the training set, bounded at $\\ll 20\\%$.\n\nWe choose this task because it emphasises contextual reasoning in a small-scale setting, is configurable for complexity, and allows us to define meaningful out-of-distribution test sets. The \\texttt{Pre-Norm} (\\texttt{QKV-Norm}) model achieves an in-distribution per-token accuracy of $91.4\\%$ ($91.0\\%$), dropping to $87.5\\%$ ($82.5\\%$) when generalising out-of-distribution to intermediate complexity, and $66.7\\%$ ($46.8\\%$) for increased complexity. Statistical uncertainties are below $0.1\\%$. The in-distribution performance is comparable, but \\texttt{QKV-Norm} generalises worse in this task, implying it has learned less task-appropriate solutions.  Appendix~\\ref{appendix: Pre vs QKV norm} shows additional comparisons suggesting that the \\texttt{Pre-Norm} and \\texttt{QKV-Norm} models behave differently, supporting the observations of \\cite{DBLP:journals/corr/abs-2010-04245,wortsman2023smallscale,dehghani2023scaling}.\n\n%\\clearpage\n\\textbf{Embedding structure}\n\nOur theory predicts that \\texttt{Pre-Norm} attention is stable with respect to information in non-attended subspaces if all input embeddings have similar $L_2$-norms, whereas \\texttt{QKV-Norm} imposes no norm constraint. We seek to experimentally bound the degree to which this structure is learned in practice.\n\nWe do this by plotting \\textit{the spread of norms with respect to their median}. A confounding effect is that the norms may differ for (i) embeddings attended to by different heads, (ii) the same head acting in different contexts, and (iii) embeddings that are never attended. We therefore measure the ratio \\textit{per-head}, and weight each embedding by its assigned attention. We remove the begin-sequence token from consideration. Figure~\\ref{fig: embedding spread} shows the resulting spread for all attention layers. On the LHS, we see that 90\\% of the distribution is contained within an interval of $\\pm20\\%$ when using \\texttt{Pre-Norm}. On the RHS, we see that \\texttt{QKV-Norm} allows a much wider spread. This is consistent with our theory, and experimentally bounds the representation effect on \\texttt{Pre-Norm} to $\\lesssim 20\\%$ in this model. Supplementary Figures~\\ref{fig: embedding spread: model variations MID}-\\ref{fig: embedding spread: model variations END} show consistent results for two model variations, although we note that \\texttt{Pre-Norm} and \\texttt{QKV-Norm} are more comparable for the variation labelled \\texttt{Alternate}.\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/baseline_model/Embedding_length_comparison_0.0.pdf}\n    \\caption{Spread of embedding $L_2$-norms experienced by attention heads at increasing model depth, excluding the \\texttt{[} token. For \\texttt{Pre-Norm}, 90\\% of the spread is observed within an interval of $\\pm20\\%$. Supplementary Figure~\\ref{fig: embedding L2 norms prenorm head} shows the distributions used to make this plot. Supplementary Figures~\\ref{fig: embedding spread: model variations MID}-\\ref{fig: embedding spread: model variations END} replicate the analysis for two model variations.}\n    \\label{fig: embedding spread}\n\\end{figure}\n\n\\textbf{Model stability with simulated interference}\n\nSection~\\ref{sec: theory: circuit stability} theoretically modelled the semantic interference induced by \\texttt{Pre-Norm} as a random perturbation on the norms of $\\{q,k_t,m_t\\}$. To estimate the real-world sensitivity to such an effect, we artificially introduce uncorrelated uniform noise onto these norms inside our trained \\texttt{Pre-Norm} model. Even though Gaussian noise is expected in the large-$N_s$ limit, we use uniform noise to avoid outliers. Figure~\\ref{fig: noise sensitivity} shows the evolution of in-distribution per-token accuracy with increasing RMS. On the LHS, we see that performance falls by $\\gtrsim10\\%$ at only a $1\\%$ noise level. We also show the trend excluding the end-sequence token, which contributes a significant fraction of the metric. On the RHS, we introduce $\\{q,k_t\\}$ noise only to sparse heads (when $\\max_t a_t \\geq 95\\%$) and non-sparse heads (when $\\max_t a_t < 70\\%$). We see that the model is stable with respect to $\\%$-scale noise on sparse-attention, and this regime is dominated by the non-sparse case. \n\nFigure~\\ref{fig: noise sensitivity} (right) is consistent with the stability predictions of Theorems~\\ref{theorem: stability: sparse}-\\ref{theorem: stability: isotropic}. However, it may also be explained if non-sparse distributions are simply more important to the model. This could be caused by non-sparse distributions being more common, as well as depth-dependence. This is because artificial noise is applied to all layers during the forward pass, therefore later layers are perturbed by both the noise component \\textit{and} the shifting of their inputs due to previous layers, which is expected to compound with depth. We are interested in capturing this effect, however it may increase the importance of early layers. See Figure~\\ref{fig: attention map prenorm} for a visualisation of the observed attention maps.\n%It may be instructive to analyse Figure~\\ref{fig: noise sensitivity}(left and right) on a per-layer basis.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[page=3, width=0.9\\textwidth, clip=True, trim=0 8.cm 0 1cm]{figures/Paper_diagrams.pdf}\n    \\caption{\\textbf{Left:} evolution of per-token accuracy as we increase noise on the $L_2$-norms of $\\{q,k_t,m_t\\}$. A $\\gtrsim 10\\%$ drop in performance is observed when $1\\%$ noise is applied to all layers. \\textbf{Right:} applying noise only to $\\{q,k_t\\}$, we see that non-sparse attention drives the drop at small noise, whereas the sparse case is stable. This is consistent with Theorems~\\ref{theorem: stability: sparse}-\\ref{theorem: stability: isotropic}, but this interpretation is confounded by the relative importance of non-sparse distributions caused by frequency and depth-dependence.}\n    \\label{fig: noise sensitivity}\n    \\vspace{0.1cm}\n    \\includegraphics[width=0.9\\textwidth, clip=True, trim=0 0 0 0]{figures/collapse/circuit_collapse_probability_min_att_0.95_0.0_loose.pdf}\n    \\caption{Probability of \\textit{circuit collapse} vs increasing noise. This observes the effect predicted in Section~\\ref{sec: theory: circuit stability}, and measures that $1\\%$ of sparse distributions collapse at a noise level of $11\\%$.}\n    \\label{fig: circuit collapse}\n    %\\vspace*{-7mm}\n    \\vspace*{-5mm}\n\\end{figure}\n\n\\vspace{0.2cm}\n\\textbf{Circuit collapse}\n\nFigure~\\ref{fig: circuit collapse} shows the probability that our artificial noise causes the circuit collapse phenomenon as defined in section~\\ref{sec: theory: circuit stability}. In this experiment, we add noise to every layer independently. This prevents the confounding effect of shifting inputs due to noise in previous layers. We observe that $1\\%$ of sparse attention distributions collapse when they experience noise at a level of $11\\%$. This reduces to $7.5\\%$ and $5.5\\%$ for the two model variations shown in Appendix~\\ref{app: model variations}.\n\n% \\vspace{0.2cm}\n% \\textbf{Comparisons of \\texttt{Pre-Norm} and \\texttt{QKV-Norm}}  ~~~[Visualisations in Appendix~\\ref{appendix: Pre vs QKV norm}]\n\n% We see several hints that normalisation placement affects model behaviour:\n\n% 1) \\texttt{Pre-Norm} and \\texttt{QKV-Norm} have comparable in-distribution per-token accuracies of $91.4\\%$ and $91.0\\%$ respectively. However, performance drops to $87.5\\%$ ($82.6\\%$) for generalisation to intermediate task difficulty, and $66.5\\%$ ($46.8\\%$) for increased difficulty. The performance drop of \\texttt{QKV-Norm} implies that it has learned a less generalisable solution.\n\n% 2) For \\texttt{Pre-Norm}, many heads exhibit sparse-attention. By contrast, sparse attention is much less common when using \\texttt{QKV-Norm}. Similar behaviour is reported for \\texttt{QK-Norm} in \\cite{DBLP:journals/corr/abs-2010-04245}.\n\n% 3) \\texttt{QKV-Norm} trains at a comparable rate to \\texttt{Pre-Norm}, but with improved stability at large learning rates. Similar behaviour is reported for \\texttt{QK-Norm} in \\cite{wortsman2023smallscale}.%, who explain it via logit regularisation rather than circuit stability.\n\n\n% \\begin{figure}[t]\n%     \\centering\n%     \\includegraphics[width=\\textwidth]{figures/training_curve_addition_addition_any_masked_categorical_accuracy.pdf}\n%     \\caption{Training curves for \\texttt{Pre-Norm} and \\texttt{QKV-Norm}  when training with different model sizes and learning rate. For large models, \\texttt{QKV-Norm} is more stable for large learning rates (top right), whilst convergence is comparable for small learning rate (bottom right).}\n%     \\label{fig: circuit stability}\n% \\end{figure}\n==== END OF /2406.17837/sections/experiments.tex ====\n==== BEGINNING OF /2406.17837/appendices/supplementary_theorems.tex ====\n\\section{Supplementary theorems}\n\\label{appendix: supplementary theorems}\n\nThis appendix contains theorems that support the main results, providing additional context or being pre-requisite for the proofs in appendix~\\ref{appendix: proofs}. We use the formulation of section~\\ref{sec: formulation}, where $1 \\leq \\{t,t'\\} \\leq T$ are indices over tokens, $x\\in\\mathbb{R}^{N_x}$ is the message receiving embedding, $\\{y_t\\in\\mathbb{R}^{N_y}\\}$ are the message senders, $w_t = x^T W_{QK} y_t$, and $a_t = \\texttt{softmax}_t w_t$ is the attention distribution\n\n\\vspace{0.5cm}\n\n\\begin{mdframed}[backgroundcolor=green!5,skipabove=-2pt,skipbelow=0]\n\\begin{theorem}\n\tShifting attention scores $w_t$ by a constant offset does not affect the attention distribution. Therefore attention is fully determined by differences in scores.\n \\label{theorem: att shift operator}\n\\end{theorem}\n\n\\textit{Proof.} ~Applying the shift $w_t \\xrightarrow[\\mathrm{offset~w}]{} w_t + \\delta w ~\\forall~t$ with fixed $\\delta w$, we have\n\\begin{equation}\n\\begin{split}\na_t ~&=~ \\frac{e^{w_t}}{\\sum_{t'} e^{w_{t'}}} \\\\\n&\\xrightarrow[\\mathrm{offset~w}]{} ~ \\frac{e^{\\delta w}e^{w_t}}{\\sum_{t'} e^{\\delta w}e^{w_{t'}}} ~=~ \\frac{e^{\\delta w}}{e^{\\delta w}}\\frac{e^{w_t}}{\\sum_{t'} e^{w_{t'}}} ~=~ 1 \\cdot a_t~=~ a_t\n\\end{split}\n\\end{equation}\nAlternatively we may write\n\\begin{equation}\na_t ~=~ \\frac{e^{w_t}}{\\sum_{t'} e^{w_{t'}}} ~=~ \\frac{e^{w_t}}{e^{w_{t'}}\\sum_{t'} e^{w_{t'}-w_t}} ~=~ \\frac{1}{\\sum_{t'} e^{w_{t'}-w_t}}\n\\end{equation}\nwhere $\\left(w_{t'}+\\delta t\\right)-\\left(w_t+\\delta t\\right) = w_{t'}-w_t$.\n\\end{mdframed}\n\n\n\\begin{mdframed}[backgroundcolor=green!5]\n\\begin{theorem}\n\tMultiplying attention scores by a positive factor changes the inverse-temperature of the attention distribution, modulating its sparsity (low temperature = less entropy = more sparse). Corollary: In the sparse limit, attention is fully determined by the order of $w_t$.\n \\label{theorem: att scale operator}\n\\end{theorem}\n\n\\textit{Proof.} ~Applying the scaling $w_t \\xrightarrow[\\mathrm{scale~w}]{} \\kappa w_t ~\\forall~ t$ with fixed $\\kappa > 0$, we have\n\\begin{equation}\n\\begin{split}\n\\frac{e^{\\kappa w_t}}{\\sum_{t'} e^{\\kappa w_{t'}}} ~~&=~~ \\frac{1}{\\sum_{t'} e^{\\kappa (w_{t'}-w_t)}}   \\\\\n~~&\\xrightarrow[\\kappa\\rightarrow 0]{} ~~~~ \\frac{1}{\\sum_{t'} e^0} ~=~ \\frac{1}{T} ~\\forall~ t ~~~~~~~~~~~~~~~~~~~~~\\text{[fully isotropic distribution]} \\\\\n~~&\\xrightarrow[\\kappa\\rightarrow \\infty]{} ~~ \n\\begin{cases}\n1 ~~ ~\\mathrm{for}~ t = \\mathrm{argmax}_{t'} w_{t'} \\\\\n0 ~~ ~~~\\forall~ t \\neq \\mathrm{argmax}_{t'} w_{t'} \\\\\n\\end{cases}\n~~~\\text{[fully sparse distribution]}\n\\end{split}\n\\end{equation}\nwhere the \\texttt{argmax} operator is fully determined by the order of $w_t$.\n\\end{mdframed}\n\n\\begin{mdframed}[backgroundcolor=green!5]\n\\begin{theorem}\n    In the \\texttt{No-Norm} case, the attention distribution $a_t$ is defined by the projection of $y_t$ onto a fixed vector $y_x$ for a given $x$. The length of $y_x$ is an inverse-temperature parameter.\n\\end{theorem}\n\n\\textit{Proof.} ~Write $w_t = x^TW_{QK}y_t = (W_{QK}^Tx)^T y_t \\equiv y_x^T y_t$ where $y_x \\triangleq W_{QK}^T x\\in\\mathbb{R}^{N_y}$, which is the dot-product between $y_t$ and a fixed vector $y_x$ on the row space of $W_{QK}$. Then, re-writing in terms of the vector lengths and the enclosing angle $\\theta_{y_t} = y_x\\wedge y_t$, we have $w = |y_x||y_t|\\cos\\theta_{y_t}$. The factor $|y_x|$ is identical for all $t$, making it an inverse-temperature.\n\\end{mdframed}\n\n% \\begin{mdframed}[backgroundcolor=green!5]\n% \\begin{theorem}\n%     In the \\texttt{No-Norm} case, for parallel heads to attend to a different ordering of tokens, the difference must be driven by information encoded in angles rather than norms. Corollary: parallel circuits cannot attend to different tokens based on information in norms.\n% \\end{theorem}\n\n% \\textit{Proof.} ~Consider the form $w_t = |y_x||y_t|\\cos\\theta_{y_t}$. The factor $|y_x|$ does not affect the order of $w_t$. The factor $|y_t|$ is identical for different heads. Any difference in order must therefore be encoded in $\\cos\\theta_{y_t}$.\n% \\end{mdframed}\n\n\n\n\n\\clearpage\n\\begin{mdframed}[backgroundcolor=green!5]\n\\begin{theorem}\nIn the \\texttt{No-Norm} case, bias parameters in the construction of query and key vectors are nullified by the \\texttt{softmax}, or only contribute terms that may be recovered if $x$ contains a constant direction.\n\\label{theorem: decomposition}\n\\end{theorem}\n\n\\textit{Proof.} ~Consider a modification to the construction of query and key vectors that uses the affine transformations $q = W_Qx+b_Q$ and $k_t = W_Ky_t+b_K$, with $W_{Q}\\in\\mathbb{R}^{N_{qkv}\\times N_x}$, $W_{K}\\in\\mathbb{R}^{N_{qkv}\\times N_y}$, $W_{QK}\\triangleq W_Q^TW_K$, and $b_Q,b_K\\in\\mathbb{R}^{N_{qkv}}$. The dot-product attention scores are then:\n\\begin{equation}\n\\begin{split}\n    w_t ~&=~ q^T k_t \\\\\n       &=~ \\left(W_Qx + b_Q\\right)^T \\left(W_Ky_t + b_K\\right)  \\\\\n       &=~ x^TW_{QK}y_t ~+~ ({W_Q}^Tb_K)^Tx ~+~ ({W_K}^Tb_Q)^Ty_t ~+~ b_Q^Tb_K \\\\\n    w_t ~+~ const~ &=~ x^TW_{QK}y_t ~+~ ({W_Q}^Tb_K)^Tx ~+~ ({W_K}^Tb_Q)^Ty_t  \\\\\n       &\\triangleq~ x^TW_{QK}y_t ~+~ \\rho_x^Tx ~+~ \\rho_y^Ty_t  ~~~~~~~~~~~~~\\rightarrow ~\\rho_x^Tx =const ~\\mathrm{given}~x\\rightarrow \\\\\n       &=~ x^TW_{QK}y_t ~+~ \\rho_y^Ty_t  ~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\rightarrow ~W_{QK} \\triangleq \\Omega^T \\Lambda \\Sigma~\\text{via SVD}~\\rightarrow  \\\\\n       &=~ x^T\\Omega^T\\Lambda\\Sigma y_t ~+~ \\rho_y^Ty_t ~~~~~~~~~~~~~~~~~~~~~~~~~~ \\rightarrow ~x' \\triangleq \\Omega x, ~~y'_t \\triangleq \\Sigma y_t ~\\rightarrow \\\\\n       &=~ {x'}^T\\Lambda y'_t ~+~ \\rho_y^Ty_t\n\\end{split}\n\\end{equation}\nAfter expanding the terms, we find an additive constant $b_Q^Tb_K$, and move this onto the LHS. Theorem~\\ref{theorem: att shift operator} states that this has no impact on the output of the \\texttt{softmax} operator. We identify $\\rho_x\\triangleq W_Q^Tb_k$ and $\\rho_y\\triangleq W_K^Tb_q$ as vectors on the \\textbf{row-spaces of $W_Q$ and $W_K$ respectively}, defined as linear maps of the special directions $b_K$ and $b_Q$. Since $x$ is constant for each \\texttt{softmax}, $\\rho_x^Tx$ is constant, and we absorb it into the LHS. We perform the singular value decomposition $W_{QK} \\triangleq \\Omega^T \\Lambda \\Sigma$ where $\\{\\Omega\\in\\mathbb{R}^{N_x\\times N_x},~\\Sigma\\in\\mathbb{R}^{N_y\\times N_y}\\}$ are orthonormal matrices and $\\Lambda \\in \\mathbb{R}^{N_x\\times N_y}$ is a diagonal matrix of positive-semidefinite singular values with maximum rank $\\min(N_x,N_y,N_{qkv})$. Orthonormal matrices apply a basis change to the embedding space using rotations and reflections. We write the transformed embeddings as $x' \\triangleq \\Omega x$ and $y_t' \\triangleq \\Sigma y_t$. The dot-product then has two terms:\n\\begin{enumerate}\n    \\item ${x'}^T \\Lambda y'_t = \\sum_i \\Lambda_{ii} x'_i y'_{ti}$ sculpts the attention distribution according to \\textit{pairwise relationships} between embeddings. We can say that $\\{\\Omega,\\Sigma\\}$ align the bases of $x$ and $y_t$, mapping them onto a common orthonormal coordinate system. $\\Lambda_{ii}$ then assigns an importance weight to each coordinate $i$, determining the contribution of $x'_iy'_{ti}$.\n    %\\footnote{``Every direction'' is true when $M=N_x=N_y\\equiv N$, otherwise the rank of $Q^TK$ is $min(M,N_x,N_y)$.}\n    %\\item $\\rho_x^Tx$ means ``$(s)$ receives from all senders when $x \\parallel \\rho_x$'', where $\\rho_x$ must be a vector on the column-space of $Q$.\n    \\item $\\rho_y^Ty$ means ``token $t$ sends to all receivers when $y_t \\parallel \\rho_y$'', where $\\rho_y$ must be a vector on the row-space of $W_K$. This may be recovered in the expansion of ${x'}^T\\Lambda y'_t$ if there exists a direction $i$ for which $x'_i=const$.\n\\end{enumerate}\n\\end{mdframed}\n==== END OF /2406.17837/appendices/supplementary_theorems.tex ====\n==== BEGINNING OF /2406.17837/appendices/proofs.tex ====\n\\section{Proofs of theorems in the main text}\n\\label{appendix: proofs}\n\nThis appendix provides proofs for the theorems presented in section~\\ref{sec: theory: residual structure}-\\ref{sec: theory: circuit stability}.\n\n\\vspace{0.5cm}\n\n\\begin{mdframed}[backgroundcolor=green!5]\n\\textbf{Theorem~\\ref{theorem: structure: no-norm}.} \\textit{\\texttt{No-Norm}: If two heads with finite non-zero temperature attend to different semantic subspaces, the subspaces must be linearly independent $\\mathbb{S}^{N_\\alpha}_\\alpha \\equiv \\mathbb{R}^{N_\\alpha}$. Corollary: $W_{QK}$ is a low-rank matrix with (left and right) null-spaces that span all non-attended information.}\n\n\\textit{Proof.} ~\nLet $\\theta_A$ and $\\theta_B$ be co-ordinates for the subspaces of $x$ attended to by heads A and B respectively, and $\\phi$ be all other information. Let $\\theta_A\\perp\\theta_B\\perp\\phi$ and $x \\perp y_t$, where $\\perp$ denotes independence. Without loss of generality, write\n\\begin{equation}\n    x(\\theta_A,\\theta_B,\\phi) ~=~ x_A(\\theta_A) ~+~ x_B(\\theta_B) ~+~ x_{other}(\\theta_A,\\theta_B,\\phi)\n\\end{equation}\nThen write\n\\begin{equation}\n\\begin{split}\n    w_t^{(A)}(\\theta_A) ~&=~ \\left(W_{QK}^{(A)} y_t\\right)^T x(\\theta_A,\\theta_B,\\phi)  \\\\\n    &=~ \\left(W_{QK}^{(A)} y_t\\right)^T x_A(\\theta_A) ~+~ \\left(W_{QK}^{(A)} y_t\\right)^T x_B(\\theta_B) ~+~ \\left(W_{QK}^{(A)} y_t\\right)^T x_{other}(\\theta_A,\\theta_B,\\phi) \\\\\n\\end{split}\n\\end{equation}\nwhich requires $\\left(W_{QK}^{(A)} y_t\\right)^T x_B(\\theta_B)=0$ and $\\left(W_{QK}^{(A)} y_t\\right)^T x_{other}(\\theta_A,\\theta_B,\\phi)=0$, since any cancellation between the two terms must be independent of $\\theta_A,\\phi$ and so can be absorbed entirely into the function $x_{B}(\\theta_B)$. This means that $x_B(\\theta_B)$ and $x_{other}(\\theta_A,\\theta_B,\\phi)$ must both be orthogonal to $W_{QK}^{(A)} y_t$, meaning that they reside on the \\textit{left null space} of $W_{QK}^{(A)}$, or are projected by ${W_{QK}^{(A)}}^T$onto a null space of $y_t$.\n\nHead A can only attend to $\\theta_A$ if $x_A(\\theta_A)$ it is not on either of these null spaces, meaning that $x_A(\\theta_A)$ is linearly independent of $x_{B}(\\theta_B)$ and $x_{other}(\\theta_A,\\theta_B,\\phi)$. Likewise for head B\n\\begin{equation}\n\\begin{split}\n    w_t^{(B)}(\\theta_B) ~&=~ \\left(W_{QK}^{(B)} y_t\\right)^T x(\\theta_A,\\theta_B,\\phi)  \\\\\n    &=~ \\left(W_{QK}^{(B)} y_t\\right)^T x_A(\\theta_A) ~+~ \\left(W_{QK}^{(B)} y_t\\right)^T x_B(\\theta_B) ~+~ \\left(W_{QK}^{(B)} y_t\\right)^T x_{other}(\\theta_A,\\theta_B,\\phi) \\\\\n\\end{split}\n\\end{equation}\nrequires that $x_{B}(\\theta_B)$ is linearly independent of both $x_A(\\theta_A)$ and $x_{other}(\\theta_A,\\theta_B,\\phi)$. Since $x_{other}$ resides on both null spaces, it is linearly independent of both $x_A(\\theta_A)$ and $x_B(\\theta_B)$, and may be seen as a third subspace that passes information through to subsequent layers.\n\nWe can also write $w_t = \\left(W_{QK}^T x\\right)^T y_t$, and so the same argument also holds for subspaces on $y_t$. In this case, non-attended subspaces are spanned by the \\textit{right null space} of $W_{QK}$.\n\\end{mdframed}\n\n% \\begin{mdframed}[backgroundcolor=blue!5]\n% \\textbf{Theorem~\\ref{theorem: structure: no-norm}.} \\textit{\\texttt{No-Norm}: If two heads with finite non-zero temperature attend to different semantic subspaces, the subspaces must be linearly independent $\\mathbb{S}^{N_\\alpha}_\\alpha \\equiv \\mathbb{R}^{N_\\alpha}$. Corollary: $W_{QK}$ is a low-rank matrix with (left and right) null-spaces that span all non-attended information.}\n\n% \\begin{proof}\n% Let $\\theta_x$ and $\\theta_y$ be parameters describing the attended subspaces on $x$ and $y_t$ respectively. Let $\\phi_x$ and $\\phi_y$ be the parameters describing the non-attended subspaces. Let $\\theta_x\\perp\\phi_x$, $\\theta_y\\perp\\phi_y$, and $\\theta_x,\\phi_x \\perp \\theta_y,\\phi_y$, where $\\perp$ denotes independence. Writing $x'$ and $y'_t$ using the basis defined in Theorem~\\ref{theorem: decomposition}, we write the embedding vectors as\n% \\begin{equation}\n%     x' = \\begin{pmatrix} f_1(\\theta_x,\\phi_x) \\\\ \\dots \\\\ f_{N_x}(\\theta_x,\\phi_x) \\end{pmatrix}\n%     ~~~~~~~~~~~~~~~~~\n%     y'_t = \\begin{pmatrix} g_1(\\theta_y,\\phi_y) \\\\ \\dots \\\\ g_{N_y}(\\theta_y,\\phi_y) \\end{pmatrix}\n% \\end{equation}\n% where $f_i$ and $g_i$ are differentiable components of the embeddings such that\n% \\begin{equation}\n%     w_t ~=~ {x'}^T \\Lambda y'_t ~=~ \\sum_{i=1}^{\\min(N_x,N_y)} \\Lambda_{ii} f_i(\\theta_x,\\phi_x) g_i(\\theta_y,\\phi_y) \n% \\end{equation}\n% Since $\\theta_x$ are attended, we must have\n% \\begin{equation}\n%     \\frac{\\partial w_t}{\\partial \\theta_x} ~=~ \\sum_i \\lambda_{ii} \\frac{\\partial f_i}{\\partial\\theta_x} g_i ~\\neq~ 0  ~~~~\\forall~~~~ \\phi_x\n% \\end{equation}\n% This requires that there exist directions $A$ with $\\Lambda_{AA}\\neq0$, $g_A \\neq 0$, and $\\frac{\\partial f_A}{\\partial\\theta_x}\\neq0$. Since $\\phi_x$ are non-attended, we must have\n% \\begin{equation}\n%     \\frac{\\partial w_t}{\\partial \\phi_x} ~=~ \\sum_i \\lambda_{ii} \\frac{\\partial f_i}{\\partial\\phi_x} g_i ~=~ 0 ~~~~\\forall~~~~ \\theta_x,\\phi_x\n% \\end{equation}\n% This means that all contributing directions $A$ must satisfy $\\forall ~\\theta_x,\\phi_x$\n% \\begin{equation}\n%     (1)~~~\\frac{\\partial f_A}{\\partial\\phi_x}=0\n%     ~~~~~~~~~\\mathrm{or}~~~~~~~~~\n%     (2)~~~\\frac{\\partial f_A}{\\partial \\phi_x} = - \\frac{1}{\\Lambda_{AA}g_A} \\sum_{i\\neq A} \\Lambda_{ii}\\frac{\\partial f_i}{\\partial \\phi_x} g_i\n% \\end{equation}\n% Case (1) indicates that direction $A$ is independent of $\\phi_x$. Case (2) can be integrated to obtain\n% \\begin{equation}\n%     f_A(\\theta_x,\\phi_x) ~=~ f_A^{\\theta_x}(\\theta_x) ~-~ \\frac{1}{\\Lambda_{AA}g_A} \\sum_{i\\neq A} \\Lambda_{ii} f_i(\\theta_x,\\phi_x) g_i(\\theta_y,\\phi_y)\n% \\label{eq: no-norm phi eqn}\n% \\end{equation}\n% with any function $f_A^{\\theta_x}(\\theta_x)$. \n\n% Integrating this expression with respect to $\\theta_x$ then gives\n% \\begin{equation}\n%     f_J(\\theta_x,\\phi_x) ~\\neq~ f_J^{\\phi_x}(\\phi_x) ~-~ \\frac{1}{\\Lambda_{JJ}g_J} \\sum_{i\\neq J} \\Lambda_{ii} f_i(\\theta_x,\\phi_x) g_i(\\theta_y,\\phi_y)\n% \\label{eq: no-norm theta eqn}\n% \\end{equation}\n% with any function $f_J^{\\phi_x}(\\phi_x)$. If $\\theta_x,\\phi_x$ induce variations in any common direction $I=J$ s.t. $\\frac{\\partial f_I}{\\partial\\theta_x}\\neq0$ and $\\frac{\\partial f_I}{\\partial\\phi_x}\\neq0$, which is attended to s.t. $\\Lambda_{II}>0$, then this direction must satisfy both Eq~\\ref{eq: no-norm phi eqn} and Eq~\\ref{eq: no-norm theta eqn}.\n% \\end{proof}\n% \\end{mdframed}\n\n\\clearpage\n\\begin{mdframed}[backgroundcolor=green!5]\n\\textbf{Theorem~\\ref{theorem: structure: pre-norm}.} \\textit{\\texttt{Pre-Norm}: Semantic subspaces must be represented as orthogonal spheres $\\mathbb{S}^{N_\\alpha}\\equiv\\mathcal{S}^{N_\\alpha-1}$ defined using the $L_2$-norm. Corollary: if either orthogonality or constant-norm are violated, semantic subspaces interfere through a multiplicative factor on $w_t$.}\n\n\\textit{Proof.} ~Write \n\\begin{equation}\nx(\\theta_A,\\theta_B,\\phi) ~=~ x_A(\\theta_A) ~+~ x_B(\\theta_B) ~+~ x_{AB}(\\theta_A,\\theta_B) ~+~ x_{other}(\\theta_A,\\theta_B,\\phi)\n\\end{equation}\nThen for head A we have\n\\begin{equation}\n    w_t^{(A)}(\\theta_A) ~=~ \\frac{1}{\\left|y_t\\right|\\left|x(\\theta_A,\\theta_B,\\phi) \\right|} {w^*_t}^{(A)}(\\theta_A)\n\\end{equation}\nwhere $w^*_t$ are the attention scores from the \\texttt{No-Norm} case, which requires $x_A(\\theta_A)$ and $x_B(\\theta_B)$ to be linearly independent. Now we additionally require $\\left|x(\\theta_A,\\theta_B,\\phi) \\right| \\perp \\theta_B,\\phi$, with\n\\begin{equation}\n|x| ~=~ \\sqrt{|x_A|^2 ~+~ |x_B ~+~ x_{AB} ~+~ x_{other}|^2 ~+~ 2 x_A^T \\left(x_B ~+~ x_{AB} ~+~ x_{other}\\right)}\n\\end{equation}\nwhere we suppress parameter dependence for readability. Since $\\sqrt{\\cdot}$ is a monotonic function, this can only be satisfied if\n\\begin{equation}\n|x_A|^2 ~+~ |x_B ~+~ x_{AB} ~+~ x_{other}|^2 ~+~ 2 x_A^T \\left(x_B ~+~ x_{AB} ~+~ x_{other}\\right) ~\\perp~ \\theta_B,\\phi\n\\end{equation}\nRepeating this process for head B gives\n\\begin{equation}\n|x_B|^2 ~+~ |x_A ~+~ x_{AB} ~+~ x_{other}|^2 ~+~ 2 x_B^T \\left(x_A ~+~ x_{AB} ~+~ x_{other}\\right) ~\\perp~ \\theta_A,\\phi\n\\end{equation}\nCombining and collecting dependencies, we then have\n\\begin{align}\n    |x_A|^2 ~=~ const ~~~&\\forall~~~ \\theta_A \\\\\n    |x_B|^2 ~=~ const ~~~&\\forall~~~ \\theta_B \\\\\n    %|x_{AB}|^2 ~+~ 2x_A^Tx_B ~+~ 2x_{AB}^T\\left(x_A + x_B\\right) ~=~ const ~~~&\\forall~~~ \\theta_A,\\theta_B \\\\\n    \\left( x_{AB} ~+~ 2x_A ~+~ 2x_B \\right)^T x_{AB} ~+~ 2x_A^Tx_B ~=~ const ~~~&\\forall~~~ \\theta_A,\\theta_B \\\\\n    \\left(x_{other} + 2x_A + 2x_B + 2x_{AB}\\right)^T x_{other} ~=~ const ~~~&\\forall~~~ \\theta_A,\\theta_B,\\phi\n\\end{align}\nWe can go one step further, noticing that each individual term carries a different functional dependence, and so must independently be constant\\footnote{N.B. If $|x_{AB}|^2 \\propto x_A^Tx_B$ then $|x_{AB}|^2=const$ reduces to $x_A^Tx_B=const$, which is already required.}. We then have $\\forall~~\\mu,\\nu\\in\\{A,B,AB,other\\}$\n\\begin{equation}\n    |x_\\mu|=const   ~~~~~~\\mathrm{and}~~~~~~  x_\\mu^Tx_\\nu=const \n\\end{equation}\nThe requirements $|x_A(\\theta_A)|=const ~\\forall~\\theta_A$ and $|x_B(\\theta_B)|=const ~\\forall~\\theta_B$ mean that the semantic subspaces have a spherical structure defined by the $L_2$-norm $|\\cdot|$.\n\nNow consider the requirement $x_A(\\theta_A)^Tx_B(\\theta_B)=const$. Say that $\\theta_A$ and $\\theta_B$ have $N_A$ and $N_B$ degrees of freedom, meaning that $x_A$ and $x_B$ have $N_A-1$ and $N_B-1$ respectively, since they each lose one by confinement to the sphere. Say that the constant is nonzero such that $x_A^Tx_B \\neq 0$. This means that there must be some direction $i$ for which $x_{Ai}x_{Bi} \\neq0$. If we know all $N_A-1$ coordinates of $x_A$, and all $N_B - 2$ coordinates of $x_B$ except for direction $i$, then we also know the value of $x_{Bi}$, because it is fixed by the constant. However, this would mean that $x_A$ and $x_B$ are not independent, violating the condition $\\theta_A \\perp \\theta_B$. The only way to satisfy independence is if $x_{Ai}x_{Bi}=0~\\forall~i$, ensuring that degrees of freedom on $x_A$ and $x_B$ never become entangled. Therefore, to satisfy semantic independence, we must have $x_A(\\theta_A)^Tx_B(\\theta_B)=0 ~\\forall~\\theta_A,\\theta_B$. This means that the subspaces are not just linearly independent, but orthogonal.\n\nWe have shown the proof for semantic subspaces of $x$. As for Theorem~\\ref{theorem: structure: no-norm}, the same structure must be true for $y_t$ by symmetry.\n\\end{mdframed}\n\n\\clearpage\n\\begin{mdframed}[backgroundcolor=green!5]\n\\textbf{Theorem~\\ref{theorem: structure: qkv-norm}.} \\textit{\\texttt{QKV-Norm}: Semantic subspaces must be linearly separable, reproducing the \\texttt{No-Norm} case.}\n\n\\textit{Proof.} ~We have\n\\begin{equation}\n    w_t^{(A)}(\\theta_A) ~=~ \\frac{1}{\\left|k_t^{(A)}\\right|\\left|q^{(A)}\\right|} {w^*_t}^{(A)}(\\theta_A)\n\\end{equation}\nwhere $w^*_t$ are the attention scores from the \\texttt{No-Norm} case, which requires $x_A(\\theta_A)$ and $x_B(\\theta_B)$ to be linearly independent. Use\n\\begin{equation}\nx(\\theta_A,\\theta_B,\\phi) ~=~ x_A(\\theta_A) ~+~ x_B(\\theta_B) ~+~ x_{other}(\\theta_A,\\theta_B,\\phi)\n\\end{equation}\nand \n\\begin{equation}\n\\begin{split}\n    q^{(A)}(\\theta_A) ~&=~ W_Q^{(A)} x(\\theta_A,\\theta_B,\\phi)  \\\\\n    &=~ W_Q^{(A)} x_A(\\theta_A) ~+~ W_Q^{(A)} x_B(\\theta_B) ~+~ W_Q^{(A)} x_{other}(\\theta_A,\\theta_B,\\phi) \\\\\n\\end{split}\n\\end{equation}\nSince we already have the condition of linearly independent $x_A,x_B$, there must exist a linear projection operator $P_A$ such that $P_A x_A = x_A$. Defining $W_Q^{(A)}=P_A$, we then have\n\\begin{equation}\n    q^{(A)}(\\theta_A) ~=~ W_Q^{(A)} x_A(\\theta_A) \n\\end{equation}\nThis demonstrates that it is possible to separate linearly independent semantic subspaces on $x$. By symmetry of $w_t^{(A)}(\\theta_A)$, the same must be true for $y_t$.\n\\end{mdframed}\n\n\\begin{mdframed}[backgroundcolor=green!5]\n\\textbf{Theorem~\\ref{theorem: stability: general}.} Consider independent infinitesimal perturbations on queries $\\epsilon^q \\in \\mathbb{R}^{N_{qkv}}$, keys $\\epsilon^k_t \\in \\mathbb{R}^{N_{qkv}}$, and messages $\\epsilon^m_t \\in \\mathbb{R}^{N_{qkv}}$. These propagate onto $\\Delta x = \\sum_{t}a_tm_t$ as\n    \\begin{align}\n        \\epsilon^{\\Delta x(q)} ~~&\\xrightarrow[\\epsilon^q\\rightarrow0]{\\mathrm{~~~~perturb~q~~~~}}~~ \\mathop{\\mathbb{E}}_{a_t} \\Big[ m_t {\\tilde k}_t^T \\Big] \\epsilon^q ~~~~~~~~~~~~~~~~~  {\\tilde k}_t ~\\triangleq~ k_t ~- \\mathop{\\mathbb{E}}_{a_t} \\Big[ k_t \\Big] \n        \\label{eq: stability: general q}\\\\\n        \\epsilon^{\\Delta x(k)} ~~&\\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{~~~~perturb~k~~~~}}~~ \\mathop{\\mathbb{E}}_{a_t} \\Big[ {\\tilde m}_t {\\epsilon^k_t}^T \\Big] q ~~~~~~~~~~~~~~~~~  {\\tilde m}_t ~\\triangleq~ m_t ~- \\mathop{\\mathbb{E}}_{a_t} \\Big[ m_t \\Big]\n        \\label{eq: stability: general k} \\\\\n        \\epsilon^{\\Delta x(m)} ~~&\\xrightarrow[\\epsilon^m_t\\rightarrow0]{\\mathrm{~~~~perturb~m~~~~}}~~ \\mathop{\\mathbb{E}}_{a_t} \\Big[ \\epsilon^m_t \\Big]\n        \\label{eq: stability: general m}\n    \\end{align}\n    where ${\\tilde z}_t$ is the value of $z_t$ measured from the attention-weighted centroid $\\mathbb{E}_{a_t}[z_t] = \\sum_t a_t z_t$.\n\n\\textit{Proof.} ~Consider $q\\rightarrow q+\\epsilon^q$ where $\\epsilon^q$ are infinitesimal perturbations on $q$. Then $\\Delta x \\rightarrow \\Delta x + \\epsilon^{\\Delta x(q)}$ where by Taylor expansion we find\n\\begin{equation}\n    \\epsilon^{\\Delta x(q)} ~=~ \\frac{\\partial \\Delta x}{\\partial q}\\epsilon^q ~+~ \\mathcal{O}\\left({\\epsilon^q}^2\\right)\n\\end{equation}\nwhere the leading term is a matrix $\\frac{\\partial \\Delta x}{\\partial q}$ acting on a vector $\\epsilon^q$. Differentiating gives\n\\begin{equation}\n    \\frac{\\partial\\Delta x}{\\partial q} ~=~ \\sum_{ij} m_i \\frac{\\partial a_i}{\\partial w_j} \\frac{\\partial w_j}{\\partial q}\n\\end{equation}\nwith $a_i = \\texttt{softmax}_i(w_i)$ and $w_i=k_i^Tq$, and we are using $i,j,k$ etc to index over tokens instead of $t,t',t''$ etc, because this is more readable when we have many summations. Then\n\n\\textcolor{Maroon}{\\textit{[continued in next box...]}}\n\\end{mdframed}\n\n\\clearpage\n\\begin{mdframed}[backgroundcolor=green!5]\n\\textcolor{Maroon}{\\textit{[...continuing from previous box]}}\n\n\\begin{equation}\n\\begin{split}\n    \\frac{\\partial a_i}{\\partial w_j} ~&=~ \\frac{\\partial}{\\partial w_j} ~ \\frac{e^{w_i}}{\\sum_k e^{w_k}} \\\\\n    &=~ \\frac{\\delta_{ij}e^{w_i}}{\\sum_k e^{w_k}} ~+~ e^{w_i}\\left(-\\frac{e^{w_j}}{\\left(\\sum_ke^{w_k}\\right)^2}\\right) \\\\\n    &=~ \\frac{e^{w_i}}{\\sum_k e^{w_k}}\\left( 1 ~-~ \\frac{e^{w_j}}{\\sum_le^{w_l}}\\right) \\\\\n    &=~ a_i\\left(\\delta_{ij} ~-~ a_j\\right) \\\\\n\\end{split}\n\\end{equation}\nand $\\frac{\\partial w_i}{\\partial q} = k_i^T$, where we retain the transpose to indicate that this is an element of the dual vector space (i.e. covector). Inserting these results into our expression for $\\epsilon^{\\Delta x(q)}$ gives\n\\begin{equation}\n\\begin{split}\n    \\epsilon^{\\Delta x(q)} ~&=~ \\sum_{ij} m_i a_i\\left(\\delta_{ij} ~-~ a_j\\right) k_j^T \\epsilon^q \\\\\n    &=~ \\sum_{i} m_i a_i \\left(k_i ~-~ \\sum_j a_jk_j \\right)^T \\epsilon^q \\\\\n    &=~ \\sum_{i} m_i a_i {\\tilde k}_i^T \\epsilon^q \\\\\n    &=~ \\mathop{\\mathbb{E}}_{a_i} \\Big[m_i {\\tilde k}_i^T \\Big] \\epsilon^q \\\\\n\\end{split}\n\\end{equation}\nThis is the result for Eq.~\\ref{eq: stability: general q}. Repeating the process for perturbations on $k_i$, we have\n\\begin{equation}\n    \\epsilon^{\\Delta x(k)} ~=~ \\sum_i\\frac{\\partial \\Delta x}{\\partial k_i}\\epsilon^k_i ~+~ \\mathcal{O}\\left({\\epsilon^k}^2\\right)\n\\end{equation}\nand\n\\begin{equation}\n\\begin{split}\n    \\frac{\\partial\\Delta x}{\\partial k_i} ~&=~ \\sum_{jk} m_j \\frac{\\partial a_j}{\\partial w_k} \\frac{\\partial w_k}{\\partial k_i} \\\\\n    &=~ \\sum_{jk} m_j a_j \\left(\\delta_{jk} ~-~ a_k\\right) \\delta_{ki} q^T \\\\\n    &=~ \\sum_{j} m_j a_j \\left(\\delta_{ji} ~-~ a_i\\right) q^T \\\\\n    &=~ a_i {\\tilde m}_i q^T\n\\end{split}\n\\end{equation}\nTherefore\n\\begin{equation}\n    \\epsilon^{\\Delta x(k)} ~=~ \\sum_i a_i {\\tilde m}_i q^T \\epsilon^k_i ~=~ \\mathop{\\mathbb{E}}_{a_i} \\Big[{\\tilde m}_i {\\epsilon^k_i}^T \\Big] q\n\\end{equation}\nwhich is the result for Eq.~\\ref{eq: stability: general k}. Finally,\n\\begin{equation}\n\\begin{split}\n    \\epsilon^{\\Delta x(m)} ~&=~ \\sum_i \\frac{\\partial \\Delta x}{\\partial m_i}\\epsilon^m_i \\\\\n    &=~ \\sum_{i} a_i \\epsilon^m_i \\\\\n    &=~ \\mathop{\\mathbb{E}}_{a_i} \\Big[ \\epsilon^m_i \\Big]\n\\end{split}\n\\end{equation}\nusing $\\frac{\\partial\\Delta x}{\\partial m_i} = \\frac{\\partial}{\\partial m_i}\\sum_j a_j m_j = \\sum_j a_j \\delta_{ij} = a_i$. This is the result for Eq.~\\ref{eq: stability: general m}.\n\\end{mdframed}\n\n\\begin{mdframed}[backgroundcolor=green!5]\n\\textbf{Theorem~\\ref{theorem: stability: sparse}.} For sparse attention:\n    \\begin{equation}\n        \\epsilon^{\\Delta x(q)} \\xrightarrow[\\epsilon^q\\rightarrow0]{\\mathrm{~~perturb~q~~}} 0   ~~~~~~~~~~\n        \\epsilon^{\\Delta x(k)} \\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{~~perturb~k~~}} 0   ~~~~~~~~~~\n        \\epsilon^{\\Delta x(m)} \\xrightarrow[\\epsilon^m_t\\rightarrow0]{\\mathrm{~~perturb~m~~}} \\epsilon^m_{t^*}\n    \\end{equation}\n    i.e. the message is stable with respect to small interference in the queries and keys. Interference in the selected value is linearly transferred onto the message.\n\n\\textit{Proof.} ~For sparse attention we have $a_t = \\delta_{tt^*}$ for some $t^*$. For perturbations of $q$, the RHS of Eq.~\\ref{eq: stability: general q} becomes\n\\begin{equation}\n\\begin{split}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ m_t {\\tilde k}_t^T \\Big] \\epsilon^q ~&=~ \\sum_{t} a_t m_t {\\tilde k}_t^T \\epsilon^q \\\\\n    &=~ \\sum_{t} \\delta_{tt^*} m_t {\\tilde k}_t^T \\epsilon^q \\\\\n    &=~ m_{t^*} {\\tilde k}_{t^*}^T \\epsilon^q \\\\\n    &=~ 0 \\\\\n\\end{split}\n\\end{equation}\nwhere the final step is because ${\\tilde k}_{t^*} = k_{t^*} - \\mathbb{E}_{a_t}[k_t] = k_{t^*} - \\sum_t \\delta_{tt^*} k_t = k_{t^*}-k_{t^*} = 0$. For perturbations of $k_t$, the RHS of Eq.~\\ref{eq: stability: general k} evaluates to $0$ because \n\\begin{equation}\n\\begin{split}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ {\\tilde m}_t {\\epsilon^k_t}^T \\Big] q ~&=~ \\sum_t a_t {\\tilde m}_t q^T \\epsilon^k_t \\\\\n    &=~ \\sum_t \\delta_{tt^*} {\\tilde m}_t q^T \\epsilon^k_t \\\\\n    &=~ {\\tilde m}_{t^*} q^T \\epsilon^k_{t^*} \\\\\n    &=~ 0 \\\\\n\\end{split}\n\\end{equation}\nwhere the final step is because ${\\tilde m}_{t^*} = m_{t^*} - \\sum_t \\delta_{tt^*} m_t = m_{t^*}-m_{t^*} = 0$. For perturbations of $m_t$, the RHS of Eq.~\\ref{eq: stability: general m} evaluates to\n\\begin{equation}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ \\epsilon^m_t \\Big] ~=~ \\sum_{t} a_t \\epsilon^m_t ~=~ \\sum_{t} \\delta_{tt^*} \\epsilon^m_t ~=~ \\epsilon^m_{t^*}\n\\end{equation}\n\\end{mdframed}\n\n\\begin{mdframed}[backgroundcolor=green!5]\n\\textbf{Theorem~\\ref{theorem: stability: isotropic}.}\nFor isotropic attention:\n\\begin{equation}\n    \\epsilon^{\\Delta x(q)} \\xrightarrow[\\epsilon^q\\rightarrow0]{\\mathrm{perturb~q}} \\langle m_t {\\tilde k}_t^T \\rangle_t \\epsilon^q ~~~~~~~~\n    %\\epsilon^{\\Delta x(q)} \\xrightarrow[\\epsilon^q\\rightarrow0]{\\mathrm{~perturb~q~}} 0   ~~~~~~~~~\n    \\epsilon^{\\Delta x(k)} \\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{perturb~k}} \\langle {\\tilde m}_t {\\epsilon^k_t}^T \\rangle_t ~q   ~~~~~~~~\n    \\epsilon^{\\Delta x(m)} \\xrightarrow[\\epsilon^m_t\\rightarrow0]{\\mathrm{perturb~m}} \\langle \\epsilon^m_t \\rangle_t\n\\end{equation}\n% \\begin{align}\n%     \\epsilon^{\\Delta x(q)} ~&\\xrightarrow[\\epsilon^q\\rightarrow0]{\\mathrm{~~perturb~q~~}}~ 0 \\\\\n%     \\epsilon^{\\Delta x(k)} ~&\\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{~~perturb~k~~}}~ \\langle {\\tilde v}_t {\\epsilon^k_t}^T \\rangle_t ~q  \\\\\n%     \\epsilon^{\\Delta x(v)} ~&\\xrightarrow[\\epsilon^v_t\\rightarrow0]{\\mathrm{~~perturb~v~~}}~ \\langle \\epsilon^v_t \\rangle_t\n% \\end{alig\nN.B. isotropy requires $k_t=const$ or $q=0$. Lemma 1: the update is stable to noisy $q$ when $k_t=const$, or when $m_t \\perp k_t$ (c.f. keys and messages from independent subspaces). Lemma 2: the update is stable to noisy $k_t$ when $q=0$, or when $m_t \\perp \\epsilon_t^k$. Lemma 3: the update is stable to noisy $m_t$ when $\\langle \\epsilon^m_t \\rangle_t=0$. Other cases propagate linearly.\n\n\\textit{Proof.} ~For isotropic attention we have $a_t = \\frac{1}{T}$. For perturbations of $q$, the RHS of Eq.~\\ref{eq: stability: general q} is\n\\begin{equation}\n\\begin{split}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ m_t {\\tilde k}_t^T \\Big] \\epsilon^q ~&=~ \\sum_{t} a_t m_t {\\tilde k}_t^T \\epsilon^q \\\\\n    &=~ \\frac{1}{T} \\sum_{t=1}^T m_t {\\tilde k}_t^T \\epsilon^q \\\\\n    &=~ \\langle m_t {\\tilde k}_t^T \\rangle_t \\epsilon^q \\\\\n\\end{split}\n\\end{equation}\nFor lemma 1, we note that $k_t=const$ implies ${\\tilde k}_t=0$, and if $m_t \\perp k_t$ then $\\langle m_t {\\tilde k}_t^T \\rangle_t = \\langle m_t k_t \\rangle_t - \\langle m_t \\rangle_t \\langle k_t\\rangle_t = Cov(m_t,k_t) = 0$.\n\n\\textcolor{Maroon}{\\textit{[continued in next box...]}}\n\\end{mdframed}\n\n\\clearpage\n\\begin{mdframed}[backgroundcolor=green!5]\n\\textcolor{Maroon}{\\textit{[...continuing from previous box]}}\n\nFor perturbations of $k_t$, the RHS of Eq.~\\ref{eq: stability: general k} is\n\\begin{equation}\n\\begin{split}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ {\\tilde m}_t {\\epsilon^k_t}^T \\Big] q ~&=~ \\frac{1}{T} \\sum_{t=1}^T {\\tilde m}_t {\\epsilon^k_t}^T q \\\\\n    &=~ \\langle {\\tilde m}_t {\\epsilon^k_t}^T \\rangle_t q \\\\\n\\end{split}\n\\end{equation}\nFor lemma 2, this expression evaluates to $0$ if $q=0$, and if $m_t \\perp \\epsilon_t^k$ then $\\langle {\\tilde m}_t {\\epsilon^k_t}^T \\rangle_t = \\langle m_t {\\epsilon^k_t}^T \\rangle_t - \\langle m_t \\rangle_t \\langle {\\epsilon^k_t}^T\\rangle_t = Cov(m_t,{\\epsilon^k_t}^T) = 0$.\n\nFor perturbations of $m_t$, the RHS of Eq.~\\ref{eq: stability: general m} evaluates to\n\\begin{equation}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ \\epsilon^m_t \\Big] ~=~ \\frac{1}{T}\\sum_{t=1}^T \\epsilon^m_t ~=~ \\langle \\epsilon^m_t \\rangle_t\n\\end{equation}\n\\end{mdframed}\n\n\\begin{mdframed}[backgroundcolor=green!5]\n\\textbf{Theorem~\\ref{theorem: multiplicative stability: sparse}. } ~Sensitivity of sparse attention to multiplicative perturbations $\\epsilon^q = \\kappa^q q$ and $\\epsilon^k = \\kappa^k_t k_t$ with $\\kappa^q,\\kappa^k_t\\ll1$. Circuit collapse occurs when $\\exists~ t \\neq t^*$ for which:\n\\begin{equation}\n    \\frac{w_{t^*}}{w_t} ~\\begin{cases} ~<~ \\lambda_w & \\mathrm{if}~ w_t \\left(1 + \\kappa^q + \\kappa^k_{t^*}\\right) > 0 \\\\\n    ~>~ \\lambda_w & \\mathrm{otherwise} \\\\ \\end{cases}\n    ~~~~~~~~~~~~~ \\lambda_w ~\\triangleq~ \\frac{1 + \\kappa^q + \\kappa^k_t}{1 + \\kappa^q + \\kappa^k_{t^*}}\n\\end{equation}\nwhere temperature cancels in the fraction. \\textbf{Attention is fully stable above the critical transition point $\\lambda_w$} (c.f. $w_t \\left(1 + \\kappa^q + \\kappa^k_{t^*}\\right) > 0$). We see that query perturbations alone are insufficient, as they result in $\\lambda_w=1$. Lemma: consider the special case when all keys have similar length $k_t \\approx const$, the attended token has $\\theta_{t^*}\\approx0$, the keys are far-from-orthogonal s.t. $\\theta_t \\ll 1$, and $\\kappa^q\\approx0$. Using $w_t \\triangleq |q| |k_t| \\cos\\theta_t$, circuit collapse occurs when $\\exists~ t \\neq t^*$ for which:\n\\begin{equation}\n        \\frac{1}{2}\\theta_t^2 ~\\lesssim~ \\kappa^k_t - \\kappa^k_{t^*}   ~~~~~~~~~~~ \\mathrm{if}~ w_t \\left(1  + \\kappa^k_{t^*}\\right) > 0 ~\\text{, otherwise reverse}\n\\label{eq: app: sparse circuit collapse result}\n\\end{equation}\ni.e. stability requires either well-separated keys s.t. $\\theta_t \\gg 0$, or small perturbations $\\kappa_t-\\kappa^*_t \\ll 1$.\n\n\\textit{Proof.} ~ Apply $q\\rightarrow q+\\epsilon^q$ and $k_t\\rightarrow k_t+\\epsilon_t^k$ to $w_t = q^Tk_t$, then we have $w_t \\rightarrow w_t + \\epsilon_w$ such that $\\epsilon^w_t = q^T\\epsilon_t^k + {\\epsilon^q}^Tk_t + {\\epsilon^q}^T\\epsilon_t^k$. For multiplicative perturbations we have  $\\epsilon^q = \\kappa^q q$ and $\\epsilon^k = \\kappa^k_t k_t$, and so $\\epsilon^w_t = \\kappa^k_t q^Tk_t + \\kappa^q q^Tk_t + \\kappa^k_t\\kappa^qq^Tk_t$. Each term recovers a factor of $w_t=q^Tk_t$, which we factor out to give $\\epsilon^w_t = \\left(\\kappa^q  + \\kappa^k_t + \\kappa^k_t\\kappa^q\\right)w_t$. The final term is subleading in the limit of small perturbations, and so\n\\begin{equation}\n    \\epsilon^w_t ~\\xrightarrow[~\\kappa^q,\\kappa^k_t\\rightarrow0~]{}~ \\left(\\kappa^q  ~+~ \\kappa^k_t\\right)w_t ~+~ \\mathcal{O}\\left(\\kappa^q\\kappa^k_t\\right)\n\\end{equation}\nCircuit collapse occurs when $w_{t^*} - w_t < \\epsilon^w_t - \\epsilon^w_{t^*}$ for some $t$. Substituting our limit for $\\epsilon^w_t$ gives\n\\begin{equation}\n    w_{t^*} - w_t ~<~ \\left(\\kappa^q  ~+~ \\kappa^k_t\\right)w_t - \\left(\\kappa^q  ~+~ \\kappa^k_{t^*}\\right)w_{t^*}\n\\end{equation}\nand collecting terms gives\n\\begin{equation}\n    \\left(1 ~+~ \\kappa^q ~+~ \\kappa^k_{t^*}\\right) w_{t^*} ~<~ \\left(1 ~+~ \\kappa^q ~+~ \\kappa^k_t\\right)w_t\n\\end{equation}\nWe then divide each side by $w_t (1 + \\kappa^q + \\kappa^k_{t^*})$, taking care to reverse the sign of the inequality when this factor is negative, to give\n\n\\textcolor{Maroon}{\\textit{[continued in next box...]}}\n\\end{mdframed}\n\n\\clearpage\n\\begin{mdframed}[backgroundcolor=green!5]\n\\textcolor{Maroon}{\\textit{[...continuing from previous box]}}\n\n\\begin{equation}\n    \\frac{w_{t^*}}{w_t} ~\\begin{cases} ~<~ \\lambda_w & \\mathrm{if}~ w_t \\left(1 + \\kappa^q + \\kappa^k_{t^*}\\right) > 0 \\\\\n    ~>~ \\lambda_w & \\mathrm{otherwise} \\\\ \\end{cases}\n    ~~~~~~~~~~~~~ \\lambda_w ~\\triangleq~ \\frac{1 + \\kappa^q + \\kappa^k_t}{1 + \\kappa^q + \\kappa^k_{t^*}}\n\\end{equation}\nwhich is the first expression in the theorem.  We note that any temperature parameter cancels in the fraction, which means that the attention head cannot become more stable by reducing its temperature to become more sparse. $\\lambda_w$ has the limits\n\\begin{equation}\n    \\lambda_w ~\\xrightarrow[\\kappa^q\\rightarrow0]{~~\\mathrm{keys~only}~~}~ \\frac{1+\\kappa^k_t}{1+\\kappa^k_{t^*}}\n    ~~~~~~~~~~~~~~~~~\n    \\lambda_w ~\\xrightarrow[\\kappa^k_t,\\kappa^k_{t^*}\\rightarrow0]{~~\\mathrm{query~only}~~}~ \\frac{1 + \\kappa_q}{1 + \\kappa_q} = 1\n\\end{equation}\nmeaning that query perturbations alone are insufficient, contributing only when they co-occur with perturbations on the keys. Write $w_t = |q| |k_t| \\cos\\theta_t$ with $\\theta_t = q \\wedge k_t$, and the approximation of identical key norms $k_{t^*}=k_t\\equiv k$ turns this into $w_t = |q| |k| \\cos\\theta_t$. Then\n\\begin{equation}\n    \\frac{w_{t^*}}{w_t} ~=~ \\frac{|q| |k| \\cos\\theta_{t^*}}{|q| |k| \\cos\\theta_t} ~=~ \\frac{\\cos\\theta_{t^*}}{\\cos\\theta_t}\n\\end{equation}\nThen $\\theta_{t^*}=0$ means that $\\cos\\theta_{t^*} = \\cos0=1$, and so $\\frac{\\cos\\theta_{t^*}}{\\cos\\theta_t} = \\frac{1}{\\cos\\theta_t} = \\sec \\theta_t$. We perform a Taylor expansion in $\\theta_t$ to obtain\n\\begin{equation}\n    \\frac{w_{t^*}}{w_t} ~\\approx~ \\sec\\theta_t ~\\approx~ 1 ~+~ \\frac{1}{2}\\theta_t^2 ~+~\\mathcal{O}\\left(\\theta_t^4\\right)\n\\end{equation}\nwhich is valid when $\\theta_t \\ll 1$. This is true for any $t\\neq t^*$ for which $k_t$ is far from orthogonal with $k_{t^*}$. Substituting this into our circuit collapse condition, we have\n\\begin{equation}\n    1 ~+~ \\frac{1}{2}\\theta_t^2 ~<~ \\frac{1 + \\kappa^k_t}{1 + \\kappa^k_{t^*}} ~~~~~~~~~~~~~~ \\mathrm{if}~ w_t \\left(1 + \\kappa^k_{t^*}\\right) > 0 \n\\end{equation}\nwhere we consider the case of $\\kappa_q\\approx0$ for readability. Re-arranging gives\n\\begin{equation}\n    \\frac{1}{2}\\theta_t^2 ~\\lesssim~  \\kappa^k_t - \\kappa^k_{t^*} ~~~~~~~~~~~~~~~~\\text{Circuit~collapse~when~}k_t~\\text{similar}\n\\label{eq: app: sparse circuit collapse result duplicate}\n\\end{equation}\nif $w_t(1 + \\kappa^k_{t^*}) > 0$, and we reverse the inequality otherwise. We have approximated the denominator on the RHS as $1 + \\kappa^k_{t^*} \\approx 1$ for $\\kappa^k_{t^*}\\rightarrow0$.\n\nWhen $\\theta_t \\ll 1$, the LHS of Eq.~\\ref{eq: app: sparse circuit collapse result duplicate} is small. This means that the attention head can tolerate only very small perturbations $\\{\\kappa^k_t,\\kappa^k_{t^*}\\}$. Therefore semantic subspaces must either have a highly orthogonal substructure s.t. $\\theta_t \\gtrsim 1 ~\\forall~t\\neq t^*$, or be orthogonal s.t. $\\kappa_t\\ll1 ~\\forall~ t$.\n\n\\end{mdframed}\n\n\\clearpage\n\\begin{mdframed}[backgroundcolor=green!5]\n\\textbf{Theorem. ~\\ref{theorem: multiplicative stability: isotropic}}. ~Sensitivity of isotropic attention to multiplicative perturbations. Say $\\epsilon^k = \\kappa^k_t k_t$ with $\\kappa^k_t\\ll1$ where $\\{\\kappa_t\\}$ have comparable amplitudes. Then\n\\begin{equation}\n\\epsilon^{\\Delta x(k)} %~\\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{~~perturb~k~~}}%~ w~ \\langle {\\tilde v}_t \\kappa^k_t \\rangle_t \n~\\approx~\n    \\begin{cases}\n    0 ~&~ \\text{if~$\\kappa_t$~independent~of~${\\tilde m}_t$,~by~symmetry} \\\\\n    0 ~&~ \\text{if~$\\kappa_t\\equiv\\kappa$~for~constant~$\\kappa$} \\\\\n    0 ~&~ \\text{if~$q=0$} \\\\\n    w \\langle {\\tilde m}_t \\kappa^k_t \\rangle_t  ~&~ \\text{otherwise}\n    \\end{cases}\n\\end{equation}\n%Stability is driven by the central limit theorem, replacing isotropic perturbations with their mean-field limit. Variance creates instability that increases with the number of random variables.\n\n\\textit{Proof.} ~We begin with the following result from Theorem~\\ref{theorem: stability: isotropic}:\n\\begin{equation}\n\\epsilon^{\\Delta x(k)} ~\\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{~~perturb~k~~}}~ \\langle {\\tilde m}_t {\\epsilon^k_t}^T \\rangle_t ~q\n\\end{equation}\nSubstituting $\\epsilon^k = \\kappa^k_t k_t$ and taking $q$ inside the brackets gives\n\\begin{equation}\n\\langle{{\\tilde m}_t  \\epsilon^k_t}^T \\rangle_t ~q ~=~ \n\\langle {\\tilde m}_t \\kappa_t {k_t}^T \\rangle_t q ~=~\n ~ \\langle {\\tilde m}_t \\kappa_t w_t \\rangle_t\n\\end{equation}\nWe then notice that isotropic attention requires that $w_t$ is a constant, which we call $w$. Then\n\\begin{equation}\n\\epsilon^{\\Delta x(k)} ~\\approx~ w \\langle {\\tilde m}_t \\kappa_t \\rangle_t\n\\end{equation}\nis our general result. We then note three special cases, each resulting in $\\epsilon^{\\Delta x(k)}=0$:\n\\begin{enumerate}\n    \\item If $\\kappa_t \\perp {\\tilde m}_t$ then $\\langle {\\tilde m}_t \\kappa_t \\rangle_t = \\langle m_t \\kappa_t \\rangle_t - \\langle m_t \\rangle_t \\langle \\kappa_t \\rangle_t = Cov(m_t,\\kappa_t) = 0$. This is case when interference $\\kappa_t^k$ on the keys is not dominated by the same semantic subspace as the message $m_t$.\n    \\item If all keys are perturbed by the same factor $\\kappa_t\\equiv\\kappa$, then $\\langle {\\tilde m}_t \\kappa_t \\rangle_t = \\kappa \\langle {\\tilde m}_t \\rangle_t =0$ because $\\langle {\\tilde m}_t \\rangle_t=0$.\n    \\item Isotropic attention can be achieved by either $q=0$ or $k_t=const$. If the case is $q=0$ then this implies $w=0$ also.\n\\end{enumerate}\n\\end{mdframed}\n==== END OF /2406.17837/appendices/proofs.tex ====\n==== BEGINNING OF /2406.17837/appendices/main_experiments_extended.tex ====\n\\section{Extended main experiments}\n\\label{appendix: extended experiments}\n\nThis appendix provides an extended explanation of the experimental results in section~\\ref{sec: experimental results}.\n\n\\subsection{Embedding structure}\n\nFigure~\\ref{fig: embedding spread} presented the spread of embedding $L_2$-norms as a function of model depth. Let us now describe in detail how this plot was made. Figure~\\ref{fig: embedding L2 norms prenorm} shows the distribution of embeddings at the input to every attention layer, for \\texttt{Baseline} models trained using \\texttt{Pre-Norm} (top) and \\texttt{QKV-Norm} (bottom). Colours represent the initial token type corresponding to that embedding. Asterisks denote tokens in the \\textit{answer}, with all labels denoting the \\textit{question}.\n\n\\begin{figure}[h]\n\\includegraphics[width=\\textwidth]{figures/baseline_model/embeddings_lengths_prenorm.pdf}\n\\caption{Distribution of embedding $L_2$-norms at different model depths using the \\texttt{Baseline Pre-Norm} model.}\n\\label{fig: embedding L2 norms prenorm}\n\\vspace{0.4cm}\n\\includegraphics[width=\\textwidth]{figures/baseline_model/embeddings_lengths_qkvnorm.pdf}\n\\caption{Distribution of embedding $L_2$-norms at different model depths using the \\texttt{Baseline QKV-Norm} model.}\n\\label{fig: embedding L2 norms qkvnorm}\n\\end{figure}\n\nWe see that the begin-sequence token (\\texttt{BEG}) is often separate from the distribution, which may be because it remains non-annotated and fulfils a qualitatively different role. We remove this from our estimates to avoid erroneously inflating the spread. Interestingly, \\texttt{BEG} still tends to be close to the main bulk for \\texttt{Pre-Norm}, but can be very far for \\texttt{QKV-Norm}, and we have to use overflow panels to capture it. This is consistent with our hypothesis that \\texttt{Pre-Norm} stability requires embeddings to have similar norms, whilst \\texttt{QKV-Norm} does not require this.\n\nIt is not sufficient to simply measure the spread of Figures~\\ref{fig: embedding L2 norms prenorm} and \\ref{fig: embedding L2 norms qkvnorm}, because an attention head may not be sensitive to all embeddings in the layer. The easiest way to account for this is to weight every embedding according to its assigned attention. Secondly, the distribution is expected to be narrow only on a per-head basis, and there is no reason why distinct heads cannot be centred around different medians. We therefore calculate the weighted distribution of embeddings on a per-head basis, as shown in Figures~\\ref{fig: embedding L2 norms prenorm head}-\\ref{fig: embedding L2 norms qkvnorm head}. \n\n\\begin{figure}[!]\n\\includegraphics[width=\\textwidth]{figures/baseline_model/contributing_tokens_0_prenorm.pdf}\n\\caption{Embedding distributions at different model depths using the \\texttt{Baseline Pre-Norm} model. The categories \\texttt{0-9} and \\texttt{N} are separated into whether they occur in the question (light colour) or answer (dark colour, distinguished by $*$ label). These distributions are used to compute the LHS of Figure~\\ref{fig: embedding spread} after removing the \\texttt{BEG} tokens.}\n\\label{fig: embedding L2 norms prenorm head}\n\\vspace{0.6cm}\n\\includegraphics[width=\\textwidth]{figures/baseline_model/contributing_tokens_0_qkvnorm.pdf}\n\\caption{Embedding distributions at different model depths using the \\texttt{Baseline QKV-Norm} model. The categories \\texttt{0-9} and \\texttt{N} are separated into whether they occur in the question (light colour) or answer (dark colour, distinguished by $*$ label). Note that overflow panels are excluded from this plot for legibility. These distributions are used to compute the RHS of Figure~\\ref{fig: embedding spread} after removing the \\texttt{BEG} tokens.}\n\\label{fig: embedding L2 norms qkvnorm head}\n\\end{figure}\n\n\n\\subsection{Circuit collapse}\n\nFigure~\\ref{fig: circuit collapse} shows the probability of circuit collapse. This is the probability that an attention distribution with no noise selects embedding $i$ with high probability $a_i \\geq 95\\%$, and when noise is added, it transitions such that some $k \\neq i$ becomes the maximum attended embedding. This definition is chosen because it matches our theoretical results in section~\\ref{sec: theory: circuit stability}. However, it does not require that the distribution remains sparse after the noise addition. Figure~\\ref{fig: circuit collapse sparse} compares this baseline result (top) with an alternative definition (bottom), in which the second distribution must also be sparse, meaning $a_k\\geq95\\%$ for some $k \\neq i$. We see that $1\\%$ of sparse attention distributions collapse at a noise level of $11\\%$ when using the original definition, delayed until $17\\%$ when using the sparse definition. Therefore we observe that the sparse-to-sparse case does occur, but requires a higher noise level.\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/collapse/circuit_collapse_probability_min_att_0.95_0.0_loose.pdf}\n    \\vspace{0.4cm}\n    \\includegraphics[width=\\textwidth]{figures/collapse/circuit_collapse_probability_min_att_0.95_0.95_strict.pdf}\n    \\caption{Probability of circuit collapse vs increasing noise. \\textbf{Top:} using the baseline definition. This is a reproduction of Figure~\\ref{fig: circuit collapse}. \\textbf{Bottom:} requiring the attention distribution to remain sparse after switching to a different token.}\n    \\label{fig: circuit collapse sparse}\n\\end{figure}\n==== END OF /2406.17837/appendices/main_experiments_extended.tex ====\n==== BEGINNING OF /2406.17837/appendices/pre_vs_qkvnorm.tex ====\n\\section{Additional comparisons between Pre-Norm and QKV-Norm}\n\\label{appendix: Pre vs QKV norm}\n\n\\subsection{Model performance}\n\nTable~\\ref{table: model performance} shows the per-token accuracy performance for the trained \\texttt{Baseline} models. \\texttt{Pre-Norm} and \\texttt{QKV-Norm} have comparable in-distribution per-token accuracies of $91.4\\%$ and $91.0\\%$ respectively. However, performance drops to $87.5\\%$ ($82.5\\%$) for generalisation to intermediate task difficulty, and $66.7\\%$ ($46.8\\%$) for increased difficulty. The performance drop of \\texttt{QKV-Norm} implies that it has learned a less generalisable solution. This re-enforces our motivation that architectural changes should be important for the inductive bias of a model.\n\n\\begin{table}[h]\n\\centering\n\\begin{tabular}{rccc}\nDataset               &    \\texttt{Pre-Norm}    &    \\texttt{QKV-Norm}   \\\\\n\\hline\nIn-distribution       &    $91.38\\pm0.04\\%$     &    $90.99\\pm0.03\\%$    \\\\\nOOD (interpolation)   &    $87.46\\pm0.04\\%$     &    $82.54\\pm0.04\\%$    \\\\\nOOD (extrapolation)   &    $66.65\\pm0.05\\%$     &    $46.76\\pm0.05\\%$    \\\\\n\\end{tabular}\n\\vspace{0.2cm}\n\\caption{Per-token accuracy for the \\texttt{Baseline} models. Dataset configurations are shown in Table~\\ref{table: dataset specifications main}.}\n\\label{table: model performance}\n\\end{table}\n\n\\subsection{Training stability}\n\nChanging the normalisation layer is expected to affect the training rate and stability. To investigate this, Figure~\\ref{fig: training stability scan} shows the training curves for different model sizes and learning rates. The task is configured as presented in Table~\\ref{table: model spec stability}. The \\texttt{Depth} parameter is the number of layers, where brackets indicate the values for an encoder-decoder model. For example, $(2,2)$ means that we use $2$ encoder blocks and $2$ decoder blocks. Each decoder block has a self-attention and a cross-attention layer, and so the total model has $6$ attention layers. A single \\texttt{Depth} value indicates a decoder architecture, with the number of layers shown. \\texttt{Width} is the number of neurons per layer, and \\texttt{Latent width} is the number of neurons on the space of $\\{q,~k_t,v_t\\}$ (called $N_{qkv}$ in section~\\ref{sec: formulation}). Training curves on the top row use a learning rate of $0.001$, whilst the bottom row use a value of $0.0001$. In each panel, two training runs are shown, with different random seeds. \\texttt{Pre-Norm} is shown in blue, and \\texttt{QKV-Norm} in red.\n\nWe find that \\texttt{Pre-Norm} training is unstable for large learning rates and model sizes, as shown by the flat blue curves in the top right hand panels. Similar stabilisation improvements at large learning rate is reported for \\texttt{QK-Norm} in \\cite{wortsman2023smallscale,dehghani2023scaling}, which applies layer normalisation to $\\{q,~k_t\\}$ but not $v_t$, as for \\texttt{QKV-Norm}. However, we note that training large models with a smaller learning rate leads to improved model performance, as shown by the panels on the bottom right. Finally, we note that both methods typically train the model at similar rates, however small model training follows a very different trajectory, with \\texttt{QKV-Norm} learning more slowly at the beginning of training (bottom left panels). There is also some visible evidence that small model training is actually \\textit{less} effective when using \\texttt{QKV-Norm} with a large learning rate (top left panels).\n\n\\begin{figure}[h]\n\\includegraphics[width=\\textwidth]{figures/training_curve_addition_addition_any_masked_categorical_accuracy.pdf}\n\\caption{Training curves when learning the task configuration shown in Table~\\ref{table: model spec stability}.}\n\\label{fig: training stability scan}\n\\end{figure}\n\n\n\\subsection{Attention sparsity}\n\nWe find that our \\texttt{Pre-Norm} models often exploit sparse-attention, whereas models trained with \\texttt{QKV-Norm} do not. Similar behaviour is reported for \\texttt{QK-Norm} in \\cite{DBLP:journals/corr/abs-2010-04245}. For a systematic comparison, Figure~\\ref{fig: attention sparsity distribution} shows a histogram of the maximum attention observed per-distribution (i.e. a histogram of $\\max_i a_i$). When making this plot, we do not consider the first row of the attention matrix, in which the \\texttt{[} token attends fully to itself.\n\nWe see that the \\texttt{Pre-Norm} distribution has a sharp peak at $1$, indicating a significant use of sparse-attention. By contrast, the \\texttt{QKV-Norm} distribution is weighted towards $0$ and has no peak at $1$. To verify this behaviour, Figure~\\ref{fig: attention map prenorm} shows an attention heatmap for a randomly chosen datapoint when using the \\texttt{Baseline} \\texttt{Pre-Norm} model, and Figure~\\ref{fig: attention map qkvnorm} shows the same datapoint for \\texttt{QKV-Norm}. We observe a significantly less sparse attention matrix for \\texttt{QKV-Norm}. Note that \\cite{DBLP:journals/corr/abs-2010-04245} also shows a similar visualisation.\n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=0.9\\textwidth]{figures/baseline_model/max_attn_comparison.pdf}\n\\caption{Distribution of the maximum attention observed per-distribution, i.e. $\\max_i a_i$, in the \\texttt{Baseline} case. We observe that the \\texttt{Pre-Norm} model often utilises sparse-attention, as seen by the peak at $1$. By contrast, \\texttt{QKV-Norm} shows no such peak. Similar behaviour is reported for \\texttt{QK-Norm} in \\cite{DBLP:journals/corr/abs-2010-04245}.}\n\\label{fig: attention sparsity distribution}\n\\end{figure}\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.87\\textwidth]{figures/baseline_model/attention_map_prenorm.pdf}\n\\caption{Attention maps for a random in-distribution example using the \\texttt{Baseline} \\texttt{Pre-Norm} model. Several attention heads create sparse attention distributions.}\n\\label{fig: attention map prenorm}\n\\vspace{0.3cm}\n\\includegraphics[width=0.87\\textwidth]{figures/baseline_model/attention_map_qkvnorm.pdf}\n\\caption{Attention maps for a random in-distribution example using the \\texttt{Baseline} \\texttt{QKV-Norm} model. We observe much less sparsity than in the \\texttt{Pre-Norm} model, shown in Figure~\\ref{fig: attention map prenorm}. Similar behaviour is reported for \\texttt{QK-Norm} in \\cite{DBLP:journals/corr/abs-2010-04245}.}\n\\label{fig: attention map qkvnorm}\n\\end{figure}\n==== END OF /2406.17837/appendices/pre_vs_qkvnorm.tex ====\n==== BEGINNING OF /2406.17837/appendices/experiment_replications.tex ====\n\\section{Main experiments: results with different models}\n\\label{app: model variations}\n\nIn this appendix we reproduce the main experimental results using our model variations. \n\n\\subsection{Embedding lengths}\n\nFigure~\\ref{fig: embedding spread} shows the empirical results demonstrating the attention-weighted spread of embeddings. Figures~\\ref{fig: embedding spread: model variations BEG}-\\ref{fig: embedding spread: model variations END} show the results we obtain when we perform the same analysis using the \\texttt{Alternate} and \\texttt{Large} model variations. In all cases, we observe 90\\% of embeddings within a spread of roughly $\\pm30\\%$ when using \\texttt{Pre-Norm}. In all cases, the spread of embeddings for \\texttt{QKV-Norm} is larger, although we note that the effect is smaller when using the \\texttt{Alternate} variation.\n\n\\begin{figure}[h]\n\\includegraphics[width=\\textwidth]{figures/baseline_model/Embedding_length_comparison_0.0.pdf}\n\\caption{Attention-weighted spread of embeddings at increasing model depth using the \\texttt{Baseline} model and task configuration. This is a replication of Figure~\\ref{fig: embedding spread}.}\n\\label{fig: embedding spread: model variations BEG}\n\\includegraphics[width=\\textwidth]{figures/alternative_model/Embedding_length_comparison_0.0.pdf}\n\\caption{Attention-weighted spread of embeddings at increasing model depth using the \\texttt{Alternate} model and task configuration.}\n\\label{fig: embedding spread: model variations MID}\n\\includegraphics[width=\\textwidth]{figures/large_model/Embedding_length_comparison_0.0.pdf}\n\\caption{Attention-weighted spread of embeddings at increasing model depth using the \\texttt{Large} model and task configuration.}\n\\label{fig: embedding spread: model variations END}\n\\end{figure}\n\n\\subsection{Model stability with simulated inference}\n\nFigure~\\ref{fig: noise sensitivity}(left) shows the stability of the model predictions under simulated interference. Figure~\\ref{fig: noise sensitivity: model variations} shows the results we obtain when we perform the same analysis using the \\texttt{Alternate} model variation. The \\texttt{Large} model was not run due to its high computational load. We find that the \\texttt{Alternate} model has a larger effect size that \\texttt{Baseline}, with a $\\gtrsim20\\%$ loss of per-token accuracy with only a $1\\%$ noise effect. For completeness, we show \\texttt{QKV-Norm} on the RHS. This is stable by construction, and only jitter due to finite sampling is observed. In these plots, we estimate the statistical uncertainty by evaluating over three datasets and calculating the standard error on the mean. This is plotted as a shaded band, but tends to be narrower than the line width.\n\nFigure~\\ref{fig: noise sensitivity}(right) compares the stability when we only apply noise to sparse heads (defined as $\\max_i a_i \\geq 95\\%$, thin dashed line) and non-sparse heads (defined as $\\max_i a_i < 70\\%$, thick dashed line). Figure~\\ref{fig: noise sensitivity: model variations sparse} compares these results with the \\texttt{Alternate} model variation. In both experiments, sparse-attention is stable under \\%-level noise, and non-sparse distributions dominate this regime.\n\nNote that later layers experience both the artificial noise injection as well as perturbation of their inputs due to the compounding of errors caused by noise in the previous layers.\n\n\\begin{figure}[h]\n\\includegraphics[width=\\textwidth]{figures/baseline_model/model_stability_addition.pdf}\n\\caption{Evolution of per-token accuracy as we increase noise on the $L_2$-norms of $\\{q,k_t,m_t\\}$ for the \\texttt{Baseline} model and task configuration.}\n\\includegraphics[width=\\textwidth]{figures/alternative_model/model_stability_addition_alternate.pdf}\n\\caption{Evolution of per-token accuracy as we increase noise on the $L_2$-norms of $\\{q,k_t,m_t\\}$ for the \\texttt{Alternate} model and task configuration.}\n\\label{fig: noise sensitivity: model variations}\n\\end{figure}\n\n\\begin{figure}[h]\n\\includegraphics[width=\\textwidth]{figures/baseline_model/model_stability_addition_sparse.pdf}\n\\caption{Evolution of per-token accuracy as we increase noise on the $L_2$-norms of $\\{q,k_t,m_t\\}$ for the \\texttt{Baseline} model and task configuration.}\n\\includegraphics[width=\\textwidth]{figures/alternative_model/model_stability_addition_alternate_sparse.pdf}\n\\caption{Evolution of per-token accuracy as we increase noise on the $L_2$-norms of $\\{q,k_t,m_t\\}$ for the \\texttt{Alternate} model and task configuration.}\n\\label{fig: noise sensitivity: model variations sparse}\n\\end{figure}\n\n\\subsection{Circuit collapse}\n\nFigure~\\ref{fig: circuit collapse} shows the probability of circuit collapse. This is the probability that an attention distribution with no noise selects embedding $i$ with high probability $a_i \\geq 95\\%$, and when noise is added, it transitions such that some $k \\neq i$ becomes the maximum attended embedding. Figure~\\ref{fig: circuit collapse: model variations} shows the results we obtain when we perform the same analysis using the \\texttt{Alternate} and \\texttt{Large} model variations. In both cases, we observe the onset of circuit collapse at smaller noise levels. Whilst the \\texttt{Baseline} model observed that 1\\% of sparse attention heads collapsed with 11\\% noise, this value is 7.5\\% for \\texttt{Alternate} and 5.5\\% for \\texttt{Large}.\n\n\\begin{figure}[h]\n\\includegraphics[width=\\textwidth]{figures/collapse/circuit_collapse_probability_min_att_0.95_0.0_loose.pdf}\n\\caption{Probability of \\textit{circuit collapse} vs increasing noise using the \\texttt{Baseline} model and task configuration. This is a replication of Figure~\\ref{fig: circuit collapse}.}\n\\includegraphics[width=\\textwidth]{figures/alternative_model/circuit_collapse_probability_min_att_0.95_0.0_loose.pdf}\n\\caption{Probability of \\textit{circuit collapse} vs increasing noise using the \\texttt{Alternate} model and task configuration.}\n\\includegraphics[width=\\textwidth]{figures/large_model/circuit_collapse_probability_min_att_0.95_0.0_loose.pdf}\n\\caption{Probability of \\textit{circuit collapse} vs increasing noise using the \\texttt{Large} model and task configuration.}\n\\label{fig: circuit collapse: model variations}\n\\end{figure}\n==== END OF /2406.17837/appendices/experiment_replications.tex ====\n==== BEGINNING OF /2406.17837/appendices/experimental_setup.tex ====\n\\section{Experimental setup}\n\\label{appendix: experimental setup}\n\n\\newcommand{\\data}[1]{\\textcolor{Maroon}{\\texttt{{#1}}}}\n\n\\subsection{Data}\n\n%The anonymised code used to produce these results is provided in the supplementary \\texttt{zip} file. \nDue to the task nature, we do not require static datasets and so generate both train and test data on-the-fly. This alleviates storage and memory concerns for long training runs in which a static dataset would have to be large. Datasets are reproducible through configuration of the environment and global random seed, which is used to manually control the random seeds of \\texttt{Python}, \\texttt{TensorFlow} \\cite{tensorflow2015-whitepaper}, and \\texttt{NumPy} \\cite{harris2020array}. This also reproduces the model initialisation. %Random seeds are provided for the models shown. %The \\texttt{zip} file includes detailed log files of training runs to help debug the replication of experiments.\n\n\\subsection{Task specification}\n\nWe consider an integer addition task, where each character is a base-10 numeral \\data{0}-\\data{9}, mathematical operator \\{\\data{+}, \\data{-}, \\data{=}, \\data{N}\\}, or special character \\{\\data{[}, \\data{]}, \\data{*}\\}. The \\data{N} operator signifies that the following integer is \\textit{negative}, and is used to avoid overloading notation with the \\data{-} operator, which means \\textit{minus}. The special characters are the begin-sequence token \\data{[}, end-sequence token \\data{]}, and mask character \\data{*}. Input sequences in the same batch are right-padded with mask tokens to the same length, which do not contribute to the model. Characters that are masked in the output do not contribute to the evaluation metrics. We tokenise per-character so the model does not need to disambiguate different representations for identical patterns (e.g. if the number \\textcolor{Maroon}{\\texttt{112}} is tokenised as [11,2], and \\textcolor{Maroon}{212} is tokenised as [2,12], then the pattern \\textcolor{Maroon}{\\texttt{12}} has a context-dependent representation). The token dictionary has a length of 17. \n\nFor a decoder architecture, the model is a sequence-sequence transformer and each datapoint has a question-answer structure separated by the \\data{=} character, e.g. the first datapoint is:\n\\begin{equation}\n    \\data{[453+16+17-N846=1332}  ~~~~\\rightarrow~~~~ \\data{***************1332]}\n\\end{equation}\nThe model must therefore predict the numerical outputs and the \\data{]} token. For an encoder-decoder architecture, the encoder input is the \\textit{question} and the decoder performs next-token prediction over the \\textit{answer}, e.g.\n\\begin{equation}\n    \\data{[453+16+17-N846]}~\\text{~(encoder)},~~\\data{[1332}~\\text{~(decoder)}  ~~~~\\rightarrow~~~~ \\data{1332]}~\\text{~(decoder)}\n\\end{equation}\n\nTo help visualise the task, Figure~\\ref{fig: screenshot model begin} shows the predictions of the baseline \\texttt{Pre-Norm} model after 1 epoch. Figure~\\ref{fig: screenshot model end} shows the fully-trained model, to help visualise the attainable in-distribution performance. The final epoch per-token accuracy is logged as 92\\%; the model sometimes correctly predicts all digits of the answer, otherwise it appears to be correct in the leading digits. Figures~\\ref{fig: screenshot large model begin}-\\ref{fig: screenshot large model end} repeat this for the \\texttt{Large} model variation, which acts on a more complex task setting and achieves a lower per-token accuracy of 57\\%. Once again, the correctly predicted tokens appear to be driven by the leading digits.\n\n\\begin{figure}[!]\n\\centering\n\\includegraphics[width=0.8\\textwidth]{figures/model_training/predictions_epoch0.png}\n\\caption{\\texttt{Baseline} \\texttt{Pre-Norm} model predictions after 1 training epoch.}\n\\label{fig: screenshot model begin}\n\\vspace{0.5cm}\n\\includegraphics[width=0.8\\textwidth]{figures/model_training/predictions_final_epoch.png}\n\\caption{\\texttt{Baseline} \\texttt{Pre-Norm} model predictions after training.}\n\\label{fig: screenshot model end}\n\\vspace{0.5cm}\n\\includegraphics[width=0.8\\textwidth]{figures/large_model/predictions_epoch0.png}\n\\caption{\\texttt{Large} \\texttt{Pre-Norm} model predictions after 1 training epoch.}\n\\label{fig: screenshot large model begin}\n\\vspace{0.5cm}\n\\includegraphics[width=0.8\\textwidth]{figures/large_model/predictions_final_epoch.png}\n\\caption{\\texttt{Large} \\texttt{Pre-Norm} model predictions after training.}\n\\label{fig: screenshot large model end}\n\\end{figure}\n\n\n\\subsection{Data-generation process}\n\nOne advantage of this task is the ability to modulate its complexity. Each dataset is defined by two hyperparameters:\n\n\\begin{tabular}{cll}\n    Dataset parameter    &   Example   &   Description   \\\\\n    \\hline\n    $N$   &   [3, 4, 6]    &   The allowed number of integers per-sequence  \\\\\n    $L$   &   [2, 3]   &   The allowed number of digits per-integer  \\\\\n\\end{tabular}\n\nEach datapoint is generated by uniformly sampling a value of $N$, then uniformly sampling a value of $L$ for each integer. This ensures that examples are not simply dominated by integers with the maximum number of digits. Each integer is uniformly sampled from all positive and negative integers with that length. Between each integer, an operator is uniformly sampled from the list $[+, -]$. For example, the datapoint \n\\begin{equation}\n    \\data{[453+16+17-N846=1332}  ~~~~\\rightarrow~~~~ \\data{***************1332]}\n\\end{equation}\nwas generated by sampling a value of $N=4$ to determine that the sum contains four integers, then sampling four values of $L=[3,2,2,3]$ to determine their lengths, then sampling the numbers $[453, 16, 17, N846]$ and operators $[+, +, -]$. The inclusion of subtraction, addition of negative numbers, and double-negatives is intended to emphasise solutions that parse the \\textit{context} of each digit within the sum.\n\n\\subsection{Train/test specifications}\n\nTable~\\ref{table: dataset specifications main} shows the $N$ and $L$ parameters used for the \\texttt{Baseline} and \\texttt{Alternate} experiments. We also show the number of datapoints, and the per-datapoint sampling probability. This is a range, with higher probabilites for the simpler sums. Table~\\ref{table: dataset specifications large} shows the task specification for the \\texttt{Large} model variation, which is trained on a more complex setting. We also perform a scan over model size and learning rate to compare the training stability of \\texttt{Pre-Norm} and \\texttt{QKV-Norm}. These experiments were performed using an earlier problem configuration shown in Table~\\ref{table: dataset specifications scan}.\n\n\\begin{table}[h]\n\\centering\n\\begin{tabular}{rllll}\n    Dataset    &   $N$   &   $L$   &   Num datapoints   &   Datapoint probability   \\\\\n    \\hline\n    Train                     &   $[3, 4, 6]$   &   $[2, 3]$   &   110M, acc=90\\% @ 40M   &   $2\\times 10^{-9} $ to $5\\times10^{-24}$   \\\\\n    Validation     &   $[5]$         &   $[2, 3]$   &   6.4k   &   $1\\times 10^{-14}$ to $1\\times 10^{-19}$   \\\\\n    \\hline\n    In-distribution         &   $[3, 4, 6]$   &   $[2, 3]$   &   128k   &   $2\\times 10^{-9}$ to $5\\times10^{-24}$   \\\\\n    OOD (interpolation)     &   $[5]$         &   $[2, 3]$   &   128k   &   $1\\times 10^{-14}$ to $1\\times 10^{-19}$   \\\\\n    OOD (extrapolation)     &   $[7, 8, 9]$   &   $[2, 3]$   &   128k   &   $7\\times 10^{-21}$ to $1\\times 10^{-35}$   \\\\\n\\end{tabular}\n\\vspace{0.3cm}\n\\caption{Dataset configurations used for \\texttt{Baseline} and \\texttt{Alternate} results.}\n\\label{table: dataset specifications main}\n\\vspace{0.5cm}\n\\begin{tabular}{rllll}\n    Dataset    &   $N$   &   $L$   &   Num datapoints   &   Datapoint probability   \\\\\n    \\hline\n    Train                     &   $[4, 5, 7, 8]$   &   $[3, 4, 5]$   &   25M   &   $4\\times 10^{-17} $ to $3\\times10^{-49}$   \\\\\n    Validation     &   $[6]$         &   $[3, 4, 5]$   &   6.4k   &   $2\\times 10^{-24}$ to $2\\times 10^{-36}$   \\\\\n    \\hline\n    In-distribution         &   $[4, 5, 7, 8]$   &   $[3, 4, 5]$   &   128k   &   $4\\times 10^{-17} $ to $3\\times10^{-49}$   \\\\\n    OOD (interpolation)     &   $[6]$         &   $[3, 4, 5]$   &   128k   &   $2\\times 10^{-24}$ to $2\\times 10^{-36}$   \\\\\n    OOD (extrapolation)     &   $[9, 10, 11]$   &   $[3, 4, 5]$   &   128k   &   $3\\times 10^{-37}$ to $3\\times 10^{-67}$   \\\\\n\\end{tabular}\n\\vspace{0.3cm}\n\\caption{Dataset configurations used for \\texttt{Large} results.}\n\\label{table: dataset specifications large}\n\\vspace{0.5cm}\n\\begin{tabular}{rlll}\n    Dataset    &   $N$   &   $L$   &   Datapoint probability   \\\\\n    \\hline\n    Train set                     &   $[3, 4, 5, 7]$   &   $[3, 4, 5]$   &   $4\\times 10^{-13}$ to $3\\times 10^{-43}$   \\\\\n    In-distribution         &   $[3, 4, 5, 7]$   &   $[3, 4, 5]$   &   $4\\times 10^{-13}$ to $3\\times 10^{-43}$   \\\\\n\\end{tabular}\n\\vspace{0.3cm}\n\\caption{Dataset configurations used for training stability results (Figure~\\ref{fig: training stability scan}).}\n\\label{table: dataset specifications scan}\n\\end{table}\n\nWe halt training according to wall time, which leads to a range of observed dataset sizes. Model convergence may also occur much earlier. We therefore show an order-of-magnitude estimate for the number of observed datapoints, as well as the point at which the baseline model reaches 90\\% per-token accuracy (this represents almost-convergence, which is logged at 92.1\\%). \n\nNote that our data-generation strategy does not ensure that training examples are exclusive (there may be repetitions), nor that the in-distribution test set does not contain overlap with training examples. The final column is therefore important, because it demonstrates that the highest per-datapoint sampling probability is $2\\times10^{-9}$, whilst the model converges with $\\mathcal{O}(10^7)$ datapoints and observes $\\mathcal{O}(10^8)$ in total. Since the datapoint probability is $2\\times10^{-9}$ for the simplest configurations and $5\\times10^{-25}$ for the most complicated, this ensures that the in-distribution evaluation metric is dominated by novel examples. The validation set is only used for visual inspection of model behaviour during training, as in Figures~\\ref{fig: screenshot model begin}-\\ref{fig: screenshot large model end}.\n\n\\subsection{Model specification (main experiments)}\n\nWe use a decoder architecture, meaning that the dot-product self-attention layers are causally masked such that token $t$ can only attend to tokens $\\leq t$. The model has the following structure:\n\\begin{equation*}\n\\begin{matrix} \n    \\texttt{Embedding + positional encoding} \\\\\n    \\downarrow \\\\\n    N_{layer} \\times \n    \\begin{bmatrix} \n    \\texttt{Attention block} \\\\ \n    \\downarrow \\\\\n    \\texttt{Feed-forward block~(ReLU)} \\\\\n    \\end{bmatrix} \\\\\n    \\downarrow \\\\\n    \\texttt{Multi-layer perceptron~(ReLU)} \\\\\n    \\downarrow \\\\\n    \\texttt{Predicted~logits} \\\\\n\\end{matrix}\n\\end{equation*}\n\n\\vspace{0.2cm}\n\\textbf{Embedding + positional encoding} ~ We initialise each token embedding as $x = x_{type} + x_{pos}$, where $x_{type}$ is a token embedding with $N_{emb}$ elements, and $x_{pos}$ use cyclic positional encodings of the same form as the original transformer architecture \\cite{DBLP:journals/corr/VaswaniSPUJGKP17}, with $N_{freq}$ frequencies initialised as a base $e$ log-series between periods of $3$ and $1k$ tokens. For each sequence, all position indices are simultaneously offset by a random integer between $0$ and $50$. This augmentation is designed to encourage the use of \\textit{relative} positions rather than absolute. The frequencies are then left as trainable parameters. The positional encodings contribute the first $2N_{freq}$ components of $x_{pos}$, and the remaining are set to $0$. This configuration guarantees that the token embeddings and positional encodings can be made orthogonal in the first layer, and $x_{pos}$ have constant $L_2$-norm, consistent with our theoretical structure.\n\n\\vspace{0.2cm}\n\\textbf{Attention block} ~ $N_{layer}$ is the number of residual blocks of our model, where our baseline is $N_{layer}=10$. The update is as formulated in section~\\ref{sec: formulation}, where $H$ is the number of parallel attention heads per layer. Since the embeddings have length $N_{emb}$, we must have $N_x=N_{emb}$, whilst the latent dimension $N_{qkv}$ is configurable. Either the \\texttt{Pre-Norm} or \\texttt{QKV-Norm} strategy is used, as configured.\n\n\\vspace{0.2cm}\n\\textbf{Feed-forward block} ~ The feed-forward blocks update embeddings using the function $x \\rightarrow x + FF(\\texttt{LayerNorm}(x))$, where $FF$ is a dense network with one hidden layer of size $N_{ff}$. The network uses a \\texttt{ReLU} \\cite{agarap2019deep} activation function on the intermediate layer, followed by a linear projection back onto embedding space. To maintain consistency with other models, we apply \\texttt{LayerNorm} at the input to $FF$. Both \\texttt{LayerNorm} and $FF$ use bias parameters.\n\n\\vspace{0.2cm}\n\\textbf{Multi-layer perceptron} ~ The final embeddings $x$ are mapped onto token logits $y$ using the function $y = MLP(\\texttt{LayerNorm}(x))$, where $MLP$ is a multi-layer perceptron with two hidden layers of size $N_{MLP}$ and ReLU activation. %If configured, the activations are followed by \\texttt{LayerNorm} normalisation. \nThe final layer is a linear projection onto the space of logits, which has length 17. For the training stability scan in Figure~\\ref{fig: training stability scan}, the MLP has three hidden layers instead.\n\n\\vspace{0.2cm}\n\\textbf{Hyperparameters} ~ Table~\\ref{table: model spec main} shows the hyperparameters used to configure the networks of the main experiments. Table~\\ref{table: model spec stability} show the hyperparameters used for the training stability analysis. This experiment also uses encoder-decoder models, following the same setup as the original transformer architecture \\cite{DBLP:journals/corr/VaswaniSPUJGKP17} and with the layer configurations listed here.\n\n\\begin{table}[h]\n\\centering\n\\begin{tabular}{rlllllllll}\n    Model   &   $N_{freq}$   &   $N_{emb}$   &   $N_{layer}$   &   $H$   &   $N_{qkv}$   &   $N_{ff}$   &   $N_{MLP}$   &   seed   \\\\\n\\hline\n    \\texttt{Baseline}        &   32   &   512    &   10    &   12   &   64   &   512   &   2$\\times$512   &   100   \\\\\n    \\texttt{Alternative}   &   32   &   512    &   8     &   12   &   64   &   512   &   2$\\times$512   &   100   \\\\\n    \\texttt{Large}   &   32   &   1024   &   12    &   16   &   64   &   512   &   2$\\times$512   &   100   \\\\\n\\end{tabular}\n\\vspace{0.2cm}\n\\caption{Model hyperparameters for main experiments (i.e. other than training stability). \\texttt{Baseline} is used for the main results presented in section~\\ref{sec: experimental results} (short). \\texttt{Alternate} and \\texttt{Large} are presented in appendix~\\ref{app: model variations} to show reproducibility of observations.}\n\\label{table: model spec main}\n\\vspace{0.2cm}\n\\begin{tabular}{rlllllllll}\n    Model   &   $N_{freq}$   &   $N_{emb}$   &   $N_{layer}$   &   $H$   &   $N_{qkv}$   &   $N_{ff}$   &   $N_{MLP}$   &   seed   \\\\\n\\hline\n    \\texttt{All}        &   16   &   -   &   -    &   12   &   -   &   512   &   3$\\times$512   &   1,2   \\\\\n\\end{tabular}\n\\vspace{0.2cm}\n\\caption{Model hyperparameters for training stability experiments. Empty parameters are varied per-model and displayed in Figure~\\ref{fig: training stability scan}.}\n\\label{table: model spec stability}\n\\end{table}\n\n\\vspace{0.2cm}\n\\textbf{Loss} ~ The loss function is \\texttt{categorical cross entropy}, calculated from the output logits.\n\n\\subsection{Model initialisation}\n\nWe use a custom initialisation strategy to give control over the initial state of the model. In particular, we use \\texttt{Checkpoint} layers to ensure that the initial states are comparable between \\texttt{Pre-Norm} and \\texttt{QKV-Norm}. This ensures that any observed differences are driven by the normalisation function, rather than being confounded by the layer placement creating more/less favourable initial conditions. \n\n\\texttt{Checkpoint} layers are calibrated on the first training batch immediately prior to training. They use this data to measure the standard deviation at that point, and calculate a scale factor that fixes the standard deviation to a pre-defined hyperparameter $\\sigma$. All subsequent passes through the layer simply apply this scale factor. This ensures that the model is initialised with a standard deviation of $\\sigma$ at that point.\n\nWe apply \\texttt{Checkpoint} layers to the token embeddings $x_{type}$ ($\\sigma_{type}=0.5$), and the initial embeddings $x$ ($\\sigma_x=1.0$), ensuring they are relatively balanced and unit scale. In every attention layer, we apply \\texttt{Checkpoint} layers to re-calibrate the possibly-Pre-normalised embeddings to $\\sigma_x$ immediately before applying the $W_Q$, $W_K$, and $W_V$ operators. This counteracts the effect that transformer necessarily increases the embedding variance throughout the model at initialisation. We apply \\texttt{Checkpoint} layers to $w_t$ in every attention layer, with constant $\\sigma_w=0.1$. This controls the variance on the initial-state attention distribution. We apply \\texttt{Checkpoint} layers to $\\Delta x$ in every attention layer, with constant $\\sigma_{\\Delta x}=0.05$, calibrating it with respect to $x$. \n\nIn the attention layer, we use uniform initialisation of the weight matrices $W_Q$, $W_K$, $W_V$, and $W_O$. The limits are configured to ensure that the initial state standard deviations on $w_t$ and $\\Delta x$ are close to their target values. Defining $\\sigma_{qk} \\triangleq \\sqrt[4]{\\frac{\\sigma_w}{N_{qkv}^3}}$, the limits are calculated as follows:\n\n\\begin{tabular}{rl}\n    Weight   &   Limits   \\\\\n    \\hline\n    $W_Q$   &   $\\pm \\sqrt{3} \\sigma_{qk}$   \\\\\n    $W_K$   &   $\\pm \\sqrt{3} \\sigma_{qk}$   \\\\\n    $W_V$   &   $\\pm \\sqrt\\frac{3}{N_{qkv}}$    \\\\\n    $W_O$   &   $\\pm \\sqrt{\\frac{3}{H N_{qkv}}}$     \\\\\n\\end{tabular}\n\n\nHowever, we note that this initialisation is superseded by the calibration of the \\texttt{Checkpoint} layers for determining the initial state, and we include it only to promote numerical stability. All other feed-forward layers use \\texttt{Glorot uniform} \\cite{pmlr-v9-glorot10a} initialisation, as implemented in \\texttt{Keras} \\cite{chollet2015keras}. Normalisation gain parameters are initialised to $1$ and biases, where used, to $0$.\n\n\\subsection{Training algorithm}\n\nWe train using the \\texttt{AdamW} optimiser \\cite{loshchilov2019decoupled} with learning rate $3\\times10^{-4}$ and weight decay of $0.01$, with all other parameters following their default values  in \\texttt{TensorFlow+Keras v2.15.0}. Each epoch consists of $2000$ batches of $128$ datapoints. For the main experiments and model variations, we use an \\textit{adaptive learning rate decay} strategy. This means that the learning rate is multiplied by a factor of $0.5$ if the training loss does not improve for $3$ consecutive epochs. We find that this balances training speed with improved performance by using small learning rates later in training. Training is halted after two days of wall time, which we observe to allow model convergence, as shown in Figure~\\ref{fig: model train curves}. For the model stability scan, training is run for $60$ hours, and learning rate is not allowed to decay (stability with respect to learning rate being one of the targets of study).\n\n\\begin{figure}[h]\n\\includegraphics[width=\\textwidth]{figures/model_training/training_curves.pdf}\n\\caption{Model training curves for the \\texttt{Baseline} \\texttt{Pre-Norm} configuration.}\n\\label{fig: model train curves}\n\\end{figure}\n\n\n\\subsection{Computational resources}\n\nThe main experiments are all performed on a single \\texttt{Nvidia v100-SXM2-16GB (Volta)} GPU. The scan of models used for the stability analysis were trained on a batch cluster with a variety of compute nodes, using $8$ cores per training run. A representative compute node is \\texttt{212-core Intel Xeon E5-2690 v3 @ 2.60GHz + 128GB RAM}.\n\n\\subsection{Environment details}\n\n%Training logs provide a dump of all Python packages used in the environment. Along with the quoted random seeds (also in the log files) and supplementary code, this is sufficient for replicating our models. \nThe main contributing package versions are as follows:\n\n\\begin{tabular}{rl}\n    Package    &    Version    \\\\\n    \\hline\n    Python       &   \\texttt{3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]}  \\\\\n    TensorFlow   &   \\texttt{2.15.0}  \\\\\n    Keras        &   \\texttt{2.15.0}  \\\\\n    NumPy        &   \\texttt{1.26.2}  \\\\\n\\end{tabular}\n\n% \\subsection{Run commands}\n\n% Training runs are executed from the \\texttt{remote\\_training\\_env/run} a command such as\n\n% \\texttt{python train\\_model.py -{}-env <env.cfg> -{}-model <model\\_base.cfg> <model.cfg> -{}-training <training.cfg> -{}-strategy PRE\\_VECNORM -{}-tag RUN\\_TAG -{}-duration 2days -{}-seed 100}\n\n% along with the provided config files. Plotting is achieved by running the notebooks in the directory the \\texttt{local\\_analysis\\_env/run}, pointing the appropriate config value to the trained model. Strategies are PRE\\_VECNORM and HEAD\\_VECNORM for \\texttt{Pre-Norm} and \\texttt{QKV-Norm} respectively.\n==== END OF /2406.17837/appendices/experimental_setup.tex ====",
            "processed_original_tex": "==== BEGINNING OF /2406.17837/main.tex ====\n\\documentclass{article}\n\n\\usepackage{amsmath}\n\\usepackage{amsthm}\n\\usepackage{amssymb}\n\\usepackage{multirow}\n\\usepackage{makecell}\n\\usepackage{pbox}\n\\usepackage{graphicx}\n\\usepackage[]{mdframed}\n\\usepackage{float}\n\\usepackage[dvipsnames]{xcolor}\n\\usepackage[numbers,sort&compress]{natbib}\n\n\\newtheorem{theorem}{Theorem}\n\\newtheorem{corollary}{Corollary}[theorem]\n\n\n\\renewcommand\\theadalign{bc}\n\\renewcommand\\theadfont{\\bfseries}\n\\renewcommand\\theadgape{\\Gape[4pt]}\n\\renewcommand\\cellgape{\\Gape[4pt]}\n\n\n\n\\usepackage[preprint]{neurips_2024}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\usepackage[utf8]{inputenc} \n\\usepackage[T1]{fontenc}    \n\\usepackage{hyperref}       \n\\usepackage{url}            \n\\usepackage{booktabs}       \n\\usepackage{amsfonts}       \n\\usepackage{nicefrac}       \n\\usepackage{microtype}      \n\\usepackage{xcolor}         \n\n\n\n\\title{Transformer Normalisation Layers and the Independence of Semantic Subspaces}\n\n\n\n\n\n\n\n\n\n\n\n\n\\author{\n  Stephen Menary \\\\\n  University of Manchester, UK \\\\\n  \\texttt{stephen.menary@manchester.ac.uk} \\\\\n   \\And\n  Samuel Kaski \\\\\n  $^{1}$ University of Manchester, UK \\\\\n  $^{2}$ Aalto University, Finland \\\\\n  \\texttt{samuel.kaski@aalto.fi} \\\\\n   \\And\n  Andr{\\'e} Freitas \\\\\n    $^{1}$ Department of Computer Science, University of Manchester, UK \\\\\n    $^{2}$ Idiap Research Institute, Switzerland \\\\\n    $^{3}$ National Biomarker Centre, CRUK-MI, University of Manchester, UK\\\\\n  \\texttt{andre.freitas@manchester.ac.uk} \\\\\n}\n\n\n\\begin{document}\n\n\n\n\n\n\\maketitle\n\n\n\n\n\\input{sections/abstract}\n\n\n\n\n\\input{sections/introduction}\n\n\n\n\n\\input{sections/the_idea}\n\n\n\n\n\\input{sections/related_works}\n\n\n\n\n\\input{sections/formulation}\n\n\n\n\n\n\\input{sections/theory_structure}\n\n\n\n\n\n\\input{sections/theory_stability}\n\n\n\n\n\\input{sections/experiments}\n\n\n\n\n\\input{sections/limitations_outlook}\n\n\n\n\n\\begin{ack}\nThis work is supported by UKRI Turing AI World-Leading Researcher Fellowship (EP/W002973/1). This work was partially funded by the Swiss National Science Foundation (SNSF) project NeuMath (\\href{https://data.snf.ch/grants/grant/204617}{200021\\_204617}), by the EPSRC grant EP/T026995/1, ``EnnCore: End-to-End Conceptual Guarding of Neural Architectures'' under Security for all in an AI enabled society, by the CRUK National Biomarker Centre, and supported by the Manchester Experimental Cancer Medicine Centre and the NIHR Manchester Biomedical Research Centre.\n\\end{ack}\n\n\n\n\n\n\\bibliographystyle{unsrt} \n\\bibliography{bibliography}\n\n\n\n\n\n\n\n\n\\appendix\n\n\n\n\n\\clearpage\n\\input{appendices/experimental_setup}\n\n\n\n\n\\clearpage\n\\input{appendices/main_experiments_extended}\n\n\n\n\n\\clearpage\n\\input{appendices/experiment_replications}\n\n\n\n\n\\clearpage\n\\input{appendices/pre_vs_qkvnorm}\n\n\n\n\n\\clearpage\n\\input{appendices/supplementary_theorems}\n\n\n\n\n\\clearpage\n\\input{appendices/proofs}\n\n\n\n\\end{document}\n==== END OF /2406.17837/main.tex ====\n==== BEGINNING OF /2406.17837/sections/theory_structure.tex ====\n\\section{Theory: representation structure required for independent subspaces}\n\\label{sec: theory: residual structure}\n\nLet $\\mathbb{S}^N \\equiv \\mathbb{R}^N$ be an $N$-dimensional latent representation of $\\mathcal{X}$ or $\\mathcal{Y}$.\n\n\\begin{mdframed}[backgroundcolor=red!5]\n\\textbf{[Definition] ~ Semantic subspace:} any independent $N_\\alpha$-dimensional subspace $\\mathbb{S}_\\alpha^{N_\\alpha} \\subset \\mathbb{S}^N$ for which every element may be uniquely identified by some parameters $\\theta_\\alpha$, such that it is possible for the attention scores $w_t$ to be fully specified by $\\theta_\\alpha$. \\textbf{Semantic separability:} ability for parallel heads to be fully specified by different semantic subspaces.\n\n\\end{mdframed}\n\n\nLet $\\{\\alpha\\}$ be the set of indivisible semantic subspaces. This can be seen as a \\textit{co-ordinate system} for the attendable embedding space. Semantic separability requires that each co-ordinate $\\alpha$ be independently measurable by an attention head. Let $\\mathbb{S}^N$ contain $N_s$ indivisible semantic subspaces $1\\leq\\alpha\\leq N_s$. Then $\\mathbb{S}^N=\\prod_\\alpha \\mathbb{S}^{N_\\alpha}_\\alpha \\bigoplus\\mathbb{S}_\\mathrm{null}$ such that $\\sum_\\alpha N_\\alpha \\leq N$ satisfies semantic separability, where $\\prod_\\alpha,\\bigoplus$ are Cartesian products and $\\mathbb{S}_\\mathrm{null}$ is a separable space of non-attended information. \n\nThe following theorems derive the representation structure required for semantic separability:\n\n\\begin{mdframed}[backgroundcolor=blue!5]\n\\underline{\\textbf{Semantically separable representation structures}}\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\textit{[Proofs in appendix~\\ref{appendix: proofs}]}\n\n\\begin{theorem}\n    \\texttt{No-Norm}: If two heads with finite non-zero temperature attend to different semantic subspaces, the subspaces must be linearly independent $\\mathbb{S}^{N_\\alpha}_\\alpha \\equiv \\mathbb{R}^{N_\\alpha}$. Corollary: $W_{QK}$ is a low-rank matrix with (left and right) null-spaces that span all non-attended information.\n\\label{theorem: structure: no-norm}\n\\end{theorem}\n\n\\begin{theorem}\n    \\texttt{Pre-Norm}: Semantic subspaces must be represented as orthogonal spheres $\\mathbb{S}^{N_\\alpha}\\equiv\\mathcal{S}^{N_\\alpha-1}$ defined using the $L_2$-norm. Corollary: if either orthogonality or constant-norm are violated, semantic subspaces interfere through a multiplicative factor on $w_t$.\n\\label{theorem: structure: pre-norm}\n\\end{theorem}\n\n\\begin{theorem}\n    \\texttt{QKV-Norm}: Semantic subspaces must be linearly independent.\n\\label{theorem: structure: qkv-norm}\n\\end{theorem}\n\\end{mdframed}\n\nWe note that every linear subspace $\\mathbb{R}^{N_\\alpha}$ has $N_\\alpha$ continuous degrees of freedom, whilst $\\mathcal{S}^{N_\\alpha-1}$ has only $N_\\alpha-1$, the other being removed by the fixed-norm constraint. The subspace $\\mathcal{S}^0$ is allowed and may be seen as a binary variable with values $\\pm const_\\alpha$, and the total representation can store $N_s$ such variables. For \\texttt{QKV-Norm}, we note that the residual subspace $\\mathbb{R}^{N_\\alpha}$ only contributes $N_\\alpha-1$ continuous degrees of freedom to the attention calculation, because we apply the projection $\\mathbb{R}^{N_\\alpha} \\rightarrow \\mathcal{S}^{N_\\alpha-1}$ after extracting the subspace. Table~\\ref{table:residual structures main} provides a summary.\n\n\\vspace{0.2cm}\n\\noindent\n\\textbf{Structure of messages}\n\nWe note the special case of compositional annotation, in which a layer creates a semantic subspace that is extracted by a later layer. This is used by circuits including the \\textit{induction circuit} \\cite{elhage2021mathematical} described in section~\\ref{sec: idea}. By normalising the inputs, \\texttt{Pre-Norm} induces a \\textit{spheroid} message structure close to the \\textit{sphere} required for separability in later layers. This may facilitate compositional annotation, aiding in circuit-formation. Message structures are summarised in Table~\\ref{table:value structures main}.\n\n\\begin{table}[h]\n\\centering\n\\begin{tabular}{rlll}\n    Strategy                 &  $\\mathbb{S}^N$  &  Representation structure   &  Attendable d.o.f.  \\\\\n    \\hline\n    \\texttt{No-Norm}         &  $\\prod_\\alpha \\mathbb{R}^{N_\\alpha}$     &  Linearly independent subspaces  &  $N$            \\\\\n    \\texttt{Pre-LayerNorm}   &  $\\prod_\\alpha \\mathcal{S}^{N_\\alpha-1}$  &  Orthogonal spheres $\\perp 1^N$  &  $N - N_s - 1$  \\\\\n    \\texttt{Pre-RMSNorm}  &  $\\prod_\\alpha \\mathcal{S}^{N_\\alpha-1}$  &  Orthogonal spheres              &  $N - N_s$      \\\\\n    \\texttt{QKV-Norm}     &  $\\prod_\\alpha \\mathbb{R}^{N_\\alpha}$     &  Linearly independent subspaces  &  $N - N_s$      \\\\\n\\end{tabular}\n\\vspace{0.1cm}\n\\caption{Representation structure required for semantic separability; \\textit{d.o.f.} means \\textit{degrees of freedom}.}\n\\label{table:residual structures main}\n\\vspace{0.1cm}\n\\begin{tabular}{rlll}\n    Strategy                     &  $m_t$  &  Structure of $m_t$  &  Compositional annotation if  \\\\\n    \\hline\n    \\texttt{No-Norm}    &   $W_{OV} y_t$                       &  Linear      &   $m_t$ on independent subspace  \\\\\n    \\texttt{Pre-Norm}   &   $W_{OV} \\texttt{N}(y_t;\\alpha_v)$  &  Spheroid    &   $m_t$ on orthogonal sphere     \\\\\n    \\texttt{QKV-Norm}   &   $W_O\\texttt{N}(W_Vy_t;\\alpha_v)$   &  Spheroid    &   $m_t$ on independent subspace  \\\\\n\\end{tabular}\n\\vspace{0.1cm}\n\\caption{Summary of message structures induced by different placements of normalisation layer.}\n\\label{table:value structures main}\n\\end{table}\n==== END OF /2406.17837/sections/theory_structure.tex ====\n==== BEGINNING OF /2406.17837/sections/the_idea.tex ====\n\\section{The idea}\n\\label{sec: idea}\n\n\\noindent\n\\textbf{Independent subspaces are observed in real-world transformer circuits}\n\nBefore providing a formal definition in section~\\ref{sec: theory: residual structure}, we explain what we mean by a \\textit{semantic subspace} of the latent representation. To emphasise that this is observed in real-world models, we use a known example: the \\textit{induction circuit} \\cite{olsson2022context,elhage2021mathematical}. This two-layer circuit emerges in next-token-prediction models and implements a simple contextual reasoning algorithm called \\textit{prefix-matching}. \n\nConsider text to be a sequence of tokens\\footnote{In this example, we tokenise per-word to help with visualisation.}, and our task is to predict the next token at every point. The induction circuit solves this by copying a previous example from the context window: e.g. if the input includes the phrase ``Harry Potter'' and the last observed word was ``Harry'', the induction circuit will predict that ``Potter'' comes next. This solves the task even if the combination ``Harry Potter'' never occurred in the training data.\n \n\nTo achieve this, we initially create an embedding for each token, encoding it's \\textit{position} and \\textit{type}. Attention layers then \\textit{copy information between embeddings in a directed way}, using two components that determine (i) \\textit{which} embeddings to extract information from, and (ii) \\textit{what} to extract. Remarkably, the model learns to implement logical gates that we will call ``match\\&pass'', internally composing the algorithm:\n\n\\includegraphics[page=2, width=\\textwidth, clip=True, trim=0 17cm 0 0cm]{figures/Paper_diagrams.pdf}\n\nEach match\\&pass step operates only on an independent subspace of information, which we will call a \\textit{semantic subspace}. In this example, there are four semantic subspaces corresponding to \\textit{position}, \\textit{type}, \\textit{prev-type}, and \\textit{pred-suffix}. We observe that the latent embeddings can contain various information, and it is instructive to think of them as memory buffers rather than tokens. The principle of \\textit{composing logical operations that act on latent semantic subspaces} is also observed in the more complex example of indirect-object identification in GPT2-Small \\cite{wang2022interpretability}.\n\n\n\n\n\n\n\n\n\n\n\n\\vspace{0.2cm}\n\\noindent\n\\textbf{The problem with Pre-Norm}\n\nWe express the latent embeddings as $x = \\sum_\\alpha x_\\alpha$ where $x_\\alpha$ encodes the value of concept $\\alpha$. This is important, because linear-attention layers extract information from $x$ using linear operators (section~\\ref{sec: formulation}), and can only isolate $x_\\alpha$ if each subspace $\\{x_\\alpha~|~\\alpha\\}$ is \\textit{linearly independent}. In other words, there must always exist a linear projection operator $P_\\alpha$ such that $P_\\alpha x = x_\\alpha$.\n\nMost transformers use either \\texttt{RMSNorm} \\cite{NEURIPS2019_1e8a1942} or \\texttt{LayerNorm} \\cite{DBLP:journals/corr/BaKH16} for their internal normalisation layers. Geometrically, \\texttt{RMSNorm} projects a vector $z \\in \\mathbb{R}^N$ onto the unit-sphere $S^{N-1}$ according to\n\\begin{equation}\n\tz ~\\rightarrow~ \\frac{z}{|z|} ~~~~~~~~~~~~~~~~~~~\\mathrm{where}~|z| ~\\triangleq~ \\sqrt[+]{\\sum_{i=1}^N z_i^2} ~~\\mathrm{is~the}~L_2\\mathrm{-norm.}\n\\end{equation}\n\\texttt{LayerNorm} is similar, projecting onto the sphere $S^{N-2}$ defined perpendicular to the direction $1^N$. This does not affect our analysis, and we focus on \\texttt{RMSNorm} for simplicity. Normalisation layers sometimes also include gain and/or bias parameters, applying a stretch-and-translate to the sphere. \\texttt{Pre-Norm} \\cite{DBLP:journals/corr/abs-2002-04745} normalises the latent embeddings at the \\textit{input} to every attention layer. Consider the example $x = x_\\mathrm{pos} + x_\\mathrm{type} + x_\\mathrm{prev-type}$. \n\n\n\nApplying \\texttt{Pre-Norm}, we find $P_\\mathrm{pos}  x = x_\\mathrm{pos}$ is replaced by\n\\begin{equation}\nP_\\mathrm{pos}  \\frac{x}{|x|} ~=~  \\frac{P_\\mathrm{pos} x}{|x|} ~=~ \\frac{x_\\mathrm{pos}}{|x_\\mathrm{pos} ~+~ x_\\mathrm{type} ~+~ x_\\mathrm{prev-type}|}\n\\end{equation}\n\\textbf{Therefore it is impossible for a linear-attention layer to extract $x_\\mathrm{pos}$ without interference from $x_\\mathrm{type}$ and $x_\\mathrm{prev-type}$, unless $|x_\\mathrm{pos} + x_\\mathrm{type} + x_\\mathrm{prev-type}|$ is a constant}. In general, we have $P_\\alpha \\frac{x}{|x|} = \\frac{x_\\alpha}{|\\sum_\\beta x_\\beta|}$, and semantic subspaces are entangled unless $|\\sum_\\alpha x_\\alpha|$ is constant. This is only possible if $|x_\\alpha|^2 = const_\\alpha ~\\forall~ \\alpha$, i.e. every subspace is a sphere, and $x_\\alpha^Tx_\\beta = 0 ~\\forall~ x_\\alpha,x_{\\beta \\neq \\alpha}$, i.e. all spheres are orthogonal (to maintain independence). This has several possible implications:\n\\begin{enumerate}\n    \\item It is a restrictive structure that must be learned during training, with unknown difficulty. Finite steps of gradient descent may separate the model from the manifold of acceptable representations, hindering the learning of circuit components that require semantic separation, like match\\&pass, especially when training with large learning rates.\n    \\item The constraint $|x_\\alpha|^2 = const_\\alpha$ removes a degree of freedom for every $\\alpha$, reducing the information capacity of the embedding space. For example, an embedding on $\\mathbb{R}^5$ could have the two-subspace structure $\\mathcal{S}^2 \\bigoplus \\mathcal{S}^1$ but not $\\mathcal{S}^2 \\bigoplus \\mathcal{S}^2$.\n\t\\item We hypothesise that the structure may be violated by (i) a tradeoff with other representational effects, (ii) imperfect model training, or (iii) encountering unexpected semantic combinations at inference-time when generalising out-of-distribution. These would cause semantic subspaces to interfere through their common normalisation factor, manifesting as noise on the $L_2$-norms of the \\{query,~key,~value\\} vectors.\n\t\\item It is a structure that we can search for empirically.\n\\end{enumerate}\n\n\n\\vspace{0.2cm}\n\\noindent\n\\textbf{A possible solution: QKV-Norm}\n\nA natural fix could be to apply the normalisation layer \\textit{after} the linear operators. In practice this means that we normalise the \\{query,~key,~value\\} vectors, called \\texttt{QKV-Norm} and defined in section~\\ref{sec: formulation}. \n\n\n\\vspace{0.2cm}\n\\noindent\n\\textbf{Paper strategy}\n\nOur work is based on three key observations: (i) semantic subspaces are observed in known circuits, (ii) they contribute to the model behaviour, and (iii) \\texttt{Pre-Norm} requires them to follow a strict latent embedding structure or else interfere through the $L_2$-norms of the \\{query,~key,~value\\} vectors.\n\nHowever, it is difficult to demonstrate specific examples of subspace interference. Firstly, a fully-converged model should learn to manage interference for in-distribution examples. Instead, we expect it to concern (i) training stability, (ii) model inductive bias, and (iii) out-of-distribution behaviour. Secondly, circuit explainability is difficult, only being achieved in simple cases. In general we expect circuits to become complicated, contain steps that are harder to interpret than match\\&pass, and exploit non-interpretable latent subspaces. Difficulty is further increased by polysemanticity \\cite{elhage2022superposition}, the ability for heads and features to change behaviour according to context.\n\nIn this work, we take an abstract approach instead. We formally define latent semantic separability, then investigate the theoretical consequences for \\texttt{Pre-Norm} architectures if this behaviour is important generally. This allows us to make testable predictions about representation structure and model stability without needing to fully reverse-engineer a network or explain subspaces in human terms. We then place some data-driven limits on the effect size. Nonetheless, direct observation remains important, and we hope that future works can confirm or falsify the importance of the proposed representation structure and interference effect.\n==== END OF /2406.17837/sections/the_idea.tex ====\n==== BEGINNING OF /2406.17837/sections/abstract.tex ====\n\n\\begin{abstract}\nRecent works have shown that transformers can solve contextual reasoning tasks by internally executing computational graphs called \\textit{circuits}. Circuits often use attention to logically match information from subspaces of the representation, e.g. using position-in-sequence to identify the previous token. In this work, we consider a \\textit{semantic subspace} to be any independent subspace of the latent representation that can fully determine an attention distribution. We show that \\texttt{Pre-Norm}, the placement of normalisation layer used by state-of-the-art transformers, violates this ability unless the model learns a strict representation structure of orthogonal spheres. This is because it causes linear subspaces to interfere through their common normalisation factor. Theoretically, we analyse circuit stability by modelling this interference as random noise on the $L_2$-norms of the query/key/value vectors, predicting a phenomenon of \\textit{circuit collapse} when sparse-attention shifts to a different token. Empirically, we investigate the sensitivity of real-world models trained for mathematical addition, observing a 1\\\n\n\n\n\n\\end{abstract}\n==== END OF /2406.17837/sections/abstract.tex ====\n==== BEGINNING OF /2406.17837/sections/limitations_outlook.tex ====\n\n\n\n\n\n\n\n\n\n\\section{Summary \\& Outlook}\n\\label{sec: summary}\n\nWe have presented the idea that transformer \\texttt{Pre-Norm} can cause interference between independent subspaces of the latent embeddings, a feature used by some real-world transformer circuits. Theoretically, we found this can only be avoided when using an embedding structure of \\textit{orthogonal spheres}. By contrast, the \\texttt{QKV-Norm} architecture requires only linearly independent subspaces. We predict that sparse attention is stable with respect to interference, until a certain threshold of noise is reached, at which point it undergoes a phase transition called \\textit{circuit collapse}. \n\nEmpirically, we observe that the $L_2$-norms of attended embeddings are contained within a spread of $\\pm20\\\n\nThis work contributes a theoretical hypothesis of model behaviour, and empirically constrains the effect size without full model reverse-engineering. We have made predictions on representation structure, interference, and circuit collapse that practitioners may search for in their own models.\n\n\\section{Limitations}\n\\label{sec: limitations}\n\nWe have not directly observed subspace independence or interference, and further work is required to establish their importance in real-world models. Experimentally, we simulate interference as being independent and similar in amplitude across heads and layers, however it is possible that it is correlated and depth-dependent. Whilst our stability experiments demonstrate that the model is more stable with respect to noise in sparse than non-sparse distributions, we have not shown whether this is due to the inherent stability of the attention distribution predicted by our theory, or the relative importance of sparse vs non-sparse distributions to the model. We show experimental results for a small model on a targeted task (with model variations in Appendix~\\ref{app: model variations}); further work is needed to study the behaviour of larger models and different corpora.\n\n\n\n==== END OF /2406.17837/sections/limitations_outlook.tex ====\n==== BEGINNING OF /2406.17837/sections/formulation.tex ====\n\\section{Formulation}\n\\label{sec: formulation}\n\n\n\n\n\nConsider the \\texttt{No-Norm} case. Let $\\mathcal{X}$ be an unordered set of message receiving tokens, and $\\mathcal{Y}$ the message senders. Let $x \\in \\mathbb{R}^{N_x}$ be the $N_x$-dimensional representation of an element in $\\mathcal{X}$, and $y_t \\in \\mathbb{R}^{N_y}$ be the $t^\\mathrm{th}$ element in $\\mathcal{Y}$, with $1 \\leq t \\leq T$. For self-attention we have $\\mathcal{X}=\\mathcal{Y}$. Let $W_Q\\in\\mathbb{R}^{N_{qkv}\\times N_x}$ and $W_K\\in\\mathbb{R}^{N_{qkv}\\times N_y}$ be the query and key weight matrices, with associated vectors $q=W_Qx\\in\\mathbb{R}^{N_{qkv}}$ and $k_t=W_Ky_t\\in\\mathbb{R}^{N_{qkv}}$ on an $N_{qkv}$-dimensional latent space. We do not include biases in $\\{q,~k_t\\}$ because they contribute terms that are nullified by the \\texttt{softmax}, or are reproduced by constant directions in $x$ (Theorem~\\ref{theorem: decomposition}).  We define dot-product attention \\textit{scores} as:\n\\begin{equation}\n    w_t ~=~ q^Tk_t ~=~ x^T W_Q^T W_K y_t ~=~ x^T W_{QK} y_t\n\\end{equation}\nwhere $W_{QK}\\triangleq W_Q^TW_K \\in \\mathbb{R}^{N_x\\times N_y}$ is a matrix with $Rank(W_{QK}) \\leq \\min(N_x,N_y,N_{qkv})$. This is the maximum span of the attended subspace in $\\{x,y_t\\}$. The attention \\textit{weights} are\n\\begin{equation}\n    a_t ~=~ \\texttt{softmax}\\left(w_t\\right) ~=~ \\frac{e^{w_t}}{\\sum_{t'} e^{w_{t'}}} ~~~~~~.\n\\end{equation}\nLet $v_t=W_Vy_t\\in\\mathbb{R}^{N_x}$ be the value vectors with $W_V\\in\\mathbb{R}^{N_{qkv}\\times N_y}$. We do not include biases in $v_t$ because they carry no dependence on the attended token. Each token emits the message $m_t = W_Ov_t \\equiv W_OW_Vy_t \\triangleq W_{OV}y_t$ where $W_O = \\mathbb{R}^{N_x\\times N_{qkv}}$ is the output-matrix. Each attention-head updates $x$ by adding the attention-weighted convex combination of messages, $x \\rightarrow x + \\Delta x$ with $\\Delta x = \\sum_{t} a_t m_t$. We usually run $H$ attention-heads in parallel, giving the total update:\n\\begin{equation}\n     x ~\\rightarrow~ x ~ + ~\\sum_{h=1}^{H} \\sum_{t=1}^{T} a_t^{(h)} m_t^{(h)} ~~~~~~~~~~~~~~~~~~ \\text{Multi-head~attention}\n\\end{equation}\nwith unique weights $\\{W_Q^{(h)},W_K^{(h)},W_V^{(h)},W_O^{(h)}\\}$ for each head index $h$.\n\n\n\nWe now introduce normalisation layers. Let $z\\in\\mathbb{R}^{N_z}$ be any $N_z$-dimensional vector, then $\\texttt{N}(z;\\alpha_z):\\mathbb{R}^{N_z}\\rightarrow\\mathbb{R}^{N_z}$ is a normalisation function with parameters $\\alpha_z$. We consider two such functions:\n\n\n\n\n\n\n\n\n\n\n\\begin{equation}\n    \\texttt{RMSNorm}\\left(z;~\\alpha_z\\right) =  \\frac{\\sqrt{N_z}}{|z|} \\mathrm{diag}\\left(\\alpha_z\\right) z \n\n~~~~~~~~~~~~~\n    \\texttt{LayerNorm}\\left(z;~\\alpha_z\\right) = \\frac{\\sqrt{N_z}}{|z_\\perp|} \\mathrm{diag}\\left(\\alpha_z\\right) z_\\perp \n\n\\end{equation}\n\\cite{NEURIPS2019_1e8a1942,DBLP:journals/corr/BaKH16} where $P_\\perp \\triangleq \\mathrm{diag}\\left(1^{N_z}\\right) - 1^{N_z}{1^{N_z}}^T$ is a linear operator that subtracts the mean of $z$ from every component, $1^{N_z}$ is vector of ones, and $z_\\perp \\triangleq P_\\perp z$ is the component of $z$ perpendicular to $1^{N_z}$.\n\nThe \\texttt{Pre-Norm} strategy means applying normalisation to the inputs $\\{x,y_t\\}$. The \\texttt{QKV-Norm} strategy means applying normalisation to the vectors $\\{q,k_t,v_t\\}$. We then have three cases:\n\n\n\\noindent\n\\begin{tabular}{rr|c|c|c}\n  ~  &  ~  &  $w_t$  &  $v_t$  &  Norm params  \\\\\n\\hline\n                & \\texttt{No-Norm}    &  $x^T ~W_{QK}~ y_t$  &  $W_V~y_t$ & - \\\\\n     (baseline) & \\texttt{Pre-Norm}   &  $\\texttt{N}\\left(x;\\alpha_x\\right)^T ~W_{QK}~ \\texttt{N}\\left(y_t;\\alpha^K_y\\right)$  & $W_V~\\texttt{N}\\left(y_t;\\alpha^V_y\\right)$ & $\\{\\alpha_x,~\\alpha^K_y,~\\alpha^V_y\\}$ \\\\\n\n     (alternate)     & \\texttt{QKV-Norm}  &  $\\texttt{N}\\left(W_Q x;\\alpha_q\\right)^T \\texttt{N}\\left(W_K y_t;\\alpha_k\\right)$  & $\\texttt{N}\\left(W_V y_t;\\alpha_v\\right)$ & $\\{\\alpha_q,~\\alpha_k,~\\alpha_v\\}$ \\\\\n\\end{tabular}\n\nWe note that several of these degrees of freedom are redundant and could be combined, e.g. $\\alpha_q$ and $\\alpha_k$. We do not consider these variations (i) because they are not relevant for the results of this paper, and (ii) to standardise the number of training parameters.\n==== END OF /2406.17837/sections/formulation.tex ====\n==== BEGINNING OF /2406.17837/sections/limitations.tex ====\n\\section{Limitations}\n\\label{sec: limitations}\n\n\\textcolor{red}{Main limitation is lack of evidence for behaviour in complex models. Generality of logical-operations on discrete semantics no yet known, or whether there are mechanisms for error-correction. Summarise theory assumptions.}\n==== END OF /2406.17837/sections/limitations.tex ====\n==== BEGINNING OF /2406.17837/sections/related_works.tex ====\n\\section{Related Works}\n\\label{sec: related works}\n\nOur work is motivated by transformer circuit discovery \\cite{olsson2022context,elhage2021mathematical,wang2022interpretability,stolfo2023mechanistic,GoldowskyDill2023LocalizingMB,Ferrando2024InformationFR} and formation \\cite{singh2023the,singh2024needs}. See \\cite{ferrando2024primer} for a recent review of interpretability for language decoder models, with a list of known logical operations implemented by attention heads. This builds upon works in BERTology \\cite{DBLP:journals/corr/abs-2002-12327,Devlin2019BERTPO}. We study normalisation, for which several formulations have been proposed \\cite{DBLP:journals/corr/abs-2002-04745,nguyen-salazar-2019-transformers,nguyen-chiang-2018-improving,shleifer2021normformer,NEURIPS2019_2f4fe03d}. Our \\texttt{QKV-Norm} variant is similar to \\texttt{QK-Norm}, which is studied by \\cite{DBLP:journals/corr/abs-2010-04245,wortsman2023smallscale,dehghani2023scaling} for asymptotic performance and training stability at large learning rates. These are motivated by logit-regularisation, whereas we are motivated by representational inductive bias and stability to latent semantic interference. \n\nWe highlight other works that study transformer normalisation through its geometric interpretation as a projection onto a sphere. \\cite{kobayashi-etal-2021-incorporating} investigated the role of normalisation in mixing the attention output with the residual stream in \\texttt{Post-Norm} models, but does not consider \\texttt{Pre-Norm}. \\cite{brody2023expressivity} studies the computational abilities of \\texttt{Pre-LayerNorm} architectures, in particular demonstrating that projection onto a sphere ensures that all keys reside on their own convex hull, preventing them from becoming ``unselectable''. \\cite{Molina2023TravelingWA} interprets the latent embeddings of \\texttt{Pre-Norm} models as a trajectory on a sphere. These works do not consider the interference of semantic subspaces. \\cite{Dong2021AttentionIN} and the contemporary work \\cite{wu2024role} study the role of \\texttt{LayerNorm} in the related phenomenon of embedding rank collapse.\n\nWe highlight the contemporary work of \\cite{wang2024understanding}, who also study multi-step contextual reasoning in transformers using matching operations over independent subspaces, for both \\texttt{Pre-Norm} and \\texttt{Post-Norm}. This builds upon \\cite{boixadsera2024when}, who study the learning of abstract symbolic reasoning in transformers, and works that manipulate the flow of information to promote algorithmic reasoning, e.g. \\cite{csordas2022the}.\n\nWe are not aware of previous works that study the impact of \\texttt{Pre-Norm}'s spherical geometry on the structure of latent subspaces. However, many works consider linear subspaces, described in the following paragraph. These results are directly applicable to the \\texttt{No-Norm} and \\texttt{QKV-Norm} methods in this work, although \\texttt{QKV-Norm} applies a subsequent spherical projection. \\cite{Lamb2021TransformersWC} design subspace separability into their model by decoupling the normalisation layers for different mechanisms.\n\nWorks on vector embeddings \\cite{word2vec1,NIPS2013_9aa42b31,Pennington2014GloVeGV} and the \\textit{linear representation hypothesis} \\cite{liguistic_regularities,Park2023TheLR,jiang2024origins} study the emergence of linear subspaces that encode separable concepts in embedding-unembedding models, using both interpretation and intervention techniques. Many works search for linear subspaces/directions in a transformer representation (e.g. linear probes \\cite{semantic_subspace_probing,belinkov-2022-probing}) or search for faithful causal abstractions (e.g. \\cite{Geiger2023FindingAB}), with a survey provided in \\cite{ferrando2024primer} sections 3-4. \nWe also highlight works that study the use of features in linear superposition \\cite{superposition1,elhage2022superposition}. This allows a model to store more features than it has dimensions, at the cost of interference in their linear projections. \n\nThe terminology of \\textit{semantic subspaces} is used more generally, e.g. \\cite{semantic_subspace_probing,5596640,Coenen2019VisualizingAM}. We consider a definition that does not require humans to define the separable concepts, only that abstract latent features remain independent in an attention layer. We also highlight works that study subspaces of static (model input) and contextual (latent or model output) embeddings in transformers, e.g. \\cite{Hewitt2019ASP,Coenen2019VisualizingAM,ethayarajh-2019-contextual,song2024uncovering,muppet2022,hernandez2024linearity,chi-etal-2020-finding,cai2021isotropy,Hernandez2021TheLL} (review in \\cite{DBLP:journals/corr/abs-2002-12327}). These are relevant because they also decompose embeddings into a combination of abstract subspaces, capturing different semantic and syntactic structures in a natural language setting. These may be used as semantic subspaces in our work. We highlight \\cite{song2024uncovering} which studies interference between positional and contextual components using a decomposition similar to ours, and also experiments using a next-token addition task.\n==== END OF /2406.17837/sections/related_works.tex ====\n==== BEGINNING OF /2406.17837/sections/introduction.tex ====\n\\section{Introduction}\n\\label{sec:intro}\n\n\n\n\n\nTransformer-based models \\cite{DBLP:journals/corr/VaswaniSPUJGKP17} are commonplace in machine learning, providing state-of-the-art contextual reasoning in domains ranging from natural language \\cite{bubeck2023sparks,touvron2023llama} to protein-folding \\cite{AlphaFold3,doi:10.1126/science.abj8754,10.7554/eLife.82819} and theoretical physics \\cite{cai2024transforming}. Recent interpretability work investigates the internal mechanisms that lead to specific model behaviours \\cite{olsson2022context,elhage2021mathematical,wang2022interpretability,ferrando2024primer,meng2022locating,DBLP:journals/corr/abs-2002-12327,liu2023transformers,goldowskydill2023localizing}. This is important for predicting behaviour in new environments, enables practitioners to match the inductive bias of a model with the structure of its task, and informs the design of architectures that promote desirable behaviour.\n\nTwo such works discovered complete \\textit{circuits} \\cite{cammarata2020thread:} in trained transformers \\cite{olsson2022context,elhage2021mathematical,wang2022interpretability}. These are computational graphs that dominate the model prediction when activated in a specialised context. They perform a type of \\textit{algorithmic reasoning} by internally executing a sequence of logical operations, using attention to pass information between memory buffers that begin as token embeddings and become increasingly abstract. Furthermore, a number of attention heads have been identified as performing logical operations (see \\cite{ferrando2024primer} section 5). To understand transformer behaviour, an important goal is to understand how logical attention heads operate, and their generality beyond the simple cases that facilitate interpretability.\n\nOne key observation is that \\textbf{the attention distribution is sometimes fully-determined by an independent subspace of the representation} - for example, an attention layer can identify the previous token by accessing a subspace that encodes only position-in-sequence. Indeed, low-rank weight matrices can only access linear subspaces by construction. A second observation is that, like most deep architectures, transformers use normalisation layers to improve training stability. A leading choice is to place normalisation at the \\textit{input} to each attention layer, which we call \\texttt{Pre-Norm} \\cite{DBLP:journals/corr/abs-2002-04745}. Some interpretability works ignore this layer because it has a linear-up-to-scale structure, absorbing the linear part into adjacent weights. In this work we argue that the layer is important, because \\textbf{Pre-Norm causes independent linear subspaces to interfere through a common normalisation factor, preventing their separation by linear attention layers}. \n\nThe purpose of this work is to ask: if the use of independent subspaces is generally important, what are the expected consequences of \\texttt{Pre-Norm} for (i) the latent representation structure, and (ii) circuit stability? To answer this, we take an abstract approach that complements direct interpretability by considering general behaviour beyond the interpretable limit. Our contributions are:\n\\begin{enumerate}\n\t\\item \\textbf{Conceptual:} we identify interference between independent subspaces as a potential destabiliser of circuits caused by \\texttt{Pre-Norm}. We suggest \\textit{separability of latent subspaces} as a target for study, and show it is easily satisfied by the alternative \\texttt{QKV-Norm}. This differs from \\texttt{Pre-Norm} by placing the normalisation layer after the linear operators. It is similar to \\texttt{QK-Norm}, for which sparse evidence currently exists \\cite{DBLP:journals/corr/abs-2010-04245,wortsman2023smallscale,dehghani2023scaling}.\n\t\\item \\textbf{Theoretical:} we formalise a \\textit{semantic subspace} as any independent subspace of the latent representation that can fully determine the attention distribution. We show that \\texttt{Pre-Norm} can only achieve this when semantic subspaces are spherical and mutually orthogonal. By contrast, \\texttt{QKV-Norm} requires only that subspaces be linearly independent, matching the \\texttt{No-Norm} case in this sense. We study the stability of attention to subspace interference, predicting a potentially problematic phenomenon of \\textit{circuit collapse} when a sparse-attention distribution changes which embedding it attends to. \n\t\n\t\\item \\textbf{Experimental:} we measure the sensitivity of trained models to simulated interference in a numerical addition task. Constraining our predictions, we find that (i) \\texttt{Pre-Norm} models induce a narrower distribution of embedding $L_2$-norms than \\texttt{QKV-Norm}, (ii) we bound the spread of $L_2$-norms to $\\pm20\\\n\\end{enumerate}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\t\n\n\n\n\n\n\n==== END OF /2406.17837/sections/introduction.tex ====\n==== BEGINNING OF /2406.17837/sections/theory_stability.tex ====\n\\section{Theory: stability to subspace interference}\n\\label{sec: theory: circuit stability}\n\nWe now investigate the impact of interfering subspaces. Consider the almost-separable limit, modelling interference as a random infinitesimal perturbation of the vectors $\\{q,k_t,m_t\\}$. Let $\\epsilon$-symbols denote perturbations such that $\\epsilon^{\\Delta x(q)} \\rightarrow \\frac{\\partial \\Delta x}{\\partial q} \\epsilon^q$ for $\\epsilon^q \\rightarrow 0$ is the change of $\\Delta x$ induced by $\\epsilon^q$. \nWe consider (i) the sparse limit, in which the attention is concentrated entirely on a single embedding, and (ii) the isotropic limit, in which it is distributed evenly among embeddings. We are particularly interested in the sparse case, since this highly directed flow of information is used by match\\&pass, although semantic separation can also be used by non-sparse heads. \n\n\\begin{mdframed}[backgroundcolor=red!5]\n\\textbf{[Definition] ~ Sparse attention:} the low-temperature limit $a_t \\approx \\delta_{tt^*}$ and $\\Delta x = m_{t^*}$, where $\\delta$ is the Kronecker delta. This occurs when there is a large difference between the top two scores: $t^* = \\mathrm{argmax}_t w_t$ and $w_{t^*} - \\max_{t\\neq t^*} w_t \\gg 1$. \\textbf{Isotropic attention:} the high-temperature limit $a_t = \\frac{1}{T}$ and $\\Delta x = \\langle m_t\\rangle_t$. This occurs when $w_t$ is constant, requiring $q=0$ or constant $k_t$.\n\\end{mdframed}\n\n\\begin{mdframed}[backgroundcolor=blue!5]\n\\underline{\\textbf{Stability of attention updates to perturbations on q, k, v}}\n~~~~~~~~~~~~~~~~~~~\\textit{[Proofs in appendix~\\ref{appendix: proofs}]}\n\n\\begin{theorem}\n    Consider independent infinitesimal perturbations on queries $\\epsilon^q \\in \\mathbb{R}^{N_{qkv}}$, keys $\\epsilon^k_t \\in \\mathbb{R}^{N_{qkv}}$, and messages $\\epsilon^m_t \\in \\mathbb{R}^{N_{qkv}}$. These propagate onto $\\Delta x = \\sum_{t}a_tm_t$ as\n    \\begin{align}\n        \\epsilon^{\\Delta x(q)} ~~&\\xrightarrow[\\epsilon^q\\rightarrow0]{\\mathrm{~~~~perturb~q~~~~}}~~ \\mathop{\\mathbb{E}}_{a_t} \\Big[ m_t {\\tilde k}_t^T \\Big] \\epsilon^q ~~~~~~~~~~~~~~~~~  {\\tilde k}_t ~\\triangleq~ k_t ~- \\mathop{\\mathbb{E}}_{a_t} \\Big[ k_t \\Big] \\\\\n        \\epsilon^{\\Delta x(k)} ~~&\\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{~~~~perturb~k~~~~}}~~ \\mathop{\\mathbb{E}}_{a_t} \\Big[ {\\tilde m}_t {\\epsilon^k_t}^T \\Big] q ~~~~~~~~~~~~~~~~~  {\\tilde m}_t ~\\triangleq~ m_t ~- \\mathop{\\mathbb{E}}_{a_t} \\Big[ m_t \\Big] \\\\\n        \\epsilon^{\\Delta x(m)} ~~&\\xrightarrow[\\epsilon^m_t\\rightarrow0]{\\mathrm{~~~~perturb~m~~~~}}~~ \\mathop{\\mathbb{E}}_{a_t} \\Big[ \\epsilon^m_t \\Big]\n    \\end{align}\n    where ${\\tilde z}_t$ is the value of $z_t$ measured from the attention-weighted centroid $\\mathbb{E}_{a_t}[z_t] = \\sum_t a_t z_t$.\n\\label{theorem: stability: general}\n\\end{theorem}\n\n\\begin{theorem}\n    For sparse attention:\n    \\begin{equation}\n        \\epsilon^{\\Delta x(q)} \\xrightarrow[\\epsilon^q\\rightarrow0]{\\mathrm{~~perturb~q~~}} 0   ~~~~~~~~~~\n        \\epsilon^{\\Delta x(k)} \\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{~~perturb~k~~}} 0   ~~~~~~~~~~\n        \\epsilon^{\\Delta x(m)} \\xrightarrow[\\epsilon^m_t\\rightarrow0]{\\mathrm{~~perturb~m~~}} \\epsilon^m_{t^*}\n    \\end{equation}\n    i.e. the message is stable with respect to small interference in the queries and keys. Interference in the selected value is linearly transferred onto the message.\n\\label{theorem: stability: sparse}\n\\end{theorem}\n\n\\begin{theorem}\n    For isotropic attention:\n    \\begin{equation}\n        \\epsilon^{\\Delta x(q)} \\xrightarrow[\\epsilon^q\\rightarrow0]{\\mathrm{perturb~q}} \\langle m_t {\\tilde k}_t^T \\rangle_t \\epsilon^q ~~~~~~~~\n        \n        \\epsilon^{\\Delta x(k)} \\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{perturb~k}} \\langle {\\tilde m}_t {\\epsilon^k_t}^T \\rangle_t ~q   ~~~~~~~~\n        \\epsilon^{\\Delta x(m)} \\xrightarrow[\\epsilon^m_t\\rightarrow0]{\\mathrm{perturb~m}} \\langle \\epsilon^m_t \\rangle_t\n    \\end{equation}\n    \n    \n    \n    \n    \n    N.B. isotropy requires $k_t=const$ or $q=0$. Lemma 1: the update is stable to noisy $q$ when $k_t=const$, or when $m_t \\perp k_t$ (c.f. keys and messages from independent subspaces). Lemma 2: the update is stable to noisy $k_t$ when $q=0$, or when $m_t \\perp \\epsilon_t^k$. Lemma 3: the update is stable to noisy $m_t$ when $\\langle \\epsilon^m_t \\rangle_t=0$. Other cases propagate linearly.\n\\label{theorem: stability: isotropic}\n\\end{theorem}\n\\end{mdframed}\n\nThe stability of sparse attention is because \\texttt{softmax} becomes an \\texttt{argmax} for low-temperature heads, which is only sensitive to the order of $w_t$. However, this introduces a different vulnerability when perturbations cause the order of $w_t$ to change, as the attention distribution undergoes a phase transition to select a different token. We call this \\textit{circuit collapse}. For example, the induction circuit collapses when the operation \\textit{attend to the previous token} attends to any other token because of interference.\n\n\\begin{mdframed}[backgroundcolor=red!5]\n\\textbf{[Definition] ~ Circuit collapse:} spontaneous phase transition in which a sparse attention distribution selects a different token due to noise on $\\{q, k_t\\}$. Let $\\epsilon^w_t = k_t^T\\epsilon^q + q^T\\epsilon^k_t + \\mathcal{O}({\\epsilon^q}^T\\epsilon^k_t)$ be perturbations on $w_t$ that result from $\\epsilon^q$ and $\\epsilon^k_t$. Circuit collapse occurs when there exists a $t \\neq t^*$ for which $w_{t^*} - w_t < \\epsilon^w_t - \\epsilon^w_{t^*}$.\n\\end{mdframed}\n\nWe now study the $L_2$-norm interference that we expect to be induced by \\texttt{Pre-Norm} when semantic separability is violated. This is characterised by perturbations that are parallel to their corresponding vector. Theorem~\\ref{theorem: multiplicative stability: sparse} shows the conditions under which we expect circuit collapse to occur.\n\n\\begin{mdframed}[backgroundcolor=blue!5]\n\\underline{\\textbf{Stability of attention updates to scaling of q, k, v}}\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\textit{[Proofs in appendix~\\ref{appendix: proofs}]}\n\n\\begin{theorem}\n    Sensitivity of sparse attention to multiplicative perturbations $\\epsilon^q = \\kappa^q q$ and $\\epsilon^k = \\kappa^k_t k_t$ with $\\kappa^q,\\kappa^k_t\\ll1$. Circuit collapse occurs when $\\exists~ t \\neq t^*$ for which:\n    \\begin{equation}\n        \\frac{w_{t^*}}{w_t} ~\\begin{cases} ~<~ \\lambda_w & \\mathrm{if}~ w_t \\left(1 + \\kappa^q + \\kappa^k_{t^*}\\right) > 0 \\\\\n        ~>~ \\lambda_w & \\mathrm{otherwise} \\\\ \\end{cases}\n        ~~~~~~~~~~~~~ \\lambda_w ~\\triangleq~ \\frac{1 + \\kappa^q + \\kappa^k_t}{1 + \\kappa^q + \\kappa^k_{t^*}}\n    \\end{equation}\n    where temperature cancels in the fraction. \\textbf{Attention is fully stable above the critical transition point $\\lambda_w$} (c.f. $w_t \\left(1 + \\kappa^q + \\kappa^k_{t^*}\\right) > 0$). We see that query perturbations alone are insufficient, as they result in $\\lambda_w=1$. Lemma: consider the special case when all keys have similar length $k_t \\approx const$, the attended token has $\\theta_{t^*}\\approx0$, the keys are far-from-orthogonal s.t. $\\theta_t \\ll 1$, and $\\kappa^q\\approx0$. Using $w_t \\triangleq |q| |k_t| \\cos\\theta_t$, circuit collapse occurs when $\\exists~ t \\neq t^*$ for which:\n    \\begin{equation}\n            \\frac{1}{2}\\theta_t^2 ~\\lesssim~ \\kappa^k_t - \\kappa^k_{t^*}   ~~~~~~~~~~~ \\mathrm{if}~ w_t \\left(1  + \\kappa^k_{t^*}\\right) > 0 ~\\text{, otherwise reverse}\n    \\label{eq: sparse circuit collapse result}\n    \\end{equation}\n    i.e. stability requires either well-separated keys s.t. $\\theta_t \\gg 0$, or small perturbations $\\kappa_t-\\kappa^*_t \\ll 1$.\n\\label{theorem: multiplicative stability: sparse}\n\\end{theorem}\n\n\\begin{theorem}\n    Sensitivity of isotropic attention to multiplicative perturbations. Say $\\epsilon^k = \\kappa^k_t k_t$ with $\\kappa^k_t\\ll1$ where $\\{\\kappa_t\\}$ have comparable amplitudes. Then\n    \\begin{equation}\n        \\epsilon^{\\Delta x(k)} \n        ~\\approx~\n        \\begin{cases}\n        0 ~&~ \\text{if~$\\kappa_t$~independent~of~${\\tilde m}_t$,~by~symmetry} \\\\\n        0 ~&~ \\text{if~$\\kappa_t\\equiv\\kappa$~for~constant~$\\kappa$} \\\\\n        0 ~&~ \\text{if~$q=0$} \\\\\n        w \\langle {\\tilde m}_t \\kappa^k_t \\rangle_t  ~&~ \\text{otherwise}\n        \\end{cases}\n    \\end{equation}\n\\label{theorem: multiplicative stability: isotropic}\n\\end{theorem}\n\\end{mdframed}\n\n\n\n\n==== END OF /2406.17837/sections/theory_stability.tex ====\n==== BEGINNING OF /2406.17837/sections/experiments.tex ====\n\\section{Experimental results}\n\\label{sec: experimental results}\n\nWe now use experiments to empirically probe (i) the real-world embedding structure, and (ii) the sensitivity to artificial noise on the \\{query, key, value\\} $L_2$-norms. Whilst this does not directly observe real-world interference, it constrains the effect importance.\n\nWe consider a base-10 integer-addition task with a question-answer structure, and train for next-token prediction. We use a decoder architecture, common for state-of-the-art language models, with $10$ layers, per-character tokenisation, and begin \\texttt{[} and end \\texttt{]} tokens. In the output, we mask \\texttt{*} tokens that precede the answer. For example, the first training sequence has input \\textcolor{Maroon}{\\texttt{[453+16+17-N846=1332}} and output \\textcolor{Maroon}{\\texttt{***************1332]}}. We compare two models that use \\texttt{Pre-Norm} and \\texttt{QKV-Norm} respectively. Appendices~\\ref{appendix: experimental setup}-\\ref{app: model variations} provide a full experimental setup and supplementary plots. \nIn this section we make all plots using an in-distribution test set that is expected to have some overlap with the training set, bounded at $\\ll 20\\\n\nWe choose this task because it emphasises contextual reasoning in a small-scale setting, is configurable for complexity, and allows us to define meaningful out-of-distribution test sets. The \\texttt{Pre-Norm} (\\texttt{QKV-Norm}) model achieves an in-distribution per-token accuracy of $91.4\\\n\n\n\\textbf{Embedding structure}\n\nOur theory predicts that \\texttt{Pre-Norm} attention is stable with respect to information in non-attended subspaces if all input embeddings have similar $L_2$-norms, whereas \\texttt{QKV-Norm} imposes no norm constraint. We seek to experimentally bound the degree to which this structure is learned in practice.\n\nWe do this by plotting \\textit{the spread of norms with respect to their median}. A confounding effect is that the norms may differ for (i) embeddings attended to by different heads, (ii) the same head acting in different contexts, and (iii) embeddings that are never attended. We therefore measure the ratio \\textit{per-head}, and weight each embedding by its assigned attention. We remove the begin-sequence token from consideration. Figure~\\ref{fig: embedding spread} shows the resulting spread for all attention layers. On the LHS, we see that 90\\\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/baseline_model/Embedding_length_comparison_0.0.pdf}\n    \\caption{Spread of embedding $L_2$-norms experienced by attention heads at increasing model depth, excluding the \\texttt{[} token. For \\texttt{Pre-Norm}, 90\\\n    \\label{fig: embedding spread}\n\\end{figure}\n\n\\textbf{Model stability with simulated interference}\n\nSection~\\ref{sec: theory: circuit stability} theoretically modelled the semantic interference induced by \\texttt{Pre-Norm} as a random perturbation on the norms of $\\{q,k_t,m_t\\}$. To estimate the real-world sensitivity to such an effect, we artificially introduce uncorrelated uniform noise onto these norms inside our trained \\texttt{Pre-Norm} model. Even though Gaussian noise is expected in the large-$N_s$ limit, we use uniform noise to avoid outliers. Figure~\\ref{fig: noise sensitivity} shows the evolution of in-distribution per-token accuracy with increasing RMS. On the LHS, we see that performance falls by $\\gtrsim10\\\n\nFigure~\\ref{fig: noise sensitivity} (right) is consistent with the stability predictions of Theorems~\\ref{theorem: stability: sparse}-\\ref{theorem: stability: isotropic}. However, it may also be explained if non-sparse distributions are simply more important to the model. This could be caused by non-sparse distributions being more common, as well as depth-dependence. This is because artificial noise is applied to all layers during the forward pass, therefore later layers are perturbed by both the noise component \\textit{and} the shifting of their inputs due to previous layers, which is expected to compound with depth. We are interested in capturing this effect, however it may increase the importance of early layers. See Figure~\\ref{fig: attention map prenorm} for a visualisation of the observed attention maps.\n\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[page=3, width=0.9\\textwidth, clip=True, trim=0 8.cm 0 1cm]{figures/Paper_diagrams.pdf}\n    \\caption{\\textbf{Left:} evolution of per-token accuracy as we increase noise on the $L_2$-norms of $\\{q,k_t,m_t\\}$. A $\\gtrsim 10\\\n    \\label{fig: noise sensitivity}\n    \\vspace{0.1cm}\n    \\includegraphics[width=0.9\\textwidth, clip=True, trim=0 0 0 0]{figures/collapse/circuit_collapse_probability_min_att_0.95_0.0_loose.pdf}\n    \\caption{Probability of \\textit{circuit collapse} vs increasing noise. This observes the effect predicted in Section~\\ref{sec: theory: circuit stability}, and measures that $1\\\n    \\label{fig: circuit collapse}\n    \n    \\vspace*{-5mm}\n\\end{figure}\n\n\\vspace{0.2cm}\n\\textbf{Circuit collapse}\n\nFigure~\\ref{fig: circuit collapse} shows the probability that our artificial noise causes the circuit collapse phenomenon as defined in section~\\ref{sec: theory: circuit stability}. In this experiment, we add noise to every layer independently. This prevents the confounding effect of shifting inputs due to noise in previous layers. We observe that $1\\\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n==== END OF /2406.17837/sections/experiments.tex ====\n==== BEGINNING OF /2406.17837/appendices/supplementary_theorems.tex ====\n\\section{Supplementary theorems}\n\\label{appendix: supplementary theorems}\n\nThis appendix contains theorems that support the main results, providing additional context or being pre-requisite for the proofs in appendix~\\ref{appendix: proofs}. We use the formulation of section~\\ref{sec: formulation}, where $1 \\leq \\{t,t'\\} \\leq T$ are indices over tokens, $x\\in\\mathbb{R}^{N_x}$ is the message receiving embedding, $\\{y_t\\in\\mathbb{R}^{N_y}\\}$ are the message senders, $w_t = x^T W_{QK} y_t$, and $a_t = \\texttt{softmax}_t w_t$ is the attention distribution\n\n\\vspace{0.5cm}\n\n\\begin{mdframed}[backgroundcolor=green!5,skipabove=-2pt,skipbelow=0]\n\\begin{theorem}\n\tShifting attention scores $w_t$ by a constant offset does not affect the attention distribution. Therefore attention is fully determined by differences in scores.\n \\label{theorem: att shift operator}\n\\end{theorem}\n\n\\textit{Proof.} ~Applying the shift $w_t \\xrightarrow[\\mathrm{offset~w}]{} w_t + \\delta w ~\\forall~t$ with fixed $\\delta w$, we have\n\\begin{equation}\n\\begin{split}\na_t ~&=~ \\frac{e^{w_t}}{\\sum_{t'} e^{w_{t'}}} \\\\\n&\\xrightarrow[\\mathrm{offset~w}]{} ~ \\frac{e^{\\delta w}e^{w_t}}{\\sum_{t'} e^{\\delta w}e^{w_{t'}}} ~=~ \\frac{e^{\\delta w}}{e^{\\delta w}}\\frac{e^{w_t}}{\\sum_{t'} e^{w_{t'}}} ~=~ 1 \\cdot a_t~=~ a_t\n\\end{split}\n\\end{equation}\nAlternatively we may write\n\\begin{equation}\na_t ~=~ \\frac{e^{w_t}}{\\sum_{t'} e^{w_{t'}}} ~=~ \\frac{e^{w_t}}{e^{w_{t'}}\\sum_{t'} e^{w_{t'}-w_t}} ~=~ \\frac{1}{\\sum_{t'} e^{w_{t'}-w_t}}\n\\end{equation}\nwhere $\\left(w_{t'}+\\delta t\\right)-\\left(w_t+\\delta t\\right) = w_{t'}-w_t$.\n\\end{mdframed}\n\n\n\\begin{mdframed}[backgroundcolor=green!5]\n\\begin{theorem}\n\tMultiplying attention scores by a positive factor changes the inverse-temperature of the attention distribution, modulating its sparsity (low temperature = less entropy = more sparse). Corollary: In the sparse limit, attention is fully determined by the order of $w_t$.\n \\label{theorem: att scale operator}\n\\end{theorem}\n\n\\textit{Proof.} ~Applying the scaling $w_t \\xrightarrow[\\mathrm{scale~w}]{} \\kappa w_t ~\\forall~ t$ with fixed $\\kappa > 0$, we have\n\\begin{equation}\n\\begin{split}\n\\frac{e^{\\kappa w_t}}{\\sum_{t'} e^{\\kappa w_{t'}}} ~~&=~~ \\frac{1}{\\sum_{t'} e^{\\kappa (w_{t'}-w_t)}}   \\\\\n~~&\\xrightarrow[\\kappa\\rightarrow 0]{} ~~~~ \\frac{1}{\\sum_{t'} e^0} ~=~ \\frac{1}{T} ~\\forall~ t ~~~~~~~~~~~~~~~~~~~~~\\text{[fully isotropic distribution]} \\\\\n~~&\\xrightarrow[\\kappa\\rightarrow \\infty]{} ~~ \n\\begin{cases}\n1 ~~ ~\\mathrm{for}~ t = \\mathrm{argmax}_{t'} w_{t'} \\\\\n0 ~~ ~~~\\forall~ t \\neq \\mathrm{argmax}_{t'} w_{t'} \\\\\n\\end{cases}\n~~~\\text{[fully sparse distribution]}\n\\end{split}\n\\end{equation}\nwhere the \\texttt{argmax} operator is fully determined by the order of $w_t$.\n\\end{mdframed}\n\n\\begin{mdframed}[backgroundcolor=green!5]\n\\begin{theorem}\n    In the \\texttt{No-Norm} case, the attention distribution $a_t$ is defined by the projection of $y_t$ onto a fixed vector $y_x$ for a given $x$. The length of $y_x$ is an inverse-temperature parameter.\n\\end{theorem}\n\n\\textit{Proof.} ~Write $w_t = x^TW_{QK}y_t = (W_{QK}^Tx)^T y_t \\equiv y_x^T y_t$ where $y_x \\triangleq W_{QK}^T x\\in\\mathbb{R}^{N_y}$, which is the dot-product between $y_t$ and a fixed vector $y_x$ on the row space of $W_{QK}$. Then, re-writing in terms of the vector lengths and the enclosing angle $\\theta_{y_t} = y_x\\wedge y_t$, we have $w = |y_x||y_t|\\cos\\theta_{y_t}$. The factor $|y_x|$ is identical for all $t$, making it an inverse-temperature.\n\\end{mdframed}\n\n\n\n\n\n\n\n\n\n\n\n\n\\clearpage\n\\begin{mdframed}[backgroundcolor=green!5]\n\\begin{theorem}\nIn the \\texttt{No-Norm} case, bias parameters in the construction of query and key vectors are nullified by the \\texttt{softmax}, or only contribute terms that may be recovered if $x$ contains a constant direction.\n\\label{theorem: decomposition}\n\\end{theorem}\n\n\\textit{Proof.} ~Consider a modification to the construction of query and key vectors that uses the affine transformations $q = W_Qx+b_Q$ and $k_t = W_Ky_t+b_K$, with $W_{Q}\\in\\mathbb{R}^{N_{qkv}\\times N_x}$, $W_{K}\\in\\mathbb{R}^{N_{qkv}\\times N_y}$, $W_{QK}\\triangleq W_Q^TW_K$, and $b_Q,b_K\\in\\mathbb{R}^{N_{qkv}}$. The dot-product attention scores are then:\n\\begin{equation}\n\\begin{split}\n    w_t ~&=~ q^T k_t \\\\\n       &=~ \\left(W_Qx + b_Q\\right)^T \\left(W_Ky_t + b_K\\right)  \\\\\n       &=~ x^TW_{QK}y_t ~+~ ({W_Q}^Tb_K)^Tx ~+~ ({W_K}^Tb_Q)^Ty_t ~+~ b_Q^Tb_K \\\\\n    w_t ~+~ const~ &=~ x^TW_{QK}y_t ~+~ ({W_Q}^Tb_K)^Tx ~+~ ({W_K}^Tb_Q)^Ty_t  \\\\\n       &\\triangleq~ x^TW_{QK}y_t ~+~ \\rho_x^Tx ~+~ \\rho_y^Ty_t  ~~~~~~~~~~~~~\\rightarrow ~\\rho_x^Tx =const ~\\mathrm{given}~x\\rightarrow \\\\\n       &=~ x^TW_{QK}y_t ~+~ \\rho_y^Ty_t  ~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\rightarrow ~W_{QK} \\triangleq \\Omega^T \\Lambda \\Sigma~\\text{via SVD}~\\rightarrow  \\\\\n       &=~ x^T\\Omega^T\\Lambda\\Sigma y_t ~+~ \\rho_y^Ty_t ~~~~~~~~~~~~~~~~~~~~~~~~~~ \\rightarrow ~x' \\triangleq \\Omega x, ~~y'_t \\triangleq \\Sigma y_t ~\\rightarrow \\\\\n       &=~ {x'}^T\\Lambda y'_t ~+~ \\rho_y^Ty_t\n\\end{split}\n\\end{equation}\nAfter expanding the terms, we find an additive constant $b_Q^Tb_K$, and move this onto the LHS. Theorem~\\ref{theorem: att shift operator} states that this has no impact on the output of the \\texttt{softmax} operator. We identify $\\rho_x\\triangleq W_Q^Tb_k$ and $\\rho_y\\triangleq W_K^Tb_q$ as vectors on the \\textbf{row-spaces of $W_Q$ and $W_K$ respectively}, defined as linear maps of the special directions $b_K$ and $b_Q$. Since $x$ is constant for each \\texttt{softmax}, $\\rho_x^Tx$ is constant, and we absorb it into the LHS. We perform the singular value decomposition $W_{QK} \\triangleq \\Omega^T \\Lambda \\Sigma$ where $\\{\\Omega\\in\\mathbb{R}^{N_x\\times N_x},~\\Sigma\\in\\mathbb{R}^{N_y\\times N_y}\\}$ are orthonormal matrices and $\\Lambda \\in \\mathbb{R}^{N_x\\times N_y}$ is a diagonal matrix of positive-semidefinite singular values with maximum rank $\\min(N_x,N_y,N_{qkv})$. Orthonormal matrices apply a basis change to the embedding space using rotations and reflections. We write the transformed embeddings as $x' \\triangleq \\Omega x$ and $y_t' \\triangleq \\Sigma y_t$. The dot-product then has two terms:\n\\begin{enumerate}\n    \\item ${x'}^T \\Lambda y'_t = \\sum_i \\Lambda_{ii} x'_i y'_{ti}$ sculpts the attention distribution according to \\textit{pairwise relationships} between embeddings. We can say that $\\{\\Omega,\\Sigma\\}$ align the bases of $x$ and $y_t$, mapping them onto a common orthonormal coordinate system. $\\Lambda_{ii}$ then assigns an importance weight to each coordinate $i$, determining the contribution of $x'_iy'_{ti}$.\n    \n    \n    \\item $\\rho_y^Ty$ means ``token $t$ sends to all receivers when $y_t \\parallel \\rho_y$'', where $\\rho_y$ must be a vector on the row-space of $W_K$. This may be recovered in the expansion of ${x'}^T\\Lambda y'_t$ if there exists a direction $i$ for which $x'_i=const$.\n\\end{enumerate}\n\\end{mdframed}\n==== END OF /2406.17837/appendices/supplementary_theorems.tex ====\n==== BEGINNING OF /2406.17837/appendices/proofs.tex ====\n\\section{Proofs of theorems in the main text}\n\\label{appendix: proofs}\n\nThis appendix provides proofs for the theorems presented in section~\\ref{sec: theory: residual structure}-\\ref{sec: theory: circuit stability}.\n\n\\vspace{0.5cm}\n\n\\begin{mdframed}[backgroundcolor=green!5]\n\\textbf{Theorem~\\ref{theorem: structure: no-norm}.} \\textit{\\texttt{No-Norm}: If two heads with finite non-zero temperature attend to different semantic subspaces, the subspaces must be linearly independent $\\mathbb{S}^{N_\\alpha}_\\alpha \\equiv \\mathbb{R}^{N_\\alpha}$. Corollary: $W_{QK}$ is a low-rank matrix with (left and right) null-spaces that span all non-attended information.}\n\n\\textit{Proof.} ~\nLet $\\theta_A$ and $\\theta_B$ be co-ordinates for the subspaces of $x$ attended to by heads A and B respectively, and $\\phi$ be all other information. Let $\\theta_A\\perp\\theta_B\\perp\\phi$ and $x \\perp y_t$, where $\\perp$ denotes independence. Without loss of generality, write\n\\begin{equation}\n    x(\\theta_A,\\theta_B,\\phi) ~=~ x_A(\\theta_A) ~+~ x_B(\\theta_B) ~+~ x_{other}(\\theta_A,\\theta_B,\\phi)\n\\end{equation}\nThen write\n\\begin{equation}\n\\begin{split}\n    w_t^{(A)}(\\theta_A) ~&=~ \\left(W_{QK}^{(A)} y_t\\right)^T x(\\theta_A,\\theta_B,\\phi)  \\\\\n    &=~ \\left(W_{QK}^{(A)} y_t\\right)^T x_A(\\theta_A) ~+~ \\left(W_{QK}^{(A)} y_t\\right)^T x_B(\\theta_B) ~+~ \\left(W_{QK}^{(A)} y_t\\right)^T x_{other}(\\theta_A,\\theta_B,\\phi) \\\\\n\\end{split}\n\\end{equation}\nwhich requires $\\left(W_{QK}^{(A)} y_t\\right)^T x_B(\\theta_B)=0$ and $\\left(W_{QK}^{(A)} y_t\\right)^T x_{other}(\\theta_A,\\theta_B,\\phi)=0$, since any cancellation between the two terms must be independent of $\\theta_A,\\phi$ and so can be absorbed entirely into the function $x_{B}(\\theta_B)$. This means that $x_B(\\theta_B)$ and $x_{other}(\\theta_A,\\theta_B,\\phi)$ must both be orthogonal to $W_{QK}^{(A)} y_t$, meaning that they reside on the \\textit{left null space} of $W_{QK}^{(A)}$, or are projected by ${W_{QK}^{(A)}}^T$onto a null space of $y_t$.\n\nHead A can only attend to $\\theta_A$ if $x_A(\\theta_A)$ it is not on either of these null spaces, meaning that $x_A(\\theta_A)$ is linearly independent of $x_{B}(\\theta_B)$ and $x_{other}(\\theta_A,\\theta_B,\\phi)$. Likewise for head B\n\\begin{equation}\n\\begin{split}\n    w_t^{(B)}(\\theta_B) ~&=~ \\left(W_{QK}^{(B)} y_t\\right)^T x(\\theta_A,\\theta_B,\\phi)  \\\\\n    &=~ \\left(W_{QK}^{(B)} y_t\\right)^T x_A(\\theta_A) ~+~ \\left(W_{QK}^{(B)} y_t\\right)^T x_B(\\theta_B) ~+~ \\left(W_{QK}^{(B)} y_t\\right)^T x_{other}(\\theta_A,\\theta_B,\\phi) \\\\\n\\end{split}\n\\end{equation}\nrequires that $x_{B}(\\theta_B)$ is linearly independent of both $x_A(\\theta_A)$ and $x_{other}(\\theta_A,\\theta_B,\\phi)$. Since $x_{other}$ resides on both null spaces, it is linearly independent of both $x_A(\\theta_A)$ and $x_B(\\theta_B)$, and may be seen as a third subspace that passes information through to subsequent layers.\n\nWe can also write $w_t = \\left(W_{QK}^T x\\right)^T y_t$, and so the same argument also holds for subspaces on $y_t$. In this case, non-attended subspaces are spanned by the \\textit{right null space} of $W_{QK}$.\n\\end{mdframed}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\clearpage\n\\begin{mdframed}[backgroundcolor=green!5]\n\\textbf{Theorem~\\ref{theorem: structure: pre-norm}.} \\textit{\\texttt{Pre-Norm}: Semantic subspaces must be represented as orthogonal spheres $\\mathbb{S}^{N_\\alpha}\\equiv\\mathcal{S}^{N_\\alpha-1}$ defined using the $L_2$-norm. Corollary: if either orthogonality or constant-norm are violated, semantic subspaces interfere through a multiplicative factor on $w_t$.}\n\n\\textit{Proof.} ~Write \n\\begin{equation}\nx(\\theta_A,\\theta_B,\\phi) ~=~ x_A(\\theta_A) ~+~ x_B(\\theta_B) ~+~ x_{AB}(\\theta_A,\\theta_B) ~+~ x_{other}(\\theta_A,\\theta_B,\\phi)\n\\end{equation}\nThen for head A we have\n\\begin{equation}\n    w_t^{(A)}(\\theta_A) ~=~ \\frac{1}{\\left|y_t\\right|\\left|x(\\theta_A,\\theta_B,\\phi) \\right|} {w^*_t}^{(A)}(\\theta_A)\n\\end{equation}\nwhere $w^*_t$ are the attention scores from the \\texttt{No-Norm} case, which requires $x_A(\\theta_A)$ and $x_B(\\theta_B)$ to be linearly independent. Now we additionally require $\\left|x(\\theta_A,\\theta_B,\\phi) \\right| \\perp \\theta_B,\\phi$, with\n\\begin{equation}\n|x| ~=~ \\sqrt{|x_A|^2 ~+~ |x_B ~+~ x_{AB} ~+~ x_{other}|^2 ~+~ 2 x_A^T \\left(x_B ~+~ x_{AB} ~+~ x_{other}\\right)}\n\\end{equation}\nwhere we suppress parameter dependence for readability. Since $\\sqrt{\\cdot}$ is a monotonic function, this can only be satisfied if\n\\begin{equation}\n|x_A|^2 ~+~ |x_B ~+~ x_{AB} ~+~ x_{other}|^2 ~+~ 2 x_A^T \\left(x_B ~+~ x_{AB} ~+~ x_{other}\\right) ~\\perp~ \\theta_B,\\phi\n\\end{equation}\nRepeating this process for head B gives\n\\begin{equation}\n|x_B|^2 ~+~ |x_A ~+~ x_{AB} ~+~ x_{other}|^2 ~+~ 2 x_B^T \\left(x_A ~+~ x_{AB} ~+~ x_{other}\\right) ~\\perp~ \\theta_A,\\phi\n\\end{equation}\nCombining and collecting dependencies, we then have\n\\begin{align}\n    |x_A|^2 ~=~ const ~~~&\\forall~~~ \\theta_A \\\\\n    |x_B|^2 ~=~ const ~~~&\\forall~~~ \\theta_B \\\\\n    \n    \\left( x_{AB} ~+~ 2x_A ~+~ 2x_B \\right)^T x_{AB} ~+~ 2x_A^Tx_B ~=~ const ~~~&\\forall~~~ \\theta_A,\\theta_B \\\\\n    \\left(x_{other} + 2x_A + 2x_B + 2x_{AB}\\right)^T x_{other} ~=~ const ~~~&\\forall~~~ \\theta_A,\\theta_B,\\phi\n\\end{align}\nWe can go one step further, noticing that each individual term carries a different functional dependence, and so must independently be constant\\footnote{N.B. If $|x_{AB}|^2 \\propto x_A^Tx_B$ then $|x_{AB}|^2=const$ reduces to $x_A^Tx_B=const$, which is already required.}. We then have $\\forall~~\\mu,\\nu\\in\\{A,B,AB,other\\}$\n\\begin{equation}\n    |x_\\mu|=const   ~~~~~~\\mathrm{and}~~~~~~  x_\\mu^Tx_\\nu=const \n\\end{equation}\nThe requirements $|x_A(\\theta_A)|=const ~\\forall~\\theta_A$ and $|x_B(\\theta_B)|=const ~\\forall~\\theta_B$ mean that the semantic subspaces have a spherical structure defined by the $L_2$-norm $|\\cdot|$.\n\nNow consider the requirement $x_A(\\theta_A)^Tx_B(\\theta_B)=const$. Say that $\\theta_A$ and $\\theta_B$ have $N_A$ and $N_B$ degrees of freedom, meaning that $x_A$ and $x_B$ have $N_A-1$ and $N_B-1$ respectively, since they each lose one by confinement to the sphere. Say that the constant is nonzero such that $x_A^Tx_B \\neq 0$. This means that there must be some direction $i$ for which $x_{Ai}x_{Bi} \\neq0$. If we know all $N_A-1$ coordinates of $x_A$, and all $N_B - 2$ coordinates of $x_B$ except for direction $i$, then we also know the value of $x_{Bi}$, because it is fixed by the constant. However, this would mean that $x_A$ and $x_B$ are not independent, violating the condition $\\theta_A \\perp \\theta_B$. The only way to satisfy independence is if $x_{Ai}x_{Bi}=0~\\forall~i$, ensuring that degrees of freedom on $x_A$ and $x_B$ never become entangled. Therefore, to satisfy semantic independence, we must have $x_A(\\theta_A)^Tx_B(\\theta_B)=0 ~\\forall~\\theta_A,\\theta_B$. This means that the subspaces are not just linearly independent, but orthogonal.\n\nWe have shown the proof for semantic subspaces of $x$. As for Theorem~\\ref{theorem: structure: no-norm}, the same structure must be true for $y_t$ by symmetry.\n\\end{mdframed}\n\n\\clearpage\n\\begin{mdframed}[backgroundcolor=green!5]\n\\textbf{Theorem~\\ref{theorem: structure: qkv-norm}.} \\textit{\\texttt{QKV-Norm}: Semantic subspaces must be linearly separable, reproducing the \\texttt{No-Norm} case.}\n\n\\textit{Proof.} ~We have\n\\begin{equation}\n    w_t^{(A)}(\\theta_A) ~=~ \\frac{1}{\\left|k_t^{(A)}\\right|\\left|q^{(A)}\\right|} {w^*_t}^{(A)}(\\theta_A)\n\\end{equation}\nwhere $w^*_t$ are the attention scores from the \\texttt{No-Norm} case, which requires $x_A(\\theta_A)$ and $x_B(\\theta_B)$ to be linearly independent. Use\n\\begin{equation}\nx(\\theta_A,\\theta_B,\\phi) ~=~ x_A(\\theta_A) ~+~ x_B(\\theta_B) ~+~ x_{other}(\\theta_A,\\theta_B,\\phi)\n\\end{equation}\nand \n\\begin{equation}\n\\begin{split}\n    q^{(A)}(\\theta_A) ~&=~ W_Q^{(A)} x(\\theta_A,\\theta_B,\\phi)  \\\\\n    &=~ W_Q^{(A)} x_A(\\theta_A) ~+~ W_Q^{(A)} x_B(\\theta_B) ~+~ W_Q^{(A)} x_{other}(\\theta_A,\\theta_B,\\phi) \\\\\n\\end{split}\n\\end{equation}\nSince we already have the condition of linearly independent $x_A,x_B$, there must exist a linear projection operator $P_A$ such that $P_A x_A = x_A$. Defining $W_Q^{(A)}=P_A$, we then have\n\\begin{equation}\n    q^{(A)}(\\theta_A) ~=~ W_Q^{(A)} x_A(\\theta_A) \n\\end{equation}\nThis demonstrates that it is possible to separate linearly independent semantic subspaces on $x$. By symmetry of $w_t^{(A)}(\\theta_A)$, the same must be true for $y_t$.\n\\end{mdframed}\n\n\\begin{mdframed}[backgroundcolor=green!5]\n\\textbf{Theorem~\\ref{theorem: stability: general}.} Consider independent infinitesimal perturbations on queries $\\epsilon^q \\in \\mathbb{R}^{N_{qkv}}$, keys $\\epsilon^k_t \\in \\mathbb{R}^{N_{qkv}}$, and messages $\\epsilon^m_t \\in \\mathbb{R}^{N_{qkv}}$. These propagate onto $\\Delta x = \\sum_{t}a_tm_t$ as\n    \\begin{align}\n        \\epsilon^{\\Delta x(q)} ~~&\\xrightarrow[\\epsilon^q\\rightarrow0]{\\mathrm{~~~~perturb~q~~~~}}~~ \\mathop{\\mathbb{E}}_{a_t} \\Big[ m_t {\\tilde k}_t^T \\Big] \\epsilon^q ~~~~~~~~~~~~~~~~~  {\\tilde k}_t ~\\triangleq~ k_t ~- \\mathop{\\mathbb{E}}_{a_t} \\Big[ k_t \\Big] \n        \\label{eq: stability: general q}\\\\\n        \\epsilon^{\\Delta x(k)} ~~&\\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{~~~~perturb~k~~~~}}~~ \\mathop{\\mathbb{E}}_{a_t} \\Big[ {\\tilde m}_t {\\epsilon^k_t}^T \\Big] q ~~~~~~~~~~~~~~~~~  {\\tilde m}_t ~\\triangleq~ m_t ~- \\mathop{\\mathbb{E}}_{a_t} \\Big[ m_t \\Big]\n        \\label{eq: stability: general k} \\\\\n        \\epsilon^{\\Delta x(m)} ~~&\\xrightarrow[\\epsilon^m_t\\rightarrow0]{\\mathrm{~~~~perturb~m~~~~}}~~ \\mathop{\\mathbb{E}}_{a_t} \\Big[ \\epsilon^m_t \\Big]\n        \\label{eq: stability: general m}\n    \\end{align}\n    where ${\\tilde z}_t$ is the value of $z_t$ measured from the attention-weighted centroid $\\mathbb{E}_{a_t}[z_t] = \\sum_t a_t z_t$.\n\n\\textit{Proof.} ~Consider $q\\rightarrow q+\\epsilon^q$ where $\\epsilon^q$ are infinitesimal perturbations on $q$. Then $\\Delta x \\rightarrow \\Delta x + \\epsilon^{\\Delta x(q)}$ where by Taylor expansion we find\n\\begin{equation}\n    \\epsilon^{\\Delta x(q)} ~=~ \\frac{\\partial \\Delta x}{\\partial q}\\epsilon^q ~+~ \\mathcal{O}\\left({\\epsilon^q}^2\\right)\n\\end{equation}\nwhere the leading term is a matrix $\\frac{\\partial \\Delta x}{\\partial q}$ acting on a vector $\\epsilon^q$. Differentiating gives\n\\begin{equation}\n    \\frac{\\partial\\Delta x}{\\partial q} ~=~ \\sum_{ij} m_i \\frac{\\partial a_i}{\\partial w_j} \\frac{\\partial w_j}{\\partial q}\n\\end{equation}\nwith $a_i = \\texttt{softmax}_i(w_i)$ and $w_i=k_i^Tq$, and we are using $i,j,k$ etc to index over tokens instead of $t,t',t''$ etc, because this is more readable when we have many summations. Then\n\n\\textcolor{Maroon}{\\textit{[continued in next box...]}}\n\\end{mdframed}\n\n\\clearpage\n\\begin{mdframed}[backgroundcolor=green!5]\n\\textcolor{Maroon}{\\textit{[...continuing from previous box]}}\n\n\\begin{equation}\n\\begin{split}\n    \\frac{\\partial a_i}{\\partial w_j} ~&=~ \\frac{\\partial}{\\partial w_j} ~ \\frac{e^{w_i}}{\\sum_k e^{w_k}} \\\\\n    &=~ \\frac{\\delta_{ij}e^{w_i}}{\\sum_k e^{w_k}} ~+~ e^{w_i}\\left(-\\frac{e^{w_j}}{\\left(\\sum_ke^{w_k}\\right)^2}\\right) \\\\\n    &=~ \\frac{e^{w_i}}{\\sum_k e^{w_k}}\\left( 1 ~-~ \\frac{e^{w_j}}{\\sum_le^{w_l}}\\right) \\\\\n    &=~ a_i\\left(\\delta_{ij} ~-~ a_j\\right) \\\\\n\\end{split}\n\\end{equation}\nand $\\frac{\\partial w_i}{\\partial q} = k_i^T$, where we retain the transpose to indicate that this is an element of the dual vector space (i.e. covector). Inserting these results into our expression for $\\epsilon^{\\Delta x(q)}$ gives\n\\begin{equation}\n\\begin{split}\n    \\epsilon^{\\Delta x(q)} ~&=~ \\sum_{ij} m_i a_i\\left(\\delta_{ij} ~-~ a_j\\right) k_j^T \\epsilon^q \\\\\n    &=~ \\sum_{i} m_i a_i \\left(k_i ~-~ \\sum_j a_jk_j \\right)^T \\epsilon^q \\\\\n    &=~ \\sum_{i} m_i a_i {\\tilde k}_i^T \\epsilon^q \\\\\n    &=~ \\mathop{\\mathbb{E}}_{a_i} \\Big[m_i {\\tilde k}_i^T \\Big] \\epsilon^q \\\\\n\\end{split}\n\\end{equation}\nThis is the result for Eq.~\\ref{eq: stability: general q}. Repeating the process for perturbations on $k_i$, we have\n\\begin{equation}\n    \\epsilon^{\\Delta x(k)} ~=~ \\sum_i\\frac{\\partial \\Delta x}{\\partial k_i}\\epsilon^k_i ~+~ \\mathcal{O}\\left({\\epsilon^k}^2\\right)\n\\end{equation}\nand\n\\begin{equation}\n\\begin{split}\n    \\frac{\\partial\\Delta x}{\\partial k_i} ~&=~ \\sum_{jk} m_j \\frac{\\partial a_j}{\\partial w_k} \\frac{\\partial w_k}{\\partial k_i} \\\\\n    &=~ \\sum_{jk} m_j a_j \\left(\\delta_{jk} ~-~ a_k\\right) \\delta_{ki} q^T \\\\\n    &=~ \\sum_{j} m_j a_j \\left(\\delta_{ji} ~-~ a_i\\right) q^T \\\\\n    &=~ a_i {\\tilde m}_i q^T\n\\end{split}\n\\end{equation}\nTherefore\n\\begin{equation}\n    \\epsilon^{\\Delta x(k)} ~=~ \\sum_i a_i {\\tilde m}_i q^T \\epsilon^k_i ~=~ \\mathop{\\mathbb{E}}_{a_i} \\Big[{\\tilde m}_i {\\epsilon^k_i}^T \\Big] q\n\\end{equation}\nwhich is the result for Eq.~\\ref{eq: stability: general k}. Finally,\n\\begin{equation}\n\\begin{split}\n    \\epsilon^{\\Delta x(m)} ~&=~ \\sum_i \\frac{\\partial \\Delta x}{\\partial m_i}\\epsilon^m_i \\\\\n    &=~ \\sum_{i} a_i \\epsilon^m_i \\\\\n    &=~ \\mathop{\\mathbb{E}}_{a_i} \\Big[ \\epsilon^m_i \\Big]\n\\end{split}\n\\end{equation}\nusing $\\frac{\\partial\\Delta x}{\\partial m_i} = \\frac{\\partial}{\\partial m_i}\\sum_j a_j m_j = \\sum_j a_j \\delta_{ij} = a_i$. This is the result for Eq.~\\ref{eq: stability: general m}.\n\\end{mdframed}\n\n\\begin{mdframed}[backgroundcolor=green!5]\n\\textbf{Theorem~\\ref{theorem: stability: sparse}.} For sparse attention:\n    \\begin{equation}\n        \\epsilon^{\\Delta x(q)} \\xrightarrow[\\epsilon^q\\rightarrow0]{\\mathrm{~~perturb~q~~}} 0   ~~~~~~~~~~\n        \\epsilon^{\\Delta x(k)} \\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{~~perturb~k~~}} 0   ~~~~~~~~~~\n        \\epsilon^{\\Delta x(m)} \\xrightarrow[\\epsilon^m_t\\rightarrow0]{\\mathrm{~~perturb~m~~}} \\epsilon^m_{t^*}\n    \\end{equation}\n    i.e. the message is stable with respect to small interference in the queries and keys. Interference in the selected value is linearly transferred onto the message.\n\n\\textit{Proof.} ~For sparse attention we have $a_t = \\delta_{tt^*}$ for some $t^*$. For perturbations of $q$, the RHS of Eq.~\\ref{eq: stability: general q} becomes\n\\begin{equation}\n\\begin{split}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ m_t {\\tilde k}_t^T \\Big] \\epsilon^q ~&=~ \\sum_{t} a_t m_t {\\tilde k}_t^T \\epsilon^q \\\\\n    &=~ \\sum_{t} \\delta_{tt^*} m_t {\\tilde k}_t^T \\epsilon^q \\\\\n    &=~ m_{t^*} {\\tilde k}_{t^*}^T \\epsilon^q \\\\\n    &=~ 0 \\\\\n\\end{split}\n\\end{equation}\nwhere the final step is because ${\\tilde k}_{t^*} = k_{t^*} - \\mathbb{E}_{a_t}[k_t] = k_{t^*} - \\sum_t \\delta_{tt^*} k_t = k_{t^*}-k_{t^*} = 0$. For perturbations of $k_t$, the RHS of Eq.~\\ref{eq: stability: general k} evaluates to $0$ because \n\\begin{equation}\n\\begin{split}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ {\\tilde m}_t {\\epsilon^k_t}^T \\Big] q ~&=~ \\sum_t a_t {\\tilde m}_t q^T \\epsilon^k_t \\\\\n    &=~ \\sum_t \\delta_{tt^*} {\\tilde m}_t q^T \\epsilon^k_t \\\\\n    &=~ {\\tilde m}_{t^*} q^T \\epsilon^k_{t^*} \\\\\n    &=~ 0 \\\\\n\\end{split}\n\\end{equation}\nwhere the final step is because ${\\tilde m}_{t^*} = m_{t^*} - \\sum_t \\delta_{tt^*} m_t = m_{t^*}-m_{t^*} = 0$. For perturbations of $m_t$, the RHS of Eq.~\\ref{eq: stability: general m} evaluates to\n\\begin{equation}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ \\epsilon^m_t \\Big] ~=~ \\sum_{t} a_t \\epsilon^m_t ~=~ \\sum_{t} \\delta_{tt^*} \\epsilon^m_t ~=~ \\epsilon^m_{t^*}\n\\end{equation}\n\\end{mdframed}\n\n\\begin{mdframed}[backgroundcolor=green!5]\n\\textbf{Theorem~\\ref{theorem: stability: isotropic}.}\nFor isotropic attention:\n\\begin{equation}\n    \\epsilon^{\\Delta x(q)} \\xrightarrow[\\epsilon^q\\rightarrow0]{\\mathrm{perturb~q}} \\langle m_t {\\tilde k}_t^T \\rangle_t \\epsilon^q ~~~~~~~~\n    \n    \\epsilon^{\\Delta x(k)} \\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{perturb~k}} \\langle {\\tilde m}_t {\\epsilon^k_t}^T \\rangle_t ~q   ~~~~~~~~\n    \\epsilon^{\\Delta x(m)} \\xrightarrow[\\epsilon^m_t\\rightarrow0]{\\mathrm{perturb~m}} \\langle \\epsilon^m_t \\rangle_t\n\\end{equation}\n\n\n\n\n\nN.B. isotropy requires $k_t=const$ or $q=0$. Lemma 1: the update is stable to noisy $q$ when $k_t=const$, or when $m_t \\perp k_t$ (c.f. keys and messages from independent subspaces). Lemma 2: the update is stable to noisy $k_t$ when $q=0$, or when $m_t \\perp \\epsilon_t^k$. Lemma 3: the update is stable to noisy $m_t$ when $\\langle \\epsilon^m_t \\rangle_t=0$. Other cases propagate linearly.\n\n\\textit{Proof.} ~For isotropic attention we have $a_t = \\frac{1}{T}$. For perturbations of $q$, the RHS of Eq.~\\ref{eq: stability: general q} is\n\\begin{equation}\n\\begin{split}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ m_t {\\tilde k}_t^T \\Big] \\epsilon^q ~&=~ \\sum_{t} a_t m_t {\\tilde k}_t^T \\epsilon^q \\\\\n    &=~ \\frac{1}{T} \\sum_{t=1}^T m_t {\\tilde k}_t^T \\epsilon^q \\\\\n    &=~ \\langle m_t {\\tilde k}_t^T \\rangle_t \\epsilon^q \\\\\n\\end{split}\n\\end{equation}\nFor lemma 1, we note that $k_t=const$ implies ${\\tilde k}_t=0$, and if $m_t \\perp k_t$ then $\\langle m_t {\\tilde k}_t^T \\rangle_t = \\langle m_t k_t \\rangle_t - \\langle m_t \\rangle_t \\langle k_t\\rangle_t = Cov(m_t,k_t) = 0$.\n\n\\textcolor{Maroon}{\\textit{[continued in next box...]}}\n\\end{mdframed}\n\n\\clearpage\n\\begin{mdframed}[backgroundcolor=green!5]\n\\textcolor{Maroon}{\\textit{[...continuing from previous box]}}\n\nFor perturbations of $k_t$, the RHS of Eq.~\\ref{eq: stability: general k} is\n\\begin{equation}\n\\begin{split}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ {\\tilde m}_t {\\epsilon^k_t}^T \\Big] q ~&=~ \\frac{1}{T} \\sum_{t=1}^T {\\tilde m}_t {\\epsilon^k_t}^T q \\\\\n    &=~ \\langle {\\tilde m}_t {\\epsilon^k_t}^T \\rangle_t q \\\\\n\\end{split}\n\\end{equation}\nFor lemma 2, this expression evaluates to $0$ if $q=0$, and if $m_t \\perp \\epsilon_t^k$ then $\\langle {\\tilde m}_t {\\epsilon^k_t}^T \\rangle_t = \\langle m_t {\\epsilon^k_t}^T \\rangle_t - \\langle m_t \\rangle_t \\langle {\\epsilon^k_t}^T\\rangle_t = Cov(m_t,{\\epsilon^k_t}^T) = 0$.\n\nFor perturbations of $m_t$, the RHS of Eq.~\\ref{eq: stability: general m} evaluates to\n\\begin{equation}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ \\epsilon^m_t \\Big] ~=~ \\frac{1}{T}\\sum_{t=1}^T \\epsilon^m_t ~=~ \\langle \\epsilon^m_t \\rangle_t\n\\end{equation}\n\\end{mdframed}\n\n\\begin{mdframed}[backgroundcolor=green!5]\n\\textbf{Theorem~\\ref{theorem: multiplicative stability: sparse}. } ~Sensitivity of sparse attention to multiplicative perturbations $\\epsilon^q = \\kappa^q q$ and $\\epsilon^k = \\kappa^k_t k_t$ with $\\kappa^q,\\kappa^k_t\\ll1$. Circuit collapse occurs when $\\exists~ t \\neq t^*$ for which:\n\\begin{equation}\n    \\frac{w_{t^*}}{w_t} ~\\begin{cases} ~<~ \\lambda_w & \\mathrm{if}~ w_t \\left(1 + \\kappa^q + \\kappa^k_{t^*}\\right) > 0 \\\\\n    ~>~ \\lambda_w & \\mathrm{otherwise} \\\\ \\end{cases}\n    ~~~~~~~~~~~~~ \\lambda_w ~\\triangleq~ \\frac{1 + \\kappa^q + \\kappa^k_t}{1 + \\kappa^q + \\kappa^k_{t^*}}\n\\end{equation}\nwhere temperature cancels in the fraction. \\textbf{Attention is fully stable above the critical transition point $\\lambda_w$} (c.f. $w_t \\left(1 + \\kappa^q + \\kappa^k_{t^*}\\right) > 0$). We see that query perturbations alone are insufficient, as they result in $\\lambda_w=1$. Lemma: consider the special case when all keys have similar length $k_t \\approx const$, the attended token has $\\theta_{t^*}\\approx0$, the keys are far-from-orthogonal s.t. $\\theta_t \\ll 1$, and $\\kappa^q\\approx0$. Using $w_t \\triangleq |q| |k_t| \\cos\\theta_t$, circuit collapse occurs when $\\exists~ t \\neq t^*$ for which:\n\\begin{equation}\n        \\frac{1}{2}\\theta_t^2 ~\\lesssim~ \\kappa^k_t - \\kappa^k_{t^*}   ~~~~~~~~~~~ \\mathrm{if}~ w_t \\left(1  + \\kappa^k_{t^*}\\right) > 0 ~\\text{, otherwise reverse}\n\\label{eq: app: sparse circuit collapse result}\n\\end{equation}\ni.e. stability requires either well-separated keys s.t. $\\theta_t \\gg 0$, or small perturbations $\\kappa_t-\\kappa^*_t \\ll 1$.\n\n\\textit{Proof.} ~ Apply $q\\rightarrow q+\\epsilon^q$ and $k_t\\rightarrow k_t+\\epsilon_t^k$ to $w_t = q^Tk_t$, then we have $w_t \\rightarrow w_t + \\epsilon_w$ such that $\\epsilon^w_t = q^T\\epsilon_t^k + {\\epsilon^q}^Tk_t + {\\epsilon^q}^T\\epsilon_t^k$. For multiplicative perturbations we have  $\\epsilon^q = \\kappa^q q$ and $\\epsilon^k = \\kappa^k_t k_t$, and so $\\epsilon^w_t = \\kappa^k_t q^Tk_t + \\kappa^q q^Tk_t + \\kappa^k_t\\kappa^qq^Tk_t$. Each term recovers a factor of $w_t=q^Tk_t$, which we factor out to give $\\epsilon^w_t = \\left(\\kappa^q  + \\kappa^k_t + \\kappa^k_t\\kappa^q\\right)w_t$. The final term is subleading in the limit of small perturbations, and so\n\\begin{equation}\n    \\epsilon^w_t ~\\xrightarrow[~\\kappa^q,\\kappa^k_t\\rightarrow0~]{}~ \\left(\\kappa^q  ~+~ \\kappa^k_t\\right)w_t ~+~ \\mathcal{O}\\left(\\kappa^q\\kappa^k_t\\right)\n\\end{equation}\nCircuit collapse occurs when $w_{t^*} - w_t < \\epsilon^w_t - \\epsilon^w_{t^*}$ for some $t$. Substituting our limit for $\\epsilon^w_t$ gives\n\\begin{equation}\n    w_{t^*} - w_t ~<~ \\left(\\kappa^q  ~+~ \\kappa^k_t\\right)w_t - \\left(\\kappa^q  ~+~ \\kappa^k_{t^*}\\right)w_{t^*}\n\\end{equation}\nand collecting terms gives\n\\begin{equation}\n    \\left(1 ~+~ \\kappa^q ~+~ \\kappa^k_{t^*}\\right) w_{t^*} ~<~ \\left(1 ~+~ \\kappa^q ~+~ \\kappa^k_t\\right)w_t\n\\end{equation}\nWe then divide each side by $w_t (1 + \\kappa^q + \\kappa^k_{t^*})$, taking care to reverse the sign of the inequality when this factor is negative, to give\n\n\\textcolor{Maroon}{\\textit{[continued in next box...]}}\n\\end{mdframed}\n\n\\clearpage\n\\begin{mdframed}[backgroundcolor=green!5]\n\\textcolor{Maroon}{\\textit{[...continuing from previous box]}}\n\n\\begin{equation}\n    \\frac{w_{t^*}}{w_t} ~\\begin{cases} ~<~ \\lambda_w & \\mathrm{if}~ w_t \\left(1 + \\kappa^q + \\kappa^k_{t^*}\\right) > 0 \\\\\n    ~>~ \\lambda_w & \\mathrm{otherwise} \\\\ \\end{cases}\n    ~~~~~~~~~~~~~ \\lambda_w ~\\triangleq~ \\frac{1 + \\kappa^q + \\kappa^k_t}{1 + \\kappa^q + \\kappa^k_{t^*}}\n\\end{equation}\nwhich is the first expression in the theorem.  We note that any temperature parameter cancels in the fraction, which means that the attention head cannot become more stable by reducing its temperature to become more sparse. $\\lambda_w$ has the limits\n\\begin{equation}\n    \\lambda_w ~\\xrightarrow[\\kappa^q\\rightarrow0]{~~\\mathrm{keys~only}~~}~ \\frac{1+\\kappa^k_t}{1+\\kappa^k_{t^*}}\n    ~~~~~~~~~~~~~~~~~\n    \\lambda_w ~\\xrightarrow[\\kappa^k_t,\\kappa^k_{t^*}\\rightarrow0]{~~\\mathrm{query~only}~~}~ \\frac{1 + \\kappa_q}{1 + \\kappa_q} = 1\n\\end{equation}\nmeaning that query perturbations alone are insufficient, contributing only when they co-occur with perturbations on the keys. Write $w_t = |q| |k_t| \\cos\\theta_t$ with $\\theta_t = q \\wedge k_t$, and the approximation of identical key norms $k_{t^*}=k_t\\equiv k$ turns this into $w_t = |q| |k| \\cos\\theta_t$. Then\n\\begin{equation}\n    \\frac{w_{t^*}}{w_t} ~=~ \\frac{|q| |k| \\cos\\theta_{t^*}}{|q| |k| \\cos\\theta_t} ~=~ \\frac{\\cos\\theta_{t^*}}{\\cos\\theta_t}\n\\end{equation}\nThen $\\theta_{t^*}=0$ means that $\\cos\\theta_{t^*} = \\cos0=1$, and so $\\frac{\\cos\\theta_{t^*}}{\\cos\\theta_t} = \\frac{1}{\\cos\\theta_t} = \\sec \\theta_t$. We perform a Taylor expansion in $\\theta_t$ to obtain\n\\begin{equation}\n    \\frac{w_{t^*}}{w_t} ~\\approx~ \\sec\\theta_t ~\\approx~ 1 ~+~ \\frac{1}{2}\\theta_t^2 ~+~\\mathcal{O}\\left(\\theta_t^4\\right)\n\\end{equation}\nwhich is valid when $\\theta_t \\ll 1$. This is true for any $t\\neq t^*$ for which $k_t$ is far from orthogonal with $k_{t^*}$. Substituting this into our circuit collapse condition, we have\n\\begin{equation}\n    1 ~+~ \\frac{1}{2}\\theta_t^2 ~<~ \\frac{1 + \\kappa^k_t}{1 + \\kappa^k_{t^*}} ~~~~~~~~~~~~~~ \\mathrm{if}~ w_t \\left(1 + \\kappa^k_{t^*}\\right) > 0 \n\\end{equation}\nwhere we consider the case of $\\kappa_q\\approx0$ for readability. Re-arranging gives\n\\begin{equation}\n    \\frac{1}{2}\\theta_t^2 ~\\lesssim~  \\kappa^k_t - \\kappa^k_{t^*} ~~~~~~~~~~~~~~~~\\text{Circuit~collapse~when~}k_t~\\text{similar}\n\\label{eq: app: sparse circuit collapse result duplicate}\n\\end{equation}\nif $w_t(1 + \\kappa^k_{t^*}) > 0$, and we reverse the inequality otherwise. We have approximated the denominator on the RHS as $1 + \\kappa^k_{t^*} \\approx 1$ for $\\kappa^k_{t^*}\\rightarrow0$.\n\nWhen $\\theta_t \\ll 1$, the LHS of Eq.~\\ref{eq: app: sparse circuit collapse result duplicate} is small. This means that the attention head can tolerate only very small perturbations $\\{\\kappa^k_t,\\kappa^k_{t^*}\\}$. Therefore semantic subspaces must either have a highly orthogonal substructure s.t. $\\theta_t \\gtrsim 1 ~\\forall~t\\neq t^*$, or be orthogonal s.t. $\\kappa_t\\ll1 ~\\forall~ t$.\n\n\\end{mdframed}\n\n\\clearpage\n\\begin{mdframed}[backgroundcolor=green!5]\n\\textbf{Theorem. ~\\ref{theorem: multiplicative stability: isotropic}}. ~Sensitivity of isotropic attention to multiplicative perturbations. Say $\\epsilon^k = \\kappa^k_t k_t$ with $\\kappa^k_t\\ll1$ where $\\{\\kappa_t\\}$ have comparable amplitudes. Then\n\\begin{equation}\n\\epsilon^{\\Delta x(k)} \n~\\approx~\n    \\begin{cases}\n    0 ~&~ \\text{if~$\\kappa_t$~independent~of~${\\tilde m}_t$,~by~symmetry} \\\\\n    0 ~&~ \\text{if~$\\kappa_t\\equiv\\kappa$~for~constant~$\\kappa$} \\\\\n    0 ~&~ \\text{if~$q=0$} \\\\\n    w \\langle {\\tilde m}_t \\kappa^k_t \\rangle_t  ~&~ \\text{otherwise}\n    \\end{cases}\n\\end{equation}\n\n\n\\textit{Proof.} ~We begin with the following result from Theorem~\\ref{theorem: stability: isotropic}:\n\\begin{equation}\n\\epsilon^{\\Delta x(k)} ~\\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{~~perturb~k~~}}~ \\langle {\\tilde m}_t {\\epsilon^k_t}^T \\rangle_t ~q\n\\end{equation}\nSubstituting $\\epsilon^k = \\kappa^k_t k_t$ and taking $q$ inside the brackets gives\n\\begin{equation}\n\\langle{{\\tilde m}_t  \\epsilon^k_t}^T \\rangle_t ~q ~=~ \n\\langle {\\tilde m}_t \\kappa_t {k_t}^T \\rangle_t q ~=~\n ~ \\langle {\\tilde m}_t \\kappa_t w_t \\rangle_t\n\\end{equation}\nWe then notice that isotropic attention requires that $w_t$ is a constant, which we call $w$. Then\n\\begin{equation}\n\\epsilon^{\\Delta x(k)} ~\\approx~ w \\langle {\\tilde m}_t \\kappa_t \\rangle_t\n\\end{equation}\nis our general result. We then note three special cases, each resulting in $\\epsilon^{\\Delta x(k)}=0$:\n\\begin{enumerate}\n    \\item If $\\kappa_t \\perp {\\tilde m}_t$ then $\\langle {\\tilde m}_t \\kappa_t \\rangle_t = \\langle m_t \\kappa_t \\rangle_t - \\langle m_t \\rangle_t \\langle \\kappa_t \\rangle_t = Cov(m_t,\\kappa_t) = 0$. This is case when interference $\\kappa_t^k$ on the keys is not dominated by the same semantic subspace as the message $m_t$.\n    \\item If all keys are perturbed by the same factor $\\kappa_t\\equiv\\kappa$, then $\\langle {\\tilde m}_t \\kappa_t \\rangle_t = \\kappa \\langle {\\tilde m}_t \\rangle_t =0$ because $\\langle {\\tilde m}_t \\rangle_t=0$.\n    \\item Isotropic attention can be achieved by either $q=0$ or $k_t=const$. If the case is $q=0$ then this implies $w=0$ also.\n\\end{enumerate}\n\\end{mdframed}\n==== END OF /2406.17837/appendices/proofs.tex ====\n==== BEGINNING OF /2406.17837/appendices/main_experiments_extended.tex ====\n\\section{Extended main experiments}\n\\label{appendix: extended experiments}\n\nThis appendix provides an extended explanation of the experimental results in section~\\ref{sec: experimental results}.\n\n\\subsection{Embedding structure}\n\nFigure~\\ref{fig: embedding spread} presented the spread of embedding $L_2$-norms as a function of model depth. Let us now describe in detail how this plot was made. Figure~\\ref{fig: embedding L2 norms prenorm} shows the distribution of embeddings at the input to every attention layer, for \\texttt{Baseline} models trained using \\texttt{Pre-Norm} (top) and \\texttt{QKV-Norm} (bottom). Colours represent the initial token type corresponding to that embedding. Asterisks denote tokens in the \\textit{answer}, with all labels denoting the \\textit{question}.\n\n\\begin{figure}[h]\n\\includegraphics[width=\\textwidth]{figures/baseline_model/embeddings_lengths_prenorm.pdf}\n\\caption{Distribution of embedding $L_2$-norms at different model depths using the \\texttt{Baseline Pre-Norm} model.}\n\\label{fig: embedding L2 norms prenorm}\n\\vspace{0.4cm}\n\\includegraphics[width=\\textwidth]{figures/baseline_model/embeddings_lengths_qkvnorm.pdf}\n\\caption{Distribution of embedding $L_2$-norms at different model depths using the \\texttt{Baseline QKV-Norm} model.}\n\\label{fig: embedding L2 norms qkvnorm}\n\\end{figure}\n\nWe see that the begin-sequence token (\\texttt{BEG}) is often separate from the distribution, which may be because it remains non-annotated and fulfils a qualitatively different role. We remove this from our estimates to avoid erroneously inflating the spread. Interestingly, \\texttt{BEG} still tends to be close to the main bulk for \\texttt{Pre-Norm}, but can be very far for \\texttt{QKV-Norm}, and we have to use overflow panels to capture it. This is consistent with our hypothesis that \\texttt{Pre-Norm} stability requires embeddings to have similar norms, whilst \\texttt{QKV-Norm} does not require this.\n\nIt is not sufficient to simply measure the spread of Figures~\\ref{fig: embedding L2 norms prenorm} and \\ref{fig: embedding L2 norms qkvnorm}, because an attention head may not be sensitive to all embeddings in the layer. The easiest way to account for this is to weight every embedding according to its assigned attention. Secondly, the distribution is expected to be narrow only on a per-head basis, and there is no reason why distinct heads cannot be centred around different medians. We therefore calculate the weighted distribution of embeddings on a per-head basis, as shown in Figures~\\ref{fig: embedding L2 norms prenorm head}-\\ref{fig: embedding L2 norms qkvnorm head}. \n\n\\begin{figure}[!]\n\\includegraphics[width=\\textwidth]{figures/baseline_model/contributing_tokens_0_prenorm.pdf}\n\\caption{Embedding distributions at different model depths using the \\texttt{Baseline Pre-Norm} model. The categories \\texttt{0-9} and \\texttt{N} are separated into whether they occur in the question (light colour) or answer (dark colour, distinguished by $*$ label). These distributions are used to compute the LHS of Figure~\\ref{fig: embedding spread} after removing the \\texttt{BEG} tokens.}\n\\label{fig: embedding L2 norms prenorm head}\n\\vspace{0.6cm}\n\\includegraphics[width=\\textwidth]{figures/baseline_model/contributing_tokens_0_qkvnorm.pdf}\n\\caption{Embedding distributions at different model depths using the \\texttt{Baseline QKV-Norm} model. The categories \\texttt{0-9} and \\texttt{N} are separated into whether they occur in the question (light colour) or answer (dark colour, distinguished by $*$ label). Note that overflow panels are excluded from this plot for legibility. These distributions are used to compute the RHS of Figure~\\ref{fig: embedding spread} after removing the \\texttt{BEG} tokens.}\n\\label{fig: embedding L2 norms qkvnorm head}\n\\end{figure}\n\n\n\\subsection{Circuit collapse}\n\nFigure~\\ref{fig: circuit collapse} shows the probability of circuit collapse. This is the probability that an attention distribution with no noise selects embedding $i$ with high probability $a_i \\geq 95\\\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/collapse/circuit_collapse_probability_min_att_0.95_0.0_loose.pdf}\n    \\vspace{0.4cm}\n    \\includegraphics[width=\\textwidth]{figures/collapse/circuit_collapse_probability_min_att_0.95_0.95_strict.pdf}\n    \\caption{Probability of circuit collapse vs increasing noise. \\textbf{Top:} using the baseline definition. This is a reproduction of Figure~\\ref{fig: circuit collapse}. \\textbf{Bottom:} requiring the attention distribution to remain sparse after switching to a different token.}\n    \\label{fig: circuit collapse sparse}\n\\end{figure}\n==== END OF /2406.17837/appendices/main_experiments_extended.tex ====\n==== BEGINNING OF /2406.17837/appendices/pre_vs_qkvnorm.tex ====\n\\section{Additional comparisons between Pre-Norm and QKV-Norm}\n\\label{appendix: Pre vs QKV norm}\n\n\\subsection{Model performance}\n\nTable~\\ref{table: model performance} shows the per-token accuracy performance for the trained \\texttt{Baseline} models. \\texttt{Pre-Norm} and \\texttt{QKV-Norm} have comparable in-distribution per-token accuracies of $91.4\\\n\n\\begin{table}[h]\n\\centering\n\\begin{tabular}{rccc}\nDataset               &    \\texttt{Pre-Norm}    &    \\texttt{QKV-Norm}   \\\\\n\\hline\nIn-distribution       &    $91.38\\pm0.04\\\nOOD (interpolation)   &    $87.46\\pm0.04\\\nOOD (extrapolation)   &    $66.65\\pm0.05\\\n\\end{tabular}\n\\vspace{0.2cm}\n\\caption{Per-token accuracy for the \\texttt{Baseline} models. Dataset configurations are shown in Table~\\ref{table: dataset specifications main}.}\n\\label{table: model performance}\n\\end{table}\n\n\\subsection{Training stability}\n\nChanging the normalisation layer is expected to affect the training rate and stability. To investigate this, Figure~\\ref{fig: training stability scan} shows the training curves for different model sizes and learning rates. The task is configured as presented in Table~\\ref{table: model spec stability}. The \\texttt{Depth} parameter is the number of layers, where brackets indicate the values for an encoder-decoder model. For example, $(2,2)$ means that we use $2$ encoder blocks and $2$ decoder blocks. Each decoder block has a self-attention and a cross-attention layer, and so the total model has $6$ attention layers. A single \\texttt{Depth} value indicates a decoder architecture, with the number of layers shown. \\texttt{Width} is the number of neurons per layer, and \\texttt{Latent width} is the number of neurons on the space of $\\{q,~k_t,v_t\\}$ (called $N_{qkv}$ in section~\\ref{sec: formulation}). Training curves on the top row use a learning rate of $0.001$, whilst the bottom row use a value of $0.0001$. In each panel, two training runs are shown, with different random seeds. \\texttt{Pre-Norm} is shown in blue, and \\texttt{QKV-Norm} in red.\n\nWe find that \\texttt{Pre-Norm} training is unstable for large learning rates and model sizes, as shown by the flat blue curves in the top right hand panels. Similar stabilisation improvements at large learning rate is reported for \\texttt{QK-Norm} in \\cite{wortsman2023smallscale,dehghani2023scaling}, which applies layer normalisation to $\\{q,~k_t\\}$ but not $v_t$, as for \\texttt{QKV-Norm}. However, we note that training large models with a smaller learning rate leads to improved model performance, as shown by the panels on the bottom right. Finally, we note that both methods typically train the model at similar rates, however small model training follows a very different trajectory, with \\texttt{QKV-Norm} learning more slowly at the beginning of training (bottom left panels). There is also some visible evidence that small model training is actually \\textit{less} effective when using \\texttt{QKV-Norm} with a large learning rate (top left panels).\n\n\\begin{figure}[h]\n\\includegraphics[width=\\textwidth]{figures/training_curve_addition_addition_any_masked_categorical_accuracy.pdf}\n\\caption{Training curves when learning the task configuration shown in Table~\\ref{table: model spec stability}.}\n\\label{fig: training stability scan}\n\\end{figure}\n\n\n\\subsection{Attention sparsity}\n\nWe find that our \\texttt{Pre-Norm} models often exploit sparse-attention, whereas models trained with \\texttt{QKV-Norm} do not. Similar behaviour is reported for \\texttt{QK-Norm} in \\cite{DBLP:journals/corr/abs-2010-04245}. For a systematic comparison, Figure~\\ref{fig: attention sparsity distribution} shows a histogram of the maximum attention observed per-distribution (i.e. a histogram of $\\max_i a_i$). When making this plot, we do not consider the first row of the attention matrix, in which the \\texttt{[} token attends fully to itself.\n\nWe see that the \\texttt{Pre-Norm} distribution has a sharp peak at $1$, indicating a significant use of sparse-attention. By contrast, the \\texttt{QKV-Norm} distribution is weighted towards $0$ and has no peak at $1$. To verify this behaviour, Figure~\\ref{fig: attention map prenorm} shows an attention heatmap for a randomly chosen datapoint when using the \\texttt{Baseline} \\texttt{Pre-Norm} model, and Figure~\\ref{fig: attention map qkvnorm} shows the same datapoint for \\texttt{QKV-Norm}. We observe a significantly less sparse attention matrix for \\texttt{QKV-Norm}. Note that \\cite{DBLP:journals/corr/abs-2010-04245} also shows a similar visualisation.\n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=0.9\\textwidth]{figures/baseline_model/max_attn_comparison.pdf}\n\\caption{Distribution of the maximum attention observed per-distribution, i.e. $\\max_i a_i$, in the \\texttt{Baseline} case. We observe that the \\texttt{Pre-Norm} model often utilises sparse-attention, as seen by the peak at $1$. By contrast, \\texttt{QKV-Norm} shows no such peak. Similar behaviour is reported for \\texttt{QK-Norm} in \\cite{DBLP:journals/corr/abs-2010-04245}.}\n\\label{fig: attention sparsity distribution}\n\\end{figure}\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.87\\textwidth]{figures/baseline_model/attention_map_prenorm.pdf}\n\\caption{Attention maps for a random in-distribution example using the \\texttt{Baseline} \\texttt{Pre-Norm} model. Several attention heads create sparse attention distributions.}\n\\label{fig: attention map prenorm}\n\\vspace{0.3cm}\n\\includegraphics[width=0.87\\textwidth]{figures/baseline_model/attention_map_qkvnorm.pdf}\n\\caption{Attention maps for a random in-distribution example using the \\texttt{Baseline} \\texttt{QKV-Norm} model. We observe much less sparsity than in the \\texttt{Pre-Norm} model, shown in Figure~\\ref{fig: attention map prenorm}. Similar behaviour is reported for \\texttt{QK-Norm} in \\cite{DBLP:journals/corr/abs-2010-04245}.}\n\\label{fig: attention map qkvnorm}\n\\end{figure}\n==== END OF /2406.17837/appendices/pre_vs_qkvnorm.tex ====\n==== BEGINNING OF /2406.17837/appendices/experiment_replications.tex ====\n\\section{Main experiments: results with different models}\n\\label{app: model variations}\n\nIn this appendix we reproduce the main experimental results using our model variations. \n\n\\subsection{Embedding lengths}\n\nFigure~\\ref{fig: embedding spread} shows the empirical results demonstrating the attention-weighted spread of embeddings. Figures~\\ref{fig: embedding spread: model variations BEG}-\\ref{fig: embedding spread: model variations END} show the results we obtain when we perform the same analysis using the \\texttt{Alternate} and \\texttt{Large} model variations. In all cases, we observe 90\\\n\n\\begin{figure}[h]\n\\includegraphics[width=\\textwidth]{figures/baseline_model/Embedding_length_comparison_0.0.pdf}\n\\caption{Attention-weighted spread of embeddings at increasing model depth using the \\texttt{Baseline} model and task configuration. This is a replication of Figure~\\ref{fig: embedding spread}.}\n\\label{fig: embedding spread: model variations BEG}\n\\includegraphics[width=\\textwidth]{figures/alternative_model/Embedding_length_comparison_0.0.pdf}\n\\caption{Attention-weighted spread of embeddings at increasing model depth using the \\texttt{Alternate} model and task configuration.}\n\\label{fig: embedding spread: model variations MID}\n\\includegraphics[width=\\textwidth]{figures/large_model/Embedding_length_comparison_0.0.pdf}\n\\caption{Attention-weighted spread of embeddings at increasing model depth using the \\texttt{Large} model and task configuration.}\n\\label{fig: embedding spread: model variations END}\n\\end{figure}\n\n\\subsection{Model stability with simulated inference}\n\nFigure~\\ref{fig: noise sensitivity}(left) shows the stability of the model predictions under simulated interference. Figure~\\ref{fig: noise sensitivity: model variations} shows the results we obtain when we perform the same analysis using the \\texttt{Alternate} model variation. The \\texttt{Large} model was not run due to its high computational load. We find that the \\texttt{Alternate} model has a larger effect size that \\texttt{Baseline}, with a $\\gtrsim20\\\n\nFigure~\\ref{fig: noise sensitivity}(right) compares the stability when we only apply noise to sparse heads (defined as $\\max_i a_i \\geq 95\\\n\nNote that later layers experience both the artificial noise injection as well as perturbation of their inputs due to the compounding of errors caused by noise in the previous layers.\n\n\\begin{figure}[h]\n\\includegraphics[width=\\textwidth]{figures/baseline_model/model_stability_addition.pdf}\n\\caption{Evolution of per-token accuracy as we increase noise on the $L_2$-norms of $\\{q,k_t,m_t\\}$ for the \\texttt{Baseline} model and task configuration.}\n\\includegraphics[width=\\textwidth]{figures/alternative_model/model_stability_addition_alternate.pdf}\n\\caption{Evolution of per-token accuracy as we increase noise on the $L_2$-norms of $\\{q,k_t,m_t\\}$ for the \\texttt{Alternate} model and task configuration.}\n\\label{fig: noise sensitivity: model variations}\n\\end{figure}\n\n\\begin{figure}[h]\n\\includegraphics[width=\\textwidth]{figures/baseline_model/model_stability_addition_sparse.pdf}\n\\caption{Evolution of per-token accuracy as we increase noise on the $L_2$-norms of $\\{q,k_t,m_t\\}$ for the \\texttt{Baseline} model and task configuration.}\n\\includegraphics[width=\\textwidth]{figures/alternative_model/model_stability_addition_alternate_sparse.pdf}\n\\caption{Evolution of per-token accuracy as we increase noise on the $L_2$-norms of $\\{q,k_t,m_t\\}$ for the \\texttt{Alternate} model and task configuration.}\n\\label{fig: noise sensitivity: model variations sparse}\n\\end{figure}\n\n\\subsection{Circuit collapse}\n\nFigure~\\ref{fig: circuit collapse} shows the probability of circuit collapse. This is the probability that an attention distribution with no noise selects embedding $i$ with high probability $a_i \\geq 95\\\n\n\\begin{figure}[h]\n\\includegraphics[width=\\textwidth]{figures/collapse/circuit_collapse_probability_min_att_0.95_0.0_loose.pdf}\n\\caption{Probability of \\textit{circuit collapse} vs increasing noise using the \\texttt{Baseline} model and task configuration. This is a replication of Figure~\\ref{fig: circuit collapse}.}\n\\includegraphics[width=\\textwidth]{figures/alternative_model/circuit_collapse_probability_min_att_0.95_0.0_loose.pdf}\n\\caption{Probability of \\textit{circuit collapse} vs increasing noise using the \\texttt{Alternate} model and task configuration.}\n\\includegraphics[width=\\textwidth]{figures/large_model/circuit_collapse_probability_min_att_0.95_0.0_loose.pdf}\n\\caption{Probability of \\textit{circuit collapse} vs increasing noise using the \\texttt{Large} model and task configuration.}\n\\label{fig: circuit collapse: model variations}\n\\end{figure}\n==== END OF /2406.17837/appendices/experiment_replications.tex ====\n==== BEGINNING OF /2406.17837/appendices/experimental_setup.tex ====\n\\section{Experimental setup}\n\\label{appendix: experimental setup}\n\n\\newcommand{\\data}[1]{\\textcolor{Maroon}{\\texttt{{#1}}}}\n\n\\subsection{Data}\n\n\nDue to the task nature, we do not require static datasets and so generate both train and test data on-the-fly. This alleviates storage and memory concerns for long training runs in which a static dataset would have to be large. Datasets are reproducible through configuration of the environment and global random seed, which is used to manually control the random seeds of \\texttt{Python}, \\texttt{TensorFlow} \\cite{tensorflow2015-whitepaper}, and \\texttt{NumPy} \\cite{harris2020array}. This also reproduces the model initialisation. \n\n\\subsection{Task specification}\n\nWe consider an integer addition task, where each character is a base-10 numeral \\data{0}-\\data{9}, mathematical operator \\{\\data{+}, \\data{-}, \\data{=}, \\data{N}\\}, or special character \\{\\data{[}, \\data{]}, \\data{*}\\}. The \\data{N} operator signifies that the following integer is \\textit{negative}, and is used to avoid overloading notation with the \\data{-} operator, which means \\textit{minus}. The special characters are the begin-sequence token \\data{[}, end-sequence token \\data{]}, and mask character \\data{*}. Input sequences in the same batch are right-padded with mask tokens to the same length, which do not contribute to the model. Characters that are masked in the output do not contribute to the evaluation metrics. We tokenise per-character so the model does not need to disambiguate different representations for identical patterns (e.g. if the number \\textcolor{Maroon}{\\texttt{112}} is tokenised as [11,2], and \\textcolor{Maroon}{212} is tokenised as [2,12], then the pattern \\textcolor{Maroon}{\\texttt{12}} has a context-dependent representation). The token dictionary has a length of 17. \n\nFor a decoder architecture, the model is a sequence-sequence transformer and each datapoint has a question-answer structure separated by the \\data{=} character, e.g. the first datapoint is:\n\\begin{equation}\n    \\data{[453+16+17-N846=1332}  ~~~~\\rightarrow~~~~ \\data{***************1332]}\n\\end{equation}\nThe model must therefore predict the numerical outputs and the \\data{]} token. For an encoder-decoder architecture, the encoder input is the \\textit{question} and the decoder performs next-token prediction over the \\textit{answer}, e.g.\n\\begin{equation}\n    \\data{[453+16+17-N846]}~\\text{~(encoder)},~~\\data{[1332}~\\text{~(decoder)}  ~~~~\\rightarrow~~~~ \\data{1332]}~\\text{~(decoder)}\n\\end{equation}\n\nTo help visualise the task, Figure~\\ref{fig: screenshot model begin} shows the predictions of the baseline \\texttt{Pre-Norm} model after 1 epoch. Figure~\\ref{fig: screenshot model end} shows the fully-trained model, to help visualise the attainable in-distribution performance. The final epoch per-token accuracy is logged as 92\\\n\n\\begin{figure}[!]\n\\centering\n\\includegraphics[width=0.8\\textwidth]{figures/model_training/predictions_epoch0.png}\n\\caption{\\texttt{Baseline} \\texttt{Pre-Norm} model predictions after 1 training epoch.}\n\\label{fig: screenshot model begin}\n\\vspace{0.5cm}\n\\includegraphics[width=0.8\\textwidth]{figures/model_training/predictions_final_epoch.png}\n\\caption{\\texttt{Baseline} \\texttt{Pre-Norm} model predictions after training.}\n\\label{fig: screenshot model end}\n\\vspace{0.5cm}\n\\includegraphics[width=0.8\\textwidth]{figures/large_model/predictions_epoch0.png}\n\\caption{\\texttt{Large} \\texttt{Pre-Norm} model predictions after 1 training epoch.}\n\\label{fig: screenshot large model begin}\n\\vspace{0.5cm}\n\\includegraphics[width=0.8\\textwidth]{figures/large_model/predictions_final_epoch.png}\n\\caption{\\texttt{Large} \\texttt{Pre-Norm} model predictions after training.}\n\\label{fig: screenshot large model end}\n\\end{figure}\n\n\n\\subsection{Data-generation process}\n\nOne advantage of this task is the ability to modulate its complexity. Each dataset is defined by two hyperparameters:\n\n\\begin{tabular}{cll}\n    Dataset parameter    &   Example   &   Description   \\\\\n    \\hline\n    $N$   &   [3, 4, 6]    &   The allowed number of integers per-sequence  \\\\\n    $L$   &   [2, 3]   &   The allowed number of digits per-integer  \\\\\n\\end{tabular}\n\nEach datapoint is generated by uniformly sampling a value of $N$, then uniformly sampling a value of $L$ for each integer. This ensures that examples are not simply dominated by integers with the maximum number of digits. Each integer is uniformly sampled from all positive and negative integers with that length. Between each integer, an operator is uniformly sampled from the list $[+, -]$. For example, the datapoint \n\\begin{equation}\n    \\data{[453+16+17-N846=1332}  ~~~~\\rightarrow~~~~ \\data{***************1332]}\n\\end{equation}\nwas generated by sampling a value of $N=4$ to determine that the sum contains four integers, then sampling four values of $L=[3,2,2,3]$ to determine their lengths, then sampling the numbers $[453, 16, 17, N846]$ and operators $[+, +, -]$. The inclusion of subtraction, addition of negative numbers, and double-negatives is intended to emphasise solutions that parse the \\textit{context} of each digit within the sum.\n\n\\subsection{Train/test specifications}\n\nTable~\\ref{table: dataset specifications main} shows the $N$ and $L$ parameters used for the \\texttt{Baseline} and \\texttt{Alternate} experiments. We also show the number of datapoints, and the per-datapoint sampling probability. This is a range, with higher probabilites for the simpler sums. Table~\\ref{table: dataset specifications large} shows the task specification for the \\texttt{Large} model variation, which is trained on a more complex setting. We also perform a scan over model size and learning rate to compare the training stability of \\texttt{Pre-Norm} and \\texttt{QKV-Norm}. These experiments were performed using an earlier problem configuration shown in Table~\\ref{table: dataset specifications scan}.\n\n\\begin{table}[h]\n\\centering\n\\begin{tabular}{rllll}\n    Dataset    &   $N$   &   $L$   &   Num datapoints   &   Datapoint probability   \\\\\n    \\hline\n    Train                     &   $[3, 4, 6]$   &   $[2, 3]$   &   110M, acc=90\\\n    Validation     &   $[5]$         &   $[2, 3]$   &   6.4k   &   $1\\times 10^{-14}$ to $1\\times 10^{-19}$   \\\\\n    \\hline\n    In-distribution         &   $[3, 4, 6]$   &   $[2, 3]$   &   128k   &   $2\\times 10^{-9}$ to $5\\times10^{-24}$   \\\\\n    OOD (interpolation)     &   $[5]$         &   $[2, 3]$   &   128k   &   $1\\times 10^{-14}$ to $1\\times 10^{-19}$   \\\\\n    OOD (extrapolation)     &   $[7, 8, 9]$   &   $[2, 3]$   &   128k   &   $7\\times 10^{-21}$ to $1\\times 10^{-35}$   \\\\\n\\end{tabular}\n\\vspace{0.3cm}\n\\caption{Dataset configurations used for \\texttt{Baseline} and \\texttt{Alternate} results.}\n\\label{table: dataset specifications main}\n\\vspace{0.5cm}\n\\begin{tabular}{rllll}\n    Dataset    &   $N$   &   $L$   &   Num datapoints   &   Datapoint probability   \\\\\n    \\hline\n    Train                     &   $[4, 5, 7, 8]$   &   $[3, 4, 5]$   &   25M   &   $4\\times 10^{-17} $ to $3\\times10^{-49}$   \\\\\n    Validation     &   $[6]$         &   $[3, 4, 5]$   &   6.4k   &   $2\\times 10^{-24}$ to $2\\times 10^{-36}$   \\\\\n    \\hline\n    In-distribution         &   $[4, 5, 7, 8]$   &   $[3, 4, 5]$   &   128k   &   $4\\times 10^{-17} $ to $3\\times10^{-49}$   \\\\\n    OOD (interpolation)     &   $[6]$         &   $[3, 4, 5]$   &   128k   &   $2\\times 10^{-24}$ to $2\\times 10^{-36}$   \\\\\n    OOD (extrapolation)     &   $[9, 10, 11]$   &   $[3, 4, 5]$   &   128k   &   $3\\times 10^{-37}$ to $3\\times 10^{-67}$   \\\\\n\\end{tabular}\n\\vspace{0.3cm}\n\\caption{Dataset configurations used for \\texttt{Large} results.}\n\\label{table: dataset specifications large}\n\\vspace{0.5cm}\n\\begin{tabular}{rlll}\n    Dataset    &   $N$   &   $L$   &   Datapoint probability   \\\\\n    \\hline\n    Train set                     &   $[3, 4, 5, 7]$   &   $[3, 4, 5]$   &   $4\\times 10^{-13}$ to $3\\times 10^{-43}$   \\\\\n    In-distribution         &   $[3, 4, 5, 7]$   &   $[3, 4, 5]$   &   $4\\times 10^{-13}$ to $3\\times 10^{-43}$   \\\\\n\\end{tabular}\n\\vspace{0.3cm}\n\\caption{Dataset configurations used for training stability results (Figure~\\ref{fig: training stability scan}).}\n\\label{table: dataset specifications scan}\n\\end{table}\n\nWe halt training according to wall time, which leads to a range of observed dataset sizes. Model convergence may also occur much earlier. We therefore show an order-of-magnitude estimate for the number of observed datapoints, as well as the point at which the baseline model reaches 90\\\n\nNote that our data-generation strategy does not ensure that training examples are exclusive (there may be repetitions), nor that the in-distribution test set does not contain overlap with training examples. The final column is therefore important, because it demonstrates that the highest per-datapoint sampling probability is $2\\times10^{-9}$, whilst the model converges with $\\mathcal{O}(10^7)$ datapoints and observes $\\mathcal{O}(10^8)$ in total. Since the datapoint probability is $2\\times10^{-9}$ for the simplest configurations and $5\\times10^{-25}$ for the most complicated, this ensures that the in-distribution evaluation metric is dominated by novel examples. The validation set is only used for visual inspection of model behaviour during training, as in Figures~\\ref{fig: screenshot model begin}-\\ref{fig: screenshot large model end}.\n\n\\subsection{Model specification (main experiments)}\n\nWe use a decoder architecture, meaning that the dot-product self-attention layers are causally masked such that token $t$ can only attend to tokens $\\leq t$. The model has the following structure:\n\\begin{equation*}\n\\begin{matrix} \n    \\texttt{Embedding + positional encoding} \\\\\n    \\downarrow \\\\\n    N_{layer} \\times \n    \\begin{bmatrix} \n    \\texttt{Attention block} \\\\ \n    \\downarrow \\\\\n    \\texttt{Feed-forward block~(ReLU)} \\\\\n    \\end{bmatrix} \\\\\n    \\downarrow \\\\\n    \\texttt{Multi-layer perceptron~(ReLU)} \\\\\n    \\downarrow \\\\\n    \\texttt{Predicted~logits} \\\\\n\\end{matrix}\n\\end{equation*}\n\n\\vspace{0.2cm}\n\\textbf{Embedding + positional encoding} ~ We initialise each token embedding as $x = x_{type} + x_{pos}$, where $x_{type}$ is a token embedding with $N_{emb}$ elements, and $x_{pos}$ use cyclic positional encodings of the same form as the original transformer architecture \\cite{DBLP:journals/corr/VaswaniSPUJGKP17}, with $N_{freq}$ frequencies initialised as a base $e$ log-series between periods of $3$ and $1k$ tokens. For each sequence, all position indices are simultaneously offset by a random integer between $0$ and $50$. This augmentation is designed to encourage the use of \\textit{relative} positions rather than absolute. The frequencies are then left as trainable parameters. The positional encodings contribute the first $2N_{freq}$ components of $x_{pos}$, and the remaining are set to $0$. This configuration guarantees that the token embeddings and positional encodings can be made orthogonal in the first layer, and $x_{pos}$ have constant $L_2$-norm, consistent with our theoretical structure.\n\n\\vspace{0.2cm}\n\\textbf{Attention block} ~ $N_{layer}$ is the number of residual blocks of our model, where our baseline is $N_{layer}=10$. The update is as formulated in section~\\ref{sec: formulation}, where $H$ is the number of parallel attention heads per layer. Since the embeddings have length $N_{emb}$, we must have $N_x=N_{emb}$, whilst the latent dimension $N_{qkv}$ is configurable. Either the \\texttt{Pre-Norm} or \\texttt{QKV-Norm} strategy is used, as configured.\n\n\\vspace{0.2cm}\n\\textbf{Feed-forward block} ~ The feed-forward blocks update embeddings using the function $x \\rightarrow x + FF(\\texttt{LayerNorm}(x))$, where $FF$ is a dense network with one hidden layer of size $N_{ff}$. The network uses a \\texttt{ReLU} \\cite{agarap2019deep} activation function on the intermediate layer, followed by a linear projection back onto embedding space. To maintain consistency with other models, we apply \\texttt{LayerNorm} at the input to $FF$. Both \\texttt{LayerNorm} and $FF$ use bias parameters.\n\n\\vspace{0.2cm}\n\\textbf{Multi-layer perceptron} ~ The final embeddings $x$ are mapped onto token logits $y$ using the function $y = MLP(\\texttt{LayerNorm}(x))$, where $MLP$ is a multi-layer perceptron with two hidden layers of size $N_{MLP}$ and ReLU activation. \nThe final layer is a linear projection onto the space of logits, which has length 17. For the training stability scan in Figure~\\ref{fig: training stability scan}, the MLP has three hidden layers instead.\n\n\\vspace{0.2cm}\n\\textbf{Hyperparameters} ~ Table~\\ref{table: model spec main} shows the hyperparameters used to configure the networks of the main experiments. Table~\\ref{table: model spec stability} show the hyperparameters used for the training stability analysis. This experiment also uses encoder-decoder models, following the same setup as the original transformer architecture \\cite{DBLP:journals/corr/VaswaniSPUJGKP17} and with the layer configurations listed here.\n\n\\begin{table}[h]\n\\centering\n\\begin{tabular}{rlllllllll}\n    Model   &   $N_{freq}$   &   $N_{emb}$   &   $N_{layer}$   &   $H$   &   $N_{qkv}$   &   $N_{ff}$   &   $N_{MLP}$   &   seed   \\\\\n\\hline\n    \\texttt{Baseline}        &   32   &   512    &   10    &   12   &   64   &   512   &   2$\\times$512   &   100   \\\\\n    \\texttt{Alternative}   &   32   &   512    &   8     &   12   &   64   &   512   &   2$\\times$512   &   100   \\\\\n    \\texttt{Large}   &   32   &   1024   &   12    &   16   &   64   &   512   &   2$\\times$512   &   100   \\\\\n\\end{tabular}\n\\vspace{0.2cm}\n\\caption{Model hyperparameters for main experiments (i.e. other than training stability). \\texttt{Baseline} is used for the main results presented in section~\\ref{sec: experimental results} (short). \\texttt{Alternate} and \\texttt{Large} are presented in appendix~\\ref{app: model variations} to show reproducibility of observations.}\n\\label{table: model spec main}\n\\vspace{0.2cm}\n\\begin{tabular}{rlllllllll}\n    Model   &   $N_{freq}$   &   $N_{emb}$   &   $N_{layer}$   &   $H$   &   $N_{qkv}$   &   $N_{ff}$   &   $N_{MLP}$   &   seed   \\\\\n\\hline\n    \\texttt{All}        &   16   &   -   &   -    &   12   &   -   &   512   &   3$\\times$512   &   1,2   \\\\\n\\end{tabular}\n\\vspace{0.2cm}\n\\caption{Model hyperparameters for training stability experiments. Empty parameters are varied per-model and displayed in Figure~\\ref{fig: training stability scan}.}\n\\label{table: model spec stability}\n\\end{table}\n\n\\vspace{0.2cm}\n\\textbf{Loss} ~ The loss function is \\texttt{categorical cross entropy}, calculated from the output logits.\n\n\\subsection{Model initialisation}\n\nWe use a custom initialisation strategy to give control over the initial state of the model. In particular, we use \\texttt{Checkpoint} layers to ensure that the initial states are comparable between \\texttt{Pre-Norm} and \\texttt{QKV-Norm}. This ensures that any observed differences are driven by the normalisation function, rather than being confounded by the layer placement creating more/less favourable initial conditions. \n\n\\texttt{Checkpoint} layers are calibrated on the first training batch immediately prior to training. They use this data to measure the standard deviation at that point, and calculate a scale factor that fixes the standard deviation to a pre-defined hyperparameter $\\sigma$. All subsequent passes through the layer simply apply this scale factor. This ensures that the model is initialised with a standard deviation of $\\sigma$ at that point.\n\nWe apply \\texttt{Checkpoint} layers to the token embeddings $x_{type}$ ($\\sigma_{type}=0.5$), and the initial embeddings $x$ ($\\sigma_x=1.0$), ensuring they are relatively balanced and unit scale. In every attention layer, we apply \\texttt{Checkpoint} layers to re-calibrate the possibly-Pre-normalised embeddings to $\\sigma_x$ immediately before applying the $W_Q$, $W_K$, and $W_V$ operators. This counteracts the effect that transformer necessarily increases the embedding variance throughout the model at initialisation. We apply \\texttt{Checkpoint} layers to $w_t$ in every attention layer, with constant $\\sigma_w=0.1$. This controls the variance on the initial-state attention distribution. We apply \\texttt{Checkpoint} layers to $\\Delta x$ in every attention layer, with constant $\\sigma_{\\Delta x}=0.05$, calibrating it with respect to $x$. \n\nIn the attention layer, we use uniform initialisation of the weight matrices $W_Q$, $W_K$, $W_V$, and $W_O$. The limits are configured to ensure that the initial state standard deviations on $w_t$ and $\\Delta x$ are close to their target values. Defining $\\sigma_{qk} \\triangleq \\sqrt[4]{\\frac{\\sigma_w}{N_{qkv}^3}}$, the limits are calculated as follows:\n\n\\begin{tabular}{rl}\n    Weight   &   Limits   \\\\\n    \\hline\n    $W_Q$   &   $\\pm \\sqrt{3} \\sigma_{qk}$   \\\\\n    $W_K$   &   $\\pm \\sqrt{3} \\sigma_{qk}$   \\\\\n    $W_V$   &   $\\pm \\sqrt\\frac{3}{N_{qkv}}$    \\\\\n    $W_O$   &   $\\pm \\sqrt{\\frac{3}{H N_{qkv}}}$     \\\\\n\\end{tabular}\n\n\nHowever, we note that this initialisation is superseded by the calibration of the \\texttt{Checkpoint} layers for determining the initial state, and we include it only to promote numerical stability. All other feed-forward layers use \\texttt{Glorot uniform} \\cite{pmlr-v9-glorot10a} initialisation, as implemented in \\texttt{Keras} \\cite{chollet2015keras}. Normalisation gain parameters are initialised to $1$ and biases, where used, to $0$.\n\n\\subsection{Training algorithm}\n\nWe train using the \\texttt{AdamW} optimiser \\cite{loshchilov2019decoupled} with learning rate $3\\times10^{-4}$ and weight decay of $0.01$, with all other parameters following their default values  in \\texttt{TensorFlow+Keras v2.15.0}. Each epoch consists of $2000$ batches of $128$ datapoints. For the main experiments and model variations, we use an \\textit{adaptive learning rate decay} strategy. This means that the learning rate is multiplied by a factor of $0.5$ if the training loss does not improve for $3$ consecutive epochs. We find that this balances training speed with improved performance by using small learning rates later in training. Training is halted after two days of wall time, which we observe to allow model convergence, as shown in Figure~\\ref{fig: model train curves}. For the model stability scan, training is run for $60$ hours, and learning rate is not allowed to decay (stability with respect to learning rate being one of the targets of study).\n\n\\begin{figure}[h]\n\\includegraphics[width=\\textwidth]{figures/model_training/training_curves.pdf}\n\\caption{Model training curves for the \\texttt{Baseline} \\texttt{Pre-Norm} configuration.}\n\\label{fig: model train curves}\n\\end{figure}\n\n\n\\subsection{Computational resources}\n\nThe main experiments are all performed on a single \\texttt{Nvidia v100-SXM2-16GB (Volta)} GPU. The scan of models used for the stability analysis were trained on a batch cluster with a variety of compute nodes, using $8$ cores per training run. A representative compute node is \\texttt{212-core Intel Xeon E5-2690 v3 @ 2.60GHz + 128GB RAM}.\n\n\\subsection{Environment details}\n\n\nThe main contributing package versions are as follows:\n\n\\begin{tabular}{rl}\n    Package    &    Version    \\\\\n    \\hline\n    Python       &   \\texttt{3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]}  \\\\\n    TensorFlow   &   \\texttt{2.15.0}  \\\\\n    Keras        &   \\texttt{2.15.0}  \\\\\n    NumPy        &   \\texttt{1.26.2}  \\\\\n\\end{tabular}\n\n\n\n\n\n\n\n\n==== END OF /2406.17837/appendices/experimental_setup.tex ====",
            "statements": {
                "definitions": [],
                "axioms": [],
                "lemmas": [],
                "theorems": [
                    {
                        "statement_id": "362f460c-83b0-4db5-b727-fb8cf1c91297",
                        "paper_id": "fa48a779-dedb-4614-9ac9-e052c1896ac6",
                        "library_nr": 4,
                        "library_name": "Theorem 4",
                        "title": "Orthogonal Attention Subspaces",
                        "statement_original_tex": "\\begin{theorem}\n    \\texttt{No-Norm}: If two heads with finite non-zero temperature attend to different semantic subspaces, the subspaces must be linearly independent $\\mathbb{S}^{N_\\alpha}_\\alpha \\equiv \\mathbb{R}^{N_\\alpha}$. Corollary: $W_{QK}$ is a low-rank matrix with (left and right) null-spaces that span all non-attended information.\n\\label{theorem: structure: no-norm}\n\\end{theorem}",
                        "statement_html": "$\\texttt{No-Norm}$: If two heads with finite non-zero temperature attend to different semantic subspaces, the subspaces must be linearly independent $\\mathbb{S}^{N_\\alpha}_\\alpha \\equiv \\mathbb{R}^{N_\\alpha}$. Corollary: $W_{QK}$ is a low-rank matrix with (left and right) null-spaces that span all non-attended information.",
                        "statement_type": "theorem",
                        "statement_motivation_html": "The $\\texttt{No-Norm}$ statement is crucial in understanding the behavior of attention mechanisms in neural networks, particularly in the context of multi-head attention. It ensures that different heads focus on distinct, linearly independent subspaces, which prevents redundancy and promotes efficient information processing. The corollary about $W_{QK}$ being a low-rank matrix highlights that it filters out non-attended information, making the model more efficient by concentrating on relevant features. This is particularly useful in designing and analyzing transformer models and other architectures that rely on attention mechanisms.",
                        "html_url": "library/theorems/theorem_4/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "6e4c7941-ca98-4989-a8ea-83d4f1fb7e8f",
                            "paper_id": null,
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\textit{Proof.} ~\nLet $\\theta_A$ and $\\theta_B$ be co-ordinates for the subspaces of $x$ attended to by heads A and B respectively, and $\\phi$ be all other information. Let $\\theta_A\\perp\\theta_B\\perp\\phi$ and $x \\perp y_t$, where $\\perp$ denotes independence. Without loss of generality, write\n\\begin{equation}\n    x(\\theta_A,\\theta_B,\\phi) ~=~ x_A(\\theta_A) ~+~ x_B(\\theta_B) ~+~ x_{other}(\\theta_A,\\theta_B,\\phi)\n\\end{equation}\nThen write\n\\begin{equation}\n\\begin{split}\n    w_t^{(A)}(\\theta_A) ~&=~ \\left(W_{QK}^{(A)} y_t\\right)^T x(\\theta_A,\\theta_B,\\phi)  \\\\\n    &=~ \\left(W_{QK}^{(A)} y_t\\right)^T x_A(\\theta_A) ~+~ \\left(W_{QK}^{(A)} y_t\\right)^T x_B(\\theta_B) ~+~ \\left(W_{QK}^{(A)} y_t\\right)^T x_{other}(\\theta_A,\\theta_B,\\phi) \\\\\n\\end{split}\n\\end{equation}\nwhich requires $\\left(W_{QK}^{(A)} y_t\\right)^T x_B(\\theta_B)=0$ and $\\left(W_{QK}^{(A)} y_t\\right)^T x_{other}(\\theta_A,\\theta_B,\\phi)=0$, since any cancellation between the two terms must be independent of $\\theta_A,\\phi$ and so can be absorbed entirely into the function $x_{B}(\\theta_B)$. This means that $x_B(\\theta_B)$ and $x_{other}(\\theta_A,\\theta_B,\\phi)$ must both be orthogonal to $W_{QK}^{(A)} y_t$, meaning that they reside on the \\textit{left null space} of $W_{QK}^{(A)}$, or are projected by ${W_{QK}^{(A)}}^T$onto a null space of $y_t$.\n\nHead A can only attend to $\\theta_A$ if $x_A(\\theta_A)$ it is not on either of these null spaces, meaning that $x_A(\\theta_A)$ is linearly independent of $x_{B}(\\theta_B)$ and $x_{other}(\\theta_A,\\theta_B,\\phi)$. Likewise for head B\n\\begin{equation}\n\\begin{split}\n    w_t^{(B)}(\\theta_B) ~&=~ \\left(W_{QK}^{(B)} y_t\\right)^T x(\\theta_A,\\theta_B,\\phi)  \\\\\n    &=~ \\left(W_{QK}^{(B)} y_t\\right)^T x_A(\\theta_A) ~+~ \\left(W_{QK}^{(B)} y_t\\right)^T x_B(\\theta_B) ~+~ \\left(W_{QK}^{(B)} y_t\\right)^T x_{other}(\\theta_A,\\theta_B,\\phi) \\\\\n\\end{split}\n\\end{equation}\nrequires that $x_{B}(\\theta_B)$ is linearly independent of both $x_A(\\theta_A)$ and $x_{other}(\\theta_A,\\theta_B,\\phi)$. Since $x_{other}$ resides on both null spaces, it is linearly independent of both $x_A(\\theta_A)$ and $x_B(\\theta_B)$, and may be seen as a third subspace that passes information through to subsequent layers.\n\nWe can also write $w_t = \\left(W_{QK}^T x\\right)^T y_t$, and so the same argument also holds for subspaces on $y_t$. In this case, non-attended subspaces are spanned by the \\textit{right null space} of $W_{QK}$.\n",
                            "statement_html": "Let $\\theta_A$ and $\\theta_B$ be co-ordinates for the subspaces of $x$ attended to by heads A and B respectively, and $\\phi$ be all other information. Let $\\theta_A\\perp\\theta_B\\perp\\phi$ and $x \\perp y_t$, where $\\perp$ denotes independence. Without loss of generality, write\n\\begin{equation}\n    x(\\theta_A,\\theta_B,\\phi) ~=~ x_A(\\theta_A) ~+~ x_B(\\theta_B) ~+~ x_{other}(\\theta_A,\\theta_B,\\phi)\n\\end{equation}\nThen write\n\\begin{equation}\n\\begin{split}\n    w_t^{(A)}(\\theta_A) ~&=~ \\left(W_{QK}^{(A)} y_t\\right)^T x(\\theta_A,\\theta_B,\\phi)  \\\\\n    &=~ \\left(W_{QK}^{(A)} y_t\\right)^T x_A(\\theta_A) ~+~ \\left(W_{QK}^{(A)} y_t\\right)^T x_B(\\theta_B) ~+~ \\left(W_{QK}^{(A)} y_t\\right)^T x_{other}(\\theta_A,\\theta_B,\\phi) \\\\\n\\end{split}\n\\end{equation}\nwhich requires $\\left(W_{QK}^{(A)} y_t\\right)^T x_B(\\theta_B)=0$ and $\\left(W_{QK}^{(A)} y_t\\right)^T x_{other}(\\theta_A,\\theta_B,\\phi)=0$, since any cancellation between the two terms must be independent of $\\theta_A,\\phi$ and so can be absorbed entirely into the function $x_{B}(\\theta_B)$. This means that $x_B(\\theta_B)$ and $x_{other}(\\theta_A,\\theta_B,\\phi)$ must both be orthogonal to $W_{QK}^{(A)} y_t$, meaning that they reside on the <em>left null space</em> of $W_{QK}^{(A)}$, or are projected by ${W_{QK}^{(A)}}^T$ onto a null space of $y_t$.\n\nHead A can only attend to $\\theta_A$ if $x_A(\\theta_A)$ it is not on either of these null spaces, meaning that $x_A(\\theta_A)$ is linearly independent of $x_{B}(\\theta_B)$ and $x_{other}(\\theta_A,\\theta_B,\\phi)$. Likewise for head B\n\\begin{equation}\n\\begin{split}\n    w_t^{(B)}(\\theta_B) ~&=~ \\left(W_{QK}^{(B)} y_t\\right)^T x(\\theta_A,\\theta_B,\\phi)  \\\\\n    &=~ \\left(W_{QK}^{(B)} y_t\\right)^T x_A(\\theta_A) ~+~ \\left(W_{QK}^{(B)} y_t\\right)^T x_B(\\theta_B) ~+~ \\left(W_{QK}^{(B)} y_t\\right)^T x_{other}(\\theta_A,\\theta_B,\\phi) \\\\\n\\end{split}\n\\end{equation}\nrequires that $x_{B}(\\theta_B)$ is linearly independent of both $x_A(\\theta_A)$ and $x_{other}(\\theta_A,\\theta_B,\\phi)$. Since $x_{other}$ resides on both null spaces, it is linearly independent of both $x_A(\\theta_A)$ and $x_B(\\theta_B)$, and may be seen as a third subspace that passes information through to subsequent layers.\n\nWe can also write $w_t = \\left(W_{QK}^T x\\right)^T y_t$, and so the same argument also holds for subspaces on $y_t$. In this case, non-attended subspaces are spanned by the <em>right null space</em> of $W_{QK}$.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "In this proof, we explore the independence and orthogonality of subspaces attended to by different heads in a multi-head attention mechanism. Here are the steps broken down:\n<br>\n<br>1. <i></i>Define Coordinates and Independence<i></i>:\n<br>   - Let \\(\\theta_A\\) and \\(\\theta_B\\) represent the coordinates for the subspaces of \\(x\\) attended to by heads A and B, respectively.\n<br>   - Let \\(\\phi\\) represent all other information.\n<br>   - Assume \\(\\theta_A \\perp \\theta_B \\perp \\phi\\) and \\(x \\perp y_t\\), where \\(\\perp\\) denotes independence.\n<br>\n<br>2. <i></i>Express \\(x\\) in Terms of Subspaces<i></i>:\n<br>   - Write \\(x\\) as a sum of functions of \\(\\theta_A\\), \\(\\theta_B\\), and \\(\\phi\\):\n<br>     \\[\n     x(\\theta_A, \\theta_B, \\phi) = x_A(\\theta_A) + x_B(\\theta_B) + x_{other}(\\theta_A, \\theta_B, \\phi)\n     \\]\n<br>\n<br>3. <i></i>Formulate Attention Weights for Head A<i></i>:\n<br>   - Write the attention weight \\(w_t^{(A)}(\\theta_A)\\) for head A:\n<br>     \\[\n     \\begin{split}\n     w_t^{(A)}(\\theta_A) &= \\left(W_{QK}^{(A)} y_t\\right)^T x(\\theta_A, \\theta_B, \\phi) \\\\\n     &= \\left(W_{QK}^{(A)} y_t\\right)^T x_A(\\theta_A) + \\left(W_{QK}^{(A)} y_t\\right)^T x_B(\\theta_B) + \\left(W_{QK}^{(A)} y_t\\right)^T x_{other}(\\theta_A, \\theta_B, \\phi)\n     \\end{split}\n     \\]\n<br>\n<br>4. <i></i>Orthogonality Conditions for Head A<i></i>:\n<br>   - For the terms involving \\(\\theta_B\\) and \\(\\phi\\) to be zero, they must be orthogonal to \\(W_{QK}^{(A)} y_t\\):\n<br>     \\[\n     \\left(W_{QK}^{(A)} y_t\\right)^T x_B(\\theta_B) = 0 \\quad \\text{and} \\quad \\left(W_{QK}^{(A)} y_t\\right)^T x_{other}(\\theta_A, \\theta_B, \\phi) = 0\n     \\]\n<br>   - This implies \\(x_B(\\theta_B)\\) and \\(x_{other}(\\theta_A, \\theta_B, \\phi)\\) reside in the left null space of \\(W_{QK}^{(A)}\\).\n<br>\n<br>5. <i></i>Independence of \\(x_A(\\theta_A)\\)<i></i>:\n<br>   - Head A can only attend to \\(\\theta_A\\) if \\(x_A(\\theta_A)\\) is not in the null spaces, meaning \\(x_A(\\theta_A)\\) is linearly independent of \\(x_B(\\theta_B)\\) and \\(x_{other}(\\theta_A, \\theta_B, \\phi)\\).\n<br>\n<br>6. <i></i>Formulate Attention Weights for Head B<i></i>:\n<br>   - Similarly, write the attention weight \\(w_t^{(B)}(\\theta_B)\\) for head B:\n<br>     \\[\n     \\begin{split}\n     w_t^{(B)}(\\theta_B) &= \\left(W_{QK}^{(B)} y_t\\right)^T x(\\theta_A, \\theta_B, \\phi) \\\\\n     &= \\left(W_{QK}^{(B)} y_t\\right)^T x_A(\\theta_A) + \\left(W_{QK}^{(B)} y_t\\right)^T x_B(\\theta_B) + \\left(W_{QK}^{(B)} y_t\\right)^T x_{other}(\\theta_A, \\theta_B, \\phi)\n     \\end{split}\n     \\]\n<br>\n<br>7. <i></i>Orthogonality Conditions for Head B<i></i>:\n<br>   - For head B, \\(x_B(\\theta_B)\\) must be linearly independent of \\(x_A(\\theta_A)\\) and \\(x_{other}(\\theta_A, \\theta_B, \\phi)\\).\n<br>\n<br>8. <i></i>Conclusion on Subspaces<i></i>:\n<br>   - Since \\(x_{other}\\) resides in both null spaces, it is linearly independent of both \\(x_A(\\theta_A)\\) and \\(x_B(\\theta_B)\\), acting as a third subspace passing information to subsequent layers.\n<br>\n<br>9. <i></i>Generalization to \\(y_t\\)<i></i>:\n<br>   - The same argument applies to subspaces on \\(y_t\\), where non-attended subspaces are spanned by the right null space of \\(W_{QK}\\).\n<br>\n<br>This proof demonstrates how the independence and orthogonality of subspaces ensure that different heads in a multi-head attention mechanism attend to distinct parts of the input, facilitating effective parallel processing."
                        }
                    },
                    {
                        "statement_id": "f16d8074-27e3-4f2d-b6c0-73826fbee538",
                        "paper_id": "fa48a779-dedb-4614-9ac9-e052c1896ac6",
                        "library_nr": 5,
                        "library_name": "Theorem 5",
                        "title": "Orthogonal Spheres Theorem",
                        "statement_original_tex": "\\begin{theorem}\n    \\texttt{Pre-Norm}: Semantic subspaces must be represented as orthogonal spheres $\\mathbb{S}^{N_\\alpha}\\equiv\\mathcal{S}^{N_\\alpha-1}$ defined using the $L_2$-norm. Corollary: if either orthogonality or constant-norm are violated, semantic subspaces interfere through a multiplicative factor on $w_t$.\n\\label{theorem: structure: pre-norm}\n\\end{theorem}",
                        "statement_html": "$\\texttt{Pre-Norm}$: Semantic subspaces must be represented as orthogonal spheres $\\mathbb{S}^{N_\\alpha}\\equiv\\mathcal{S}^{N_\\alpha-1}$ defined using the $L_2$-norm. Corollary: if either orthogonality or constant-norm are violated, semantic subspaces interfere through a multiplicative factor on $w_t$.",
                        "statement_type": "theorem",
                        "statement_motivation_html": "The concept of $\\texttt{Pre-Norm}$ is crucial in the context of semantic subspaces, particularly in ensuring that these subspaces are represented as orthogonal spheres. This orthogonality and constant-norm condition, defined using the $L_2$-norm, helps in maintaining the integrity of the subspaces. The corollary highlights the importance of these conditions by stating that any violation leads to interference in the semantic subspaces, affecting the multiplicative factor on $w_t$. This is particularly useful in fields like machine learning and data representation, where maintaining orthogonal and normalized subspaces can significantly enhance the performance and accuracy of models.",
                        "html_url": "library/theorems/theorem_5/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "dba5aec6-fa8d-4b4d-9196-7fe2b9065c1a",
                            "paper_id": null,
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\textit{Proof.} ~Write \n\\begin{equation}\nx(\\theta_A,\\theta_B,\\phi) ~=~ x_A(\\theta_A) ~+~ x_B(\\theta_B) ~+~ x_{AB}(\\theta_A,\\theta_B) ~+~ x_{other}(\\theta_A,\\theta_B,\\phi)\n\\end{equation}\nThen for head A we have\n\\begin{equation}\n    w_t^{(A)}(\\theta_A) ~=~ \\frac{1}{\\left|y_t\\right|\\left|x(\\theta_A,\\theta_B,\\phi) \\right|} {w^*_t}^{(A)}(\\theta_A)\n\\end{equation}\nwhere $w^*_t$ are the attention scores from the \\texttt{No-Norm} case, which requires $x_A(\\theta_A)$ and $x_B(\\theta_B)$ to be linearly independent. Now we additionally require $\\left|x(\\theta_A,\\theta_B,\\phi) \\right| \\perp \\theta_B,\\phi$, with\n\\begin{equation}\n|x| ~=~ \\sqrt{|x_A|^2 ~+~ |x_B ~+~ x_{AB} ~+~ x_{other}|^2 ~+~ 2 x_A^T \\left(x_B ~+~ x_{AB} ~+~ x_{other}\\right)}\n\\end{equation}\nwhere we suppress parameter dependence for readability. Since $\\sqrt{\\cdot}$ is a monotonic function, this can only be satisfied if\n\\begin{equation}\n|x_A|^2 ~+~ |x_B ~+~ x_{AB} ~+~ x_{other}|^2 ~+~ 2 x_A^T \\left(x_B ~+~ x_{AB} ~+~ x_{other}\\right) ~\\perp~ \\theta_B,\\phi\n\\end{equation}\nRepeating this process for head B gives\n\\begin{equation}\n|x_B|^2 ~+~ |x_A ~+~ x_{AB} ~+~ x_{other}|^2 ~+~ 2 x_B^T \\left(x_A ~+~ x_{AB} ~+~ x_{other}\\right) ~\\perp~ \\theta_A,\\phi\n\\end{equation}\nCombining and collecting dependencies, we then have\n\\begin{align}\n    |x_A|^2 ~=~ const ~~~&\\forall~~~ \\theta_A \\\\\n    |x_B|^2 ~=~ const ~~~&\\forall~~~ \\theta_B \\\\\n    \n    \\left( x_{AB} ~+~ 2x_A ~+~ 2x_B \\right)^T x_{AB} ~+~ 2x_A^Tx_B ~=~ const ~~~&\\forall~~~ \\theta_A,\\theta_B \\\\\n    \\left(x_{other} + 2x_A + 2x_B + 2x_{AB}\\right)^T x_{other} ~=~ const ~~~&\\forall~~~ \\theta_A,\\theta_B,\\phi\n\\end{align}\nWe can go one step further, noticing that each individual term carries a different functional dependence, and so must independently be constant\\footnote{N.B. If $|x_{AB}|^2 \\propto x_A^Tx_B$ then $|x_{AB}|^2=const$ reduces to $x_A^Tx_B=const$, which is already required.}. We then have $\\forall~~\\mu,\\nu\\in\\{A,B,AB,other\\}$\n\\begin{equation}\n    |x_\\mu|=const   ~~~~~~\\mathrm{and}~~~~~~  x_\\mu^Tx_\\nu=const \n\\end{equation}\nThe requirements $|x_A(\\theta_A)|=const ~\\forall~\\theta_A$ and $|x_B(\\theta_B)|=const ~\\forall~\\theta_B$ mean that the semantic subspaces have a spherical structure defined by the $L_2$-norm $|\\cdot|$.\n\nNow consider the requirement $x_A(\\theta_A)^Tx_B(\\theta_B)=const$. Say that $\\theta_A$ and $\\theta_B$ have $N_A$ and $N_B$ degrees of freedom, meaning that $x_A$ and $x_B$ have $N_A-1$ and $N_B-1$ respectively, since they each lose one by confinement to the sphere. Say that the constant is nonzero such that $x_A^Tx_B \\neq 0$. This means that there must be some direction $i$ for which $x_{Ai}x_{Bi} \\neq0$. If we know all $N_A-1$ coordinates of $x_A$, and all $N_B - 2$ coordinates of $x_B$ except for direction $i$, then we also know the value of $x_{Bi}$, because it is fixed by the constant. However, this would mean that $x_A$ and $x_B$ are not independent, violating the condition $\\theta_A \\perp \\theta_B$. The only way to satisfy independence is if $x_{Ai}x_{Bi}=0~\\forall~i$, ensuring that degrees of freedom on $x_A$ and $x_B$ never become entangled. Therefore, to satisfy semantic independence, we must have $x_A(\\theta_A)^Tx_B(\\theta_B)=0 ~\\forall~\\theta_A,\\theta_B$. This means that the subspaces are not just linearly independent, but orthogonal.\n\nWe have shown the proof for semantic subspaces of $x$. As for Theorem~\\ref{theorem: structure: no-norm}, the same structure must be true for $y_t$ by symmetry.",
                            "statement_html": "Write \n\\begin{equation}\nx(\\theta_A,\\theta_B,\\phi) ~=~ x_A(\\theta_A) ~+~ x_B(\\theta_B) ~+~ x_{AB}(\\theta_A,\\theta_B) ~+~ x_{other}(\\theta_A,\\theta_B,\\phi)\n\\end{equation}\nThen for head A we have\n\\begin{equation}\n    w_t^{(A)}(\\theta_A) ~=~ \\frac{1}{\\left|y_t\\right|\\left|x(\\theta_A,\\theta_B,\\phi) \\right|} {w^*_t}^{(A)}(\\theta_A)\n\\end{equation}\nwhere $w^*_t$ are the attention scores from the $\\texttt{No-Norm}$ case, which requires $x_A(\\theta_A)$ and $x_B(\\theta_B)$ to be linearly independent. Now we additionally require $\\left|x(\\theta_A,\\theta_B,\\phi) \\right| \\perp \\theta_B,\\phi$, with\n\\begin{equation}\n|x| ~=~ \\sqrt{|x_A|^2 ~+~ |x_B ~+~ x_{AB} ~+~ x_{other}|^2 ~+~ 2 x_A^T \\left(x_B ~+~ x_{AB} ~+~ x_{other}\\right)}\n\\end{equation}\nwhere we suppress parameter dependence for readability. Since $\\sqrt{\\cdot}$ is a monotonic function, this can only be satisfied if\n\\begin{equation}\n|x_A|^2 ~+~ |x_B ~+~ x_{AB} ~+~ x_{other}|^2 ~+~ 2 x_A^T \\left(x_B ~+~ x_{AB} ~+~ x_{other}\\right) ~\\perp~ \\theta_B,\\phi\n\\end{equation}\nRepeating this process for head B gives\n\\begin{equation}\n|x_B|^2 ~+~ |x_A ~+~ x_{AB} ~+~ x_{other}|^2 ~+~ 2 x_B^T \\left(x_A ~+~ x_{AB} ~+~ x_{other}\\right) ~\\perp~ \\theta_A,\\phi\n\\end{equation}\nCombining and collecting dependencies, we then have\n\\begin{align}\n    |x_A|^2 ~=~ const ~~~&\\forall~~~ \\theta_A \\\\\n    |x_B|^2 ~=~ const ~~~&\\forall~~~ \\theta_B \\\\\n    \n    \\left( x_{AB} ~+~ 2x_A ~+~ 2x_B \\right)^T x_{AB} ~+~ 2x_A^Tx_B ~=~ const ~~~&\\forall~~~ \\theta_A,\\theta_B \\\\\n    \\left(x_{other} + 2x_A + 2x_B + 2x_{AB}\\right)^T x_{other} ~=~ const ~~~&\\forall~~~ \\theta_A,\\theta_B,\\phi\n\\end{align}\nWe can go one step further, noticing that each individual term carries a different functional dependence, and so must independently be constant<sup>N.B. If $|x_{AB}|^2 \\propto x_A^Tx_B$ then $|x_{AB}|^2=const$ reduces to $x_A^Tx_B=const$, which is already required.</sup>. We then have $\\forall~~\\mu,\\nu\\in\\{A,B,AB,other\\}$\n\\begin{equation}\n    |x_\\mu|=const   ~~~~~~\\mathrm{and}~~~~~~  x_\\mu^Tx_\\nu=const \n\\end{equation}\nThe requirements $|x_A(\\theta_A)|=const ~\\forall~\\theta_A$ and $|x_B(\\theta_B)|=const ~\\forall~\\theta_B$ mean that the semantic subspaces have a spherical structure defined by the $L_2$-norm $|\\cdot|$.\n\nNow consider the requirement $x_A(\\theta_A)^Tx_B(\\theta_B)=const$. Say that $\\theta_A$ and $\\theta_B$ have $N_A$ and $N_B$ degrees of freedom, meaning that $x_A$ and $x_B$ have $N_A-1$ and $N_B-1$ respectively, since they each lose one by confinement to the sphere. Say that the constant is nonzero such that $x_A^Tx_B \\neq 0$. This means that there must be some direction $i$ for which $x_{Ai}x_{Bi} \\neq0$. If we know all $N_A-1$ coordinates of $x_A$, and all $N_B - 2$ coordinates of $x_B$ except for direction $i$, then we also know the value of $x_{Bi}$, because it is fixed by the constant. However, this would mean that $x_A$ and $x_B$ are not independent, violating the condition $\\theta_A \\perp \\theta_B$. The only way to satisfy independence is if $x_{Ai}x_{Bi}=0~\\forall~i$, ensuring that degrees of freedom on $x_A$ and $x_B$ never become entangled. Therefore, to satisfy semantic independence, we must have $x_A(\\theta_A)^Tx_B(\\theta_B)=0 ~\\forall~\\theta_A,\\theta_B$. This means that the subspaces are not just linearly independent, but orthogonal.\n\nWe have shown the proof for semantic subspaces of $x$. As for <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_4/index.html#theorem%3A+structure%3A+no-norm\">Theorem 4</a>, the same structure must be true for $y_t$ by symmetry.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "The proof involves several steps to demonstrate the orthogonality and independence of semantic subspaces. Here is a breakdown of the steps:\n<br>\n<br>1. <i></i>Expression of \\( x \\)<i></i>:\n<br>   The function \\( x \\) is decomposed into four components:\n<br>   \\[\n   x(\\theta_A,\\theta_B,\\phi) = x_A(\\theta_A) + x_B(\\theta_B) + x_{AB}(\\theta_A,\\theta_B) + x_{other}(\\theta_A,\\theta_B,\\phi)\n   \\]\n<br>\n<br>2. <i></i>Weight for Head A<i></i>:\n<br>   The weight \\( w_t^{(A)} \\) for head A is defined as:\n<br>   \\[\n   w_t^{(A)}(\\theta_A) = \\frac{1}{\\left|y_t\\right|\\left|x(\\theta_A,\\theta_B,\\phi) \\right|} {w^*_t}^{(A)}(\\theta_A)\n   \\]\n<br>   Here, \\( w^*_t \\) are the attention scores from the \\(\\texttt{No-Norm}\\) case, requiring \\( x_A(\\theta_A) \\) and \\( x_B(\\theta_B) \\) to be linearly independent.\n<br>\n<br>3. <i></i>Norm Condition<i></i>:\n<br>   The norm \\( \\left|x(\\theta_A,\\theta_B,\\phi) \\right| \\) must be independent of \\( \\theta_B \\) and \\( \\phi \\):\n<br>   \\[\n   |x| = \\sqrt{|x_A|^2 + |x_B + x_{AB} + x_{other}|^2 + 2 x_A^T (x_B + x_{AB} + x_{other})}\n   \\]\n<br>\n<br>4. <i></i>Orthogonality Condition for Head A<i></i>:\n<br>   This condition can only be satisfied if:\n<br>   \\[\n   |x_A|^2 + |x_B + x_{AB} + x_{other}|^2 + 2 x_A^T (x_B + x_{AB} + x_{other}) \\perp \\theta_B,\\phi\n   \\]\n<br>\n<br>5. <i></i>Repeating for Head B<i></i>:\n<br>   Similarly, for head B:\n<br>   \\[\n   |x_B|^2 + |x_A + x_{AB} + x_{other}|^2 + 2 x_B^T (x_A + x_{AB} + x_{other}) \\perp \\theta_A,\\phi\n   \\]\n<br>\n<br>6. <i></i>Combining Dependencies<i></i>:\n<br>   Combining the conditions, we get:\n<br>   \\[\n   |x_A|^2 = const \\quad \\forall \\theta_A\n   \\]\n<br>   \\[\n   |x_B|^2 = const \\quad \\forall \\theta_B\n   \\]\n<br>   \\[\n   (x_{AB} + 2x_A + 2x_B)^T x_{AB} + 2x_A^T x_B = const \\quad \\forall \\theta_A,\\theta_B\n   \\]\n<br>   \\[\n   (x_{other} + 2x_A + 2x_B + 2x_{AB})^T x_{other} = const \\quad \\forall \\theta_A,\\theta_B,\\phi\n   \\]\n<br>\n<br>7. <i></i>Independence of Terms<i></i>:\n<br>   Each term must independently be constant:\n<br>   \\[\n   |x_\\mu| = const \\quad \\text{and} \\quad x_\\mu^T x_\\nu = const \\quad \\forall \\mu,\\nu \\in \\{A,B,AB,other\\}\n   \\]\n<br>\n<br>8. <i></i>Spherical Structure<i></i>:\n<br>   The requirements \\( |x_A(\\theta_A)| = const \\forall \\theta_A \\) and \\( |x_B(\\theta_B)| = const \\forall \\theta_B \\) imply that the semantic subspaces have a spherical structure defined by the \\( L_2 \\)-norm.\n<br>\n<br>9. <i></i>Orthogonality of Subspaces<i></i>:\n<br>   To ensure semantic independence, \\( x_A(\\theta_A)^T x_B(\\theta_B) = const \\) must be zero:\n<br>   \\[\n   x_A(\\theta_A)^T x_B(\\theta_B) = 0 \\quad \\forall \\theta_A,\\theta_B\n   \\]\n<br>   This means the subspaces are orthogonal.\n<br>\n<br>10. <i></i>Symmetry for \\( y_t \\)<i></i>:\n<br>    By symmetry, the same structure must hold for \\( y_t \\) as stated in Theorem 4.\n<br>\n<br>This proof demonstrates that the semantic subspaces are not only linearly independent but also orthogonal, ensuring the required independence and structure."
                        }
                    },
                    {
                        "statement_id": "6b2d26aa-bacf-4ff5-be0f-794ae2b8b70f",
                        "paper_id": "fa48a779-dedb-4614-9ac9-e052c1896ac6",
                        "library_nr": 6,
                        "library_name": "Theorem 6",
                        "title": "Semantic Orthogonality Theorem",
                        "statement_original_tex": "\\begin{theorem}\n    \\texttt{QKV-Norm}: Semantic subspaces must be linearly independent.\n\\label{theorem: structure: qkv-norm}\n\\end{theorem}",
                        "statement_html": "$\\texttt{QKV-Norm}$: Semantic subspaces must be linearly independent.",
                        "statement_type": "theorem",
                        "statement_motivation_html": "The concept of $\\texttt{QKV-Norm}$, which states that semantic subspaces must be linearly independent, is crucial in the field of machine learning and natural language processing. This principle ensures that the different semantic dimensions captured by the model do not overlap, thereby preserving the distinctiveness of each semantic feature. It is particularly useful when designing and training transformer models, as it helps in maintaining the integrity and clarity of the learned representations, leading to more accurate and meaningful results.",
                        "html_url": "library/theorems/theorem_6/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "8aa7a054-4012-4c35-8dc8-82f1751b1df9",
                            "paper_id": null,
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\textit{Proof.} ~We have\n\\begin{equation}\n    w_t^{(A)}(\\theta_A) ~=~ \\frac{1}{\\left|k_t^{(A)}\\right|\\left|q^{(A)}\\right|} {w^*_t}^{(A)}(\\theta_A)\n\\end{equation}\nwhere $w^*_t$ are the attention scores from the \\texttt{No-Norm} case, which requires $x_A(\\theta_A)$ and $x_B(\\theta_B)$ to be linearly independent. Use\n\\begin{equation}\nx(\\theta_A,\\theta_B,\\phi) ~=~ x_A(\\theta_A) ~+~ x_B(\\theta_B) ~+~ x_{other}(\\theta_A,\\theta_B,\\phi)\n\\end{equation}\nand \n\\begin{equation}\n\\begin{split}\n    q^{(A)}(\\theta_A) ~&=~ W_Q^{(A)} x(\\theta_A,\\theta_B,\\phi)  \\\\\n    &=~ W_Q^{(A)} x_A(\\theta_A) ~+~ W_Q^{(A)} x_B(\\theta_B) ~+~ W_Q^{(A)} x_{other}(\\theta_A,\\theta_B,\\phi) \\\\\n\\end{split}\n\\end{equation}\nSince we already have the condition of linearly independent $x_A,x_B$, there must exist a linear projection operator $P_A$ such that $P_A x_A = x_A$. Defining $W_Q^{(A)}=P_A$, we then have\n\\begin{equation}\n    q^{(A)}(\\theta_A) ~=~ W_Q^{(A)} x_A(\\theta_A) \n\\end{equation}\nThis demonstrates that it is possible to separate linearly independent semantic subspaces on $x$. By symmetry of $w_t^{(A)}(\\theta_A)$, the same must be true for $y_t$.\n",
                            "statement_html": "We have\n\\begin{equation}\n    w_t^{(A)}(\\theta_A) ~=~ \\frac{1}{\\left|k_t^{(A)}\\right|\\left|q^{(A)}\\right|} {w^*_t}^{(A)}(\\theta_A)\n\\end{equation}\nwhere $w^*_t$ are the attention scores from the $\\texttt{No-Norm}$ case, which requires $x_A(\\theta_A)$ and $x_B(\\theta_B)$ to be linearly independent. Use\n\\begin{equation}\nx(\\theta_A,\\theta_B,\\phi) ~=~ x_A(\\theta_A) ~+~ x_B(\\theta_B) ~+~ x_{other}(\\theta_A,\\theta_B,\\phi)\n\\end{equation}\nand \n\\begin{equation}\n\\begin{split}\n    q^{(A)}(\\theta_A) ~&=~ W_Q^{(A)} x(\\theta_A,\\theta_B,\\phi)  \\\\\n    &=~ W_Q^{(A)} x_A(\\theta_A) ~+~ W_Q^{(A)} x_B(\\theta_B) ~+~ W_Q^{(A)} x_{other}(\\theta_A,\\theta_B,\\phi) \\\\\n\\end{split}\n\\end{equation}\nSince we already have the condition of linearly independent $x_A,x_B$, there must exist a linear projection operator $P_A$ such that $P_A x_A = x_A$. Defining $W_Q^{(A)}=P_A$, we then have\n\\begin{equation}\n    q^{(A)}(\\theta_A) ~=~ W_Q^{(A)} x_A(\\theta_A) \n\\end{equation}\nThis demonstrates that it is possible to separate linearly independent semantic subspaces on $x$. By symmetry of $w_t^{(A)}(\\theta_A)$, the same must be true for $y_t$.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, we can break it down into several key steps:\n<br>\n<br>1. <i></i>Initial Equation<i></i>:\n<br>   The proof starts with the given equation:\n<br>   \\[\n   w_t^{(A)}(\\theta_A) = \\frac{1}{\\left|k_t^{(A)}\\right|\\left|q^{(A)}\\right|} {w^*_t}^{(A)}(\\theta_A)\n   \\]\n<br>   Here, \\( w^*_t \\) represents the attention scores from the \\(\\texttt{No-Norm}\\) case, which assumes that \\( x_A(\\theta_A) \\) and \\( x_B(\\theta_B) \\) are linearly independent.\n<br>\n<br>2. <i></i>Definition of \\( x(\\theta_A, \\theta_B, \\phi) \\)<i></i>:\n<br>   The proof introduces a function \\( x \\) defined as:\n<br>   \\[\n   x(\\theta_A, \\theta_B, \\phi) = x_A(\\theta_A) + x_B(\\theta_B) + x_{other}(\\theta_A, \\theta_B, \\phi)\n   \\]\n<br>   This function combines the components \\( x_A \\), \\( x_B \\), and an additional term \\( x_{other} \\).\n<br>\n<br>3. <i></i>Expression for \\( q^{(A)}(\\theta_A) \\)<i></i>:\n<br>   The proof then expresses \\( q^{(A)}(\\theta_A) \\) as:\n<br>   \\[\n   \\begin{split}\n   q^{(A)}(\\theta_A) &= W_Q^{(A)} x(\\theta_A, \\theta_B, \\phi) \\\\\n   &= W_Q^{(A)} x_A(\\theta_A) + W_Q^{(A)} x_B(\\theta_B) + W_Q^{(A)} x_{other}(\\theta_A, \\theta_B, \\phi)\n   \\end{split}\n   \\]\n<br>   This step shows how \\( q^{(A)} \\) is derived from the linear transformation of \\( x \\).\n<br>\n<br>4. <i></i>Linear Independence and Projection Operator<i></i>:\n<br>   Given the condition that \\( x_A \\) and \\( x_B \\) are linearly independent, there must exist a linear projection operator \\( P_A \\) such that \\( P_A x_A = x_A \\). By defining \\( W_Q^{(A)} = P_A \\), we simplify \\( q^{(A)} \\) to:\n<br>   \\[\n   q^{(A)}(\\theta_A) = W_Q^{(A)} x_A(\\theta_A)\n   \\]\n<br>\n<br>5. <i></i>Conclusion<i></i>:\n<br>   This demonstrates that it is possible to separate linearly independent semantic subspaces on \\( x \\). By symmetry of \\( w_t^{(A)}(\\theta_A) \\), the same must be true for \\( y_t \\).\n<br>\n<br>This step-by-step breakdown shows how the proof establishes the separation of linearly independent semantic subspaces using projection operators and linear transformations."
                        }
                    },
                    {
                        "statement_id": "11793c7d-e742-4b70-8386-c6f4212cab08",
                        "paper_id": "fa48a779-dedb-4614-9ac9-e052c1896ac6",
                        "library_nr": 7,
                        "library_name": "Theorem 7",
                        "title": "Propagation of Infinitesimal Perturbations in Attention Mechanisms",
                        "statement_original_tex": "\\begin{theorem}\n    Consider independent infinitesimal perturbations on queries $\\epsilon^q \\in \\mathbb{R}^{N_{qkv}}$, keys $\\epsilon^k_t \\in \\mathbb{R}^{N_{qkv}}$, and messages $\\epsilon^m_t \\in \\mathbb{R}^{N_{qkv}}$. These propagate onto $\\Delta x = \\sum_{t}a_tm_t$ as\n    \\begin{align}\n        \\epsilon^{\\Delta x(q)} ~~&\\xrightarrow[\\epsilon^q\\rightarrow0]{\\mathrm{~~~~perturb~q~~~~}}~~ \\mathop{\\mathbb{E}}_{a_t} \\Big[ m_t {\\tilde k}_t^T \\Big] \\epsilon^q ~~~~~~~~~~~~~~~~~  {\\tilde k}_t ~\\triangleq~ k_t ~- \\mathop{\\mathbb{E}}_{a_t} \\Big[ k_t \\Big] \\\\\n        \\epsilon^{\\Delta x(k)} ~~&\\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{~~~~perturb~k~~~~}}~~ \\mathop{\\mathbb{E}}_{a_t} \\Big[ {\\tilde m}_t {\\epsilon^k_t}^T \\Big] q ~~~~~~~~~~~~~~~~~  {\\tilde m}_t ~\\triangleq~ m_t ~- \\mathop{\\mathbb{E}}_{a_t} \\Big[ m_t \\Big] \\\\\n        \\epsilon^{\\Delta x(m)} ~~&\\xrightarrow[\\epsilon^m_t\\rightarrow0]{\\mathrm{~~~~perturb~m~~~~}}~~ \\mathop{\\mathbb{E}}_{a_t} \\Big[ \\epsilon^m_t \\Big]\n    \\end{align}\n    where ${\\tilde z}_t$ is the value of $z_t$ measured from the attention-weighted centroid $\\mathbb{E}_{a_t}[z_t] = \\sum_t a_t z_t$.\n\\label{theorem: stability: general}\n\\end{theorem}",
                        "statement_html": "Consider independent infinitesimal perturbations on queries $\\epsilon^q \\in \\mathbb{R}^{N_{qkv}}$, keys $\\epsilon^k_t \\in \\mathbb{R}^{N_{qkv}}$, and messages $\\epsilon^m_t \\in \\mathbb{R}^{N_{qkv}}$. These propagate onto $\\Delta x = \\sum_{t}a_tm_t$ as\n\\begin{align}\n    \\epsilon^{\\Delta x(q)} ~~&\\xrightarrow[\\epsilon^q\\rightarrow0]{\\mathrm{~~~~perturb~q~~~~}}~~ \\mathop{\\mathbb{E}}_{a_t} \\Big[ m_t {\\tilde k}_t^T \\Big] \\epsilon^q ~~~~~~~~~~~~~~~~~  {\\tilde k}_t ~\\triangleq~ k_t ~- \\mathop{\\mathbb{E}}_{a_t} \\Big[ k_t \\Big] \\\\\n    \\epsilon^{\\Delta x(k)} ~~&\\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{~~~~perturb~k~~~~}}~~ \\mathop{\\mathbb{E}}_{a_t} \\Big[ {\\tilde m}_t {\\epsilon^k_t}^T \\Big] q ~~~~~~~~~~~~~~~~~  {\\tilde m}_t ~\\triangleq~ m_t ~- \\mathop{\\mathbb{E}}_{a_t} \\Big[ m_t \\Big] \\\\\n    \\epsilon^{\\Delta x(m)} ~~&\\xrightarrow[\\epsilon^m_t\\rightarrow0]{\\mathrm{~~~~perturb~m~~~~}}~~ \\mathop{\\mathbb{E}}_{a_t} \\Big[ \\epsilon^m_t \\Big]\n\\end{align}\nwhere ${\\tilde z}_t$ is the value of $z_t$ measured from the attention-weighted centroid $\\mathbb{E}_{a_t}[z_t] = \\sum_t a_t z_t$.",
                        "statement_type": "theorem",
                        "statement_motivation_html": "Understanding how infinitesimal perturbations propagate in the context of queries, keys, and messages is crucial for analyzing the stability and sensitivity of attention mechanisms in neural networks. This can help in designing more robust models by identifying how small changes in input can affect the output, thereby improving the interpretability and reliability of the model's predictions.",
                        "html_url": "library/theorems/theorem_7/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "e77c2bc7-6b3a-4d13-b48d-77f58945d44b",
                            "paper_id": null,
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\textit{Proof.} ~Consider $q\\rightarrow q+\\epsilon^q$ where $\\epsilon^q$ are infinitesimal perturbations on $q$. Then $\\Delta x \\rightarrow \\Delta x + \\epsilon^{\\Delta x(q)}$ where by Taylor expansion we find\n\\begin{equation}\n    \\epsilon^{\\Delta x(q)} ~=~ \\frac{\\partial \\Delta x}{\\partial q}\\epsilon^q ~+~ \\mathcal{O}\\left({\\epsilon^q}^2\\right)\n\\end{equation}\nwhere the leading term is a matrix $\\frac{\\partial \\Delta x}{\\partial q}$ acting on a vector $\\epsilon^q$. Differentiating gives\n\\begin{equation}\n    \\frac{\\partial\\Delta x}{\\partial q} ~=~ \\sum_{ij} m_i \\frac{\\partial a_i}{\\partial w_j} \\frac{\\partial w_j}{\\partial q}\n\\end{equation}\nwith $a_i = \\texttt{softmax}_i(w_i)$ and $w_i=k_i^Tq$, and we are using $i,j,k$ etc to index over tokens instead of $t,t',t''$ etc, because this is more readable when we have many summations. Then\n\n\\begin{equation}\n\\begin{split}\n    \\frac{\\partial a_i}{\\partial w_j} ~&=~ \\frac{\\partial}{\\partial w_j} ~ \\frac{e^{w_i}}{\\sum_k e^{w_k}} \\\\\n    &=~ \\frac{\\delta_{ij}e^{w_i}}{\\sum_k e^{w_k}} ~+~ e^{w_i}\\left(-\\frac{e^{w_j}}{\\left(\\sum_ke^{w_k}\\right)^2}\\right) \\\\\n    &=~ \\frac{e^{w_i}}{\\sum_k e^{w_k}}\\left( 1 ~-~ \\frac{e^{w_j}}{\\sum_le^{w_l}}\\right) \\\\\n    &=~ a_i\\left(\\delta_{ij} ~-~ a_j\\right) \\\\\n\\end{split}\n\\end{equation}\nand $\\frac{\\partial w_i}{\\partial q} = k_i^T$, where we retain the transpose to indicate that this is an element of the dual vector space (i.e. covector). Inserting these results into our expression for $\\epsilon^{\\Delta x(q)}$ gives\n\\begin{equation}\n\\begin{split}\n    \\epsilon^{\\Delta x(q)} ~&=~ \\sum_{ij} m_i a_i\\left(\\delta_{ij} ~-~ a_j\\right) k_j^T \\epsilon^q \\\\\n    &=~ \\sum_{i} m_i a_i \\left(k_i ~-~ \\sum_j a_jk_j \\right)^T \\epsilon^q \\\\\n    &=~ \\sum_{i} m_i a_i {\\tilde k}_i^T \\epsilon^q \\\\\n    &=~ \\mathop{\\mathbb{E}}_{a_i} \\Big[m_i {\\tilde k}_i^T \\Big] \\epsilon^q \\\\\n\\end{split}\n\\end{equation}\nThis is the result for Eq.~\\ref{eq: stability: general q}. Repeating the process for perturbations on $k_i$, we have\n\\begin{equation}\n    \\epsilon^{\\Delta x(k)} ~=~ \\sum_i\\frac{\\partial \\Delta x}{\\partial k_i}\\epsilon^k_i ~+~ \\mathcal{O}\\left({\\epsilon^k}^2\\right)\n\\end{equation}\nand\n\\begin{equation}\n\\begin{split}\n    \\frac{\\partial\\Delta x}{\\partial k_i} ~&=~ \\sum_{jk} m_j \\frac{\\partial a_j}{\\partial w_k} \\frac{\\partial w_k}{\\partial k_i} \\\\\n    &=~ \\sum_{jk} m_j a_j \\left(\\delta_{jk} ~-~ a_k\\right) \\delta_{ki} q^T \\\\\n    &=~ \\sum_{j} m_j a_j \\left(\\delta_{ji} ~-~ a_i\\right) q^T \\\\\n    &=~ a_i {\\tilde m}_i q^T\n\\end{split}\n\\end{equation}\nTherefore\n\\begin{equation}\n    \\epsilon^{\\Delta x(k)} ~=~ \\sum_i a_i {\\tilde m}_i q^T \\epsilon^k_i ~=~ \\mathop{\\mathbb{E}}_{a_i} \\Big[{\\tilde m}_i {\\epsilon^k_i}^T \\Big] q\n\\end{equation}\nwhich is the result for Eq.~\\ref{eq: stability: general k}. Finally,\n\\begin{equation}\n\\begin{split}\n    \\epsilon^{\\Delta x(m)} ~&=~ \\sum_i \\frac{\\partial \\Delta x}{\\partial m_i}\\epsilon^m_i \\\\\n    &=~ \\sum_{i} a_i \\epsilon^m_i \\\\\n    &=~ \\mathop{\\mathbb{E}}_{a_i} \\Big[ \\epsilon^m_i \\Big]\n\\end{split}\n\\end{equation}\nusing $\\frac{\\partial\\Delta x}{\\partial m_i} = \\frac{\\partial}{\\partial m_i}\\sum_j a_j m_j = \\sum_j a_j \\delta_{ij} = a_i$. This is the result for Eq.~\\ref{eq: stability: general m}.\n",
                            "statement_html": "Consider $q \\rightarrow q + \\epsilon^q$ where $\\epsilon^q$ are infinitesimal perturbations on $q$. Then $\\Delta x \\rightarrow \\Delta x + \\epsilon^{\\Delta x(q)}$ where by Taylor expansion we find\n\\begin{equation}\n    \\epsilon^{\\Delta x(q)} ~=~ \\frac{\\partial \\Delta x}{\\partial q}\\epsilon^q ~+~ \\mathcal{O}\\left({\\epsilon^q}^2\\right)\n\\end{equation}\nwhere the leading term is a matrix $\\frac{\\partial \\Delta x}{\\partial q}$ acting on a vector $\\epsilon^q$. Differentiating gives\n\\begin{equation}\n    \\frac{\\partial\\Delta x}{\\partial q} ~=~ \\sum_{ij} m_i \\frac{\\partial a_i}{\\partial w_j} \\frac{\\partial w_j}{\\partial q}\n\\end{equation}\nwith $a_i = \\texttt{softmax}_i(w_i)$ and $w_i = k_i^T q$, and we are using $i, j, k$ etc to index over tokens instead of $t, t', t''$ etc, because this is more readable when we have many summations. Then\n\n\\begin{equation}\n\\begin{split}\n    \\frac{\\partial a_i}{\\partial w_j} ~&=~ \\frac{\\partial}{\\partial w_j} ~ \\frac{e^{w_i}}{\\sum_k e^{w_k}} \\\\\n    &=~ \\frac{\\delta_{ij}e^{w_i}}{\\sum_k e^{w_k}} ~+~ e^{w_i}\\left(-\\frac{e^{w_j}}{\\left(\\sum_k e^{w_k}\\right)^2}\\right) \\\\\n    &=~ \\frac{e^{w_i}}{\\sum_k e^{w_k}}\\left( 1 ~-~ \\frac{e^{w_j}}{\\sum_l e^{w_l}}\\right) \\\\\n    &=~ a_i\\left(\\delta_{ij} ~-~ a_j\\right) \\\\\n\\end{split}\n\\end{equation}\nand $\\frac{\\partial w_i}{\\partial q} = k_i^T$, where we retain the transpose to indicate that this is an element of the dual vector space (i.e. covector). Inserting these results into our expression for $\\epsilon^{\\Delta x(q)}$ gives\n\\begin{equation}\n\\begin{split}\n    \\epsilon^{\\Delta x(q)} ~&=~ \\sum_{ij} m_i a_i\\left(\\delta_{ij} ~-~ a_j\\right) k_j^T \\epsilon^q \\\\\n    &=~ \\sum_{i} m_i a_i \\left(k_i ~-~ \\sum_j a_j k_j \\right)^T \\epsilon^q \\\\\n    &=~ \\sum_{i} m_i a_i {\\tilde k}_i^T \\epsilon^q \\\\\n    &=~ \\mathop{\\mathbb{E}}_{a_i} \\Big[m_i {\\tilde k}_i^T \\Big] \\epsilon^q \\\\\n\\end{split}\n\\end{equation}\nThis is the result for Eq. (39) [in <a href=\"https://arxiv.org/pdf/2406.17837#equation.F.39\"> original paper</a>]. Repeating the process for perturbations on $k_i$, we have\n\\begin{equation}\n    \\epsilon^{\\Delta x(k)} ~=~ \\sum_i \\frac{\\partial \\Delta x}{\\partial k_i}\\epsilon^k_i ~+~ \\mathcal{O}\\left({\\epsilon^k}^2\\right)\n\\end{equation}\nand\n\\begin{equation}\n\\begin{split}\n    \\frac{\\partial\\Delta x}{\\partial k_i} ~&=~ \\sum_{jk} m_j \\frac{\\partial a_j}{\\partial w_k} \\frac{\\partial w_k}{\\partial k_i} \\\\\n    &=~ \\sum_{jk} m_j a_j \\left(\\delta_{jk} ~-~ a_k\\right) \\delta_{ki} q^T \\\\\n    &=~ \\sum_{j} m_j a_j \\left(\\delta_{ji} ~-~ a_i\\right) q^T \\\\\n    &=~ a_i {\\tilde m}_i q^T\n\\end{split}\n\\end{equation}\nTherefore\n\\begin{equation}\n    \\epsilon^{\\Delta x(k)} ~=~ \\sum_i a_i {\\tilde m}_i q^T \\epsilon^k_i ~=~ \\mathop{\\mathbb{E}}_{a_i} \\Big[{\\tilde m}_i {\\epsilon^k_i}^T \\Big] q\n\\end{equation}\nwhich is the result for Eq. (40) [in <a href=\"https://arxiv.org/pdf/2406.17837#equation.F.40\"> original paper</a>]. Finally,\n\\begin{equation}\n\\begin{split}\n    \\epsilon^{\\Delta x(m)} ~&=~ \\sum_i \\frac{\\partial \\Delta x}{\\partial m_i}\\epsilon^m_i \\\\\n    &=~ \\sum_{i} a_i \\epsilon^m_i \\\\\n    &=~ \\mathop{\\mathbb{E}}_{a_i} \\Big[ \\epsilon^m_i \\Big]\n\\end{split}\n\\end{equation}\nusing $\\frac{\\partial\\Delta x}{\\partial m_i} = \\frac{\\partial}{\\partial m_i}\\sum_j a_j m_j = \\sum_j a_j \\delta_{ij} = a_i$. This is the result for Eq. (41) [in <a href=\"https://arxiv.org/pdf/2406.17837#equation.F.41\"> original paper</a>].",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, we break it down into several key steps:\n<br>\n<br>1. <i></i>Initial Perturbation<i></i>:\n<br>   - We start by considering a perturbation on \\( q \\), denoted as \\( q \\rightarrow q + \\epsilon^q \\), where \\( \\epsilon^q \\) represents an infinitesimal change.\n<br>   - Consequently, the change in \\( \\Delta x \\) is perturbed to \\( \\Delta x + \\epsilon^{\\Delta x(q)} \\).\n<br>\n<br>2. <i></i>Taylor Expansion<i></i>:\n<br>   - Using a Taylor expansion, we express \\( \\epsilon^{\\Delta x(q)} \\) as:\n<br>     \\[\n     \\epsilon^{\\Delta x(q)} = \\frac{\\partial \\Delta x}{\\partial q}\\epsilon^q + \\mathcal{O}\\left({\\epsilon^q}^2\\right)\n     \\]\n<br>   - Here, the leading term involves the matrix \\( \\frac{\\partial \\Delta x}{\\partial q} \\) acting on the vector \\( \\epsilon^q \\).\n<br>\n<br>3. <i></i>Differentiation<i></i>:\n<br>   - We differentiate \\( \\Delta x \\) with respect to \\( q \\):\n<br>     \\[\n     \\frac{\\partial\\Delta x}{\\partial q} = \\sum_{ij} m_i \\frac{\\partial a_i}{\\partial w_j} \\frac{\\partial w_j}{\\partial q}\n     \\]\n<br>   - Where \\( a_i = \\texttt{softmax}_i(w_i) \\) and \\( w_i = k_i^T q \\).\n<br>\n<br>4. <i></i>Softmax Derivative<i></i>:\n<br>   - We compute the derivative of the softmax function:\n<br>     \\[\n     \\frac{\\partial a_i}{\\partial w_j} = a_i\\left(\\delta_{ij} - a_j\\right)\n     \\]\n<br>   - And \\( \\frac{\\partial w_i}{\\partial q} = k_i^T \\).\n<br>\n<br>5. <i></i>Substitution<i></i>:\n<br>   - Substituting these results into the expression for \\( \\epsilon^{\\Delta x(q)} \\):\n<br>     \\[\n     \\epsilon^{\\Delta x(q)} = \\sum_{i} m_i a_i {\\tilde k}_i^T \\epsilon^q = \\mathop{\\mathbb{E}}_{a_i} \\Big[m_i {\\tilde k}_i^T \\Big] \\epsilon^q\n     \\]\n<br>\n<br>6. <i></i>Perturbation on \\( k_i \\)<i></i>:\n<br>   - Repeating the process for perturbations on \\( k_i \\):\n<br>     \\[\n     \\epsilon^{\\Delta x(k)} = \\sum_i \\frac{\\partial \\Delta x}{\\partial k_i}\\epsilon^k_i + \\mathcal{O}\\left({\\epsilon^k}^2\\right)\n     \\]\n<br>   - And differentiating:\n<br>     \\[\n     \\frac{\\partial\\Delta x}{\\partial k_i} = a_i {\\tilde m}_i q^T\n     \\]\n<br>   - Leading to:\n<br>     \\[\n     \\epsilon^{\\Delta x(k)} = \\mathop{\\mathbb{E}}_{a_i} \\Big[{\\tilde m}_i {\\epsilon^k_i}^T \\Big] q\n     \\]\n<br>\n<br>7. <i></i>Perturbation on \\( m_i \\)<i></i>:\n<br>   - Finally, for perturbations on \\( m_i \\):\n<br>     \\[\n     \\epsilon^{\\Delta x(m)} = \\sum_i \\frac{\\partial \\Delta x}{\\partial m_i}\\epsilon^m_i = \\mathop{\\mathbb{E}}_{a_i} \\Big[ \\epsilon^m_i \\Big]\n     \\]\n<br>   - Using \\( \\frac{\\partial\\Delta x}{\\partial m_i} = a_i \\).\n<br>\n<br>These steps collectively demonstrate how infinitesimal perturbations on \\( q \\), \\( k_i \\), and \\( m_i \\) propagate through the system, leading to the final results for each case."
                        }
                    },
                    {
                        "statement_id": "dfa8874b-1868-4a82-b832-55f6bf151677",
                        "paper_id": "fa48a779-dedb-4614-9ac9-e052c1896ac6",
                        "library_nr": 8,
                        "library_name": "Theorem 8",
                        "title": "Stability of Sparse Attention",
                        "statement_original_tex": "\\begin{theorem}\n    For sparse attention:\n    \\begin{equation}\n        \\epsilon^{\\Delta x(q)} \\xrightarrow[\\epsilon^q\\rightarrow0]{\\mathrm{~~perturb~q~~}} 0   ~~~~~~~~~~\n        \\epsilon^{\\Delta x(k)} \\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{~~perturb~k~~}} 0   ~~~~~~~~~~\n        \\epsilon^{\\Delta x(m)} \\xrightarrow[\\epsilon^m_t\\rightarrow0]{\\mathrm{~~perturb~m~~}} \\epsilon^m_{t^*}\n    \\end{equation}\n    i.e. the message is stable with respect to small interference in the queries and keys. Interference in the selected value is linearly transferred onto the message.\n\\label{theorem: stability: sparse}\n\\end{theorem}",
                        "statement_html": "For sparse attention:\n\\begin{equation}\n    \\epsilon^{\\Delta x(q)} \\xrightarrow[\\epsilon^q\\rightarrow0]{\\mathrm{~~perturb~q~~}} 0   ~~~~~~~~~~\n    \\epsilon^{\\Delta x(k)} \\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{~~perturb~k~~}} 0   ~~~~~~~~~~\n    \\epsilon^{\\Delta x(m)} \\xrightarrow[\\epsilon^m_t\\rightarrow0]{\\mathrm{~~perturb~m~~}} \\epsilon^m_{t^*}\n\\end{equation}\ni.e. the message is stable with respect to small interference in the queries and keys. Interference in the selected value is linearly transferred onto the message.",
                        "statement_type": "theorem",
                        "statement_motivation_html": "Sparse attention is useful in the context of neural networks, particularly in transformer models. It ensures that the message remains stable despite small perturbations in the queries and keys, which is crucial for maintaining the integrity of the information being processed. This stability allows for more efficient and reliable computations, especially when dealing with large-scale data.",
                        "html_url": "library/theorems/theorem_8/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "dc39fcbe-bf8e-485e-85df-876f868beb53",
                            "paper_id": null,
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\textit{Proof.} ~For sparse attention we have $a_t = \\delta_{tt^*}$ for some $t^*$. For perturbations of $q$, the RHS of Eq.~\\ref{eq: stability: general q} becomes\n\\begin{equation}\n\\begin{split}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ m_t {\\tilde k}_t^T \\Big] \\epsilon^q ~&=~ \\sum_{t} a_t m_t {\\tilde k}_t^T \\epsilon^q \\\\\n    &=~ \\sum_{t} \\delta_{tt^*} m_t {\\tilde k}_t^T \\epsilon^q \\\\\n    &=~ m_{t^*} {\\tilde k}_{t^*}^T \\epsilon^q \\\\\n    &=~ 0 \\\\\n\\end{split}\n\\end{equation}\nwhere the final step is because ${\\tilde k}_{t^*} = k_{t^*} - \\mathbb{E}_{a_t}[k_t] = k_{t^*} - \\sum_t \\delta_{tt^*} k_t = k_{t^*}-k_{t^*} = 0$. For perturbations of $k_t$, the RHS of Eq.~\\ref{eq: stability: general k} evaluates to $0$ because \n\\begin{equation}\n\\begin{split}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ {\\tilde m}_t {\\epsilon^k_t}^T \\Big] q ~&=~ \\sum_t a_t {\\tilde m}_t q^T \\epsilon^k_t \\\\\n    &=~ \\sum_t \\delta_{tt^*} {\\tilde m}_t q^T \\epsilon^k_t \\\\\n    &=~ {\\tilde m}_{t^*} q^T \\epsilon^k_{t^*} \\\\\n    &=~ 0 \\\\\n\\end{split}\n\\end{equation}\nwhere the final step is because ${\\tilde m}_{t^*} = m_{t^*} - \\sum_t \\delta_{tt^*} m_t = m_{t^*}-m_{t^*} = 0$. For perturbations of $m_t$, the RHS of Eq.~\\ref{eq: stability: general m} evaluates to\n\\begin{equation}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ \\epsilon^m_t \\Big] ~=~ \\sum_{t} a_t \\epsilon^m_t ~=~ \\sum_{t} \\delta_{tt^*} \\epsilon^m_t ~=~ \\epsilon^m_{t^*}\n\\end{equation}\n",
                            "statement_html": "For sparse attention we have $a_t = \\delta_{tt^*}$ for some $t^*$. For perturbations of $q$, the RHS of Eq. (39) [in <a href=\"https://arxiv.org/pdf/2406.17837#equation.F.39\"> original paper</a>] becomes\n\\begin{equation}\n\\begin{split}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ m_t {\\tilde k}_t^T \\Big] \\epsilon^q ~&=~ \\sum_{t} a_t m_t {\\tilde k}_t^T \\epsilon^q \\\\\n    &=~ \\sum_{t} \\delta_{tt^*} m_t {\\tilde k}_t^T \\epsilon^q \\\\\n    &=~ m_{t^*} {\\tilde k}_{t^*}^T \\epsilon^q \\\\\n    &=~ 0 \\\\\n\\end{split}\n\\end{equation}\nwhere the final step is because ${\\tilde k}_{t^*} = k_{t^*} - \\mathbb{E}_{a_t}[k_t] = k_{t^*} - \\sum_t \\delta_{tt^*} k_t = k_{t^*}-k_{t^*} = 0$. For perturbations of $k_t$, the RHS of Eq. (40) [in <a href=\"https://arxiv.org/pdf/2406.17837#equation.F.40\"> original paper</a>] evaluates to $0$ because \n\\begin{equation}\n\\begin{split}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ {\\tilde m}_t {\\epsilon^k_t}^T \\Big] q ~&=~ \\sum_t a_t {\\tilde m}_t q^T \\epsilon^k_t \\\\\n    &=~ \\sum_t \\delta_{tt^*} {\\tilde m}_t q^T \\epsilon^k_t \\\\\n    &=~ {\\tilde m}_{t^*} q^T \\epsilon^k_{t^*} \\\\\n    &=~ 0 \\\\\n\\end{split}\n\\end{equation}\nwhere the final step is because ${\\tilde m}_{t^*} = m_{t^*} - \\sum_t \\delta_{tt^*} m_t = m_{t^*}-m_{t^*} = 0$. For perturbations of $m_t$, the RHS of Eq. (41) [in <a href=\"https://arxiv.org/pdf/2406.17837#equation.F.41\"> original paper</a>] evaluates to\n\\begin{equation}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ \\epsilon^m_t \\Big] ~=~ \\sum_{t} a_t \\epsilon^m_t ~=~ \\sum_{t} \\delta_{tt^*} \\epsilon^m_t ~=~ \\epsilon^m_{t^*}\n\\end{equation}",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, we need to break down the steps involved in evaluating the right-hand side (RHS) of the given equations under different perturbations. Here is a step-by-step explanation:\n\n1. <i></i>Sparse Attention Definition<i></i>:\n   - We start with the definition of sparse attention: \\( a_t = \\delta_{tt^*} \\) for some \\( t^* \\). This means that the attention is focused on a specific time step \\( t^* \\).\n\n2. <i></i>Perturbations of \\( q \\)<i></i>:\n   - We consider the perturbations of \\( q \\) and evaluate the RHS of Eq. (39) [in <a href=\"https://arxiv.org/pdf/2406.17837#equation.F.39\"> original paper</a>]:\n   \\[\n   \\mathop{\\mathbb{E}}_{a_t} \\Big[ m_t {\\tilde k}_t^T \\Big] \\epsilon^q = \\sum_{t} a_t m_t {\\tilde k}_t^T \\epsilon^q\n   \\]\n   - Substituting \\( a_t = \\delta_{tt^*} \\):\n   \\[\n   \\sum_{t} \\delta_{tt^*} m_t {\\tilde k}_t^T \\epsilon^q = m_{t^*} {\\tilde k}_{t^*}^T \\epsilon^q\n   \\]\n   - Since \\( {\\tilde k}_{t^*} = k_{t^*} - \\mathbb{E}_{a_t}[k_t] = k_{t^*} - \\sum_t \\delta_{tt^*} k_t = k_{t^*} - k_{t^*} = 0 \\):\n   \\[\n   m_{t^*} {\\tilde k}_{t^*}^T \\epsilon^q = 0\n   \\]\n\n3. <i></i>Perturbations of \\( k_t \\)<i></i>:\n   - Next, we consider the perturbations of \\( k_t \\) and evaluate the RHS of Eq. (40) [in <a href=\"https://arxiv.org/pdf/2406.17837#equation.F.40\"> original paper</a>]:\n   \\[\n   \\mathop{\\mathbb{E}}_{a_t} \\Big[ {\\tilde m}_t {\\epsilon^k_t}^T \\Big] q = \\sum_t a_t {\\tilde m}_t q^T \\epsilon^k_t\n   \\]\n   - Substituting \\( a_t = \\delta_{tt^*} \\):\n   \\[\n   \\sum_t \\delta_{tt^*} {\\tilde m}_t q^T \\epsilon^k_t = {\\tilde m}_{t^*} q^T \\epsilon^k_{t^*}\n   \\]\n   - Since \\( {\\tilde m}_{t^*} = m_{t^*} - \\sum_t \\delta_{tt^*} m_t = m_{t^*} - m_{t^*} = 0 \\):\n   \\[\n   {\\tilde m}_{t^*} q^T \\epsilon^k_{t^*} = 0\n   \\]\n\n4. <i></i>Perturbations of \\( m_t \\)<i></i>:\n   - Finally, we consider the perturbations of \\( m_t \\) and evaluate the RHS of Eq. (41) [in <a href=\"https://arxiv.org/pdf/2406.17837#equation.F.41\"> original paper</a>]:\n   \\[\n   \\mathop{\\mathbb{E}}_{a_t} \\Big[ \\epsilon^m_t \\Big] = \\sum_{t} a_t \\epsilon^m_t\n   \\]\n   - Substituting \\( a_t = \\delta_{tt^*} \\):\n   \\[\n   \\sum_{t} \\delta_{tt^*} \\epsilon^m_t = \\epsilon^m_{t^*}\n   \\]\n\nIn summary, the proof shows that for sparse attention, the perturbations of \\( q \\) and \\( k_t \\) result in zero contributions due to the properties of \\( {\\tilde k}_{t^*} \\) and \\( {\\tilde m}_{t^*} \\). However, the perturbation of \\( m_t \\) directly results in \\( \\epsilon^m_{t^*} \\)."
                        }
                    },
                    {
                        "statement_id": "284bd1c6-c3ec-4d4e-82e0-1324bf888aea",
                        "paper_id": "fa48a779-dedb-4614-9ac9-e052c1896ac6",
                        "library_nr": 9,
                        "library_name": "Theorem 9",
                        "title": "Stability of Isotropic Attention",
                        "statement_original_tex": "\\begin{theorem}\n    For isotropic attention:\n    \\begin{equation}\n        \\epsilon^{\\Delta x(q)} \\xrightarrow[\\epsilon^q\\rightarrow0]{\\mathrm{perturb~q}} \\langle m_t {\\tilde k}_t^T \\rangle_t \\epsilon^q ~~~~~~~~\n        \n        \\epsilon^{\\Delta x(k)} \\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{perturb~k}} \\langle {\\tilde m}_t {\\epsilon^k_t}^T \\rangle_t ~q   ~~~~~~~~\n        \\epsilon^{\\Delta x(m)} \\xrightarrow[\\epsilon^m_t\\rightarrow0]{\\mathrm{perturb~m}} \\langle \\epsilon^m_t \\rangle_t\n    \\end{equation}\n    \n    \n    \n    \n    \n    N.B. isotropy requires $k_t=const$ or $q=0$. Lemma 1: the update is stable to noisy $q$ when $k_t=const$, or when $m_t \\perp k_t$ (c.f. keys and messages from independent subspaces). Lemma 2: the update is stable to noisy $k_t$ when $q=0$, or when $m_t \\perp \\epsilon_t^k$. Lemma 3: the update is stable to noisy $m_t$ when $\\langle \\epsilon^m_t \\rangle_t=0$. Other cases propagate linearly.\n\\label{theorem: stability: isotropic}\n\\end{theorem}",
                        "statement_html": "For isotropic attention:\n\\begin{equation}\n    \\epsilon^{\\Delta x(q)} \\xrightarrow[\\epsilon^q\\rightarrow0]{\\mathrm{perturb~q}} \\langle m_t {\\tilde k}_t^T \\rangle_t \\epsilon^q ~~~~~~~~\n    \n    \\epsilon^{\\Delta x(k)} \\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{perturb~k}} \\langle {\\tilde m}_t {\\epsilon^k_t}^T \\rangle_t ~q   ~~~~~~~~\n    \\epsilon^{\\Delta x(m)} \\xrightarrow[\\epsilon^m_t\\rightarrow0]{\\mathrm{perturb~m}} \\langle \\epsilon^m_t \\rangle_t\n\\end{equation}\n\nN.B. isotropy requires $k_t=const$ or $q=0$. Lemma 1: the update is stable to noisy $q$ when $k_t=const$, or when $m_t \\perp k_t$ (c.f. keys and messages from independent subspaces). Lemma 2: the update is stable to noisy $k_t$ when $q=0$, or when $m_t \\perp \\epsilon_t^k$. Lemma 3: the update is stable to noisy $m_t$ when $\\langle \\epsilon^m_t \\rangle_t=0$. Other cases propagate linearly.",
                        "statement_type": "theorem",
                        "statement_motivation_html": "Understanding the stability of updates in isotropic attention mechanisms is crucial for designing robust neural networks. This set of lemmas provides conditions under which the updates remain stable despite the presence of noise in the queries ($q$), keys ($k_t$), or messages ($m_t$). Specifically, it tells us that:\n\n1. The update is stable to noisy queries when the keys are constant or when the messages are orthogonal to the keys.\n2. The update is stable to noisy keys when the queries are zero or when the messages are orthogonal to the noisy keys.\n3. The update is stable to noisy messages when the average noise in the messages is zero.\n\nThese insights are particularly useful when implementing attention mechanisms in neural networks, ensuring that the model remains reliable even in the presence of noise.",
                        "html_url": "library/theorems/theorem_9/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "5cd41d02-3a01-4490-b557-c6b61cbc0b22",
                            "paper_id": null,
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\textit{Proof.} ~For isotropic attention we have $a_t = \\frac{1}{T}$. For perturbations of $q$, the RHS of Eq.~\\ref{eq: stability: general q} is\n\\begin{equation}\n\\begin{split}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ m_t {\\tilde k}_t^T \\Big] \\epsilon^q ~&=~ \\sum_{t} a_t m_t {\\tilde k}_t^T \\epsilon^q \\\\\n    &=~ \\frac{1}{T} \\sum_{t=1}^T m_t {\\tilde k}_t^T \\epsilon^q \\\\\n    &=~ \\langle m_t {\\tilde k}_t^T \\rangle_t \\epsilon^q \\\\\n\\end{split}\n\\end{equation}\nFor lemma 1, we note that $k_t=const$ implies ${\\tilde k}_t=0$, and if $m_t \\perp k_t$ then $\\langle m_t {\\tilde k}_t^T \\rangle_t = \\langle m_t k_t \\rangle_t - \\langle m_t \\rangle_t \\langle k_t\\rangle_t = Cov(m_t,k_t) = 0$.\n\nFor perturbations of $k_t$, the RHS of Eq.~\\ref{eq: stability: general k} is\n\\begin{equation}\n\\begin{split}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ {\\tilde m}_t {\\epsilon^k_t}^T \\Big] q ~&=~ \\frac{1}{T} \\sum_{t=1}^T {\\tilde m}_t {\\epsilon^k_t}^T q \\\\\n    &=~ \\langle {\\tilde m}_t {\\epsilon^k_t}^T \\rangle_t q \\\\\n\\end{split}\n\\end{equation}\nFor lemma 2, this expression evaluates to $0$ if $q=0$, and if $m_t \\perp \\epsilon_t^k$ then $\\langle {\\tilde m}_t {\\epsilon^k_t}^T \\rangle_t = \\langle m_t {\\epsilon^k_t}^T \\rangle_t - \\langle m_t \\rangle_t \\langle {\\epsilon^k_t}^T\\rangle_t = Cov(m_t,{\\epsilon^k_t}^T) = 0$.\n\nFor perturbations of $m_t$, the RHS of Eq.~\\ref{eq: stability: general m} evaluates to\n\\begin{equation}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ \\epsilon^m_t \\Big] ~=~ \\frac{1}{T}\\sum_{t=1}^T \\epsilon^m_t ~=~ \\langle \\epsilon^m_t \\rangle_t\n\\end{equation}",
                            "statement_html": "For isotropic attention we have $a_t = \\frac{1}{T}$. For perturbations of $q$, the RHS of Eq. (39) [in <a href=\"https://arxiv.org/pdf/2406.17837#equation.F.39\"> original paper</a>] is\n\\begin{equation}\n\\begin{split}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ m_t {\\tilde k}_t^T \\Big] \\epsilon^q ~&=~ \\sum_{t} a_t m_t {\\tilde k}_t^T \\epsilon^q \\\\\n    &=~ \\frac{1}{T} \\sum_{t=1}^T m_t {\\tilde k}_t^T \\epsilon^q \\\\\n    &=~ \\langle m_t {\\tilde k}_t^T \\rangle_t \\epsilon^q \\\\\n\\end{split}\n\\end{equation}\nFor lemma 1, we note that $k_t=const$ implies ${\\tilde k}_t=0$, and if $m_t \\perp k_t$ then $\\langle m_t {\\tilde k}_t^T \\rangle_t = \\langle m_t k_t \\rangle_t - \\langle m_t \\rangle_t \\langle k_t\\rangle_t = Cov(m_t,k_t) = 0$.\n\nFor perturbations of $k_t$, the RHS of Eq. (40) [in <a href=\"https://arxiv.org/pdf/2406.17837#equation.F.40\"> original paper</a>] is\n\\begin{equation}\n\\begin{split}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ {\\tilde m}_t {\\epsilon^k_t}^T \\Big] q ~&=~ \\frac{1}{T} \\sum_{t=1}^T {\\tilde m}_t {\\epsilon^k_t}^T q \\\\\n    &=~ \\langle {\\tilde m}_t {\\epsilon^k_t}^T \\rangle_t q \\\\\n\\end{split}\n\\end{equation}\nFor lemma 2, this expression evaluates to $0$ if $q=0$, and if $m_t \\perp \\epsilon_t^k$ then $\\langle {\\tilde m}_t {\\epsilon^k_t}^T \\rangle_t = \\langle m_t {\\epsilon^k_t}^T \\rangle_t - \\langle m_t \\rangle_t \\langle {\\epsilon^k_t}^T\\rangle_t = Cov(m_t,{\\epsilon^k_t}^T) = 0$.\n\nFor perturbations of $m_t$, the RHS of Eq. (41) [in <a href=\"https://arxiv.org/pdf/2406.17837#equation.F.41\"> original paper</a>] evaluates to\n\\begin{equation}\n    \\mathop{\\mathbb{E}}_{a_t} \\Big[ \\epsilon^m_t \\Big] ~=~ \\frac{1}{T}\\sum_{t=1}^T \\epsilon^m_t ~=~ \\langle \\epsilon^m_t \\rangle_t\n\\end{equation}",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, we break it down into several steps, each addressing different perturbations and their implications:\n<br>\n<br>1. <i></i>Isotropic Attention<i></i>:\n<br>   - Given isotropic attention, we have \\( a_t = \\frac{1}{T} \\).\n<br>   - For perturbations of \\( q \\), the right-hand side (RHS) of Eq. (39) [in <a href=\"https://arxiv.org/pdf/2406.17837#equation.F.39\"> original paper</a>] is evaluated as follows:\n<br>   \\[\n   \\mathop{\\mathbb{E}}_{a_t} \\Big[ m_t {\\tilde k}_t^T \\Big] \\epsilon^q = \\sum_{t} a_t m_t {\\tilde k}_t^T \\epsilon^q = \\frac{1}{T} \\sum_{t=1}^T m_t {\\tilde k}_t^T \\epsilon^q = \\langle m_t {\\tilde k}_t^T \\rangle_t \\epsilon^q\n   \\]\n<br>2. <i></i>Lemma 1<i></i>:\n<br>   - If \\( k_t = \\text{const} \\), then \\( {\\tilde k}_t = 0 \\).\n<br>   - If \\( m_t \\perp k_t \\), then:\n<br>   \\[\n   \\langle m_t {\\tilde k}_t^T \\rangle_t = \\langle m_t k_t \\rangle_t - \\langle m_t \\rangle_t \\langle k_t \\rangle_t = \\text{Cov}(m_t, k_t) = 0\n   \\]\n<br>3. <i></i>Perturbations of \\( k_t \\)<i></i>:\n<br>   - For perturbations of \\( k_t \\), the RHS of Eq. (40) [in <a href=\"https://arxiv.org/pdf/2406.17837#equation.F.40\"> original paper</a>] is:\n<br>   \\[\n   \\mathop{\\mathbb{E}}_{a_t} \\Big[ {\\tilde m}_t {\\epsilon^k_t}^T \\Big] q = \\frac{1}{T} \\sum_{t=1}^T {\\tilde m}_t {\\epsilon^k_t}^T q = \\langle {\\tilde m}_t {\\epsilon^k_t}^T \\rangle_t q\n   \\]\n<br>4. <i></i>Lemma 2<i></i>:\n<br>   - This expression evaluates to \\( 0 \\) if \\( q = 0 \\).\n<br>   - If \\( m_t \\perp \\epsilon_t^k \\), then:\n<br>   \\[\n   \\langle {\\tilde m}_t {\\epsilon^k_t}^T \\rangle_t = \\langle m_t {\\epsilon^k_t}^T \\rangle_t - \\langle m_t \\rangle_t \\langle {\\epsilon^k_t}^T \\rangle_t = \\text{Cov}(m_t, {\\epsilon^k_t}^T) = 0\n   \\]\n<br>5. <i></i>Perturbations of \\( m_t \\)<i></i>:\n<br>   - For perturbations of \\( m_t \\), the RHS of Eq. (41) [in <a href=\"https://arxiv.org/pdf/2406.17837#equation.F.41\"> original paper</a>] evaluates to:\n<br>   \\[\n   \\mathop{\\mathbb{E}}_{a_t} \\Big[ \\epsilon^m_t \\Big] = \\frac{1}{T} \\sum_{t=1}^T \\epsilon^m_t = \\langle \\epsilon^m_t \\rangle_t\n   \\]\n<br>Each step systematically addresses the perturbations and their effects, leading to the conclusion of the proof."
                        }
                    },
                    {
                        "statement_id": "27255a8a-07b0-4dc0-ac96-c9f2cd0bdb02",
                        "paper_id": "fa48a779-dedb-4614-9ac9-e052c1896ac6",
                        "library_nr": 10,
                        "library_name": "Theorem 10",
                        "title": "Sensitivity of Sparse Attention to Multiplicative Perturbations",
                        "statement_original_tex": "\\begin{theorem}\n    Sensitivity of sparse attention to multiplicative perturbations $\\epsilon^q = \\kappa^q q$ and $\\epsilon^k = \\kappa^k_t k_t$ with $\\kappa^q,\\kappa^k_t\\ll1$. Circuit collapse occurs when $\\exists~ t \\neq t^*$ for which:\n    \\begin{equation}\n        \\frac{w_{t^*}}{w_t} ~\\begin{cases} ~<~ \\lambda_w & \\mathrm{if}~ w_t \\left(1 + \\kappa^q + \\kappa^k_{t^*}\\right) > 0 \\\\\n        ~>~ \\lambda_w & \\mathrm{otherwise} \\\\ \\end{cases}\n        ~~~~~~~~~~~~~ \\lambda_w ~\\triangleq~ \\frac{1 + \\kappa^q + \\kappa^k_t}{1 + \\kappa^q + \\kappa^k_{t^*}}\n    \\end{equation}\n    where temperature cancels in the fraction. \\textbf{Attention is fully stable above the critical transition point $\\lambda_w$} (c.f. $w_t \\left(1 + \\kappa^q + \\kappa^k_{t^*}\\right) > 0$). We see that query perturbations alone are insufficient, as they result in $\\lambda_w=1$. Lemma: consider the special case when all keys have similar length $k_t \\approx const$, the attended token has $\\theta_{t^*}\\approx0$, the keys are far-from-orthogonal s.t. $\\theta_t \\ll 1$, and $\\kappa^q\\approx0$. Using $w_t \\triangleq |q| |k_t| \\cos\\theta_t$, circuit collapse occurs when $\\exists~ t \\neq t^*$ for which:\n    \\begin{equation}\n            \\frac{1}{2}\\theta_t^2 ~\\lesssim~ \\kappa^k_t - \\kappa^k_{t^*}   ~~~~~~~~~~~ \\mathrm{if}~ w_t \\left(1  + \\kappa^k_{t^*}\\right) > 0 ~\\text{, otherwise reverse}\n    \\label{eq: sparse circuit collapse result}\n    \\end{equation}\n    i.e. stability requires either well-separated keys s.t. $\\theta_t \\gg 0$, or small perturbations $\\kappa_t-\\kappa^*_t \\ll 1$.\n\\label{theorem: multiplicative stability: sparse}\n\\end{theorem}",
                        "statement_html": "Sensitivity of sparse attention to multiplicative perturbations $\\epsilon^q = \\kappa^q q$ and $\\epsilon^k = \\kappa^k_t k_t$ with $\\kappa^q,\\kappa^k_t\\ll1$. Circuit collapse occurs when $\\exists~ t \\neq t^*$ for which:\n\\begin{equation}\n    \\frac{w_{t^*}}{w_t} ~\\begin{cases} ~&lt;~ \\lambda_w & \\mathrm{if}~ w_t \\left(1 + \\kappa^q + \\kappa^k_{t^*}\\right) &gt; 0 \\\\\n    ~&gt;~ \\lambda_w & \\mathrm{otherwise} \\\\ \\end{cases}\n    ~~~~~~~~~~~~~ \\lambda_w ~\\triangleq~ \\frac{1 + \\kappa^q + \\kappa^k_t}{1 + \\kappa^q + \\kappa^k_{t^*}}\n\\end{equation}\nwhere temperature cancels in the fraction. <strong>Attention is fully stable above the critical transition point $\\lambda_w$</strong> (c.f. $w_t \\left(1 + \\kappa^q + \\kappa^k_{t^*}\\right) &gt; 0$). We see that query perturbations alone are insufficient, as they result in $\\lambda_w=1$. Lemma: consider the special case when all keys have similar length $k_t \\approx const$, the attended token has $\\theta_{t^*}\\approx0$, the keys are far-from-orthogonal s.t. $\\theta_t \\ll 1$, and $\\kappa^q\\approx0$. Using $w_t \\triangleq |q| |k_t| \\cos\\theta_t$, circuit collapse occurs when $\\exists~ t \\neq t^*$ for which:\n\\begin{equation}\n        \\frac{1}{2}\\theta_t^2 ~\\lesssim~ \\kappa^k_t - \\kappa^k_{t^*}   ~~~~~~~~~~~ \\mathrm{if}~ w_t \\left(1  + \\kappa^k_{t^*}\\right) &gt; 0 ~\\text{, otherwise reverse}\n\\label{eq: sparse circuit collapse result}\n\\end{equation}\ni.e. stability requires either well-separated keys s.t. $\\theta_t \\gg 0$, or small perturbations $\\kappa_t-\\kappa^*_t \\ll 1$.",
                        "statement_type": "theorem",
                        "statement_motivation_html": "Understanding the stability of sparse attention mechanisms is crucial for designing robust neural networks, especially in natural language processing tasks. This lemma provides insights into how multiplicative perturbations in queries and keys can lead to circuit collapse, affecting the model's performance. By identifying the critical transition point $\\lambda_w$, we can ensure that attention remains stable, thereby improving the reliability of the model. This is particularly useful when dealing with large-scale data where even minor perturbations can significantly impact the results.",
                        "html_url": "library/theorems/theorem_10/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "4dbc2447-8ee1-4a09-a57b-2d73bda803b3",
                            "paper_id": null,
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\textit{Proof.} ~ Apply $q\\rightarrow q+\\epsilon^q$ and $k_t\\rightarrow k_t+\\epsilon_t^k$ to $w_t = q^Tk_t$, then we have $w_t \\rightarrow w_t + \\epsilon_w$ such that $\\epsilon^w_t = q^T\\epsilon_t^k + {\\epsilon^q}^Tk_t + {\\epsilon^q}^T\\epsilon_t^k$. For multiplicative perturbations we have  $\\epsilon^q = \\kappa^q q$ and $\\epsilon^k = \\kappa^k_t k_t$, and so $\\epsilon^w_t = \\kappa^k_t q^Tk_t + \\kappa^q q^Tk_t + \\kappa^k_t\\kappa^qq^Tk_t$. Each term recovers a factor of $w_t=q^Tk_t$, which we factor out to give $\\epsilon^w_t = \\left(\\kappa^q  + \\kappa^k_t + \\kappa^k_t\\kappa^q\\right)w_t$. The final term is subleading in the limit of small perturbations, and so\n\\begin{equation}\n    \\epsilon^w_t ~\\xrightarrow[~\\kappa^q,\\kappa^k_t\\rightarrow0~]{}~ \\left(\\kappa^q  ~+~ \\kappa^k_t\\right)w_t ~+~ \\mathcal{O}\\left(\\kappa^q\\kappa^k_t\\right)\n\\end{equation}\nCircuit collapse occurs when $w_{t^*} - w_t < \\epsilon^w_t - \\epsilon^w_{t^*}$ for some $t$. Substituting our limit for $\\epsilon^w_t$ gives\n\\begin{equation}\n    w_{t^*} - w_t ~<~ \\left(\\kappa^q  ~+~ \\kappa^k_t\\right)w_t - \\left(\\kappa^q  ~+~ \\kappa^k_{t^*}\\right)w_{t^*}\n\\end{equation}\nand collecting terms gives\n\\begin{equation}\n    \\left(1 ~+~ \\kappa^q ~+~ \\kappa^k_{t^*}\\right) w_{t^*} ~<~ \\left(1 ~+~ \\kappa^q ~+~ \\kappa^k_t\\right)w_t\n\\end{equation}\nWe then divide each side by $w_t (1 + \\kappa^q + \\kappa^k_{t^*})$, taking care to reverse the sign of the inequality when this factor is negative, to give\n\\begin{equation}\n    \\frac{w_{t^*}}{w_t} ~\\begin{cases} ~<~ \\lambda_w & \\mathrm{if}~ w_t \\left(1 + \\kappa^q + \\kappa^k_{t^*}\\right) &gt; 0 \\\\\n    ~&gt;~ \\lambda_w & \\mathrm{otherwise} \\\\ \\end{cases}\n    ~~~~~~~~~~~~~ \\lambda_w ~\\triangleq~ \\frac{1 + \\kappa^q + \\kappa^k_t}{1 + \\kappa^q + \\kappa^k_{t^*}}\n\\end{equation}\nwhich is the first expression in the theorem.  We note that any temperature parameter cancels in the fraction, which means that the attention head cannot become more stable by reducing its temperature to become more sparse. $\\lambda_w$ has the limits\n\\begin{equation}\n    \\lambda_w ~\\xrightarrow[\\kappa^q\\rightarrow0]{~~\\mathrm{keys~only}~~}~ \\frac{1+\\kappa^k_t}{1+\\kappa^k_{t^*}}\n    ~~~~~~~~~~~~~~~~~\n    \\lambda_w ~\\xrightarrow[\\kappa^k_t,\\kappa^k_{t^*}\\rightarrow0]{~~\\mathrm{query~only}~~}~ \\frac{1 + \\kappa_q}{1 + \\kappa_q} = 1\n\\end{equation}\nmeaning that query perturbations alone are insufficient, contributing only when they co-occur with perturbations on the keys. Write $w_t = |q| |k_t| \\cos\\theta_t$ with $\\theta_t = q \\wedge k_t$, and the approximation of identical key norms $k_{t^*}=k_t\\equiv k$ turns this into $w_t = |q| |k| \\cos\\theta_t$. Then\n\\begin{equation}\n    \\frac{w_{t^*}}{w_t} ~=~ \\frac{|q| |k| \\cos\\theta_{t^*}}{|q| |k| \\cos\\theta_t} ~=~ \\frac{\\cos\\theta_{t^*}}{\\cos\\theta_t}\n\\end{equation}\nThen $\\theta_{t^*}=0$ means that $\\cos\\theta_{t^*} = \\cos0=1$, and so $\\frac{\\cos\\theta_{t^*}}{\\cos\\theta_t} = \\frac{1}{\\cos\\theta_t} = \\sec \\theta_t$. We perform a Taylor expansion in $\\theta_t$ to obtain\n\\begin{equation}\n    \\frac{w_{t^*}}{w_t} ~\\approx~ \\sec\\theta_t ~\\approx~ 1 ~+~ \\frac{1}{2}\\theta_t^2 ~+~\\mathcal{O}\\left(\\theta_t^4\\right)\n\\end{equation}\nwhich is valid when $\\theta_t \\ll 1$. This is true for any $t\\neq t^*$ for which $k_t$ is far from orthogonal with $k_{t^*}$. Substituting this into our circuit collapse condition, we have\n\\begin{equation}\n    1 ~+~ \\frac{1}{2}\\theta_t^2 ~<~ \\frac{1 + \\kappa^k_t}{1 + \\kappa^k_{t^*}} ~~~~~~~~~~~~~~ \\mathrm{if}~ w_t \\left(1 + \\kappa^k_{t^*}\\right) &gt; 0 \n\\end{equation}\nwhere we consider the case of $\\kappa_q\\approx0$ for readability. Re-arranging gives\n\\begin{equation}\n    \\frac{1}{2}\\theta_t^2 ~\\lesssim~  \\kappa^k_t - \\kappa^k_{t^*} ~~~~~~~~~~~~~~~~\\text{Circuit~collapse~when~}k_t~\\text{similar}\n\\label{eq: app: sparse circuit collapse result duplicate}\n\\end{equation}\nif $w_t(1 + \\kappa^k_{t^*}) &gt; 0$, and we reverse the inequality otherwise. We have approximated the denominator on the RHS as $1 + \\kappa^k_{t^*} \\approx 1$ for $\\kappa^k_{t^*}\\rightarrow0$.\n\nWhen $\\theta_t \\ll 1$, the LHS of Eq.~\\ref{eq: app: sparse circuit collapse result duplicate} is small. This means that the attention head can tolerate only very small perturbations $\\{\\kappa^k_t,\\kappa^k_{t^*}\\}$. Therefore semantic subspaces must either have a highly orthogonal substructure s.t. $\\theta_t \\gtrsim 1 ~\\forall~t\\neq t^*$, or be orthogonal s.t. $\\kappa_t\\ll1 ~\\forall~ t$.",
                            "statement_html": "Apply $q \\rightarrow q + \\epsilon^q$ and $k_t \\rightarrow k_t + \\epsilon_t^k$ to $w_t = q^T k_t$, then we have $w_t \\rightarrow w_t + \\epsilon_w$ such that $\\epsilon^w_t = q^T \\epsilon_t^k + {\\epsilon^q}^T k_t + {\\epsilon^q}^T \\epsilon_t^k$. For multiplicative perturbations we have $\\epsilon^q = \\kappa^q q$ and $\\epsilon^k = \\kappa^k_t k_t$, and so $\\epsilon^w_t = \\kappa^k_t q^T k_t + \\kappa^q q^T k_t + \\kappa^k_t \\kappa^q q^T k_t$. Each term recovers a factor of $w_t = q^T k_t$, which we factor out to give $\\epsilon^w_t = \\left(\\kappa^q + \\kappa^k_t + \\kappa^k_t \\kappa^q\\right) w_t$. The final term is subleading in the limit of small perturbations, and so\n\\begin{equation}\n    \\epsilon^w_t ~\\xrightarrow[~\\kappa^q,\\kappa^k_t\\rightarrow0~]{}~ \\left(\\kappa^q  ~+~ \\kappa^k_t\\right)w_t ~+~ \\mathcal{O}\\left(\\kappa^q \\kappa^k_t\\right)\n\\end{equation}\nCircuit collapse occurs when $w_{t^*} - w_t &lt; \\epsilon^w_t - \\epsilon^w_{t^*}$ for some $t$. Substituting our limit for $\\epsilon^w_t$ gives\n\\begin{equation}\n    w_{t^*} - w_t ~&lt;~ \\left(\\kappa^q  ~+~ \\kappa^k_t\\right)w_t - \\left(\\kappa^q  ~+~ \\kappa^k_{t^*}\\right)w_{t^*}\n\\end{equation}\nand collecting terms gives\n\\begin{equation}\n    \\left(1 ~+~ \\kappa^q ~+~ \\kappa^k_{t^*}\\right) w_{t^*} ~&lt;~ \\left(1 ~+~ \\kappa^q ~+~ \\kappa^k_t\\right)w_t\n\\end{equation}\nWe then divide each side by $w_t (1 + \\kappa^q + \\kappa^k_{t^*})$, taking care to reverse the sign of the inequality when this factor is negative, to give\n\\begin{equation}\n    \\frac{w_{t^*}}{w_t} ~\\begin{cases} ~&lt;~ \\lambda_w & \\mathrm{if}~ w_t \\left(1 + \\kappa^q + \\kappa^k_{t^*}\\right) &gt; 0 \\\\\n    ~&gt;~ \\lambda_w & \\mathrm{otherwise} \\\\ \\end{cases}\n    ~~~~~~~~~~~~~ \\lambda_w ~\\triangleq~ \\frac{1 + \\kappa^q + \\kappa^k_t}{1 + \\kappa^q + \\kappa^k_{t^*}}\n\\end{equation}\nwhich is the first expression in the theorem. We note that any temperature parameter cancels in the fraction, which means that the attention head cannot become more stable by reducing its temperature to become more sparse. $\\lambda_w$ has the limits\n\\begin{equation}\n    \\lambda_w ~\\xrightarrow[\\kappa^q\\rightarrow0]{~~\\mathrm{keys~only}~~}~ \\frac{1+\\kappa^k_t}{1+\\kappa^k_{t^*}}\n    ~~~~~~~~~~~~~~~~~\n    \\lambda_w ~\\xrightarrow[\\kappa^k_t,\\kappa^k_{t^*}\\rightarrow0]{~~\\mathrm{query~only}~~}~ \\frac{1 + \\kappa_q}{1 + \\kappa_q} = 1\n\\end{equation}\nmeaning that query perturbations alone are insufficient, contributing only when they co-occur with perturbations on the keys. Write $w_t = |q| |k_t| \\cos\\theta_t$ with $\\theta_t = q \\wedge k_t$, and the approximation of identical key norms $k_{t^*} = k_t \\equiv k$ turns this into $w_t = |q| |k| \\cos\\theta_t$. Then\n\\begin{equation}\n    \\frac{w_{t^*}}{w_t} ~=~ \\frac{|q| |k| \\cos\\theta_{t^*}}{|q| |k| \\cos\\theta_t} ~=~ \\frac{\\cos\\theta_{t^*}}{\\cos\\theta_t}\n\\end{equation}\nThen $\\theta_{t^*} = 0$ means that $\\cos\\theta_{t^*} = \\cos0 = 1$, and so $\\frac{\\cos\\theta_{t^*}}{\\cos\\theta_t} = \\frac{1}{\\cos\\theta_t} = \\sec \\theta_t$. We perform a Taylor expansion in $\\theta_t$ to obtain\n\\begin{equation}\n    \\frac{w_{t^*}}{w_t} ~\\approx~ \\sec\\theta_t ~\\approx~ 1 ~+~ \\frac{1}{2}\\theta_t^2 ~+~\\mathcal{O}\\left(\\theta_t^4\\right)\n\\end{equation}\nwhich is valid when $\\theta_t \\ll 1$. This is true for any $t \\neq t^*$ for which $k_t$ is far from orthogonal with $k_{t^*}$. Substituting this into our circuit collapse condition, we have\n\\begin{equation}\n    1 ~+~ \\frac{1}{2}\\theta_t^2 ~&lt;~ \\frac{1 + \\kappa^k_t}{1 + \\kappa^k_{t^*}} ~~~~~~~~~~~~~~ \\mathrm{if}~ w_t \\left(1 + \\kappa^k_{t^*}\\right) &gt; 0 \n\\end{equation}\nwhere we consider the case of $\\kappa_q \\approx 0$ for readability. Re-arranging gives\n\\begin{equation}\n    \\frac{1}{2}\\theta_t^2 ~\\lesssim~  \\kappa^k_t - \\kappa^k_{t^*} ~~~~~~~~~~~~~~~~\\text{Circuit~collapse~when~}k_t~\\text{similar}\n\\label{eq: app: sparse circuit collapse result duplicate}\n\\end{equation}\nif $w_t(1 + \\kappa^k_{t^*}) &gt; 0$, and we reverse the inequality otherwise. We have approximated the denominator on the RHS as $1 + \\kappa^k_{t^*} \\approx 1$ for $\\kappa^k_{t^*} \\rightarrow 0$.\n\nWhen $\\theta_t \\ll 1$, the LHS of Eq. \\ref{eq: app: sparse circuit collapse result duplicate} is small. This means that the attention head can tolerate only very small perturbations $\\{\\kappa^k_t, \\kappa^k_{t^*}\\}$. Therefore semantic subspaces must either have a highly orthogonal substructure s.t. $\\theta_t \\gtrsim 1 ~\\forall~ t \\neq t^*$, or be orthogonal s.t. $\\kappa_t \\ll 1 ~\\forall~ t$.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several key steps:\n<br>\n<br>1. <i></i>Initial Perturbations<i></i>:\n<br>   - We start by applying small perturbations to the vectors \\( q \\) and \\( k_t \\), denoted as \\( \\epsilon^q \\) and \\( \\epsilon_t^k \\) respectively.\n<br>   - This results in a perturbation in \\( w_t = q^T k_t \\), denoted as \\( \\epsilon^w_t \\).\n<br>\n<br>2. <i></i>Expression for Perturbation<i></i>:\n<br>   - The perturbation \\( \\epsilon^w_t \\) is expressed in terms of \\( \\epsilon^q \\) and \\( \\epsilon_t^k \\).\n<br>   - For multiplicative perturbations, \\( \\epsilon^q = \\kappa^q q \\) and \\( \\epsilon^k = \\kappa^k_t k_t \\), leading to \\( \\epsilon^w_t = (\\kappa^q + \\kappa^k_t + \\kappa^q \\kappa^k_t) w_t \\).\n<br>\n<br>3. <i></i>Simplification for Small Perturbations<i></i>:\n<br>   - In the limit of small perturbations (\\( \\kappa^q, \\kappa^k_t \\rightarrow 0 \\)), the term \\( \\kappa^q \\kappa^k_t \\) becomes negligible.\n<br>   - Thus, \\( \\epsilon^w_t \\approx (\\kappa^q + \\kappa^k_t) w_t \\).\n<br>\n<br>4. <i></i>Circuit Collapse Condition<i></i>:\n<br>   - Circuit collapse occurs when the difference between \\( w_{t^*} \\) and \\( w_t \\) is less than the difference in their perturbations.\n<br>   - Substituting the expression for \\( \\epsilon^w_t \\) into this condition, we derive an inequality involving \\( w_{t^*} \\) and \\( w_t \\).\n<br>\n<br>5. <i></i>Normalization and Inequality<i></i>:\n<br>   - By normalizing the inequality, we obtain a condition involving the ratio \\( \\frac{w_{t^*}}{w_t} \\) and a parameter \\( \\lambda_w \\).\n<br>   - \\( \\lambda_w \\) is defined in terms of the perturbation factors \\( \\kappa^q \\) and \\( \\kappa^k_t \\).\n<br>\n<br>6. <i></i>Limits of \\( \\lambda_w \\)<i></i>:\n<br>   - We explore the limits of \\( \\lambda_w \\) for cases where only keys or only queries are perturbed.\n<br>   - For key-only perturbations, \\( \\lambda_w \\) simplifies to \\( \\frac{1+\\kappa^k_t}{1+\\kappa^k_{t^*}} \\).\n<br>   - For query-only perturbations, \\( \\lambda_w \\) equals 1, indicating that query perturbations alone are insufficient.\n<br>\n<br>7. <i></i>Geometric Interpretation<i></i>:\n<br>   - Expressing \\( w_t \\) in terms of the magnitudes and angle between \\( q \\) and \\( k_t \\), we derive \\( \\frac{w_{t^*}}{w_t} = \\frac{\\cos\\theta_{t^*}}{\\cos\\theta_t} \\).\n<br>   - For small angles \\( \\theta_t \\), this ratio approximates to \\( \\sec\\theta_t \\).\n<br>\n<br>8. <i></i>Taylor Expansion<i></i>:\n<br>   - Performing a Taylor expansion of \\( \\sec\\theta_t \\) for small \\( \\theta_t \\), we get \\( \\sec\\theta_t \\approx 1 + \\frac{1}{2}\\theta_t^2 \\).\n<br>\n<br>9. <i></i>Final Condition for Circuit Collapse<i></i>:\n<br>   - Substituting the Taylor expansion into the circuit collapse condition, we derive an inequality involving \\( \\theta_t^2 \\) and the perturbation factors.\n<br>   - This shows that circuit collapse occurs when the perturbations are small and the vectors \\( k_t \\) are similar.\n<br>\n<br>10. <i></i>Conclusion<i></i>:\n<br>    - The attention head can only tolerate very small perturbations unless the semantic subspaces are highly orthogonal.\n<br>    - This ensures stability in the presence of small perturbations."
                        }
                    },
                    {
                        "statement_id": "5f4c54c0-161b-4470-9927-e2b2f8911158",
                        "paper_id": "fa48a779-dedb-4614-9ac9-e052c1896ac6",
                        "library_nr": 11,
                        "library_name": "Theorem 11",
                        "title": "Multiplicative Stability in Isotropic Attention",
                        "statement_original_tex": "\\begin{theorem}\n    Sensitivity of isotropic attention to multiplicative perturbations. Say $\\epsilon^k = \\kappa^k_t k_t$ with $\\kappa^k_t\\ll1$ where $\\{\\kappa_t\\}$ have comparable amplitudes. Then\n    \\begin{equation}\n        \\epsilon^{\\Delta x(k)} \n        ~\\approx~\n        \\begin{cases}\n        0 ~&~ \\text{if~$\\kappa_t$~independent~of~${\\tilde m}_t$,~by~symmetry} \\\\\n        0 ~&~ \\text{if~$\\kappa_t\\equiv\\kappa$~for~constant~$\\kappa$} \\\\\n        0 ~&~ \\text{if~$q=0$} \\\\\n        w \\langle {\\tilde m}_t \\kappa^k_t \\rangle_t  ~&~ \\text{otherwise}\n        \\end{cases}\n    \\end{equation}\n\\label{theorem: multiplicative stability: isotropic}\n\\end{theorem}",
                        "statement_html": "Sensitivity of isotropic attention to multiplicative perturbations. Say $\\epsilon^k = \\kappa^k_t k_t$ with $\\kappa^k_t\\ll1$ where $\\{\\kappa_t\\}$ have comparable amplitudes. Then\n\\begin{equation}\n    \\epsilon^{\\Delta x(k)} \n    ~\\approx~\n    \\begin{cases}\n    0 ~&~ \\text{if $\\kappa_t$ independent of ${\\tilde m}_t$, by symmetry} \\\\\n    0 ~&~ \\text{if $\\kappa_t\\equiv\\kappa$ for constant $\\kappa$} \\\\\n    0 ~&~ \\text{if $q=0$} \\\\\n    w \\langle {\\tilde m}_t \\kappa^k_t \\rangle_t  ~&~ \\text{otherwise}\n    \\end{cases}\n\\end{equation}",
                        "statement_type": "theorem",
                        "statement_motivation_html": "This statement is useful in the context of analyzing the sensitivity of isotropic attention mechanisms to small multiplicative perturbations. It provides conditions under which the perturbations have negligible effects, which can be crucial for understanding the stability and robustness of such systems. Specifically, it tells us that if the perturbations are independent of the variable ${\\tilde m}_t$, constant, or if $q=0$, the effect is approximately zero. Otherwise, the effect is proportional to the weighted average of the product of ${\\tilde m}_t$ and the perturbation $\\kappa^k_t$. This can be particularly useful in designing and evaluating attention mechanisms in machine learning models.",
                        "html_url": "library/theorems/theorem_11/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "042cdcc1-bfc9-4e6b-af68-24026f34f3be",
                            "paper_id": null,
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\textit{Proof.} ~We begin with the following result from Theorem~\\ref{theorem: stability: isotropic}:\n\\begin{equation}\n\\epsilon^{\\Delta x(k)} ~\\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{~~perturb~k~~}}~ \\langle {\\tilde m}_t {\\epsilon^k_t}^T \\rangle_t ~q\n\\end{equation}\nSubstituting $\\epsilon^k = \\kappa^k_t k_t$ and taking $q$ inside the brackets gives\n\\begin{equation}\n\\langle{{\\tilde m}_t  \\epsilon^k_t}^T \\rangle_t ~q ~=~ \n\\langle {\\tilde m}_t \\kappa_t {k_t}^T \\rangle_t q ~=~\n ~ \\langle {\\tilde m}_t \\kappa_t w_t \\rangle_t\n\\end{equation}\nWe then notice that isotropic attention requires that $w_t$ is a constant, which we call $w$. Then\n\\begin{equation}\n\\epsilon^{\\Delta x(k)} ~\\approx~ w \\langle {\\tilde m}_t \\kappa_t \\rangle_t\n\\end{equation}\nis our general result. We then note three special cases, each resulting in $\\epsilon^{\\Delta x(k)}=0$:\n\\begin{enumerate}\n    \\item If $\\kappa_t \\perp {\\tilde m}_t$ then $\\langle {\\tilde m}_t \\kappa_t \\rangle_t = \\langle m_t \\kappa_t \\rangle_t - \\langle m_t \\rangle_t \\langle \\kappa_t \\rangle_t = Cov(m_t,\\kappa_t) = 0$. This is case when interference $\\kappa_t^k$ on the keys is not dominated by the same semantic subspace as the message $m_t$.\n    \\item If all keys are perturbed by the same factor $\\kappa_t\\equiv\\kappa$, then $\\langle {\\tilde m}_t \\kappa_t \\rangle_t = \\kappa \\langle {\\tilde m}_t \\rangle_t =0$ because $\\langle {\\tilde m}_t \\rangle_t=0$.\n    \\item Isotropic attention can be achieved by either $q=0$ or $k_t=const$. If the case is $q=0$ then this implies $w=0$ also.\n\\end{enumerate}",
                            "statement_html": "<em>Proof.</em> We begin with the following result from <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_9/index.html#theorem%3A+stability%3A+isotropic\">Theorem 9</a>:\n\\begin{equation}\n\\epsilon^{\\Delta x(k)} ~\\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{~~perturb~k~~}}~ \\langle {\\tilde m}_t {\\epsilon^k_t}^T \\rangle_t ~q\n\\end{equation}\nSubstituting $\\epsilon^k = \\kappa^k_t k_t$ and taking $q$ inside the brackets gives\n\\begin{equation}\n\\langle{{\\tilde m}_t  \\epsilon^k_t}^T \\rangle_t ~q ~=~ \n\\langle {\\tilde m}_t \\kappa_t {k_t}^T \\rangle_t q ~=~\n ~ \\langle {\\tilde m}_t \\kappa_t w_t \\rangle_t\n\\end{equation}\nWe then notice that isotropic attention requires that $w_t$ is a constant, which we call $w$. Then\n\\begin{equation}\n\\epsilon^{\\Delta x(k)} ~\\approx~ w \\langle {\\tilde m}_t \\kappa_t \\rangle_t\n\\end{equation}\nis our general result. We then note three special cases, each resulting in $\\epsilon^{\\Delta x(k)}=0$:\n<ol>\n    <li>If $\\kappa_t \\perp {\\tilde m}_t$ then $\\langle {\\tilde m}_t \\kappa_t \\rangle_t = \\langle m_t \\kappa_t \\rangle_t - \\langle m_t \\rangle_t \\langle \\kappa_t \\rangle_t = Cov(m_t,\\kappa_t) = 0$. This is case when interference $\\kappa_t^k$ on the keys is not dominated by the same semantic subspace as the message $m_t$.</li>\n    <li>If all keys are perturbed by the same factor $\\kappa_t\\equiv\\kappa$, then $\\langle {\\tilde m}_t \\kappa_t \\rangle_t = \\kappa \\langle {\\tilde m}_t \\rangle_t =0$ because $\\langle {\\tilde m}_t \\rangle_t=0$.</li>\n    <li>Isotropic attention can be achieved by either $q=0$ or $k_t=const$. If the case is $q=0$ then this implies $w=0$ also.</li>\n</ol>",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "\n\nWe start by referencing a result from <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_9/index.html#theorem%3A+stability%3A+isotropic\">Theorem 9</a>:\n\n\\[\n\\epsilon^{\\Delta x(k)} ~\\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{~~perturb~k~~}}~ \\langle {\\tilde m}_t {\\epsilon^k_t}^T \\rangle_t ~q\n\\]\n\nNext, we substitute \\(\\epsilon^k = \\kappa^k_t k_t\\) and bring \\(q\\) inside the brackets:\n\n\\[\n\\langle{{\\tilde m}_t  \\epsilon^k_t}^T \\rangle_t ~q ~=~ \n\\langle {\\tilde m}_t \\kappa_t {k_t}^T \\rangle_t q ~=~\n ~ \\langle {\\tilde m}_t \\kappa_t w_t \\rangle_t\n\\]\n\nWe then observe that isotropic attention requires \\(w_t\\) to be a constant, denoted as \\(w\\). Thus,\n\n\\[\n\\epsilon^{\\Delta x(k)} ~\\approx~ w \\langle {\\tilde m}_t \\kappa_t \\rangle_t\n\\]\n\nThis is our general result. We then consider three special cases, each leading to \\(\\epsilon^{\\Delta x(k)}=0\\):\n<br><br>1. <i></i>Orthogonality Condition<i></i>: If \\(\\kappa_t \\perp {\\tilde m}_t\\), then \\(\\langle {\\tilde m}_t \\kappa_t \\rangle_t = \\langle m_t \\kappa_t \\rangle_t - \\langle m_t \\rangle_t \\langle \\kappa_t \\rangle_t = \\text{Cov}(m_t,\\kappa_t) = 0\\). This occurs when the interference \\(\\kappa_t^k\\) on the keys is not dominated by the same semantic subspace as the message \\(m_t\\).\n<br><br>2. <i></i>Uniform Perturbation<i></i>: If all keys are perturbed by the same factor \\(\\kappa_t \\equiv \\kappa\\), then \\(\\langle {\\tilde m}_t \\kappa_t \\rangle_t = \\kappa \\langle {\\tilde m}_t \\rangle_t = 0\\) because \\(\\langle {\\tilde m}_t \\rangle_t = 0\\).\n<br><br>3. <i></i>Isotropic Attention<i></i>: This can be achieved by either \\(q = 0\\) or \\(k_t = \\text{const}\\). If \\(q = 0\\), it implies \\(w = 0\\) as well."
                        }
                    },
                    {
                        "statement_id": "aed3d295-f283-44d0-b0c0-0f2bb3140332",
                        "paper_id": "fa48a779-dedb-4614-9ac9-e052c1896ac6",
                        "library_nr": 12,
                        "library_name": "Theorem 12",
                        "title": "Shift Invariance of Attention",
                        "statement_original_tex": "\\begin{theorem}\n\tShifting attention scores $w_t$ by a constant offset does not affect the attention distribution. Therefore attention is fully determined by differences in scores.\n \\label{theorem: att shift operator}\n\\end{theorem}",
                        "statement_html": "Shifting attention scores $w_t$ by a constant offset does not affect the attention distribution. Therefore attention is fully determined by differences in scores.\n",
                        "statement_type": "theorem",
                        "statement_motivation_html": "Understanding that shifting attention scores $w_t$ by a constant offset does not affect the attention distribution is crucial in the field of machine learning, particularly in the design and analysis of attention mechanisms. This property ensures that the relative importance of different elements remains unchanged, allowing for more stable and interpretable models. Use this theorem when you need to simplify or normalize attention scores without altering the underlying distribution of attention.",
                        "html_url": "library/theorems/theorem_12/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "9e557a5d-ca76-48ed-b616-c41dd94ce77b",
                            "paper_id": null,
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\textit{Proof.} ~Applying the shift $w_t \\xrightarrow[\\mathrm{offset~w}]{} w_t + \\delta w ~\\forall~t$ with fixed $\\delta w$, we have\n\\begin{equation}\n\\begin{split}\na_t ~&=~ \\frac{e^{w_t}}{\\sum_{t'} e^{w_{t'}}} \\\\\n&\\xrightarrow[\\mathrm{offset~w}]{} ~ \\frac{e^{\\delta w}e^{w_t}}{\\sum_{t'} e^{\\delta w}e^{w_{t'}}} ~=~ \\frac{e^{\\delta w}}{e^{\\delta w}}\\frac{e^{w_t}}{\\sum_{t'} e^{w_{t'}}} ~=~ 1 \\cdot a_t~=~ a_t\n\\end{split}\n\\end{equation}\nAlternatively we may write\n\\begin{equation}\na_t ~=~ \\frac{e^{w_t}}{\\sum_{t'} e^{w_{t'}}} ~=~ \\frac{e^{w_t}}{e^{w_{t'}}\\sum_{t'} e^{w_{t'}-w_t}} ~=~ \\frac{1}{\\sum_{t'} e^{w_{t'}-w_t}}\n\\end{equation}\nwhere $\\left(w_{t'}+\\delta t\\right)-\\left(w_t+\\delta t\\right) = w_{t'}-w_t$.",
                            "statement_html": "Applying the shift $w_t \\xrightarrow[\\mathrm{offset~w}]{} w_t + \\delta w ~\\forall~t$ with fixed $\\delta w$, we have\n\\begin{equation}\n\\begin{split}\na_t ~&=~ \\frac{e^{w_t}}{\\sum_{t'} e^{w_{t'}}} \\\\\n&\\xrightarrow[\\mathrm{offset~w}]{} ~ \\frac{e^{\\delta w}e^{w_t}}{\\sum_{t'} e^{\\delta w}e^{w_{t'}}} ~=~ \\frac{e^{\\delta w}}{e^{\\delta w}}\\frac{e^{w_t}}{\\sum_{t'} e^{w_{t'}}} ~=~ 1 \\cdot a_t~=~ a_t\n\\end{split}\n\\end{equation}\nAlternatively we may write\n\\begin{equation}\na_t ~=~ \\frac{e^{w_t}}{\\sum_{t'} e^{w_{t'}}} ~=~ \\frac{e^{w_t}}{e^{w_{t'}}\\sum_{t'} e^{w_{t'}-w_t}} ~=~ \\frac{1}{\\sum_{t'} e^{w_{t'}-w_t}}\n\\end{equation}\nwhere $\\left(w_{t'}+\\delta t\\right)-\\left(w_t+\\delta t\\right) = w_{t'}-w_t$.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, we can break it down into the following steps:\n<br>\n<br>1. <i></i>Initial Expression<i></i>:\n<br>   The proof starts with the expression for \\(a_t\\):\n<br>   \\[\n   a_t = \\frac{e^{w_t}}{\\sum_{t'} e^{w_{t'}}}\n   \\]\n<br>\n<br>2. <i></i>Applying the Shift<i></i>:\n<br>   We apply a shift \\(w_t \\xrightarrow[\\mathrm{offset~w}]{} w_t + \\delta w\\) for all \\(t\\) with a fixed \\(\\delta w\\). This modifies the expression for \\(a_t\\):\n<br>   \\[\n   a_t \\xrightarrow[\\mathrm{offset~w}]{} \\frac{e^{\\delta w}e^{w_t}}{\\sum_{t'} e^{\\delta w}e^{w_{t'}}}\n   \\]\n<br>\n<br>3. <i></i>Simplifying the Expression<i></i>:\n<br>   The exponential terms \\(e^{\\delta w}\\) in the numerator and denominator cancel out:\n<br>   \\[\n   \\frac{e^{\\delta w}e^{w_t}}{\\sum_{t'} e^{\\delta w}e^{w_{t'}}} = \\frac{e^{\\delta w}}{e^{\\delta w}}\\frac{e^{w_t}}{\\sum_{t'} e^{w_{t'}}} = 1 \\cdot a_t = a_t\n   \\]\n<br>\n<br>4. <i></i>Alternative Representation<i></i>:\n<br>   Alternatively, we can rewrite \\(a_t\\) by factoring out \\(e^{w_t}\\) from the denominator:\n<br>   \\[\n   a_t = \\frac{e^{w_t}}{\\sum_{t'} e^{w_{t'}}} = \\frac{e^{w_t}}{e^{w_{t'}}\\sum_{t'} e^{w_{t'}-w_t}} = \\frac{1}{\\sum_{t'} e^{w_{t'}-w_t}}\n   \\]\n<br>\n<br>5. <i></i>Invariance Under Shift<i></i>:\n<br>   The expression \\(\\left(w_{t'}+\\delta t\\right)-\\left(w_t+\\delta t\\right) = w_{t'}-w_t\\) shows that the difference between the terms remains unchanged under the shift, confirming that \\(a_t\\) is invariant under the shift \\(w_t \\xrightarrow[\\mathrm{offset~w}]{} w_t + \\delta w\\).\n<br>\n<br>This proof demonstrates that the expression for \\(a_t\\) remains unchanged when a constant shift is applied to all \\(w_t\\) values."
                        }
                    },
                    {
                        "statement_id": "1e810612-48c2-4fde-a918-b041e3de6689",
                        "paper_id": "fa48a779-dedb-4614-9ac9-e052c1896ac6",
                        "library_nr": 13,
                        "library_name": "Theorem 13",
                        "title": "Attention Scaling Theorem",
                        "statement_original_tex": "\\begin{theorem}\n\tMultiplying attention scores by a positive factor changes the inverse-temperature of the attention distribution, modulating its sparsity (low temperature = less entropy = more sparse). Corollary: In the sparse limit, attention is fully determined by the order of $w_t$.\n \\label{theorem: att scale operator}\n\\end{theorem}",
                        "statement_html": "Multiplying attention scores by a positive factor changes the inverse-temperature of the attention distribution, modulating its sparsity (low temperature = less entropy = more sparse). Corollary: In the sparse limit, attention is fully determined by the order of $w_t$.",
                        "statement_type": "theorem",
                        "statement_motivation_html": "Understanding how multiplying attention scores by a positive factor affects the inverse-temperature of the attention distribution is crucial in the field of machine learning, particularly in the design and optimization of attention mechanisms. This insight allows us to control the sparsity of the attention distribution, which can be beneficial in various applications such as natural language processing and computer vision. In the sparse limit, where the attention becomes highly focused, the distribution is determined solely by the order of the attention scores, simplifying the model and potentially improving computational efficiency.",
                        "html_url": "library/theorems/theorem_13/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "c19678ff-1ba3-4e31-ba06-b8cae8312583",
                            "paper_id": null,
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\textit{Proof.} ~Applying the scaling $w_t \\xrightarrow[\\mathrm{scale~w}]{} \\kappa w_t ~\\forall~ t$ with fixed $\\kappa > 0$, we have\n\\begin{equation}\n\\begin{split}\n\\frac{e^{\\kappa w_t}}{\\sum_{t'} e^{\\kappa w_{t'}}} ~~&=~~ \\frac{1}{\\sum_{t'} e^{\\kappa (w_{t'}-w_t)}}   \\\\\n~~&\\xrightarrow[\\kappa\\rightarrow 0]{} ~~~~ \\frac{1}{\\sum_{t'} e^0} ~=~ \\frac{1}{T} ~\\forall~ t ~~~~~~~~~~~~~~~~~~~~~\\text{[fully isotropic distribution]} \\\\\n~~&\\xrightarrow[\\kappa\\rightarrow \\infty]{} ~~ \n\\begin{cases}\n1 ~~ ~\\mathrm{for}~ t = \\mathrm{argmax}_{t'} w_{t'} \\\\\n0 ~~ ~~~\\forall~ t \\neq \\mathrm{argmax}_{t'} w_{t'} \\\\\n\\end{cases}\n~~~\\text{[fully sparse distribution]}\n\\end{split}\n\\end{equation}\nwhere the \\texttt{argmax} operator is fully determined by the order of $w_t$.",
                            "statement_html": "Applying the scaling $w_t \\xrightarrow[\\mathrm{scale~w}]{} \\kappa w_t ~\\forall~ t$ with fixed $\\kappa > 0$, we have\n\\begin{equation}\n\\begin{split}\n\\frac{e^{\\kappa w_t}}{\\sum_{t'} e^{\\kappa w_{t'}}} ~~&=~~ \\frac{1}{\\sum_{t'} e^{\\kappa (w_{t'}-w_t)}}   \\\\\n~~&\\xrightarrow[\\kappa\\rightarrow 0]{} ~~~~ \\frac{1}{\\sum_{t'} e^0} ~=~ \\frac{1}{T} ~\\forall~ t ~~~~~~~~~~~~~~~~~~~~~\\text{[fully isotropic distribution]} \\\\\n~~&\\xrightarrow[\\kappa\\rightarrow \\infty]{} ~~ \n\\begin{cases}\n1 ~~ ~\\mathrm{for}~ t = \\mathrm{argmax}_{t'} w_{t'} \\\\\n0 ~~ ~~~\\forall~ t \\neq \\mathrm{argmax}_{t'} w_{t'} \\\\\n\\end{cases}\n~~~\\text{[fully sparse distribution]}\n\\end{split}\n\\end{equation}\nwhere the $\\texttt{argmax}$ operator is fully determined by the order of $w_t$.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break down the steps involved:\n<br>\n<br>1. <i></i>Scaling Transformation<i></i>: \n<br>   We start by applying a scaling transformation to the weights \\( w_t \\) such that \\( w_t \\) is scaled by a factor \\( \\kappa \\), where \\( \\kappa > 0 \\) is a fixed constant. This transformation is denoted as:\n<br>   \\[\n   w_t \\xrightarrow[\\mathrm{scale~w}]{} \\kappa w_t \\quad \\forall \\, t\n   \\]\n<br>\n<br>2. <i></i>Expression of Softmax Function<i></i>:\n<br>   The softmax function with the scaled weights is given by:\n<br>   \\[\n   \\frac{e^{\\kappa w_t}}{\\sum_{t'} e^{\\kappa w_{t'}}}\n   \\]\n<br>   This can be rewritten by factoring out \\( e^{\\kappa w_t} \\) from the denominator:\n<br>   \\[\n   \\frac{e^{\\kappa w_t}}{\\sum_{t'} e^{\\kappa w_{t'}}} = \\frac{1}{\\sum_{t'} e^{\\kappa (w_{t'} - w_t)}}\n   \\]\n<br>\n<br>3. <i></i>Limit as \\(\\kappa \\to 0\\)<i></i>:\n<br>   When \\(\\kappa\\) approaches 0, the exponent \\( \\kappa (w_{t'} - w_t) \\) approaches 0 for all \\( t' \\). Thus, \\( e^{\\kappa (w_{t'} - w_t)} \\) approaches 1 for all \\( t' \\). The sum in the denominator becomes \\( T \\), where \\( T \\) is the total number of terms:\n<br>   \\[\n   \\frac{1}{\\sum_{t'} e^{\\kappa (w_{t'} - w_t)}} \\xrightarrow[\\kappa\\rightarrow 0]{} \\frac{1}{\\sum_{t'} e^0} = \\frac{1}{T} \\quad \\forall \\, t\n   \\]\n<br>   This represents a fully isotropic distribution, where each term is equally likely.\n<br>\n<br>4. <i></i>Limit as \\(\\kappa \\to \\infty\\)<i></i>:\n<br>   When \\(\\kappa\\) approaches infinity, the term \\( e^{\\kappa (w_{t'} - w_t)} \\) will be dominated by the largest \\( w_{t'} \\). Specifically, if \\( t = \\mathrm{argmax}_{t'} w_{t'} \\), then \\( e^{\\kappa (w_{t'} - w_t)} \\) will be very large for \\( t' = t \\) and very small (approaching 0) for \\( t' \\neq t \\):\n<br>   \\[\n   \\frac{1}{\\sum_{t'} e^{\\kappa (w_{t'} - w_t)}} \\xrightarrow[\\kappa\\rightarrow \\infty]{} \n   \\begin{cases}\n   1 & \\text{for } t = \\mathrm{argmax}_{t'} w_{t'} \\\\\n   0 & \\forall \\, t \\neq \\mathrm{argmax}_{t'} w_{t'}\n   \\end{cases}\n   \\]\n<br>   This represents a fully sparse distribution, where only the term with the maximum weight has a probability of 1, and all other terms have a probability of 0.\n<br>\n<br>5. <i></i>Conclusion<i></i>:\n<br>   The \\(\\texttt{argmax}\\) operator is used to determine the index \\( t \\) that maximizes \\( w_t \\). This operator is fully determined by the order of the weights \\( w_t \\).\n<br>\n<br>In summary, the proof demonstrates how the softmax function transitions from a fully isotropic distribution to a fully sparse distribution as the scaling factor \\(\\kappa\\) varies from 0 to infinity."
                        }
                    },
                    {
                        "statement_id": "81ce0884-13ce-475b-a34c-f60f02a9d9a6",
                        "paper_id": "fa48a779-dedb-4614-9ac9-e052c1896ac6",
                        "library_nr": 14,
                        "library_name": "Theorem 14",
                        "title": "Inverse-Temperature Projection Theorem",
                        "statement_original_tex": "\\begin{theorem}\n    In the \\texttt{No-Norm} case, the attention distribution $a_t$ is defined by the projection of $y_t$ onto a fixed vector $y_x$ for a given $x$. The length of $y_x$ is an inverse-temperature parameter.\n\\end{theorem}",
                        "statement_html": "In the $\\texttt{No-Norm}$ case, the attention distribution $a_t$ is defined by the projection of $y_t$ onto a fixed vector $y_x$ for a given $x$. The length of $y_x$ is an inverse-temperature parameter.",
                        "statement_type": "theorem",
                        "statement_motivation_html": "The $\\texttt{No-Norm}$ case in attention mechanisms is useful for simplifying the computation of attention distributions. By projecting $y_t$ onto a fixed vector $y_x$, we can efficiently determine the relevance of $y_t$ with respect to $y_x$. This approach is particularly beneficial in scenarios where computational efficiency is crucial, such as in real-time applications or when dealing with large datasets. The inverse-temperature parameter, represented by the length of $y_x$, allows for fine-tuning the sharpness of the attention distribution, providing flexibility in how attention is allocated.",
                        "html_url": "library/theorems/theorem_14/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "b43dfe8d-1c80-46c2-bb34-4477e44731f4",
                            "paper_id": null,
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\textit{Proof.} ~Write $w_t = x^TW_{QK}y_t = (W_{QK}^Tx)^T y_t \\equiv y_x^T y_t$ where $y_x \\triangleq W_{QK}^T x\\in\\mathbb{R}^{N_y}$, which is the dot-product between $y_t$ and a fixed vector $y_x$ on the row space of $W_{QK}$. Then, re-writing in terms of the vector lengths and the enclosing angle $\\theta_{y_t} = y_x\\wedge y_t$, we have $w = |y_x||y_t|\\cos\\theta_{y_t}$. The factor $|y_x|$ is identical for all $t$, making it an inverse-temperature.",
                            "statement_html": "Write $w_t = x^TW_{QK}y_t = (W_{QK}^Tx)^T y_t \\equiv y_x^T y_t$ where $y_x \\triangleq W_{QK}^T x\\in\\mathbb{R}^{N_y}$, which is the dot-product between $y_t$ and a fixed vector $y_x$ on the row space of $W_{QK}$. Then, re-writing in terms of the vector lengths and the enclosing angle $\\theta_{y_t} = y_x\\wedge y_t$, we have $w = |y_x||y_t|\\cos\\theta_{y_t}$. The factor $|y_x|$ is identical for all $t$, making it an inverse-temperature.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, we can break it down into the following steps:\n<br>\n<br>1. <i></i>Initial Expression<i></i>:\n<br>   - We start with the expression \\( w_t = x^T W_{QK} y_t \\).\n<br>   - This can be rewritten as \\( w_t = (W_{QK}^T x)^T y_t \\).\n<br>\n<br>2. <i></i>Definition of \\( y_x \\)<i></i>:\n<br>   - Define \\( y_x \\triangleq W_{QK}^T x \\in \\mathbb{R}^{N_y} \\).\n<br>   - This allows us to express \\( w_t \\) as \\( w_t \\equiv y_x^T y_t \\).\n<br>\n<br>3. <i></i>Dot-Product Interpretation<i></i>:\n<br>   - The expression \\( y_x^T y_t \\) represents the dot-product between \\( y_t \\) and a fixed vector \\( y_x \\) in the row space of \\( W_{QK} \\).\n<br>\n<br>4. <i></i>Rewriting in Terms of Vector Lengths and Angle<i></i>:\n<br>   - We rewrite the dot-product in terms of the magnitudes (lengths) of the vectors and the cosine of the angle between them.\n<br>   - This gives us \\( w = |y_x||y_t|\\cos\\theta_{y_t} \\), where \\( \\theta_{y_t} \\) is the angle between \\( y_x \\) and \\( y_t \\).\n<br>\n<br>5. <i></i>Inverse-Temperature Factor<i></i>:\n<br>   - The magnitude \\( |y_x| \\) is constant for all \\( t \\), making it an inverse-temperature factor.\n<br>\n<br>This step-by-step breakdown helps in understanding how the initial expression is transformed and interpreted in terms of vector magnitudes and angles."
                        }
                    },
                    {
                        "statement_id": "373011d7-d16c-4aa2-884f-3ff90517a6b8",
                        "paper_id": "fa48a779-dedb-4614-9ac9-e052c1896ac6",
                        "library_nr": 15,
                        "library_name": "Theorem 15",
                        "title": "Bias Nullification Theorem",
                        "statement_original_tex": "\\begin{theorem}\nIn the \\texttt{No-Norm} case, bias parameters in the construction of query and key vectors are nullified by the \\texttt{softmax}, or only contribute terms that may be recovered if $x$ contains a constant direction.\n\\label{theorem: decomposition}\n\\end{theorem}",
                        "statement_html": "In the $\\texttt{No-Norm}$ case, bias parameters in the construction of query and key vectors are nullified by the $\\texttt{softmax}$, or only contribute terms that may be recovered if $x$ contains a constant direction.\n",
                        "statement_type": "theorem",
                        "statement_motivation_html": "The No-Norm case is useful in simplifying the computation of attention mechanisms in neural networks. By nullifying the bias parameters in the construction of query and key vectors, the softmax function ensures that only the essential directional information is retained. This can be particularly beneficial when dealing with input vectors that contain a constant direction, as it allows for a more efficient and focused computation.",
                        "html_url": "library/theorems/theorem_15/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "f4dcdfe8-6702-422f-91c8-681724731af9",
                            "paper_id": null,
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\textit{Proof.} ~Consider a modification to the construction of query and key vectors that uses the affine transformations $q = W_Qx+b_Q$ and $k_t = W_Ky_t+b_K$, with $W_{Q}\\in\\mathbb{R}^{N_{qkv}\\times N_x}$, $W_{K}\\in\\mathbb{R}^{N_{qkv}\\times N_y}$, $W_{QK}\\triangleq W_Q^TW_K$, and $b_Q,b_K\\in\\mathbb{R}^{N_{qkv}}$. The dot-product attention scores are then:\n\\begin{equation}\n\\begin{split}\n    w_t ~&=~ q^T k_t \\\\\n       &=~ \\left(W_Qx + b_Q\\right)^T \\left(W_Ky_t + b_K\\right)  \\\\\n       &=~ x^TW_{QK}y_t ~+~ ({W_Q}^Tb_K)^Tx ~+~ ({W_K}^Tb_Q)^Ty_t ~+~ b_Q^Tb_K \\\\\n    w_t ~+~ const~ &=~ x^TW_{QK}y_t ~+~ ({W_Q}^Tb_K)^Tx ~+~ ({W_K}^Tb_Q)^Ty_t  \\\\\n       &\\triangleq~ x^TW_{QK}y_t ~+~ \\rho_x^Tx ~+~ \\rho_y^Ty_t  ~~~~~~~~~~~~~\\rightarrow ~\\rho_x^Tx =const ~\\mathrm{given}~x\\rightarrow \\\\\n       &=~ x^TW_{QK}y_t ~+~ \\rho_y^Ty_t  ~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\rightarrow ~W_{QK} \\triangleq \\Omega^T \\Lambda \\Sigma~\\text{via SVD}~\\rightarrow  \\\\\n       &=~ x^T\\Omega^T\\Lambda\\Sigma y_t ~+~ \\rho_y^Ty_t ~~~~~~~~~~~~~~~~~~~~~~~~~~ \\rightarrow ~x' \\triangleq \\Omega x, ~~y'_t \\triangleq \\Sigma y_t ~\\rightarrow \\\\\n       &=~ {x'}^T\\Lambda y'_t ~+~ \\rho_y^Ty_t\n\\end{split}\n\\end{equation}\nAfter expanding the terms, we find an additive constant $b_Q^Tb_K$, and move this onto the LHS. Theorem~\\ref{theorem: att shift operator} states that this has no impact on the output of the \\texttt{softmax} operator. We identify $\\rho_x\\triangleq W_Q^Tb_k$ and $\\rho_y\\triangleq W_K^Tb_q$ as vectors on the \\textbf{row-spaces of $W_Q$ and $W_K$ respectively}, defined as linear maps of the special directions $b_K$ and $b_Q$. Since $x$ is constant for each \\texttt{softmax}, $\\rho_x^Tx$ is constant, and we absorb it into the LHS. We perform the singular value decomposition $W_{QK} \\triangleq \\Omega^T \\Lambda \\Sigma$ where $\\{\\Omega\\in\\mathbb{R}^{N_x\\times N_x},~\\Sigma\\in\\mathbb{R}^{N_y\\times N_y}\\}$ are orthonormal matrices and $\\Lambda \\in \\mathbb{R}^{N_x\\times N_y}$ is a diagonal matrix of positive-semidefinite singular values with maximum rank $\\min(N_x,N_y,N_{qkv})$. Orthonormal matrices apply a basis change to the embedding space using rotations and reflections. We write the transformed embeddings as $x' \\triangleq \\Omega x$ and $y_t' \\triangleq \\Sigma y_t$. The dot-product then has two terms:\n\\begin{enumerate}\n    \\item ${x'}^T \\Lambda y'_t = \\sum_i \\Lambda_{ii} x'_i y'_{ti}$ sculpts the attention distribution according to \\textit{pairwise relationships} between embeddings. We can say that $\\{\\Omega,\\Sigma\\}$ align the bases of $x$ and $y_t$, mapping them onto a common orthonormal coordinate system. $\\Lambda_{ii}$ then assigns an importance weight to each coordinate $i$, determining the contribution of $x'_iy'_{ti}$.\n    %\\footnote{``Every direction'' is true when $M=N_x=N_y\\equiv N$, otherwise the rank of $Q^TK$ is $min(M,N_x,N_y)$.}\n    %\\item $\\rho_x^Tx$ means ``$(s)$ receives from all senders when $x \\parallel \\rho_x$'', where $\\rho_x$ must be a vector on the column-space of $Q$.\n    \\item $\\rho_y^Ty$ means ``token $t$ sends to all receivers when $y_t \\parallel \\rho_y$'', where $\\rho_y$ must be a vector on the row-space of $W_K$. This may be recovered in the expansion of ${x'}^T\\Lambda y'_t$ if there exists a direction $i$ for which $x'_i=const$.\n\\end{enumerate}",
                            "statement_html": "Consider a modification to the construction of query and key vectors that uses the affine transformations $q = W_Qx+b_Q$ and $k_t = W_Ky_t+b_K$, with $W_{Q}\\in\\mathbb{R}^{N_{qkv}\\times N_x}$, $W_{K}\\in\\mathbb{R}^{N_{qkv}\\times N_y}$, $W_{QK}\\triangleq W_Q^TW_K$, and $b_Q,b_K\\in\\mathbb{R}^{N_{qkv}}$. The dot-product attention scores are then:\n\\begin{equation}\n\\begin{split}\n    w_t ~&=~ q^T k_t \\\\\n       &=~ \\left(W_Qx + b_Q\\right)^T \\left(W_Ky_t + b_K\\right)  \\\\\n       &=~ x^TW_{QK}y_t ~+~ ({W_Q}^Tb_K)^Tx ~+~ ({W_K}^Tb_Q)^Ty_t ~+~ b_Q^Tb_K \\\\\n    w_t ~+~ const~ &=~ x^TW_{QK}y_t ~+~ ({W_Q}^Tb_K)^Tx ~+~ ({W_K}^Tb_Q)^Ty_t  \\\\\n       &\\triangleq~ x^TW_{QK}y_t ~+~ \\rho_x^Tx ~+~ \\rho_y^Ty_t  ~~~~~~~~~~~~~\\rightarrow ~\\rho_x^Tx =const ~\\mathrm{given}~x\\rightarrow \\\\\n       &=~ x^TW_{QK}y_t ~+~ \\rho_y^Ty_t  ~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\rightarrow ~W_{QK} \\triangleq \\Omega^T \\Lambda \\Sigma~\\text{via SVD}~\\rightarrow  \\\\\n       &=~ x^T\\Omega^T\\Lambda\\Sigma y_t ~+~ \\rho_y^Ty_t ~~~~~~~~~~~~~~~~~~~~~~~~~~ \\rightarrow ~x' \\triangleq \\Omega x, ~~y'_t \\triangleq \\Sigma y_t ~\\rightarrow \\\\\n       &=~ {x'}^T\\Lambda y'_t ~+~ \\rho_y^Ty_t\n\\end{split}\n\\end{equation}\nAfter expanding the terms, we find an additive constant $b_Q^Tb_K$, and move this onto the LHS. <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_12/index.html#theorem%3A+att+shift+operator\">Theorem 12</a> states that this has no impact on the output of the $\\texttt{softmax}$ operator. We identify $\\rho_x\\triangleq W_Q^Tb_k$ and $\\rho_y\\triangleq W_K^Tb_q$ as vectors on the <b>row-spaces of $W_Q$ and $W_K$ respectively</b>, defined as linear maps of the special directions $b_K$ and $b_Q$. Since $x$ is constant for each $\\texttt{softmax}$, $\\rho_x^Tx$ is constant, and we absorb it into the LHS. We perform the singular value decomposition $W_{QK} \\triangleq \\Omega^T \\Lambda \\Sigma$ where $\\{\\Omega\\in\\mathbb{R}^{N_x\\times N_x},~\\Sigma\\in\\mathbb{R}^{N_y\\times N_y}\\}$ are orthonormal matrices and $\\Lambda \\in \\mathbb{R}^{N_x\\times N_y}$ is a diagonal matrix of positive-semidefinite singular values with maximum rank $\\min(N_x,N_y,N_{qkv})$. Orthonormal matrices apply a basis change to the embedding space using rotations and reflections. We write the transformed embeddings as $x' \\triangleq \\Omega x$ and $y_t' \\triangleq \\Sigma y_t$. The dot-product then has two terms:\n<ol>\n    <li>${x'}^T \\Lambda y'_t = \\sum_i \\Lambda_{ii} x'_i y'_{ti}$ sculpts the attention distribution according to <i>pairwise relationships</i> between embeddings. We can say that $\\{\\Omega,\\Sigma\\}$ align the bases of $x$ and $y_t$, mapping them onto a common orthonormal coordinate system. $\\Lambda_{ii}$ then assigns an importance weight to each coordinate $i$, determining the contribution of $x'_iy'_{ti}$.</li>\n    <li>$\\rho_y^Ty$ means \"token $t$ sends to all receivers when $y_t \\parallel \\rho_y$\", where $\\rho_y$ must be a vector on the row-space of $W_K$. This may be recovered in the expansion of ${x'}^T\\Lambda y'_t$ if there exists a direction $i$ for which $x'_i=const$.</li>\n</ol>",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "Consider a modification to the construction of query and key vectors that uses the affine transformations \\( q = W_Qx + b_Q \\) and \\( k_t = W_Ky_t + b_K \\), with \\( W_{Q} \\in \\mathbb{R}^{N_{qkv} \\times N_x} \\), \\( W_{K} \\in \\mathbb{R}^{N_{qkv} \\times N_y} \\), \\( W_{QK} \\triangleq W_Q^T W_K \\), and \\( b_Q, b_K \\in \\mathbb{R}^{N_{qkv}} \\). The dot-product attention scores are then:\n\n\\[\n\\begin{split}\n    w_t ~&=~ q^T k_t \\\\\n       &=~ \\left(W_Q x + b_Q\\right)^T \\left(W_K y_t + b_K\\right)  \\\\\n       &=~ x^T W_{QK} y_t ~+~ ({W_Q}^T b_K)^T x ~+~ ({W_K}^T b_Q)^T y_t ~+~ b_Q^T b_K \\\\\n    w_t ~+~ \\text{const}~ &=~ x^T W_{QK} y_t ~+~ ({W_Q}^T b_K)^T x ~+~ ({W_K}^T b_Q)^T y_t  \\\\\n       &\\triangleq~ x^T W_{QK} y_t ~+~ \\rho_x^T x ~+~ \\rho_y^T y_t  ~~~~~~~~~~~~~\\rightarrow ~\\rho_x^T x = \\text{const} ~\\text{given}~ x \\rightarrow \\\\\n       &=~ x^T W_{QK} y_t ~+~ \\rho_y^T y_t  ~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\rightarrow ~W_{QK} \\triangleq \\Omega^T \\Lambda \\Sigma~\\text{via SVD}~\\rightarrow  \\\\\n       &=~ x^T \\Omega^T \\Lambda \\Sigma y_t ~+~ \\rho_y^T y_t ~~~~~~~~~~~~~~~~~~~~~~~~~~ \\rightarrow ~x' \\triangleq \\Omega x, ~~y'_t \\triangleq \\Sigma y_t ~\\rightarrow \\\\\n       &=~ {x'}^T \\Lambda y'_t ~+~ \\rho_y^T y_t\n\\end{split}\n\\]\n\nAfter expanding the terms, we find an additive constant \\( b_Q^T b_K \\), and move this onto the LHS. <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_12/index.html#theorem%3A+att+shift+operator\">Theorem 12</a> states that this has no impact on the output of the \\(\\texttt{softmax}\\) operator. We identify \\(\\rho_x \\triangleq W_Q^T b_K\\) and \\(\\rho_y \\triangleq W_K^T b_Q\\) as vectors on the <i></i>row-spaces of \\( W_Q \\) and \\( W_K \\) respectively<i></i>, defined as linear maps of the special directions \\( b_K \\) and \\( b_Q \\). Since \\( x \\) is constant for each \\(\\texttt{softmax}\\), \\(\\rho_x^T x\\) is constant, and we absorb it into the LHS. We perform the singular value decomposition \\( W_{QK} \\triangleq \\Omega^T \\Lambda \\Sigma \\) where \\(\\{\\Omega \\in \\mathbb{R}^{N_x \\times N_x},~\\Sigma \\in \\mathbb{R}^{N_y \\times N_y}\\}\\) are orthonormal matrices and \\(\\Lambda \\in \\mathbb{R}^{N_x \\times N_y}\\) is a diagonal matrix of positive-semidefinite singular values with maximum rank \\(\\min(N_x, N_y, N_{qkv})\\). Orthonormal matrices apply a basis change to the embedding space using rotations and reflections. We write the transformed embeddings as \\( x' \\triangleq \\Omega x \\) and \\( y_t' \\triangleq \\Sigma y_t \\). The dot-product then has two terms:\n\n1. \\({x'}^T \\Lambda y'_t = \\sum_i \\Lambda_{ii} x'_i y'_{ti}\\) sculpts the attention distribution according to <i>pairwise relationships</i> between embeddings. We can say that \\(\\{\\Omega, \\Sigma\\}\\) align the bases of \\( x \\) and \\( y_t \\), mapping them onto a common orthonormal coordinate system. \\(\\Lambda_{ii}\\) then assigns an importance weight to each coordinate \\( i \\), determining the contribution of \\( x'_i y'_{ti} \\).\n\n2. \\(\\rho_y^T y\\) means \"token \\( t \\) sends to all receivers when \\( y_t \\parallel \\rho_y \\)\", where \\(\\rho_y\\) must be a vector on the row-space of \\( W_K \\). This may be recovered in the expansion of \\({x'}^T \\Lambda y'_t\\) if there exists a direction \\( i \\) for which \\( x'_i = \\text{const} \\)."
                        }
                    }
                ],
                "corollaries": []
            },
            "mathjax_macros": [
                "emph: [\"\\\\textit{#1}\", 1]",
                "bm: [\"\\\\boldsymbol{\\\\mathbf{#1}}\", 1]",
                "mathds: [\"\\\\mathbf{#1}\", 1]",
                "textsl: [\"\\\\textit{#1}\", 1]",
                "vspace: [\"\", 1]",
                "xspace: \"\"",
                "data: [\"\\\\textcolor{Maroon}{\\\\texttt{{#1}}}\", 1]"
            ],
            "mathjax_environments": [
                "subequations: [\"{\", \"}\"]"
            ],
            "label2statementid": {
                "theorem: structure: no-norm": "362f460c-83b0-4db5-b727-fb8cf1c91297",
                "theorem: structure: pre-norm": "f16d8074-27e3-4f2d-b6c0-73826fbee538",
                "theorem: structure: qkv-norm": "6b2d26aa-bacf-4ff5-be0f-794ae2b8b70f",
                "theorem: stability: general": "11793c7d-e742-4b70-8386-c6f4212cab08",
                "theorem: stability: sparse": "dfa8874b-1868-4a82-b832-55f6bf151677",
                "theorem: stability: isotropic": "284bd1c6-c3ec-4d4e-82e0-1324bf888aea",
                "eq: sparse circuit collapse result": "27255a8a-07b0-4dc0-ac96-c9f2cd0bdb02",
                "theorem: multiplicative stability: sparse": "27255a8a-07b0-4dc0-ac96-c9f2cd0bdb02",
                "theorem: multiplicative stability: isotropic": "5f4c54c0-161b-4470-9927-e2b2f8911158",
                "theorem: att shift operator": "aed3d295-f283-44d0-b0c0-0f2bb3140332",
                "theorem: att scale operator": "1e810612-48c2-4fde-a918-b041e3de6689",
                "theorem: decomposition": "373011d7-d16c-4aa2-884f-3ff90517a6b8"
            }
        },
        {
            "paper_id": "9210081a-2305-4894-99c6-c1631ba1cabe",
            "title": "The Geometry of Categorical and Hierarchical Concepts in Large Language Models",
            "authors": [
                "Kiho Park",
                "Yo Joong Choe",
                "Yibo Jiang",
                "Victor Veitch"
            ],
            "year": 2024,
            "source_url": "https://arxiv.org/abs/2406.01506",
            "html_url": "library/papers/the_geometry_of_categorical_and_hierarchical_concepts_in_large_language_models/index.html",
            "bibtex": "@misc{park2024geometrycategoricalhierarchicalconcepts,\n\ttitle={The Geometry of Categorical and Hierarchical Concepts in Large Language Models}, \n\tauthor={Kiho Park and Yo Joong Choe and Yibo Jiang and Victor Veitch},\n\tyear={2024},\n\teprint={2406.01506},\n\tarchivePrefix={arXiv},\n\tprimaryClass={cs.CL},\n\turl={https://arxiv.org/abs/2406.01506}}",
            "original_tex": "==== BEGINNING OF /2406.01506/your-paper.tex ====\n\\pdfoutput=1\n\\documentclass{article}\n\\usepackage{verbatim}\n\n\\input{preamble/preamble.tex}\n\\input{preamble/preamble_math.tex}\n\\input{preamble/definitions_basic.tex}\n\\input{preamble/minimalist_biblatex.tex}\n\n\\addbibresource{bibs/hierarchical.bib}\n\n\\usepackage{thm-restate}\n\n\\crefformat{equation}{(#2#1#3)}\n\\crefformat{figure}{Figure~#2#1#3}\n\\crefname{definition}{Definition}{Definitions}\n\\crefname{example}{Example}{Examples}\n\\crefname{lemma}{Lemma}{Lemmas}\n\\crefname{cor}{Corollary}{Corollaries}\n\\crefname{theorem}{Theorem}{Theorems}\n\\crefname{assumption}{Assumption}{Assumptions}\n\n\n\n\\DeclareMathOperator{\\lmOp}{\\lambda_{\\text{LM}}}\n\\newcommand{\\lmText}[1]{\\lmOp(\\text{``#1''})}\n\\newcommand{\\lm}[1]{\\lmOp(#1)}\n\n\\DeclareMathOperator{\\repOp}{Rep}\n\\newcommand{\\repText}[1]{\\repOp(\\text{``#1''})}\n\\newcommand{\\rep}[1]{\\repOp(#1)}\n\n\\DeclareMathOperator{\\genOp}{gen}\n\\newcommand{\\promptSpace}{\\mathcal{X}}\n\\newcommand{\\conceptSpace}{\\mathcal{C}}\n\\newcommand{\\repSpace}{\\mathcal{R}}\n\\newcommand{\\responseSpace}{\\mathcal{Y}}\n\\newcommand{\\defas}{:=}\n\\newcommand{\\nullconcept}{\\phi}\n\\newcommand{\\conceptDistSpace}{\\mathcal{Q}}\n\n\\newcommand{\\ConceptName}[1]{$\\mathtt{#1}$}\n\\newcommand{\\ConceptValue}[1]{\\texttt{#1}}\n\\newcommand{\\ConceptDirName}[2]{\\texttt{#1}\\Rightarrow\\texttt{#2}}\n\\newcommand{\\ConceptDirMath}[2]{#1 \\Rightarrow #2}\n\n\\DeclareMathOperator{\\SpanOp}{Span}\n\\newcommand{\\SpanText}[1]{\\SpanOp(\\text{``#1''})}\n\\newcommand{\\Span}[1]{\\SpanOp(#1)}\n\n\\DeclareMathOperator{\\ConeOp}{Cone}\n\\newcommand{\\ConeText}[1]{\\ConeOp(\\text{``#1''})}\n\\newcommand{\\Cone}[1]{\\ConeOp(#1)}\n\\newcommand{\\cov}{\\mathrm{Cov}}\n\\newcommand{\\yquad}{\\mathcal{Y}}\n\\newcommand{\\ip}[2]{\\langle #1,#2\\rangle}\n\n\\usepackage{enumitem}\n\n\\usepackage[affil-it]{authblk}\n\n\n\\title{The Geometry of Categorical and \\\\ Hierarchical Concepts in Large Language Models}\n\n\\date{}\n\\author[1]{Kiho Park}\n\\author[2]{Yo Joong Choe}\n\\author[3]{Yibo Jiang}\n\\author[1,2]{Victor Veitch}\n\\affil[1]{Department of Statistics, University of Chicago}\n\\affil[2]{Data Science Institute, University of Chicago}\n\\affil[3]{Department of Computer Science, University of Chicago}\n\n\\begin{document}\n\\maketitle\n\n\\begin{abstract}\n  Understanding how semantic meaning is encoded in the representation spaces of large language models is a fundamental problem in interpretability.\n  In this paper, we study the two foundational questions in this area. First, how are categorical concepts, such as $\\{\\ConceptValue{mammal}, \\ConceptValue{bird}, \\ConceptValue{reptile}, \\ConceptValue{fish}\\}$, represented? Second, how are hierarchical relations between concepts encoded? For example, how is the fact that \\ConceptName{dog} is a kind of \\ConceptName{mammal} encoded?\n  We show how to extend the linear representation hypothesis to answer these questions. We find a remarkably simple structure: simple categorical concepts are represented as simplices, hierarchically related concepts are orthogonal in a sense we make precise, and (in consequence) complex concepts are represented as polytopes constructed from direct sums of simplices, reflecting the hierarchical structure.\n  We validate these theoretical results on the Gemma large language model, estimating representations for 957 hierarchically related concepts using data from WordNet.\n  Code is available at \\href{https://github.com/KihoPark/LLM_Categorical_Hierarchical_Representations}{github.com/KihoPark/LLM\\_Categorical\\_Hierarchical\\_Representations}.\n\\end{abstract}\n\n\n\n\\section{Introduction}\\label{sec:introduction}\n\nThis paper concerns how high-level semantic concepts are encoded in the representation spaces of large language models (LLMs).\nUnderstanding this is crucial for model interpretability and control. The ultimate aspiration is to monitor (and manipulate) the semantic behavior of LLMs (e.g., is the model's response truthful) by directly measuring (and editing) the internal vector representations of the model \\Citep[e.g.,][]{li2023emergent, zou2023representation, ghandeharioun2024patchscope}.\nAchieving this requires understanding how the geometric structure of the representation spaces corresponds to the high-level semantic concepts that humans understand.\nIn this paper, we are concerned with two fundamental questions in this direction:\n\\begin{enumerate}\n  \\item How are categorical concepts represented? For example, what is the representation of the concept $\\ConceptValue{animal} = \\{\\ConceptValue{mammal}, \\ConceptValue{bird}, \\ConceptValue{reptile}, \\ConceptValue{fish}\\}$?\n  \\item How are hierarchical relations between concepts represented? For example, what is the relationship between the representations of $\\ConceptValue{animal}$, $\\ConceptValue{mammal}$, $\\ConceptValue{dog}$, and $\\ConceptValue{poodle}$?\n\\end{enumerate}\n\nOur starting point is the \\emph{linear representation hypothesis}, the informal idea that high-level concepts are linearly encoded in the representation spaces of LLMs \\Citep[e.g.,][]{marks2023geometry, tigges2023linear, gurnee2024language}.\nA main challenge for the linear representation hypothesis is that, in general, it's not clear what ``linear'' means, nor what constitutes a ``high-level concept''.\n\\Citet{park2024linear} give a formalization in the limited setting of binary concepts that can be defined by counterfactual pairs of words. For example, the concept of $\\ConceptDirName{male}{female}$ is formalized using the counterfactual pairs $\\{(\\text{``man''}, \\text{``woman''}), (\\text{``king''}, \\text{``queen''}), \\dots\\}$. \nThey prove that such binary concepts have a well-defined linear representation as a direction in the representation space.\nThey further connect semantic structure and representation geometry by showing that, under a \\emph{causal inner product}, concepts that can be freely manipulated (e.g., $\\ConceptDirName{male}{female}$ and $\\ConceptDirName{french}{english}$) are represented by orthogonal directions.\n\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[trim={3cm 0.7cm 1.5cm 1cm}, clip, width=1.0\\linewidth]{figures/diagram.pdf}\n  \\caption{\n    In large language models, categorical concepts are represented as simplices in the representation space.\n    Further, hierarchically related concepts (such as \\ConceptName{animal} and $\\ConceptDirName{mammal}{bird}$) live in orthogonal subspaces.\n    The top panel illustrates the structure, the bottom panels show the measured representation structure in the Gemma LLM. See \\Cref{sec:experiments} for details.}\n  \\label{fig:diagram}\n\\end{figure}\n\nOur aim is to extend this formalization beyond binary concepts represented as counterfactual word pairs. For example, the \\ConceptName{animal} concept does not have a natural counterfactual definition, but such concepts are fundamental to human semantic understanding. Further, we aim to understand how the geometry of the representation space encodes relationships between concepts that cannot be freely manipulated, such as \\ConceptName{animal} and \\ConceptName{mammal}. \n\nTo that end, we make the following contributions:\n\\begin{enumerate}\n  \\item We show how to move from representations of binary concepts as \\emph{directions} to representations as \\emph{vectors}. This allows us to use vector operations to compose representations.\n  \\item Using this result, we show that semantic hierarchy between concepts is encoded geometrically as orthogonality between representations, in a manner we make precise.\n  \\item Then, we construct the representation of categorical variables (e.g., \\ConceptName{animal}) as the polytope where the vertices are the representations of the binary features that define the category (e.g., $\\ConceptValue{mammal}, \\ConceptValue{bird}, \\dots$). We show that for ``natural'' concepts, the representation is a simplex.\n  \\item Finally, we empirically validate these theoretical results on the Gemma large language model \\Citep[][]{team2024gemma}. To that end, we extract concepts from the WordNet hierarchy \\Citep[][]{miller1995wordnet}, estimate their representations, and show that the geometric structure of the representations align with the semantic hierarchy of WordNet.\n\\end{enumerate}\nThe final structure is remarkably simple, and is summarized in \\Cref{fig:diagram}.\nIn totality, these results provide a foundation for understanding how high-level semantic concepts are encoded in the representation spaces of LLMs. \n\n\n\\section{Preliminaries}\\label{sec:preliminaries}\nWe begin with some necessary background.\n\\subsection{Large Language Models}\nFor the purposes of this paper, we consider a large language model to consist of two parts. \nThe first part is a function $\\lambda$ that maps input texts $x$ to vectors $\\lambda(x)$ in a representation space $\\Lambda  \\simeq \\Reals^d$. This is the function given by the stacked transformer blocks. We take $\\lambda(x)$ to be the output of the final layer at the final token position. The second part is an unembedding layer that assigns a vector $\\gamma(y)$ in an unembedding space $\\Gamma  \\simeq \\Reals^d$ to each token $y$ in the vocabulary. Together, these define a sampling distribution over tokens via the softmax distribution:\n\\begin{equation}\n  \\Pr(y\\given x) = \\frac{\\exp(\\lambda(x)^\\top \\gamma(y))}{\\sum_{y' \\in \\mathrm{Vocab}}\\exp(\\lambda(x)^\\top \\gamma(y'))}.\n\\end{equation}\n\nThe broad goal is to understand how semantic structure is encoded in the geometry of the spaces $\\Lambda$ and $\\Gamma$.\n(We do not address the ``internal'' structure of the LLMs in this paper, though we are optimistic that a clear understanding of the softmax geometry will shed light on this as well.)\n\n\\subsection{Concepts}\nWe formalize a concept as a latent variable $W$ that is caused by the context $X$ and causes the output $Y$. \nThat is, a concept is a thing that could---in principle---be manipulated to affect the output of the language model.\nIn the particular case where a concept is a binary variable with a word-level counterfactual, we can identify the variable $W$ with the counterfactual pair of outputs $(Y(0), Y(1))$. Concretely, we can identify $\\ConceptDirName{male}{female}$ with $(Y(0), Y(1)) \\in_R \\{(\\text{``man''}, \\text{``woman''}), (\\text{``king''}, \\text{``queen''}), (\\text{``he''}, \\text{``her''}), \\dots\\}$. We emphasize that the notion of a concept as a latent variable that affects the output is more general than the counterfactual binary case.\n\nGiven a pair of concept variables $W$ and $Z$, we say that $W$ is \\emph{causally separable} with $Z$ if the potential outcome $Y(W=w, Z=z)$ is well-defined for all $w,z$.\nThat is, two variables are causally separable if they can be freely manipulated---e.g., we can change the output language and the sex of the subject freely, so these concepts are causally separable.\n\n\\subsection{Causal Inner Product and Linear Representations}\nWe are trying to understand how concepts are represented.\nAt this stage, there are two distinct representation spaces: $\\Lambda$ and $\\Gamma$. The former is the space of context embeddings, and the latter is the space of token unembeddings. \nWe would like to unify these spaces so that there is just a single notion of representation. \n\n\\Citet{park2024linear} show how to achieve this unification via a ``Causal Inner Product''. This is a particular choice of inner product that respects the semantics of language in the sense that the linear representations of (binary, counterfactual) causally separable concepts are orthogonal under the inner product. Their result can be understood as saying that there is some invertible matrix $A$ and constant vector $\\bar{\\gamma}_0$ such that, if we transform the embedding and unembedding spaces as\n\\begin{equation}\\label{eq:transformation}\n  g(y) \\leftarrow A (\\gamma(y) - \\bar{\\gamma}_0), \\quad \\ell(x) \\leftarrow A^{-\\top} \\lambda(x)\n\\end{equation}\nthen the Euclidean inner product in the transformed spaces is the causal inner product, and the Riesz isomorphism between the embedding and unembedding spaces is simply the usual vector transpose operation. \nWe can estimate $A$ as the whitening operation for the unembedding matrix.\nFollowing this transformation, we can think of the embedding and unembedding spaces as the same space, equipped with the Euclidean inner product.\\footnote{We are glossing over some technical details here; see \\citep{park2024linear} for details.}\n\nNotice that the softmax probabilities are unchanged for any $A$ and $\\bar{\\gamma}_0$, so this transformation does not affect the model's behavior. The vector $\\bar{\\gamma}_0$ defines an origin for the unembedding space, and can be chosen arbitrarily. We give a particularly convenient choice below.\n\nIn this unified space, the linear representation of a binary concept $W\\in_R \\{0, 1\\}$ is defined as:\n\\begin{definition}\\label{def:linear_representation_orig}\n  A vector $\\bar{\\ell}_W$ is a linear representation of a binary concept $W$ if for all contexts $\\ell$, and all concept variables $Z$ that are causally separable with $W$, we have, for all $\\alpha>0$,\n  \\begin{align}\n    \\Pr(W = 1 \\given \\ell + \\alpha \\bar{\\ell}_W) &> \\Pr(W = 1 \\given \\ell), \\text{ and} \\\\\n    \\Pr(Z \\given \\ell + \\alpha \\bar{\\ell}_W) &= \\Pr(Z\\given \\ell).\n  \\end{align} \n\\end{definition}\nThat is, the linear representation is a direction in the representation space that, when added to the context, increases the probability of the concept, but does not affect the probability of any off-target concept. \nThe representation is only a direction because $\\alpha \\bar{\\ell}_W$ is also a linear representation for any $\\alpha>0$ (i.e., there is no notion of magnitude).\nIn the case of binary concepts that can be represented as counterfactual pairs of words, this direction can be shown to be proportional to the ``linear probing'' direction, and proportional to $g(Y(1)) - g(Y(0))$ for any counterfactual pair $Y(1), Y(0)$ that differ on $W$.\n\n\n\n\\section{Binary Concepts and Hierarchical Structure}\nOur high-level strategy will be to build up from binary concepts to more complex structure. We begin by defining the basic building blocks.\n\n\\paragraph{Binary and Categorical Concepts}\nWe consider two kinds of binary concept.\nA \\emph{binary feature} $W \\in_R \\{\\ConceptValue{not\\_w}, \\ConceptValue{is\\_w}\\}$ is an indicator of whether the output has the attribute $w$. For example, if the feature \\ConceptName{is\\_animal} is true then the output will be about an animal. \nA \\emph{binary contrast} $\\ConceptDirName{a}{b} \\in_R \\{a, b \\}$ is a binary variable that contrasts two specific attribute values. For example, the variable $\\ConceptDirName{mammal}{bird}$ is a binary contrast.\nIn the particular case where the concept may be represented as counterfactual pairs of words, we can identify the contrast with the notion of linear representation in \\citet{park2024linear}.\n\nWe also define a categorical concept to be any concept corresponding to a categorical latent variable. This includes binary concepts as a special case.\n\n\\paragraph{Hierarchical Structure}\nThe next step is to define what we mean by a hierarchical relation between concepts.\nTo that end, to each attribute $w$, we associate a set of tokens $\\yquad(w)$ that have the attribute.\nFor example, $\\yquad(\\ConceptValue{mammal}) = \\{\\text{`` dog''}, \\text{`` cats''}, \\text{`` Tiger''}, \\dots\\}$.\nThen,\n\\begin{definition}\\label{def:hierarchical_relation}\n  A value $z$ is \\defnphrase{subordinate} to a value $w$ (denoted by $z \\prec w$) if $\\yquad(z) \\subseteq \\yquad(w)$.\n  We say a categorical concept $Z \\in_R \\{z_0, \\dots, z_{k-1}\\}$ is subordinate to a categorical concept $W \\in_R \\{w_0, \\dots, w_{n-1}\\}$ if there exists a value $w_Z$ of $W$ such that each value $z_i$ of $Z$ is subordinate to $w_Z$.\n\\end{definition}\n\nFor example, the binary contrast  $\\ConceptDirName{dog}{cat}$ is subordinate to the binary feature $\\{\\ConceptValue{is\\_mammal},$ $\\ConceptValue{not\\_mammal} \\}$, and the binary contrast $\\ConceptDirName{parrot}{eagle}$ is subordinate to the categorical concept \\{\\ConceptName{mammal}, \\ConceptName{bird}, \\ConceptName{fish}\\}.\nOn the other hand, $\\ConceptDirName{dog}{eagle}$ is not subordinate to $\\ConceptDirName{bird}{mammal}$, and $\\ConceptDirName{bird}{mammal}$ and $\\ConceptDirName{live\\_in\\_house}{live\\_in\\_water}$ are not subordinate to each other.\n\n\\paragraph{Linear Representations of Binary Concepts}\nNow we return to the question of how binary concepts are represented.\nA key desideratum is that if $\\bar{\\ell}_W$ is a linear representation then moving the representation in this direction should modify the probability of the target concept \\emph{in isolation}.\nIf adding $\\bar{\\ell}_W$ also modified off-target concepts, it would not be natural to identify it with $W$. In \\cref{def:linear_representation_orig}, this idea is formalized by the requirement that the probability of causally separable concepts is unchanged when the representation is added to the context. \n\nWe now observe that, when there is hierarchical structure, this requirement is not strong enough to capture `off-target' behavior. For example, if $\\bar{\\ell}_{\\ConceptValue{animal}}$ captures the concept of animal vs not-animal, then moving in this direction should not affect the relative probability of the output being about a mammal versus a bird. If it did, then the representation would actually capture some amalgamation of the animal and mammal concepts. Accordingly, we must strengthen our definition: \n\\begin{definition}\\label{def:linear_representation}\n    A vector $\\bar{\\ell}_W$ is a linear representation of a binary concept $W$ if\n    \\begin{align}\n      \\Pr(W = 1 \\given \\ell + \\alpha \\bar{\\ell}_W) &> \\Pr(W = 1 \\given \\ell) \\text{, and} \\label{eq:linear_cond1} \\\\\n      \\Pr(Z \\given \\ell + \\alpha\\bar{\\ell}_W) &= \\Pr(Z \\given \\ell), \\label{eq:linear_cond2}\n    \\end{align}   \n    for all contexts $\\ell$, all $\\alpha>0$, and all concept variables $Z$ that are either subordinate or causally separable with $W$.\n    Here, if $W$ is a binary feature for an attribute $w$, $W=1$ denotes $W = \\ConceptValue{is\\_w}$.\n\\end{definition}   \n\nNotice that, in the case of binary concepts defined by counterfactual pairs, this definition is equivalent to \\Cref{def:linear_representation_orig}, because such variables have no subordinate concepts.\n\n\n\n\n\\section{Representations of Complex Concepts}\nWe now turn to how complex concepts are represented. The high-level strategy is to show how to represent binary features as vectors, show how geometry encodes semantic composition, and then use this to construct representations of complex concepts.\n\n\\subsection{Vector Representations of Binary Features}\nTo build up to complex concepts, we need to understand how to compose representations of binary concepts. At this stage, the representations are \\emph{directions} in the representation space---they do not have a natural notion of magnitude. In particular, this means we cannot use vector operations (such as addition) to compose representations. To overcome this, we now show how to associate a magnitude to the representation of a binary concept.\n\nThe key is the following result, which connects binary feature representations and word unembeddings:\n\\begin{restatable}[Magnitudes of Linear Representations]{theorem}{Magnitude}\\label{thm:magnitude}\n  Suppose there exists a linear representation (normalized direction) $\\bar\\ell_W$ of a binary feature $W$ for an attribute $w$.  \n  Then, there is a constant $b_w>0$ and a choice of unembedding space origin $\\bar{\\gamma}_0^w$ in \\cref{eq:transformation} such that\n  \\begin{equation}\\label{eq:magnitude}\n    \\begin{cases}\n      \\bar\\ell_W^\\top g(y) = b_w & \\text{if } y\\in \\yquad(w)\\\\\n      \\bar\\ell_W^\\top g(y) = 0 & \\text{if } y \\not\\in \\yquad(w).\n    \\end{cases}\n  \\end{equation}\n  Further, if there are $d$ causally separable attributes $\\{w_0, \\dots, w_{d-1}\\}$ with linear representations, we can choose a canonical origin $\\bar\\gamma_0$ in \\cref{eq:transformation} as\n  \\begin{equation}\n    \\bar\\gamma_0 = \\sum_i \\bar{\\gamma}_0^{w_i}.\n  \\end{equation}\n\\end{restatable}\nAll proofs are given in \\Cref{sec:proofs}.\n\nIn words, this theorem says that if a (perfect) linear representation of the \\ConceptName{animal} feature exists, then every token that has the animal attribute has the \\emph{same} dot product with the representation vector; i.e., ``cat'' is exactly as much \\ConceptName{animal} as ``dog'' is. If this weren't true, then increasing the probability that the output is about an animal would also increase the relative probability that the output is about a dog rather than a cat.\nIn practice, such exact representations are unlikely to be found by gradient descent in LLM training. Rather, we expect $\\bar\\ell_W^\\top g(y)$ to be isotropically distributed around $b_w$ with variance that is small compared to $b_w$ (so that animal and non-animal words are well-separated.) \n\nWith this result in hand, we can define a notion of vector representation for a binary feature:\n\\begin{definition}\\label{def:vector_representation}\n  We say that binary feature $W$ for an attribute $w$ has a \\emph{vector representation} $\\bar{\\ell}_w \\in \\Reals^d$ if $\\bar{\\ell}_w$ satisfies \\cref{def:linear_representation} and $\\|\\bar{\\ell}_w\\|_2=b_w$ in \\cref{thm:magnitude}.\n  If the vector representation of a binary feature is not unique, we say $\\bar\\ell_w$ is the vector representation that maximizes $b_w$.\n\\end{definition} \n\n\n\\subsection{Hierarchical Orthogonality}\nWe have now moved from representations as directions to representations as vectors.\nUsing this result, we now establish how hierarchical relations between concepts are encoded in the vector space structure of the representation space. The structure is illustrated in \\Cref{fig:three_2d_plots}. Formally, we have the following connections between vector and semantic structure:\n\n\\begin{restatable}[Hierarchical Orthogonality]{theorem}{Orthogonality}\\label{thm:orthogonality}\n  Suppose there exist the vector representations for all the following binary features.\n  Then, we have that\n  \\begin{enumerate}[label=(\\alph*)]\n    \\item $\\bar\\ell_{w_1} - \\bar\\ell_{w_0}$ is a linear representation $\\bar{\\ell}_{\\ConceptDirMath{w_0}{w_1}}$ defined in \\cref{def:linear_representation}; \\label{item:a}\n    \\item $\\bar\\ell_{w} \\perp \\bar\\ell_{z} - \\bar\\ell_w$ for $z \\prec w$; \\label{item:left}\n    \\item $\\bar{\\ell}_{w} \\perp \\bar\\ell_{z_1} - \\bar\\ell_{z_0}$ for $Z \\in_R \\{z_0, z_1\\}$ subordinate to $W \\in_R \\{\\ConceptValue{not\\_w}, \\ConceptValue{is\\_w}\\}$; \\label{item:middle}\n    \\item $\\bar{\\ell}_{w_1} - \\bar{\\ell}_{w_0}  \\perp \\bar\\ell_{z_1} - \\bar\\ell_{z_0}$ for $Z \\in_R \\{z_0, z_1\\}$ subordinate to $W \\in_R \\{w_0, w_1\\}$; and \\label{item:right}\n    \\item $\\bar\\ell_{w_1}-\\bar\\ell_{w_0} \\perp \\bar\\ell_{w_2} - \\bar\\ell_{w_1}$ for $w_2 \\prec w_1 \\prec w_0$.\n  \\end{enumerate}\n\\end{restatable}\nWe emphasize that these results---involving differences of representations---are only possible because we now have \\emph{vector} representations (mere direction would not suffice).\n\n\n\\subsection{Categorical Concepts as Simplices}\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=1.0\\linewidth]{figures/three_2d_plots.png}\n  \\caption{Hierarchical semantics are encoded as orthogonality in the representation space, as predicted in \\cref{thm:orthogonality}.    \n  The plots show the projection of the unembedding vectors on the 2D subspaces: $\\mathrm{span}\\{\\bar\\ell_{\\ConceptValue{animal}}, \\bar\\ell_{\\ConceptValue{mammal}}\\}$ (left; statement~\\ref{item:left}), $\\mathrm{span}\\{\\bar\\ell_{\\ConceptValue{animal}},\\bar\\ell_{\\ConceptValue{bird}} - \\bar\\ell_{\\ConceptValue{mammal}}\\}$ (middle; statement~\\ref{item:middle}), and $\\mathrm{span}\\{\\bar\\ell_{\\ConceptValue{animal}} - \\bar\\ell_{\\ConceptValue{plant}}, \\bar\\ell_{\\ConceptValue{bird}} - \\bar\\ell_{\\ConceptValue{mammal}}\\}$ (right; statement~\\ref{item:right}).\n  The gray points indicate all 256K tokens in the vocabulary, and the colored points are the tokens in $\\yquad(\\ConceptValue{w})$.\n  The blue and red vectors are used to span the 2D subspaces.}\n  \\label{fig:three_2d_plots}\n\\end{figure}\nThe power of having a vector representation is that now we can use ordinary vector space operations to construct representation of other concepts.\nWe now turn to the representation of categorical concepts, e.g., $\\{\\ConceptValue{mammal}, \\ConceptValue{reptile}, \\ConceptValue{bird}, \\ConceptValue{fish}\\}.$\nThere is now a straightforward way to define the representation of such concepts:\n\\begin{definition}\n  The \\emph{polytope representation} of a categorical concept $Z = \\{z_0, \\dots, z_{k-1}\\}$ is the convex hull of the vector representations of the elements of the concept.\n\\end{definition}\n\nPolytopes are quite general objects. The definition here also includes representations of categorical variables that are semantically unnatural, e.g., $\\{\\ConceptValue{dog}, \\ConceptValue{sandwich}, \\ConceptValue{running}\\}$. We would like to make a more precise statement about the representation of ``natural'' concepts. One possible notion of a ``natural'' concept is one where the model can freely manipulate the output values. The next theorem shows such concepts have particularly simple structure: \n\\begin{restatable}[Categorical Concepts are Represented as Simplices]{theorem}{Simplex}\\label{thm:simplex}\n  Suppose that $\\{w_0, \\dots, w_{k-1}\\}$ is a collection of $k$ mutually exclusive attributes such that for every joint distribution $Q(w_0, \\dots w_{k-1})$ there is some $\\ell_{i}$ such that $\\Pr(W = w_i \\given \\ell_{i}) = Q(W=w_i)$ for every $i$.\n  Then, the vector representations $\\bar\\ell_{w_0}, \\dots, \\bar\\ell_{w_{k-1}}$ form a $(k-1)$-simplex in the representation space. In this case, we take the simplex to be the representation of the categorical concept $W = \\{w_0, \\dots, w_{k-1}\\}$.\n\\end{restatable}\n\n\\paragraph*{Summary} Together, \\cref{thm:orthogonality,thm:simplex} give the simple structure illustrated in \\cref{fig:diagram}: hierarchical concepts are represented as direct sums of simplices. The direct sum structure is immediate from the orthogonality in \\cref{thm:orthogonality}.\n\n\n\n\\section{Experiments}\\label{sec:experiments}\nWe now turn to empirically testing the theoretical results in the representation space of the Gemma-2B large language model \\Citep[][]{team2024gemma}.\\footnote{Code is available at \\href{https://github.com/KihoPark/LLM_Categorical_Hierarchical_Representations}{github.com/KihoPark/LLM\\_Categorical\\_Hierarchical\\_Representations}.}\n\n\\subsection{Setup}\n\\paragraph*{Canonical representation space}\nThe results in this paper rely on transforming the representation space so that the Euclidean inner product is a causal inner product, aligning the embedding and unembedding representations. Following \\citet{park2024linear}, we estimate the required transformation as:\n\\begin{equation}\n  g(y) = \\cov(\\gamma)^{-1/2}(\\gamma(y) - \\EE[\\gamma])\n\\end{equation}\nwhere $\\gamma$ is the unembedding vector of a word sampled uniformly from the vocabulary.\nCentering by $\\EE[\\gamma]$ is a reasonable approximation of centering by $\\bar{\\gamma}_0$ defined in \\cref{thm:magnitude} because this makes the projection of a random $g(y)$ on an arbitrary direction close to $0$. This matches the requirement that the projection of a word onto a concept the words does not belong to should be close to $0$.\n\n\\paragraph*{WordNet}\nWe define a large collection of binary concepts using WordNet \\Citep[][]{miller1995wordnet}.\nBriefly, WordNet organizes English words into a hierarchy of synsets, where each synset is a set of synonyms. The WordNet hierarchy is based on word hyponym relations, and reflects the semantic hierarchy of interest in this paper. We take each synset as an attribute $w$ and define $\\yquad(w)$ as the collection of all words belonging to any synset that is a descendant of $w$. For example, the synset \\texttt{mammal.n.01} is a descendant of \\texttt{animal.n.01}, so both $\\mathcal{Y}(\\texttt{mammal.n.01})$ and $\\mathcal{Y}(\\texttt{animal.n.01})$ contain the word ``dog''. We collect all noun and verb synsets, and augment the word collections by including plural forms of the nouns, multiple tenses of each verb, and capital and lower case versions of each word. We filter to include only those synsets with at least 50 words in the Gemma vocabulary. This leaves us with 593 noun and 364 verb synsets, each defining an attribute.\n\n\\paragraph{Estimation via Linear Discriminant Analysis}\nNow, we want to estimate the vector representation $\\bar\\ell_w$ for each attribute $w$.\nTo do this, we make use of vocabulary sets $\\yquad(w)$.\nFollowing \\cref{thm:magnitude}, the vector associated to the concept $w$ should have two properties. First, when the full vocabulary is projected onto this vector, the words in $\\yquad(w)$ should be well-separated from the rest of the vocabulary. Second, the projection of the unembedding vectors for $y\\in \\yquad(w)$ should be approximately the same value. Equivalently, the variance of the projection of the unembedding vectors for $y\\in \\yquad(w)$ should be small.\nTo capture these requirements, we estimate the directions using a variant of Linear Discriminant Analysis (LDA),\nwhich finds a projection minimizing within-class variance and maximizing between-class variance.\nFormally, we estimate the vector representation of a binary feature $W$ for an attribute $w$ as\n\\begin{equation}\\label{eq:estimated_vector}\n  \\bar\\ell_w =   \\left(\\tilde{g}_w^\\top\\EE(g_w)\\right)\\tilde{g}_w, \\quad \\text{with}\\quad \\tilde{g}_w= \\frac{\\cov(g_w)^{\\dagger} \\EE(g_w)}{\\|\\cov(g_w)^{\\dagger} \\EE(g_w)\\|_2},\n\\end{equation}\nwhere $g_w$ is the unembedding vector of a word sampled uniformly from $\\yquad(w)$ and $\\cov(g_w)^{\\dagger}$ is a pseudo-inverse of the covariance matrix. We estimate the covariance matrix $\\cov(g_w)$ using the Ledoit-Wolf shrinkage estimator \\Citep[][]{ledoit2004well}, because the dimension of the representation spaces is much higher than the number of samples.\n\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[trim={1cm 1cm 0.5cm 2cm}, clip, width=1.0\\linewidth]{figures/two_3D_plots.png}\n  \\caption{Categorical concepts are represented as simplices.\n  The plots show the projection of the unembedding vectors on the 3D subspaces: $\\mathrm{span}\\{\\bar\\ell_{\\ConceptValue{mammal}}, \\bar\\ell_{\\ConceptValue{bird}},\\bar\\ell_{\\ConceptValue{fish}}\\}$ (left) and $\\mathrm{span}\\{\\bar\\ell_{\\ConceptValue{bird}} - \\bar\\ell_{\\ConceptValue{mammal}}, \\bar\\ell_{\\ConceptValue{fish}} - \\bar\\ell_{\\ConceptValue{mammal}}, \\bar\\ell_{\\ConceptValue{reptile}} - \\bar\\ell_{\\ConceptValue{mammal}} \\}$ (right).\n  The gray points indicate all 256K tokens in the vocabulary, and the colored points are the tokens in $\\yquad(\\ConceptValue{w})$.\n  The left plot further shows the orthogonality between the triangle and the projection of $\\bar\\ell_{\\ConceptValue{animal}}$ (black arrow).}\n  \\label{fig:two_3D_plots}\n\\end{figure}\n\\subsection{Visualization of \\ConceptName{animal}}\\label{sec:visualization}\nAs a concrete example, we check the theoretical predictions for the concept \\ConceptName{animal}.\nFor this, we generated two sets of tokens $\\yquad(\\ConceptValue{animal})$ and $\\yquad(\\ConceptValue{plant})$ using ChatGPT-4 \\Citep{openai2023gpt4} and manually inspected them.\n$\\yquad(\\ConceptValue{animal})$ is separated to six sets of tokens for each subcategory $\\{\\ConceptValue{mammal}, \\ConceptValue{bird},\\ConceptValue{fish},\\ConceptValue{reptile},\\ConceptValue{amphibian},\\ConceptValue{insect}\\}$.\n\n\\Cref{fig:three_2d_plots} illustrates the geometric relationships between various representation vectors. The main takeaway is that the semantic hierarchy is encoded as orthogonality in the manner predicted by \\cref{thm:orthogonality}. The figure also illustrates \\cref{thm:magnitude}, showing that the projection of the unembedding vectors for $y\\in \\yquad(w)$ is approximately constant, while the projection of $y\\not \\in \\yquad(w)$ is zero.\n\n\\Cref{fig:two_3D_plots} illustrates that the representation of a categorical concept is a simplex, as predicted in \\cref{thm:simplex}. It also shows that, as predicted, the simplex for $\\ConceptValue{fish}, \\ConceptValue{mammal}, \\ConceptValue{bird}$ is orthogonal to the vector representation of \\ConceptName{animal}.\n\n\n\n\n\\subsection{WordNet Hierarchy}\nWe now turn to using the WordNet hierarchy to evaluate the theoretical predictions at scale.\nWe report the noun hierarchy here and defer the verb hierarchy to \\Cref{sec:additional_results}.\n\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=0.95\\linewidth]{figures/noun_evaluation.pdf}\n  \\caption{Linear representations exist for most binary features in the WordNet noun hierarchy.  \n  Comparison of projection of test and random words on estimated vector representations for each WordNet feature.\n  The values are divided by the norm of the estimated vector representation. The $x$-axis indices denote all features in the noun hierarchy.\n  The thick lines present the mean of the projections for each feature and the error bars indicate the 1.96 $\\times$ standard error.}\n  \\label{fig:noun_evaluation}\n\\end{figure}\n\n\\paragraph{Existence of Vector Representations for Binary Features}\nTo evaluate whether vector representations exist, for each synset $w$ we split $\\yquad(w)$ into train words (80\\%) and test words (20\\%), fit the LDA estimator to the train words, and examine the projection of the unembedding vectors for the test words onto the estimated vector representation. \\Cref{fig:noun_evaluation} shows the mean and standard error of the test projections, divided by the magnitude of each estimated $\\bar{\\ell}_w$. If a vector representation exists for an attribute, we would expect these values to be close to 1. We see that this is indeed the case, giving evidence that vector representations do indeed exist for these features.\n\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=1.0\\linewidth]{figures/noun_single_three_heatmap.pdf}\n  \\caption{Hierarchical semantics in WordNet are encoded in Gemma representation space, with the orthogonal structure predicted in \\cref{thm:orthogonality}.    \n  The adjacency matrix of the hierarchical relations between features in the noun hierarchy (left), the cosine similarity between the vector representations $\\bar\\ell_w$ for each feature (middle), and the cosine similarity between child-parent vectors $\\bar\\ell_w - \\bar\\ell_{\\text{parent of }w}$ for each feature (right).\n  The features are ordered by the hierarchy.\n  }\n  \\label{fig:three_heatmap}\n\\end{figure}\n\n\\paragraph{Hierarchical Orthogonality}\nIt remains to evaluate the prediction that hierarchical relations are encoded as orthogonality in the representation space.\n\\Cref{fig:three_heatmap} shows the adjacency matrix of the WordNet noun hyponym inclusion graph (left), the cosine similarity between the vector representations $\\bar\\ell_w$ for each feature (middle), and the cosine similarity between child-parent vectors $\\bar\\ell_w - \\bar\\ell_{\\text{parent of }w}$ for each feature (right). Strikingly, the cosine similarity clearly reflects the semantic hierarchy---the adjacency matrix is clearly visible in the middle heatmap. This is because, e.g., \\ConceptName{mammal.n.01} and \\ConceptName{animal.n.01} have high cosine similarity. By contrast, as predicted by \\cref{thm:orthogonality}, the child-parent and parent-grandparent vectors are orthogonal. This also straightforwardly implies all other theoretical connections between orthogonality and semantic hierarchy.\n\nIn \\Cref{sec:additional_results}, we present zoomed-in heatmaps for the subtree of descendants of ``animal'', and the results for the verb hierarchy.\n\n\n\n\\section{Discussion and Related Work}\\label{sec:discussion_relatedwork}\nWe set out to understand how semantic structure is encoded in the geometry of representation space.\nWe have arrived an astonishingly simple structure, summarized in \\Cref{fig:diagram}. \nThe key contributions are moving from representing concepts as directions to representing them as vectors (and polytopes), and connecting semantic hierarchy to orthogonality. \n\n\\paragraph{Related work}\nThe results here connect closely to the study of linear representations in language models\n \\citep[e.g.,][]{mikolov2013linguistic,pennington2014glove,arora2016latent, elhage2022toy, burns2022discovering, tigges2023linear, nanda2023emergent, moschella2022relative, li2023inference, gurnee2023finding, wang2023concept, jiang2024origins,park2024linear}.\n In particular, \\citet{park2024linear} formalize the linear representation hypothesis by unifying three distinct notions of linearity: word2vec-like embedding differences, logistic probing, and steering vectors. Our work relies on this unification, and just focuses on the steering vector notion. \nOur work also connects to work aimed at theoretically understanding the existence of linear representations. Specifically, \\cite{arora2016latent, arora2018linear, frandsen2019understanding} use RAND-WALK model where the latent vectors are modeled to drift on the unit sphere. \n\\cite{blei2006dynamic, rudolph2016exponential, rudolph2017dynamic} consider a similar dynamic topic modeling. \\citet{Gittens2017SkipGramZ} and subsequent works \\citep{allen2019analogies,allen2019vec} propose a paraphrasing model where a subset of words is semantically equivalent to a single word. \n\\citet{ethayarajh2018towards} try to explain linear representations by decomposing the pointwise mutual information matrix while \\citet{ri2023contrastive} connect it to contrastive loss. \\Citet{jiang2024origins} connect the existence of linear representations to the implicit bias of gradient descent. In this paper, we do not seek to justify the \\emph{existence} of linear representations, but rather to understand their \\emph{structure} if they do exist. \nThough, by empirically estimating vector representations for thousands of concepts, we add to the body of evidence supporting the existence of linear representations. \\citet{elhage2022toy} also empirically observe the formation of polytopes in the representation space of a toy model, and the present work can be viewed in part as giving an explanation for this phenomenon.\n\nThere is also a growing literature studying the representation geometry of natural language \\citep{Mimno2017TheSG, reif2019visualizing, volpi2021natural, Volpi2020EvaluatingNA, li2020sentence, chen2021probing, chang2022geometry, liang2022mind, jiang2023uncovering,wang2023concept, park2024linear, valeriani2024geometry}. \nMuch of this work focuses on connections to hyperbolic geometry \\citep[][]{nickel2017poincare, ganea2018hyperbolic, chen2021probing, he2024language}. We do not find such a connection in existing LLMs, but it is an interesting direction for future work to determine if more efficient LLM representations could be constructed in hyperbolic space. \\Citet{jiang2023uncovering} hypothesize that very general  \"independence structures\" are naturally represented by partial orthogonality in vector spaces \\citep{amini2022non}. The results here confirm and expand on this hypothesis in the case of hierarchical structure in language models.\n\n\\paragraph{Implications and Future Work}\nThe results in this paper are foundational for understanding the structure of representation space in language models.\nOf course, the ultimate purpose of foundations is to build upon them.\nOne immediate direction is to refine the attempts to interpret LLM structure to explicitly account for hierarchical semantics. As a concrete example, there is currently significant interest in using sparse autoencoders to extract interpretable features from LLMs \\Citep[e.g.,][]{cunningham2023sparse, bricken2023monosemanticity, attention_saes, braun2024identifying}.\nThis work searches for representations in terms of distinct binary features. \nConcretely, it hopes to find features for, e.g., \\ConceptName{animal}, \\ConceptName{mammal}, \\ConceptName{bird}, etc. Based on the results here, these representations are strongly co-linear, and potentially difficult to disentangle. \nOn the other hand, a representation in terms of $\\bar\\ell_{\\ConceptValue{animal}}$, $\\bar\\ell_{\\ConceptValue{mammal}} - \\bar\\ell_\\ConceptValue{animal}$, $\\bar\\ell_{\\ConceptValue{bird}} - \\bar\\ell_\\ConceptValue{animal}$, etc., will be cleanly separated and equally interpretable. Fundamentally, semantic meaning has hierarchical structure, so interpretability methods should respect this structure. Understanding the geometric representation makes it possible to design such methods.\n\nIn a separate, foundational, direction: the results in this paper rely on using the canonical representation space. We estimate this using the whitening transformation of the unembedding layer. However, this technique only works for the final layer representation. It is an important open question how to make sense of the geometry of internal layers.\n\n\n\n\n\\clearpage\n\\printbibliography\n\n\n\n\n\\clearpage\n\\appendix\n\n\n\n\\section{Proofs}\\label{sec:proofs}\n\\subsection{Proof of \\Cref{thm:magnitude}}\n\\Magnitude*\n\\begin{proof}\n  For any $y_1, y_0 \\in \\yquad(w)$ or $y_1, y_0 \\not\\in \\yquad(w)$, let $Z$ be a binary concept where $\\yquad(Z = 0) = \\{y_0\\}$ and $\\yquad(Z = 1) = \\{y_1\\}$.\n  Since $Z$ is subordinate to $W$, \\cref{eq:linear_cond2} implies that\n  \\begin{align}\n    & \\logit \\Pr(Y = y_1 \\given Y \\in \\{y_0, y_1\\} , \\ell +  \\bar\\ell_W )= \\logit \\Pr(Y = y_1 \\given Y \\in \\{y_0, y_1\\} , \\ell)\\\\\n    & \\iff  \\bar\\ell_W ^\\top  (g(y_1) - g(y_0))=\\bar\\ell_W ^\\top  A(\\gamma(y_1) - \\gamma(y_0)) = 0\n  \\end{align}\n  where $A$ is the invertible matrix in \\cref{eq:transformation}.\n  This means that $\\bar\\ell_W^\\top A \\gamma(y)$ is the same for all $y\\in \\yquad(w)$, and it is also the same for all $y\\not\\in \\yquad(w)$.\n\n  Furthermore, for any $y_1 \\in \\yquad(w)$ and $y_0 \\not\\in \\yquad(w)$, \\cref{eq:linear_cond1} implies that\n  \\begin{align}\n    & \\logit \\Pr(Y = y_1 \\given Y \\in \\{y_0, y_1\\} , \\ell + \\bar\\ell_W )> \\logit \\Pr(Y = y_1 \\given Y \\in \\{y_0, y_1\\} , \\ell)\\\\\n    & \\iff  \\bar\\ell_W ^\\top  (g(y_1) - g(y_0)) =  \\bar\\ell_W ^\\top  A(\\gamma(y_1) - \\gamma(y_0))> 0.\n  \\end{align}\n\n  Thus, by setting $b_w^0 = \\bar\\ell_W^\\top A \\gamma(y)$ for any $y \\not\\in \\yquad(w)$, and $b_w = \\bar\\ell_W^\\top A \\gamma(y_1) - \\bar\\ell_W^\\top A \\gamma(y_0) > 0$ for any $y_1 \\in \\yquad(w)$ and $y_0 \\not\\in \\yquad(w)$, we get\n  \\begin{equation}\\label{eq:feature_vector}\n    \\begin{cases}\n      \\bar\\ell_W^\\top A \\gamma(y) = b_w^0 + b_w & \\text{if } y\\in \\yquad(w)\\\\\n      \\bar\\ell_W^\\top A \\gamma(y) = b_w^0 & \\text{if } y \\not\\in \\yquad(w).\n    \\end{cases}\n  \\end{equation}\n  Then, we can choose an origin as\n  \\begin{equation}\n    \\bar\\gamma_0^w = b_w^0 A^{-1} \\bar\\ell_W\n  \\end{equation}\n  satisfying \\cref{eq:magnitude}.\n\n  On the other hand, if there exist $\\bar\\ell_W$ and $\\bar\\ell_Z$ for causally separable attributes $w$ and $z$, then $\\bar\\ell_W$ and $\\bar\\ell_Z$ are orthogonal by the property of the causal inner product.\n  If they are not orthogonal, adding $\\bar\\ell_Z$ can change the other concept $W$, and it is a contradiction.\n  Now if there exist the linear representation for $d$ binary features for causally separable attributes $\\{w_0, \\dots, w_{d-1}\\}$, we can choose a canonical $\\bar\\gamma_0$ in \\cref{eq:transformation} as\n  \\begin{equation}\n    \\bar\\gamma_0 = \\sum_i \\bar{\\gamma}_0^{w_i}.\n  \\end{equation}\n  with \\cref{eq:magnitude} satisfied.\n\\end{proof}\n\n\n\n\\subsection{Proof of \\Cref{thm:orthogonality}}\n\\Orthogonality*\n\\begin{proof}\n  \\begin{enumerate}[label=(\\alph*)]\n    \\item For $\\bar\\ell_{w_1}$ and $\\bar\\ell_{w_0}$, by \\Cref{thm:magnitude}, we have\n    \\begin{equation}\n      \\begin{cases}\n        (\\bar\\ell_{w_1} - \\bar\\ell_{w_0})^\\top g(y) = 0-b_{w_0} = -b_{w_0} & \\text{if } y\\in \\yquad(w_0)\\\\\n        (\\bar\\ell_{w_1} - \\bar\\ell_{w_0})^\\top g(y) = b_{w_1} - 0 = b_{w_1} & \\text{if } y \\in \\yquad(w_1)\\\\\n        (\\bar\\ell_{w_1} - \\bar\\ell_{w_0})^\\top g(y) = 0 - 0 = 0 & \\text{if } y\\not\\in \\yquad(w_0)\\cup \\yquad(w_1).\n      \\end{cases}\n    \\end{equation}\n    Since $\\bar\\ell_{w_1} - \\bar\\ell_{w_0}$ can change the target concept $\\ConceptDirMath{w_0}{w_1}$ without changing any other concept subordinate or causally separable to the target concept, $\\bar\\ell_{w_1} - \\bar\\ell_{w_0}$ is the linear representation $\\bar{\\ell}_{w_0\\Rightarrow w_1}$.\n\n    \\item For $\\bar\\ell_{w}$ and $\\bar\\ell_{z}$ where $z \\prec w$, by \\Cref{thm:magnitude}, we have\n    \\begin{equation}\n      \\begin{cases}\n        (\\bar\\ell_{z} - \\bar\\ell_{w})^\\top g(y) = b_z - b_w & \\text{if } y\\in \\yquad(z)\\\\\n        (\\bar\\ell_{z} - \\bar\\ell_{w})^\\top g(y) =  0 -b_w =  -b_w & \\text{if } y \\in \\yquad(w)\\setminus \\yquad(z)\\\\\n        (\\bar\\ell_{z} - \\bar\\ell_{w})^\\top g(y) = 0 - 0 = 0 & \\text{if } y\\not\\in \\yquad(w).\n      \\end{cases}\n    \\end{equation}\n    When $w\\setminus z$ denotes an attribute defined by $\\yquad(w)\\setminus \\yquad(z)$, $\\bar\\ell_{z} - \\bar\\ell_{w}$ can change the target concept $\\ConceptDirMath{w\\setminus z}{z}$ without changing any other concept subordinate or causally separable to the target concept.\n    Thus, $\\bar\\ell_{z} - \\bar\\ell_{w}$ is the linear representation $\\bar{\\ell}_{w\\setminus z \\Rightarrow z}$.\n    This concept means $\\ConceptValue{not\\_z}  \\Rightarrow \\ConceptValue{is\\_z}$ conditioned on $w$, and hence it is subordinate to $w$.\n    \n    Therefore, $\\bar\\ell_w$ is orthogonal to the linear representation $\\bar{\\ell}_{w\\setminus z \\Rightarrow z} = \\bar\\ell_z - \\bar\\ell_w$ by the property of the causal inner product.\n    If they are not orthogonal, adding $\\bar\\ell_w$ can change the other concept $\\ConceptDirMath{w\\setminus z}{z}$, and it is a contradiction.\n\n    \\item By the above result \\ref{item:left}, $\\bar\\ell_w^\\top (\\bar\\ell_{z_1} - \\bar\\ell_w) = \\bar\\ell_w^\\top (\\bar\\ell_{z_0} - \\bar\\ell_w) = 0$.\n    Therefore, $\\bar\\ell_w^\\top (\\bar\\ell_{z_1} - \\bar\\ell_{z_0}) = 0$.\n\n    \\item Let's say that $w_1$ is $w_Z$ defined in \\Cref{def:hierarchical_relation}.\n    The binary contrast $\\ConceptDirMath{z_0}{z_1}$ is subordinate to the binary feature for the attribute $w_0$.\n    By the property of the causal inner product, $\\bar\\ell_{w_0}$ is orthogonal to the linear representation $\\bar{\\ell}_{z_0\\Rightarrow z_1} = \\bar\\ell_{z_1} - \\bar\\ell_{z_0}$ (by \\ref{item:a}).\n    Then, with the above result \\ref{item:middle}, we have $(\\bar\\ell_{w_1} - \\bar\\ell_{w_0})^\\top (\\bar\\ell_{z_1} - \\bar\\ell_{z_0})$.\n\n    \\item By the above result \\ref{item:left}, we have\n    \\begin{equation}\n      \\begin{cases}\n        \\|\\bar\\ell_{w_1} - \\bar\\ell_{w_0}\\|_2^2 = \\|\\bar\\ell_{w_1}\\|_2^2  - \\|\\bar\\ell_{w_0}\\|_2^2\\\\\n        \\|\\bar\\ell_{w_2} - \\bar\\ell_{w_1}\\|_2^2 = \\|\\bar\\ell_{w_2}\\|_2^2  - \\|\\bar\\ell_{w_1}\\|_2^2\\\\\n        \\|\\bar\\ell_{w_2} - \\bar\\ell_{w_0}\\|_2^2 = \\|\\bar\\ell_{w_2}\\|_2^2  - \\|\\bar\\ell_{w_0}\\|_2^2.\n      \\end{cases}\n    \\end{equation}\n    Then,\n    \\begin{align}\n      &\\|\\bar\\ell_{w_1} - \\bar\\ell_{w_0}\\|_2^2 + \\|\\bar\\ell_{w_2} - \\bar\\ell_{w_1}\\|_2^2\\\\\n      &= \\|\\bar\\ell_{w_1}\\|_2^2  - \\|\\bar\\ell_{w_0}\\|_2^2 + \\|\\bar\\ell_{w_2}\\|_2^2  - \\|\\bar\\ell_{w_1}\\|_2^2\\\\\n      &= \\|\\bar\\ell_{w_2}\\|_2^2  - \\|\\bar\\ell_{w_0}\\|_2^2\\\\\n      & = \\|\\bar\\ell_{w_2} - \\bar\\ell_{w_0}\\|_2^2.\n    \\end{align}\n    Therefore, $\\bar\\ell_{w_1} - \\bar\\ell_{w_0}$ is orthogonal to $\\bar\\ell_{w_2} - \\bar\\ell_{w_1}$.\n  \\end{enumerate}\n\\end{proof}\n\n\n\\subsection{Proof of \\Cref{thm:simplex}}\n\\Simplex*\n\\begin{proof}\n  If we can represent arbitrary joint distributions, this means, in particular, that we can change the probability of one attribute without changing the relative probability between a pair of other attributes.\n  Consider the case where $k=3$, as illustrated in \\Cref{fig:proof}.\n  If $\\bar\\ell_{w_0}, \\bar\\ell_{w_1}, \\bar\\ell_{w_2}$ are on a line, then there is no direction in that line (to change the value in the categorical concept) such that adding the direction can change the probability of $w_2$ without changing the relative probabilities between $w_0$ and $w_1$.\n  However, if $\\bar\\ell_{w_0}, \\bar\\ell_{w_1}, \\bar\\ell_{w_2}$ are not on a line, they form a triangle.\n  Then, there exists a line that is toward $\\bar\\ell_{w_2}$ and perpendicular to the opposite side of the triangle.\n  Now adding the direction $\\tilde{\\ell}$ can manipulate the probability of $w_2$ without changing the relative probabilities between $w_0$ and $w_1$.\n  That is, for any $\\alpha > 0$ and context embedding $\\ell$,\n  \\begin{equation}\n    \\begin{cases}\n      \\Pr(W = w_2 \\given \\ell + \\alpha \\tilde{\\ell}) > \\Pr(W = w_2 \\given \\ell), \\text{ and}\\\\\n      \\frac{\\Pr(W = w_1 \\given \\ell + \\alpha \\tilde{\\ell})}{\\Pr(W = w_0 \\given \\ell + \\alpha \\tilde{\\ell})}  = \\frac{\\Pr(W = w_1 \\given \\ell )}{\\Pr(W = w_0 \\given \\ell)}.\n    \\end{cases}\n  \\end{equation}\n  Therefore, the vectors $\\bar\\ell_{w_0}, \\bar\\ell_{w_1}, \\bar\\ell_{w_2}$ form a 2-simplex.\n  \n  This argument extends immediately to higher $k$ by induction.\n  For each $i \\in \\{0, \\dots, k-1\\}$, there should exist a direction that is toward $\\bar\\ell_{w_i}$ and orthogonal to the opposite hyperplane ($(k-2)$-simplex) formed by the other $\\bar\\ell_{w_{i'}}$'s.\n  Then, the vectors $\\bar\\ell_{w_0}, \\dots, \\bar\\ell_{w_{k-1}}$ form a $(k-1)$-simplex.\n\\end{proof}\n\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[trim={4cm 8cm 3cm 5cm}, clip, width=0.8\\linewidth]{figures/Theorem.pdf}\n  \\caption{Illustration of the case $k=3$ in the proof of \\Cref{thm:simplex}.}\n  \\label{fig:proof}\n\\end{figure}\n\n\n\n\\section{Experiment Details}\\label{sec:experiment_details}\nWe employ the \\texttt{Gemma-2B} version of the Gemma model \\Citep[][]{team2024gemma}, which is accessible online via the \\texttt{huggingface} library.\nIts two billion parameters are pre-trained on three trillion tokens.\nThis model utilizes 256K tokens and 2,048 dimensions for the representation space.\n\nWe always use tokens that start with a space (`$\\backslash$u2581') in front of the word, as they are used for next-word generation with full meaning.\nAdditionally, like WordNet data we use, we include plural forms, and both capital and lowercase versions of the words in $\\yquad(\\ConceptValue{animal})$ and $\\yquad(\\ConceptValue{plant})$ for visualization in \\Cref{sec:visualization}.\n\nIn the WordNet synset data, each content of the synset \\texttt{mammal.n.01} indicates that \"mammal\" is a word, \"n\" denotes \"noun,\" and \"01\" signifies the first meaning of the word.\nIn the WordNet hierarchy, if a parent has only one child, we combine the two features into one.\nAdditionally, since the WordNet hierarchy is not a perfect tree, a child can have more than one parent.\nWe use one of the parents when computing the $\\bar\\ell_{w} - \\bar\\ell_{\\text{parent of }w}$.\n\n\n\\section{Additional Results}\\label{sec:additional_results}\n\\subsection{Zooming in on a Subtree of Noun Hierarchy}\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=1.0\\linewidth]{figures/sub_graph_tree.pdf}\n  \\caption{Subtree in WordNet noun hierarchy for descendants of \\ConceptValue{animal}.\n  }\n  \\label{fig:sub_tree}\n\\end{figure}\n\n\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=1.0\\linewidth]{figures/sub_tree_three_heatmap_noun_single.pdf}\n  \\caption{Zoomed-in Heatmaps of the subtree for \\ConceptValue{animal} in \\Cref{fig:sub_tree}.\n  }\n  \\label{fig:sub_tree_three_heatmap}\n\\end{figure}\n\nAs it is difficult to understand the entire WordNet hierarchy at once from the heatmaps in \\Cref{fig:three_heatmap}, we present a zoomed-in heatmap for the subtree (\\Cref{fig:sub_tree}) for the feature \\ConceptValue{animal} in \\Cref{fig:sub_tree_three_heatmap}.\nThe left heatmap displays the adjacency matrix of the hierarchical relations between features, aligned with the subtree in \\Cref{fig:sub_tree}.\nThe middle heatmap shows that the cosine similarities between the vector representations $\\bar\\ell_w$ correspond to the adjacency matrix.\nThe final heatmap demonstrates that the child-parent vector $\\bar\\ell_w - \\bar\\ell_{\\text{parent of }w}$ and $\\bar\\ell_{\\text{parent of }w} - \\bar\\ell_{\\text{grandparent of }w}$ are orthogonal, as predicted in \\Cref{thm:orthogonality}.\n\n\n\\subsection{WordNet Verb Hierarchy}\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=0.95\\linewidth]{figures/verb_evaluation.pdf}\n  \\caption{Linear representations exist for most binary features in the WordNet verb hierarchy.  \n  Comparison of projection of test and random words on estimated vector representations for each feature.\n  The values are divided by the norm of the estimated vector representation. The $x$-axis indices denote all features in the verb hierarchy.\n  The thick lines present the mean of the projections for each feature and the error bars indicate the 1.96 $\\times$ standard error.}\n  \\label{fig:verb_evaluation}\n\\end{figure}\n\n\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=1.0\\linewidth]{figures/verb_single_three_heatmap.pdf}\n  \\caption{The adjacency matrix of the hierarchical relations between features in the WordNet verb hierarchy (left), the cosine similarity between the vector representations $\\bar\\ell_w$ for each feature (middle), and the cosine similarity between child-parent vectors $\\bar\\ell_w - \\bar\\ell_{\\text{parent of }w}$ for each feature (right).\n  The features are ordered by the hierarchy.\n  }\n  \\label{fig:verb_three_heatmap}\n\\end{figure}\n\nIn the same way as for the noun hierarchy, we estimate the vector representations for the WordNet verb hierarchy.\nTo evaluate whether vector representations exist, we split $\\yquad(w)$ for each synset $w$ into train words (80\\%) and test words (20\\%), fit the LDA estimator to the train words, and examine the projection of the unembedding vectors for the test words onto the estimated vector representation.\n\\Cref{fig:verb_evaluation} shows the mean and standard error of the test projections, divided by the magnitude of each estimated $\\bar{\\ell}_w$. If a vector representation exists for an attribute, we would expect these values to be close to 1. This is indeed the case, providing evidence that vector representations do indeed exist for these features.\n\n\\Cref{fig:verb_three_heatmap} displays the adjacency matrix of the WordNet verb hyponym inclusion graph (left), the cosine similarity between the vector representations $\\bar\\ell_w$ for each feature (middle), and the cosine similarity between child-parent vectors $\\bar\\ell_w - \\bar\\ell_{\\text{parent of }w}$ for each feature (right).\nThe cosine similarity clearly reflects the semantic hierarchy---the adjacency matrix is clearly visible in the middle heatmap.\nBy contrast, as predicted by \\cref{thm:orthogonality}, the child-parent and parent-grandparent vectors are orthogonal. This straightforwardly implies all other theoretical connections between orthogonality and the semantic hierarchy.\n\n\n\\end{document}\n\n\n\n\n\n\n==== END OF /2406.01506/your-paper.tex ====\n==== BEGINNING OF /2406.01506/preamble/preamble.tex ====\n%\\RequirePackage[l2tabu, orthodox]{nag}\n\n% FONTS\n\\usepackage[utf8]{inputenc} % allow utf-8 input\n\\usepackage[T1]{fontenc}    % use 8-bit T1 fonts\n\n% Replace default Latin Modern typewriter with its proportional counterpart\n% http://www.tug.dk/FontCatalogue/lmoderntypewriterprop/\n\\renewcommand*\\ttdefault{lmvtt}\n\n\n%%% OPTION 1 - Fourier Math + New Century Schoolbook + ParaType Sans\n\n% % Import Fourier Math (this imposes its own New Century Schoolbook type)\n% % http://www.ctan.org/tex-archive/fonts/fouriernc/\n% \\usepackage{fouriernc}\n% \\usepackage{amsmath}\n% % Replace with TeX Gyre Schola version of New Century Schoolbook (must scale!)\n% % http://www.tug.dk/FontCatalogue/tgschola/\n% \\usepackage[scale=0.92]{tgschola}\n% \\usepackage[scaled=0.88]{PTSans}\n\n%% OPTION 2 - MathDesign Math + Bitstream Charter + ParaType Sans\n\n% Import MathDesign (this brings along Bitstream Charter)\n% http://www.ctan.org/tex-archive/fonts/mathdesign/\n\\usepackage[bitstream-charter]{mathdesign}\n\\usepackage{amsmath}\n\\usepackage[scaled=0.92]{PTSans}\n\n\n% %%% OPTION 3 - MTPRO 2 Math + Termes Times + ParaType Sans\n\n% \\usepackage{tgtermes}\n% \\usepackage{amsmath}\n% \\usepackage[subscriptcorrection,\n%             amssymbols,\n%             mtpbb,\n%             mtpcal,\n%             nofontinfo  % suppresses all warnings\n%            ]{mtpro2}\n% \\usepackage{scalefnt,letltxmacro}\n% \\LetLtxMacro{\\oldtextsc}{\\textsc}\n% \\renewcommand{\\textsc}[1]{\\oldtextsc{\\scalefont{1.10}#1}}\n% \\usepackage[scaled=0.92]{PTSans}\n\n% GEOMETRY\n\\usepackage[\n  paper  = letterpaper,\n  left   = 1.65in,\n  right  = 1.65in,\n  top    = 1.0in,\n  bottom = 1.0in,\n  ]{geometry}\n\n% COLOR\n\\usepackage[usenames,dvipsnames,table]{xcolor}\n% \\usepackage[]{xcolor}\n%\\usepackage[usenames,table]{xcolor}\n\\definecolor{shadecolor}{gray}{0.9}\n\n% SPACING and TEXT\n\\usepackage[final,expansion=alltext]{microtype}\n\\usepackage[english]{babel}\n\\usepackage[parfill]{parskip}\n\\usepackage{afterpage}\n\\usepackage{framed}\n\n%redefine the leftbar environment to accept a width and coloring options\n\\renewenvironment{leftbar}[1][\\hsize]\n{%\n  \\def\\FrameCommand\n  {%\n    {\\color{Gray}\\vrule width 3pt}%\n    \\hspace{10pt}%\n    %\\hspace{0pt}\\fboxsep=\\FrameSep\\colorbox{black!10}%\n  }%\n  \\MakeFramed{\\hsize#1\\advance\\hsize-\\width\\FrameRestore}%\n}%\n{\\endMakeFramed}\n\n% define a paragraph header function\n\\DeclareRobustCommand{\\parhead}[1]{\\textbf{#1}~}\n\n% EDITING\n% line numbering in left margin\n\\usepackage{lineno}\n\\renewcommand\\linenumberfont{\\normalfont\n                             \\footnotesize\n                             \\sffamily\n                             \\color{SkyBlue}}\n% ragged paragraphs in right margin\n\\usepackage{ragged2e}\n\\DeclareRobustCommand{\\sidenote}[1]{\\marginpar{\n                                    \\RaggedRight\n                                    \\textcolor{Plum}{\\textsf{#1}}}}\n% paragraph counter in right margin\n\\newcommand{\\parnum}{\\bfseries\\P\\arabic{parcount}}\n\\newcounter{parcount}\n\\newcommand\\p{%\n    \\stepcounter{parcount}%\n    \\leavevmode\\marginpar[\\hfill\\parnum]{\\parnum}%\n}\n% paragraph helper\n\\DeclareRobustCommand{\\PP}{\\textcolor{Plum}{\\P} }\n\n% COUNTERS\n\\renewcommand{\\labelenumi}{\\color{black!67}{\\arabic{enumi}.}}\n\\renewcommand{\\labelenumii}{{\\color{black!67}(\\alph{enumii})}}\n\\renewcommand{\\labelitemi}{{\\color{black!67}\\textbullet}}\n\n% FIGURES\n\\usepackage{graphicx}\n\\usepackage{wrapfig}\n\\usepackage[labelfont=bf,font=footnotesize,width=.9\\textwidth]{caption}\n\\usepackage[format=hang]{subcaption}\n\n% TABLES\n\\usepackage{booktabs,multirow,multicol}       % professional-quality tables\n\n% ALGORITHMS\n\\usepackage[algoruled,algo2e]{algorithm2e}\n\\usepackage{listings}\n\\usepackage{fancyvrb}\n\\fvset{fontsize=\\normalsize}\n\n% BIBLIOGRAPHY\n%\\usepackage{natbib}\n\n% HYPERREF\n\\usepackage[colorlinks,linktoc=all]{hyperref}\n\\usepackage[all]{hypcap}\n\\hypersetup{citecolor=MidnightBlue}\n\\hypersetup{linkcolor=MidnightBlue}\n\\hypersetup{urlcolor=MidnightBlue}\n\n% CLEVEREF must come after HYPERREF\n\\usepackage[nameinlink]{cleveref}\n\n% ACRONYMS\n\\usepackage[acronym,nowarn]{glossaries}\n% \\makeglossaries\n\n% COLOR DEFINITIONS\n\\newcommand{\\red}[1]{\\textcolor{BrickRed}{#1}}\n\\newcommand{\\orange}[1]{\\textcolor{BurntOrange}{#1}}\n\\newcommand{\\green}[1]{\\textcolor{OliveGreen}{#1}}\n\\newcommand{\\blue}[1]{\\textcolor{MidnightBlue}{#1}}\n\\newcommand{\\gray}[1]{\\textcolor{black!60}{#1}}\n\n% LISTINGS DEFINTIONS\n\\lstdefinestyle{mystyle}{\n    commentstyle=\\color{OliveGreen},\n    keywordstyle=\\color{BurntOrange},\n    numberstyle=\\tiny\\color{black!60},\n    stringstyle=\\color{MidnightBlue},\n    basicstyle=\\ttfamily,\n    breakatwhitespace=false,\n    breaklines=true,\n    captionpos=b,\n    keepspaces=true,\n    numbers=left,\n    numbersep=5pt,\n    showspaces=false,\n    showstringspaces=false,\n    showtabs=false,\n    tabsize=2\n}\n\\lstset{style=mystyle}\n==== END OF /2406.01506/preamble/preamble.tex ====\n==== BEGINNING OF /2406.01506/preamble/preamble_math.tex ====\n% !TEX root = main.tex\n\n\n% redundant w/ stuff loaded in preamble.tex\n\\usepackage{amsthm}\n% \\usepackage{amsfonts}       % blackboard math symbols\n% \\usepackage{amssymb}\n\n\\usepackage{centernot}\n\\usepackage{nicefrac}       % compact symbols for 1/2, etc.\n\\usepackage{mathtools}\n\\usepackage{amsbsy}\n\\usepackage{amstext}\n\\usepackage{thmtools}\n\\usepackage{thm-restate}\n\n% compatibility w/ parskip https://tex.stackexchange.com/questions/25346/wrong-spacing-before-theorem-environment-amsthm\n\\begingroup\n    \\makeatletter\n    \\@for\\theoremstyle:=definition,remark,plain\\do{%\n        \\expandafter\\g@addto@macro\\csname th@\\theoremstyle\\endcsname{%\n            \\addtolength\\thm@preskip\\parskip\n            }%\n        }\n\\endgroup\n\n\\DeclareRobustCommand{\\mb}[1]{\\ensuremath{\\mathbf{\\boldsymbol{#1}}}}\n% \\DeclareRobustCommand{\\mb}[1]{\\mathbold{#1}}\n\n\\DeclareRobustCommand{\\KL}[2]{\\ensuremath{\\textrm{KL}\\left(#1\\;\\|\\;#2\\right)}}\n\n% \\DeclareMathOperator*{\\argmax}{arg\\,max}\n% \\DeclareMathOperator*{\\argmin}{arg\\,min}\n\n\\crefname{lemma}{lemma}{lemmas}\n\\Crefname{lemma}{Lemma}{Lemmas}\n\\crefname{thm}{theorem}{theorems}\n\\Crefname{thm}{Theorem}{Theorems}\n\\crefname{prop}{proposition}{propositions}\n\\Crefname{prop}{Proposition}{Propositions}\n\\crefname{assumption}{assumption}{assumptions}\n\\crefname{assumption}{Assumption}{Assumptions}\n\n\n% \\newtheorem{thm}{Theorem} % reset theorem numbering for each chapter\n% \\newtheorem{defn}{Definition} % definition numbers are dependent on theorem numbers\n% \\newtheorem{prop}[thm]{Proposition}\n% \\newtheorem{exmp}[thm]{Example} % same for example numbers\n% \\newtheorem{lemma}[thm]{Lemma}\n% \\newtheorem{assumption}{Assumption}\n% \\newtheorem{corollary}[thm]{Corollary}\n\\newcommand\\independent{\\protect\\mathpalette{\\protect\\independenT}{\\perp}}\n\\def\\independenT#1#2{\\mathrel{\\rlap{$#1#2$}\\mkern2mu{#1#2}}}\n\n\\newcommand{\\grad}{\\nabla}\n\n\\renewcommand{\\mid}{~\\vert~}\n\\newcommand{\\prm}{\\:;\\:}\n\n\\newcommand{\\mbw}{\\mb{w}}\n\\newcommand{\\mbW}{\\mb{W}}\n\n\\newcommand{\\mbx}{\\mb{x}}\n\\newcommand{\\mbX}{\\mb{X}}\n\n\\newcommand{\\mby}{\\mb{y}}\n\\newcommand{\\mbY}{\\mb{Y}}\n\n\\newcommand{\\mbz}{\\mb{z}}\n\\newcommand{\\mbZ}{\\mb{Z}}\n\\newcommand{\\mbT}{\\mb{T}}\n\\newcommand{\\mbA}{\\mb{A}}\n\\newcommand{\\mba}{\\mb{a}}\n\n\\newcommand{\\mbI}{\\mb{I}}\n\\newcommand{\\mbone}{\\mb{1}}\n\n\\newcommand{\\mbL}{\\mb{L}}\n\n\\newcommand{\\mbtheta}{\\mb{\\theta}}\n\\newcommand{\\mbTheta}{\\mb{\\Theta}}\n\\newcommand{\\mbomega}{\\mb{\\omega}}\n\\newcommand{\\mbOmega}{\\mb{\\Omega}}\n\\newcommand{\\mbsigma}{\\mb{\\sigma}}\n\\newcommand{\\mbSigma}{\\mb{\\Sigma}}\n\\newcommand{\\mbphi}{\\mb{\\phi}}\n\\newcommand{\\mbPhi}{\\mb{\\Phi}}\n\n\\newcommand{\\mbalpha}{\\mb{\\alpha}}\n\\newcommand{\\mbbeta}{\\mb{\\beta}}\n\\newcommand{\\mbgamma}{\\mb{\\gamma}}\n\\newcommand{\\mbeta}{\\mb{\\eta}}\n\\newcommand{\\mbmu}{\\mb{\\mu}}\n\\newcommand{\\mbrho}{\\mb{\\rho}}\n\\newcommand{\\mblambda}{\\mb{\\lambda}}\n\\newcommand{\\mbzeta}{\\mb{\\zeta}}\n\n\\newcommand\\dif{\\mathop{}\\!\\mathrm{d}}\n\\newcommand{\\diag}{\\textrm{diag}}\n\\newcommand{\\supp}{\\textrm{supp}}\n\n\\newcommand{\\V}{\\mathbb{V}}\n\\newcommand{\\bbH}{\\mathbb{H}}\n\n\\newcommand{\\bbN}{\\mathbb{N}}\n\\newcommand{\\bbZ}{\\mathbb{Z}}\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\bbS}{\\mathbb{S}}\n\n\\newcommand{\\cL}{\\mathcal{L}}\n\\newcommand{\\cD}{\\mathcal{D}}\n\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cT}{\\mathcal{T}}\n\\newcommand{\\Gam}{\\textrm{Gam}}\n\\newcommand{\\InvGam}{\\textrm{InvGam}}\n\n% \\newcommand{\\qedsymbol}{\\rule{0.7em}{0.7em}}\n\n\\newcommand{\\g}{\\, | \\,}\n\\newcommand{\\s}{\\, ; \\,}\n\n\\newcommand{\\indpt}{\\protect\\mathpalette{\\protect\\independenT}{\\perp}}\n\\newcommand{\\E}[2]{\\mathbb{E}_{#1}\\left[#2\\right]}\n\n\\def\\checkmark{\\tikz\\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} \n\n\n\\usepackage{booktabs,arydshln}\n\\makeatletter\n\\def\\adl@drawiv#1#2#3{%\n        \\hskip.5\\tabcolsep\n        \\xleaders#3{#2.5\\@tempdimb #1{1}#2.5\\@tempdimb}%\n                #2\\z@ plus1fil minus1fil\\relax\n        \\hskip.5\\tabcolsep}\n\\newcommand{\\cdashlinelr}[1]{%\n  \\noalign{\\vskip\\aboverulesep\n           \\global\\let\\@dashdrawstore\\adl@draw\n           \\global\\let\\adl@draw\\adl@drawiv}\n  \\cdashline{#1}\n  \\noalign{\\global\\let\\adl@draw\\@dashdrawstore\n           \\vskip\\belowrulesep}}\n\\makeatother\n\n\\newenvironment{proofsk}{%\n  \\renewcommand{\\proofname}{Proof sketch}\\proof}{\\endproof}\n\n\\renewcommand{\\epsilon}{\\varepsilon}\n\n%********************************************************************\n% Extra theorem environments\n%********************************************************************\n\n\\declaretheorem[style=plain,name=Theorem]{theorem}\n\\declaretheorem[style=plain,sibling=theorem,name=Lemma]{lemma}\n\\declaretheorem[style=plain,sibling=theorem,name=Proposition]{proposition}\n\\declaretheorem[style=plain,sibling=theorem,name=Corollary]{cor}\n\\declaretheorem[style=plain,sibling=theorem,name=Claim]{claim}\n\\declaretheorem[style=plain,sibling=theorem,name=Conjecture]{conjecture}\n\\declaretheorem[style=definition,sibling=theorem,name=Definition]{definition}\n\\declaretheorem[style=definition,name=Assumption]{assumption}\n\\declaretheorem[style=definition,sibling=theorem,name=Example]{example}\n\\declaretheorem[style=remark,sibling=theorem,name=Remark]{remark}\n\n\\newenvironment{example*}\n {\\pushQED{\\qed}\\example}\n {\\popQED\\endexample}\n\\numberwithin{equation}{section}\n\n==== END OF /2406.01506/preamble/preamble_math.tex ====\n==== BEGINNING OF /2406.01506/preamble/minimalist_biblatex.tex ====\n\\usepackage{csquotes}\n\\usepackage[%\nminnames=1,maxnames=99,maxcitenames=2,\nstyle=alphabetic,\n%style=authoryear,\ndoi=false,\nurl=false,\ngiveninits=true,\nhyperref,\nnatbib,\nbackend=bibtex,\nsorting=nyt,\nbackref=true\n]{biblatex}%\n\\renewbibmacro{in:}{%\n  \\ifentrytype{article}{}{\\printtext{\\bibstring{in}\\intitlepunct}}}\n%\\setcitestyle{authoryear,round,citesep={;},aysep={,},yysep={;}}\n\n\\renewbibmacro*{journal}{%\n  \\iffieldundef{journaltitle}\n    {}\n    {\\printtext[journaltitle]{%\n       \\printfield[noformat]{journaltitle}%\n       \\setunit{\\subtitlepunct}%\n       \\printfield[noformat]{journalsubtitle}}}}\n\n%\\DeclareFieldFormat[article,inbook,incollection,inproceedings,patent,thesis,unpublished]{titlecase}{\\MakeSentenceCase*{#1}}\n% print the title of articles and any in* type entry in sentence case\n\\DeclareFieldFormat{sentencecase}{\\MakeSentenceCase*{#1}}\n\n\\renewbibmacro*{title}{%\n  \\ifthenelse{\\iffieldundef{title}\\AND\\iffieldundef{subtitle}}\n    {}\n    {\\ifthenelse{\\ifentrytype{article}\\OR\\ifentrytype{inbook}%\n      \\OR\\ifentrytype{incollection}\\OR\\ifentrytype{inproceedings}%\n      \\OR\\ifentrytype{inreference}\\OR\\ifentrytype{misc}}\n      {\\printtext[title]{%\n        \\printfield[sentencecase]{title}%\n        \\setunit{\\subtitlepunct}%\n        \\printfield[sentencecase]{subtitle}}}%\n      {\\printtext[title]{%\n        \\printfield[titlecase]{title}%\n        \\setunit{\\subtitlepunct}%\n        \\printfield[titlecase]{subtitle}}}%\n     \\newunit}%\n  \\printfield{titleaddon}}\n\n\n\n\\AtEveryBibitem{%\n\\ifentrytype{article}{\n%    \\clearfield{url}%\n    \\clearfield{urldate}%\n    \\clearfield{eprint}\n    \\clearfield{eid}\n}{}\n\\ifentrytype{book}{\n    \\clearfield{url}%\n    \\clearfield{urldate}%\n    \\clearfield{eprint}\n}{}\n\\ifentrytype{collection}{\n    \\clearfield{url}%\n    \\clearfield{urldate}%\n    \\clearfield{eprint}\n}{}\n\\ifentrytype{incollection}{\n    \\clearfield{url}%\n    \\clearfield{urldate}%\n    \\clearfield{eprint}\n}{}\n}\n\n\\AtEveryBibitem{\n    \\clearfield{pages}\n    \\clearfield{review}%\n    \\clearfield{series}%%\n    \\clearfield{volume}\n    \\clearfield{month}\n    % \\clearfield{eprint}\n    \\clearfield{isbn}\n    \\clearfield{issn}\n    \\clearlist{location}\n    \\clearfield{series}\n    \\clearlist{publisher}\n    \\clearname{editor}\n}{}\n\n==== END OF /2406.01506/preamble/minimalist_biblatex.tex ====\n==== BEGINNING OF /2406.01506/preamble/definitions_basic.tex ====\n% This file contains definitions of custom macros\n% ------------------------------------------------------------------------------\n\n\\newcommand{\\defnphrase}[1]{\\emph{#1}}\n\n\\global\\long\\def\\floor#1{\\lfloor#1\\rfloor}\n\n%\\newcommand{\\st}{\\,:\\,}\n\\newcommand{\\defeq}{\\coloneqq}\n\\newcommand{\\asympeq}{\\ \\sim\\ }\n\n\\newcommand{\\Reals}{\\mathbb{R}}\n\\newcommand{\\Nats}{\\mathbb{N}}\n\\newcommand{\\NNReals}{\\Reals_{+}}\n\n\n\\newcommand{\\exclude}{\\backslash}\n\n\\newcommand{\\eps}{\\varepsilon}\n\n\\renewcommand{\\Re}{\\mathrm{Re}}\n\\renewcommand{\\Im}{\\mathrm{Im}}\n\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\logit}{logit}\n\n\n% Graph theory\n\\newcommand{\\edges}{e}\n\\newcommand{\\vertices}{v}\n\\newcommand{\\loops}{l}\n\n\n% Probability\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\var}{\\mathrm{var}}\n\\renewcommand{\\Pr}{\\mathbb{P}}\n\\newcommand{\\convPr}{\\xrightarrow{\\,p\\,}}\n\\newcommand{\\convDist}{\\xrightarrow{\\,d\\,}}\n\\newcommand{\\equaldist}{\\overset{d}{=}}\n\\newcommand{\\upto}{\\!\\uparrow\\!}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\as}{\\textrm{ a.s.}}\n\\newcommand{\\equalas}{\\overset{\\mathrm{a.s.}}{=}}\n\\newcommand{\\abs}[1]{\\left\\lvert#1 \\right\\rvert}\n\\newcommand{\\intd}{\\mathrm{d}}\n\\newcommand{\\dist}{\\ \\sim\\ }\n\\newcommand{\\distiid}{\\overset{\\mathrm{iid}}{\\dist}}\n\\newcommand{\\distind}{\\overset{ind}{\\dist}}\n\\newcommand{\\dtv}[1]{\\|#1\\|_{\\mathrm{TV}}}\n%\\newcommand{\\PP}{\\Pi}\n\\newcommand{\\PPDist}{\\mathrm{PP}}\n\n\\newcommand{\\Lebesgue}{\\Lambda}\n\\newcommand{\\NatSubs}[1]{\\tilde \\Nats_{#1}}\n\n% Causality\n\\newcommand{\\cdo}{\\mathrm{do}} \n\n% Distributions\n\\newcommand{\\normalDist}{\\mathrm{Normal}}\n\\newcommand{\\diriDist}{\\mathrm{Diri}}\n\\newcommand{\\categDist}{\\mathrm{Cat}}\n\\newcommand{\\betaDist}{\\mathrm{Beta}}\n\\newcommand{\\bernDist}{\\mathrm{Bern}}\n\\newcommand{\\binDist}{\\mathrm{Bin}}\n\\newcommand{\\uniDist}{\\mathrm{Uni}}\n\\newcommand{\\poiDist}{\\mathrm{Poi}}\n\\newcommand{\\gammaDist}{\\mathrm{Gamma}}\n\\newcommand{\\multiDist}{\\mathrm{Multi}}\n\n\n% \\Set command\n\\providecommand\\given{} % so it exists\n\\newcommand\\SetSymbol[1][]{\n  \\nonscript\\,#1:\\nonscript\\,\\mathopen{}\\allowbreak}\n\\DeclarePairedDelimiterX\\Set[1]{\\lbrace}{\\rbrace}%\n{ \\renewcommand\\given{\\SetSymbol[]} #1 }\n\n% Indicator\n\\newcommand{\\Ind}{\\mathbbm{1}}\n\n\n%%% Local Variables:\n%%% mode: latex\n%%% TeX-master: \"main\"\n%%% End:\n\n==== END OF /2406.01506/preamble/definitions_basic.tex ====",
            "processed_original_tex": "==== BEGINNING OF /2406.01506/your-paper.tex ====\n\\pdfoutput=1\n\\documentclass{article}\n\\usepackage{verbatim}\n\n\\input{preamble/preamble.tex}\n\\input{preamble/preamble_math.tex}\n\\input{preamble/definitions_basic.tex}\n\\input{preamble/minimalist_biblatex.tex}\n\n\\addbibresource{bibs/hierarchical.bib}\n\n\\usepackage{thm-restate}\n\n\\crefformat{equation}{(#2#1#3)}\n\\crefformat{figure}{Figure~#2#1#3}\n\\crefname{definition}{Definition}{Definitions}\n\\crefname{example}{Example}{Examples}\n\\crefname{lemma}{Lemma}{Lemmas}\n\\crefname{cor}{Corollary}{Corollaries}\n\\crefname{theorem}{Theorem}{Theorems}\n\\crefname{assumption}{Assumption}{Assumptions}\n\n\n\n\\DeclareMathOperator{\\lmOp}{\\lambda_{\\text{LM}}}\n\\newcommand{\\lmText}[1]{\\lmOp(\\text{``#1''})}\n\\newcommand{\\lm}[1]{\\lmOp(#1)}\n\n\\DeclareMathOperator{\\repOp}{Rep}\n\\newcommand{\\repText}[1]{\\repOp(\\text{``#1''})}\n\\newcommand{\\rep}[1]{\\repOp(#1)}\n\n\\DeclareMathOperator{\\genOp}{gen}\n\\newcommand{\\promptSpace}{\\mathcal{X}}\n\\newcommand{\\conceptSpace}{\\mathcal{C}}\n\\newcommand{\\repSpace}{\\mathcal{R}}\n\\newcommand{\\responseSpace}{\\mathcal{Y}}\n\\newcommand{\\defas}{:=}\n\\newcommand{\\nullconcept}{\\phi}\n\\newcommand{\\conceptDistSpace}{\\mathcal{Q}}\n\n\\newcommand{\\ConceptName}[1]{$\\mathtt{#1}$}\n\\newcommand{\\ConceptValue}[1]{\\texttt{#1}}\n\\newcommand{\\ConceptDirName}[2]{\\texttt{#1}\\Rightarrow\\texttt{#2}}\n\\newcommand{\\ConceptDirMath}[2]{#1 \\Rightarrow #2}\n\n\\DeclareMathOperator{\\SpanOp}{Span}\n\\newcommand{\\SpanText}[1]{\\SpanOp(\\text{``#1''})}\n\\newcommand{\\Span}[1]{\\SpanOp(#1)}\n\n\\DeclareMathOperator{\\ConeOp}{Cone}\n\\newcommand{\\ConeText}[1]{\\ConeOp(\\text{``#1''})}\n\\newcommand{\\Cone}[1]{\\ConeOp(#1)}\n\\newcommand{\\cov}{\\mathrm{Cov}}\n\\newcommand{\\yquad}{\\mathcal{Y}}\n\\newcommand{\\ip}[2]{\\langle #1,#2\\rangle}\n\n\\usepackage{enumitem}\n\n\\usepackage[affil-it]{authblk}\n\n\n\\title{The Geometry of Categorical and \\\\ Hierarchical Concepts in Large Language Models}\n\n\\date{}\n\\author[1]{Kiho Park}\n\\author[2]{Yo Joong Choe}\n\\author[3]{Yibo Jiang}\n\\author[1,2]{Victor Veitch}\n\\affil[1]{Department of Statistics, University of Chicago}\n\\affil[2]{Data Science Institute, University of Chicago}\n\\affil[3]{Department of Computer Science, University of Chicago}\n\n\\begin{document}\n\\maketitle\n\n\\begin{abstract}\n  Understanding how semantic meaning is encoded in the representation spaces of large language models is a fundamental problem in interpretability.\n  In this paper, we study the two foundational questions in this area. First, how are categorical concepts, such as $\\{\\ConceptValue{mammal}, \\ConceptValue{bird}, \\ConceptValue{reptile}, \\ConceptValue{fish}\\}$, represented? Second, how are hierarchical relations between concepts encoded? For example, how is the fact that \\ConceptName{dog} is a kind of \\ConceptName{mammal} encoded?\n  We show how to extend the linear representation hypothesis to answer these questions. We find a remarkably simple structure: simple categorical concepts are represented as simplices, hierarchically related concepts are orthogonal in a sense we make precise, and (in consequence) complex concepts are represented as polytopes constructed from direct sums of simplices, reflecting the hierarchical structure.\n  We validate these theoretical results on the Gemma large language model, estimating representations for 957 hierarchically related concepts using data from WordNet.\n  Code is available at \\href{https://github.com/KihoPark/LLM_Categorical_Hierarchical_Representations}{github.com/KihoPark/LLM\\_Categorical\\_Hierarchical\\_Representations}.\n\\end{abstract}\n\n\n\n\\section{Introduction}\\label{sec:introduction}\n\nThis paper concerns how high-level semantic concepts are encoded in the representation spaces of large language models (LLMs).\nUnderstanding this is crucial for model interpretability and control. The ultimate aspiration is to monitor (and manipulate) the semantic behavior of LLMs (e.g., is the model's response truthful) by directly measuring (and editing) the internal vector representations of the model \\Citep[e.g.,][]{li2023emergent, zou2023representation, ghandeharioun2024patchscope}.\nAchieving this requires understanding how the geometric structure of the representation spaces corresponds to the high-level semantic concepts that humans understand.\nIn this paper, we are concerned with two fundamental questions in this direction:\n\\begin{enumerate}\n  \\item How are categorical concepts represented? For example, what is the representation of the concept $\\ConceptValue{animal} = \\{\\ConceptValue{mammal}, \\ConceptValue{bird}, \\ConceptValue{reptile}, \\ConceptValue{fish}\\}$?\n  \\item How are hierarchical relations between concepts represented? For example, what is the relationship between the representations of $\\ConceptValue{animal}$, $\\ConceptValue{mammal}$, $\\ConceptValue{dog}$, and $\\ConceptValue{poodle}$?\n\\end{enumerate}\n\nOur starting point is the \\emph{linear representation hypothesis}, the informal idea that high-level concepts are linearly encoded in the representation spaces of LLMs \\Citep[e.g.,][]{marks2023geometry, tigges2023linear, gurnee2024language}.\nA main challenge for the linear representation hypothesis is that, in general, it's not clear what ``linear'' means, nor what constitutes a ``high-level concept''.\n\\Citet{park2024linear} give a formalization in the limited setting of binary concepts that can be defined by counterfactual pairs of words. For example, the concept of $\\ConceptDirName{male}{female}$ is formalized using the counterfactual pairs $\\{(\\text{``man''}, \\text{``woman''}), (\\text{``king''}, \\text{``queen''}), \\dots\\}$. \nThey prove that such binary concepts have a well-defined linear representation as a direction in the representation space.\nThey further connect semantic structure and representation geometry by showing that, under a \\emph{causal inner product}, concepts that can be freely manipulated (e.g., $\\ConceptDirName{male}{female}$ and $\\ConceptDirName{french}{english}$) are represented by orthogonal directions.\n\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[trim={3cm 0.7cm 1.5cm 1cm}, clip, width=1.0\\linewidth]{figures/diagram.pdf}\n  \\caption{\n    In large language models, categorical concepts are represented as simplices in the representation space.\n    Further, hierarchically related concepts (such as \\ConceptName{animal} and $\\ConceptDirName{mammal}{bird}$) live in orthogonal subspaces.\n    The top panel illustrates the structure, the bottom panels show the measured representation structure in the Gemma LLM. See \\Cref{sec:experiments} for details.}\n  \\label{fig:diagram}\n\\end{figure}\n\nOur aim is to extend this formalization beyond binary concepts represented as counterfactual word pairs. For example, the \\ConceptName{animal} concept does not have a natural counterfactual definition, but such concepts are fundamental to human semantic understanding. Further, we aim to understand how the geometry of the representation space encodes relationships between concepts that cannot be freely manipulated, such as \\ConceptName{animal} and \\ConceptName{mammal}. \n\nTo that end, we make the following contributions:\n\\begin{enumerate}\n  \\item We show how to move from representations of binary concepts as \\emph{directions} to representations as \\emph{vectors}. This allows us to use vector operations to compose representations.\n  \\item Using this result, we show that semantic hierarchy between concepts is encoded geometrically as orthogonality between representations, in a manner we make precise.\n  \\item Then, we construct the representation of categorical variables (e.g., \\ConceptName{animal}) as the polytope where the vertices are the representations of the binary features that define the category (e.g., $\\ConceptValue{mammal}, \\ConceptValue{bird}, \\dots$). We show that for ``natural'' concepts, the representation is a simplex.\n  \\item Finally, we empirically validate these theoretical results on the Gemma large language model \\Citep[][]{team2024gemma}. To that end, we extract concepts from the WordNet hierarchy \\Citep[][]{miller1995wordnet}, estimate their representations, and show that the geometric structure of the representations align with the semantic hierarchy of WordNet.\n\\end{enumerate}\nThe final structure is remarkably simple, and is summarized in \\Cref{fig:diagram}.\nIn totality, these results provide a foundation for understanding how high-level semantic concepts are encoded in the representation spaces of LLMs. \n\n\n\\section{Preliminaries}\\label{sec:preliminaries}\nWe begin with some necessary background.\n\\subsection{Large Language Models}\nFor the purposes of this paper, we consider a large language model to consist of two parts. \nThe first part is a function $\\lambda$ that maps input texts $x$ to vectors $\\lambda(x)$ in a representation space $\\Lambda  \\simeq \\Reals^d$. This is the function given by the stacked transformer blocks. We take $\\lambda(x)$ to be the output of the final layer at the final token position. The second part is an unembedding layer that assigns a vector $\\gamma(y)$ in an unembedding space $\\Gamma  \\simeq \\Reals^d$ to each token $y$ in the vocabulary. Together, these define a sampling distribution over tokens via the softmax distribution:\n\\begin{equation}\n  \\Pr(y\\given x) = \\frac{\\exp(\\lambda(x)^\\top \\gamma(y))}{\\sum_{y' \\in \\mathrm{Vocab}}\\exp(\\lambda(x)^\\top \\gamma(y'))}.\n\\end{equation}\n\nThe broad goal is to understand how semantic structure is encoded in the geometry of the spaces $\\Lambda$ and $\\Gamma$.\n(We do not address the ``internal'' structure of the LLMs in this paper, though we are optimistic that a clear understanding of the softmax geometry will shed light on this as well.)\n\n\\subsection{Concepts}\nWe formalize a concept as a latent variable $W$ that is caused by the context $X$ and causes the output $Y$. \nThat is, a concept is a thing that could---in principle---be manipulated to affect the output of the language model.\nIn the particular case where a concept is a binary variable with a word-level counterfactual, we can identify the variable $W$ with the counterfactual pair of outputs $(Y(0), Y(1))$. Concretely, we can identify $\\ConceptDirName{male}{female}$ with $(Y(0), Y(1)) \\in_R \\{(\\text{``man''}, \\text{``woman''}), (\\text{``king''}, \\text{``queen''}), (\\text{``he''}, \\text{``her''}), \\dots\\}$. We emphasize that the notion of a concept as a latent variable that affects the output is more general than the counterfactual binary case.\n\nGiven a pair of concept variables $W$ and $Z$, we say that $W$ is \\emph{causally separable} with $Z$ if the potential outcome $Y(W=w, Z=z)$ is well-defined for all $w,z$.\nThat is, two variables are causally separable if they can be freely manipulated---e.g., we can change the output language and the sex of the subject freely, so these concepts are causally separable.\n\n\\subsection{Causal Inner Product and Linear Representations}\nWe are trying to understand how concepts are represented.\nAt this stage, there are two distinct representation spaces: $\\Lambda$ and $\\Gamma$. The former is the space of context embeddings, and the latter is the space of token unembeddings. \nWe would like to unify these spaces so that there is just a single notion of representation. \n\n\\Citet{park2024linear} show how to achieve this unification via a ``Causal Inner Product''. This is a particular choice of inner product that respects the semantics of language in the sense that the linear representations of (binary, counterfactual) causally separable concepts are orthogonal under the inner product. Their result can be understood as saying that there is some invertible matrix $A$ and constant vector $\\bar{\\gamma}_0$ such that, if we transform the embedding and unembedding spaces as\n\\begin{equation}\\label{eq:transformation}\n  g(y) \\leftarrow A (\\gamma(y) - \\bar{\\gamma}_0), \\quad \\ell(x) \\leftarrow A^{-\\top} \\lambda(x)\n\\end{equation}\nthen the Euclidean inner product in the transformed spaces is the causal inner product, and the Riesz isomorphism between the embedding and unembedding spaces is simply the usual vector transpose operation. \nWe can estimate $A$ as the whitening operation for the unembedding matrix.\nFollowing this transformation, we can think of the embedding and unembedding spaces as the same space, equipped with the Euclidean inner product.\\footnote{We are glossing over some technical details here; see \\citep{park2024linear} for details.}\n\nNotice that the softmax probabilities are unchanged for any $A$ and $\\bar{\\gamma}_0$, so this transformation does not affect the model's behavior. The vector $\\bar{\\gamma}_0$ defines an origin for the unembedding space, and can be chosen arbitrarily. We give a particularly convenient choice below.\n\nIn this unified space, the linear representation of a binary concept $W\\in_R \\{0, 1\\}$ is defined as:\n\\begin{definition}\\label{def:linear_representation_orig}\n  A vector $\\bar{\\ell}_W$ is a linear representation of a binary concept $W$ if for all contexts $\\ell$, and all concept variables $Z$ that are causally separable with $W$, we have, for all $\\alpha>0$,\n  \\begin{align}\n    \\Pr(W = 1 \\given \\ell + \\alpha \\bar{\\ell}_W) &> \\Pr(W = 1 \\given \\ell), \\text{ and} \\\\\n    \\Pr(Z \\given \\ell + \\alpha \\bar{\\ell}_W) &= \\Pr(Z\\given \\ell).\n  \\end{align} \n\\end{definition}\nThat is, the linear representation is a direction in the representation space that, when added to the context, increases the probability of the concept, but does not affect the probability of any off-target concept. \nThe representation is only a direction because $\\alpha \\bar{\\ell}_W$ is also a linear representation for any $\\alpha>0$ (i.e., there is no notion of magnitude).\nIn the case of binary concepts that can be represented as counterfactual pairs of words, this direction can be shown to be proportional to the ``linear probing'' direction, and proportional to $g(Y(1)) - g(Y(0))$ for any counterfactual pair $Y(1), Y(0)$ that differ on $W$.\n\n\n\n\\section{Binary Concepts and Hierarchical Structure}\nOur high-level strategy will be to build up from binary concepts to more complex structure. We begin by defining the basic building blocks.\n\n\\paragraph{Binary and Categorical Concepts}\nWe consider two kinds of binary concept.\nA \\emph{binary feature} $W \\in_R \\{\\ConceptValue{not\\_w}, \\ConceptValue{is\\_w}\\}$ is an indicator of whether the output has the attribute $w$. For example, if the feature \\ConceptName{is\\_animal} is true then the output will be about an animal. \nA \\emph{binary contrast} $\\ConceptDirName{a}{b} \\in_R \\{a, b \\}$ is a binary variable that contrasts two specific attribute values. For example, the variable $\\ConceptDirName{mammal}{bird}$ is a binary contrast.\nIn the particular case where the concept may be represented as counterfactual pairs of words, we can identify the contrast with the notion of linear representation in \\citet{park2024linear}.\n\nWe also define a categorical concept to be any concept corresponding to a categorical latent variable. This includes binary concepts as a special case.\n\n\\paragraph{Hierarchical Structure}\nThe next step is to define what we mean by a hierarchical relation between concepts.\nTo that end, to each attribute $w$, we associate a set of tokens $\\yquad(w)$ that have the attribute.\nFor example, $\\yquad(\\ConceptValue{mammal}) = \\{\\text{`` dog''}, \\text{`` cats''}, \\text{`` Tiger''}, \\dots\\}$.\nThen,\n\\begin{definition}\\label{def:hierarchical_relation}\n  A value $z$ is \\defnphrase{subordinate} to a value $w$ (denoted by $z \\prec w$) if $\\yquad(z) \\subseteq \\yquad(w)$.\n  We say a categorical concept $Z \\in_R \\{z_0, \\dots, z_{k-1}\\}$ is subordinate to a categorical concept $W \\in_R \\{w_0, \\dots, w_{n-1}\\}$ if there exists a value $w_Z$ of $W$ such that each value $z_i$ of $Z$ is subordinate to $w_Z$.\n\\end{definition}\n\nFor example, the binary contrast  $\\ConceptDirName{dog}{cat}$ is subordinate to the binary feature $\\{\\ConceptValue{is\\_mammal},$ $\\ConceptValue{not\\_mammal} \\}$, and the binary contrast $\\ConceptDirName{parrot}{eagle}$ is subordinate to the categorical concept \\{\\ConceptName{mammal}, \\ConceptName{bird}, \\ConceptName{fish}\\}.\nOn the other hand, $\\ConceptDirName{dog}{eagle}$ is not subordinate to $\\ConceptDirName{bird}{mammal}$, and $\\ConceptDirName{bird}{mammal}$ and $\\ConceptDirName{live\\_in\\_house}{live\\_in\\_water}$ are not subordinate to each other.\n\n\\paragraph{Linear Representations of Binary Concepts}\nNow we return to the question of how binary concepts are represented.\nA key desideratum is that if $\\bar{\\ell}_W$ is a linear representation then moving the representation in this direction should modify the probability of the target concept \\emph{in isolation}.\nIf adding $\\bar{\\ell}_W$ also modified off-target concepts, it would not be natural to identify it with $W$. In \\cref{def:linear_representation_orig}, this idea is formalized by the requirement that the probability of causally separable concepts is unchanged when the representation is added to the context. \n\nWe now observe that, when there is hierarchical structure, this requirement is not strong enough to capture `off-target' behavior. For example, if $\\bar{\\ell}_{\\ConceptValue{animal}}$ captures the concept of animal vs not-animal, then moving in this direction should not affect the relative probability of the output being about a mammal versus a bird. If it did, then the representation would actually capture some amalgamation of the animal and mammal concepts. Accordingly, we must strengthen our definition: \n\\begin{definition}\\label{def:linear_representation}\n    A vector $\\bar{\\ell}_W$ is a linear representation of a binary concept $W$ if\n    \\begin{align}\n      \\Pr(W = 1 \\given \\ell + \\alpha \\bar{\\ell}_W) &> \\Pr(W = 1 \\given \\ell) \\text{, and} \\label{eq:linear_cond1} \\\\\n      \\Pr(Z \\given \\ell + \\alpha\\bar{\\ell}_W) &= \\Pr(Z \\given \\ell), \\label{eq:linear_cond2}\n    \\end{align}   \n    for all contexts $\\ell$, all $\\alpha>0$, and all concept variables $Z$ that are either subordinate or causally separable with $W$.\n    Here, if $W$ is a binary feature for an attribute $w$, $W=1$ denotes $W = \\ConceptValue{is\\_w}$.\n\\end{definition}   \n\nNotice that, in the case of binary concepts defined by counterfactual pairs, this definition is equivalent to \\Cref{def:linear_representation_orig}, because such variables have no subordinate concepts.\n\n\n\n\n\\section{Representations of Complex Concepts}\nWe now turn to how complex concepts are represented. The high-level strategy is to show how to represent binary features as vectors, show how geometry encodes semantic composition, and then use this to construct representations of complex concepts.\n\n\\subsection{Vector Representations of Binary Features}\nTo build up to complex concepts, we need to understand how to compose representations of binary concepts. At this stage, the representations are \\emph{directions} in the representation space---they do not have a natural notion of magnitude. In particular, this means we cannot use vector operations (such as addition) to compose representations. To overcome this, we now show how to associate a magnitude to the representation of a binary concept.\n\nThe key is the following result, which connects binary feature representations and word unembeddings:\n\nAll proofs are given in \\Cref{sec:proofs}.\n\nIn words, this theorem says that if a (perfect) linear representation of the \\ConceptName{animal} feature exists, then every token that has the animal attribute has the \\emph{same} dot product with the representation vector; i.e., ``cat'' is exactly as much \\ConceptName{animal} as ``dog'' is. If this weren't true, then increasing the probability that the output is about an animal would also increase the relative probability that the output is about a dog rather than a cat.\nIn practice, such exact representations are unlikely to be found by gradient descent in LLM training. Rather, we expect $\\bar\\ell_W^\\top g(y)$ to be isotropically distributed around $b_w$ with variance that is small compared to $b_w$ (so that animal and non-animal words are well-separated.) \n\nWith this result in hand, we can define a notion of vector representation for a binary feature:\n\\begin{definition}\\label{def:vector_representation}\n  We say that binary feature $W$ for an attribute $w$ has a \\emph{vector representation} $\\bar{\\ell}_w \\in \\Reals^d$ if $\\bar{\\ell}_w$ satisfies \\cref{def:linear_representation} and $\\|\\bar{\\ell}_w\\|_2=b_w$ in \\cref{thm:magnitude}.\n  If the vector representation of a binary feature is not unique, we say $\\bar\\ell_w$ is the vector representation that maximizes $b_w$.\n\\end{definition} \n\n\n\\subsection{Hierarchical Orthogonality}\nWe have now moved from representations as directions to representations as vectors.\nUsing this result, we now establish how hierarchical relations between concepts are encoded in the vector space structure of the representation space. The structure is illustrated in \\Cref{fig:three_2d_plots}. Formally, we have the following connections between vector and semantic structure:\n\n\nWe emphasize that these results---involving differences of representations---are only possible because we now have \\emph{vector} representations (mere direction would not suffice).\n\n\n\\subsection{Categorical Concepts as Simplices}\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=1.0\\linewidth]{figures/three_2d_plots.png}\n  \\caption{Hierarchical semantics are encoded as orthogonality in the representation space, as predicted in \\cref{thm:orthogonality}.    \n  The plots show the projection of the unembedding vectors on the 2D subspaces: $\\mathrm{span}\\{\\bar\\ell_{\\ConceptValue{animal}}, \\bar\\ell_{\\ConceptValue{mammal}}\\}$ (left; statement~\\ref{item:left}), $\\mathrm{span}\\{\\bar\\ell_{\\ConceptValue{animal}},\\bar\\ell_{\\ConceptValue{bird}} - \\bar\\ell_{\\ConceptValue{mammal}}\\}$ (middle; statement~\\ref{item:middle}), and $\\mathrm{span}\\{\\bar\\ell_{\\ConceptValue{animal}} - \\bar\\ell_{\\ConceptValue{plant}}, \\bar\\ell_{\\ConceptValue{bird}} - \\bar\\ell_{\\ConceptValue{mammal}}\\}$ (right; statement~\\ref{item:right}).\n  The gray points indicate all 256K tokens in the vocabulary, and the colored points are the tokens in $\\yquad(\\ConceptValue{w})$.\n  The blue and red vectors are used to span the 2D subspaces.}\n  \\label{fig:three_2d_plots}\n\\end{figure}\nThe power of having a vector representation is that now we can use ordinary vector space operations to construct representation of other concepts.\nWe now turn to the representation of categorical concepts, e.g., $\\{\\ConceptValue{mammal}, \\ConceptValue{reptile}, \\ConceptValue{bird}, \\ConceptValue{fish}\\}.$\nThere is now a straightforward way to define the representation of such concepts:\n\\begin{definition}\n  The \\emph{polytope representation} of a categorical concept $Z = \\{z_0, \\dots, z_{k-1}\\}$ is the convex hull of the vector representations of the elements of the concept.\n\\end{definition}\n\nPolytopes are quite general objects. The definition here also includes representations of categorical variables that are semantically unnatural, e.g., $\\{\\ConceptValue{dog}, \\ConceptValue{sandwich}, \\ConceptValue{running}\\}$. We would like to make a more precise statement about the representation of ``natural'' concepts. One possible notion of a ``natural'' concept is one where the model can freely manipulate the output values. The next theorem shows such concepts have particularly simple structure: \n\n\n\\paragraph*{Summary} Together, \\cref{thm:orthogonality,thm:simplex} give the simple structure illustrated in \\cref{fig:diagram}: hierarchical concepts are represented as direct sums of simplices. The direct sum structure is immediate from the orthogonality in \\cref{thm:orthogonality}.\n\n\n\n\\section{Experiments}\\label{sec:experiments}\nWe now turn to empirically testing the theoretical results in the representation space of the Gemma-2B large language model \\Citep[][]{team2024gemma}.\\footnote{Code is available at \\href{https://github.com/KihoPark/LLM_Categorical_Hierarchical_Representations}{github.com/KihoPark/LLM\\_Categorical\\_Hierarchical\\_Representations}.}\n\n\\subsection{Setup}\n\\paragraph*{Canonical representation space}\nThe results in this paper rely on transforming the representation space so that the Euclidean inner product is a causal inner product, aligning the embedding and unembedding representations. Following \\citet{park2024linear}, we estimate the required transformation as:\n\\begin{equation}\n  g(y) = \\cov(\\gamma)^{-1/2}(\\gamma(y) - \\EE[\\gamma])\n\\end{equation}\nwhere $\\gamma$ is the unembedding vector of a word sampled uniformly from the vocabulary.\nCentering by $\\EE[\\gamma]$ is a reasonable approximation of centering by $\\bar{\\gamma}_0$ defined in \\cref{thm:magnitude} because this makes the projection of a random $g(y)$ on an arbitrary direction close to $0$. This matches the requirement that the projection of a word onto a concept the words does not belong to should be close to $0$.\n\n\\paragraph*{WordNet}\nWe define a large collection of binary concepts using WordNet \\Citep[][]{miller1995wordnet}.\nBriefly, WordNet organizes English words into a hierarchy of synsets, where each synset is a set of synonyms. The WordNet hierarchy is based on word hyponym relations, and reflects the semantic hierarchy of interest in this paper. We take each synset as an attribute $w$ and define $\\yquad(w)$ as the collection of all words belonging to any synset that is a descendant of $w$. For example, the synset \\texttt{mammal.n.01} is a descendant of \\texttt{animal.n.01}, so both $\\mathcal{Y}(\\texttt{mammal.n.01})$ and $\\mathcal{Y}(\\texttt{animal.n.01})$ contain the word ``dog''. We collect all noun and verb synsets, and augment the word collections by including plural forms of the nouns, multiple tenses of each verb, and capital and lower case versions of each word. We filter to include only those synsets with at least 50 words in the Gemma vocabulary. This leaves us with 593 noun and 364 verb synsets, each defining an attribute.\n\n\\paragraph{Estimation via Linear Discriminant Analysis}\nNow, we want to estimate the vector representation $\\bar\\ell_w$ for each attribute $w$.\nTo do this, we make use of vocabulary sets $\\yquad(w)$.\nFollowing \\cref{thm:magnitude}, the vector associated to the concept $w$ should have two properties. First, when the full vocabulary is projected onto this vector, the words in $\\yquad(w)$ should be well-separated from the rest of the vocabulary. Second, the projection of the unembedding vectors for $y\\in \\yquad(w)$ should be approximately the same value. Equivalently, the variance of the projection of the unembedding vectors for $y\\in \\yquad(w)$ should be small.\nTo capture these requirements, we estimate the directions using a variant of Linear Discriminant Analysis (LDA),\nwhich finds a projection minimizing within-class variance and maximizing between-class variance.\nFormally, we estimate the vector representation of a binary feature $W$ for an attribute $w$ as\n\\begin{equation}\\label{eq:estimated_vector}\n  \\bar\\ell_w =   \\left(\\tilde{g}_w^\\top\\EE(g_w)\\right)\\tilde{g}_w, \\quad \\text{with}\\quad \\tilde{g}_w= \\frac{\\cov(g_w)^{\\dagger} \\EE(g_w)}{\\|\\cov(g_w)^{\\dagger} \\EE(g_w)\\|_2},\n\\end{equation}\nwhere $g_w$ is the unembedding vector of a word sampled uniformly from $\\yquad(w)$ and $\\cov(g_w)^{\\dagger}$ is a pseudo-inverse of the covariance matrix. We estimate the covariance matrix $\\cov(g_w)$ using the Ledoit-Wolf shrinkage estimator \\Citep[][]{ledoit2004well}, because the dimension of the representation spaces is much higher than the number of samples.\n\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[trim={1cm 1cm 0.5cm 2cm}, clip, width=1.0\\linewidth]{figures/two_3D_plots.png}\n  \\caption{Categorical concepts are represented as simplices.\n  The plots show the projection of the unembedding vectors on the 3D subspaces: $\\mathrm{span}\\{\\bar\\ell_{\\ConceptValue{mammal}}, \\bar\\ell_{\\ConceptValue{bird}},\\bar\\ell_{\\ConceptValue{fish}}\\}$ (left) and $\\mathrm{span}\\{\\bar\\ell_{\\ConceptValue{bird}} - \\bar\\ell_{\\ConceptValue{mammal}}, \\bar\\ell_{\\ConceptValue{fish}} - \\bar\\ell_{\\ConceptValue{mammal}}, \\bar\\ell_{\\ConceptValue{reptile}} - \\bar\\ell_{\\ConceptValue{mammal}} \\}$ (right).\n  The gray points indicate all 256K tokens in the vocabulary, and the colored points are the tokens in $\\yquad(\\ConceptValue{w})$.\n  The left plot further shows the orthogonality between the triangle and the projection of $\\bar\\ell_{\\ConceptValue{animal}}$ (black arrow).}\n  \\label{fig:two_3D_plots}\n\\end{figure}\n\\subsection{Visualization of \\ConceptName{animal}}\\label{sec:visualization}\nAs a concrete example, we check the theoretical predictions for the concept \\ConceptName{animal}.\nFor this, we generated two sets of tokens $\\yquad(\\ConceptValue{animal})$ and $\\yquad(\\ConceptValue{plant})$ using ChatGPT-4 \\Citep{openai2023gpt4} and manually inspected them.\n$\\yquad(\\ConceptValue{animal})$ is separated to six sets of tokens for each subcategory $\\{\\ConceptValue{mammal}, \\ConceptValue{bird},\\ConceptValue{fish},\\ConceptValue{reptile},\\ConceptValue{amphibian},\\ConceptValue{insect}\\}$.\n\n\\Cref{fig:three_2d_plots} illustrates the geometric relationships between various representation vectors. The main takeaway is that the semantic hierarchy is encoded as orthogonality in the manner predicted by \\cref{thm:orthogonality}. The figure also illustrates \\cref{thm:magnitude}, showing that the projection of the unembedding vectors for $y\\in \\yquad(w)$ is approximately constant, while the projection of $y\\not \\in \\yquad(w)$ is zero.\n\n\\Cref{fig:two_3D_plots} illustrates that the representation of a categorical concept is a simplex, as predicted in \\cref{thm:simplex}. It also shows that, as predicted, the simplex for $\\ConceptValue{fish}, \\ConceptValue{mammal}, \\ConceptValue{bird}$ is orthogonal to the vector representation of \\ConceptName{animal}.\n\n\n\n\n\\subsection{WordNet Hierarchy}\nWe now turn to using the WordNet hierarchy to evaluate the theoretical predictions at scale.\nWe report the noun hierarchy here and defer the verb hierarchy to \\Cref{sec:additional_results}.\n\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=0.95\\linewidth]{figures/noun_evaluation.pdf}\n  \\caption{Linear representations exist for most binary features in the WordNet noun hierarchy.  \n  Comparison of projection of test and random words on estimated vector representations for each WordNet feature.\n  The values are divided by the norm of the estimated vector representation. The $x$-axis indices denote all features in the noun hierarchy.\n  The thick lines present the mean of the projections for each feature and the error bars indicate the 1.96 $\\times$ standard error.}\n  \\label{fig:noun_evaluation}\n\\end{figure}\n\n\\paragraph{Existence of Vector Representations for Binary Features}\nTo evaluate whether vector representations exist, for each synset $w$ we split $\\yquad(w)$ into train words (80\\\n\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=1.0\\linewidth]{figures/noun_single_three_heatmap.pdf}\n  \\caption{Hierarchical semantics in WordNet are encoded in Gemma representation space, with the orthogonal structure predicted in \\cref{thm:orthogonality}.    \n  The adjacency matrix of the hierarchical relations between features in the noun hierarchy (left), the cosine similarity between the vector representations $\\bar\\ell_w$ for each feature (middle), and the cosine similarity between child-parent vectors $\\bar\\ell_w - \\bar\\ell_{\\text{parent of }w}$ for each feature (right).\n  The features are ordered by the hierarchy.\n  }\n  \\label{fig:three_heatmap}\n\\end{figure}\n\n\\paragraph{Hierarchical Orthogonality}\nIt remains to evaluate the prediction that hierarchical relations are encoded as orthogonality in the representation space.\n\\Cref{fig:three_heatmap} shows the adjacency matrix of the WordNet noun hyponym inclusion graph (left), the cosine similarity between the vector representations $\\bar\\ell_w$ for each feature (middle), and the cosine similarity between child-parent vectors $\\bar\\ell_w - \\bar\\ell_{\\text{parent of }w}$ for each feature (right). Strikingly, the cosine similarity clearly reflects the semantic hierarchy---the adjacency matrix is clearly visible in the middle heatmap. This is because, e.g., \\ConceptName{mammal.n.01} and \\ConceptName{animal.n.01} have high cosine similarity. By contrast, as predicted by \\cref{thm:orthogonality}, the child-parent and parent-grandparent vectors are orthogonal. This also straightforwardly implies all other theoretical connections between orthogonality and semantic hierarchy.\n\nIn \\Cref{sec:additional_results}, we present zoomed-in heatmaps for the subtree of descendants of ``animal'', and the results for the verb hierarchy.\n\n\n\n\\section{Discussion and Related Work}\\label{sec:discussion_relatedwork}\nWe set out to understand how semantic structure is encoded in the geometry of representation space.\nWe have arrived an astonishingly simple structure, summarized in \\Cref{fig:diagram}. \nThe key contributions are moving from representing concepts as directions to representing them as vectors (and polytopes), and connecting semantic hierarchy to orthogonality. \n\n\\paragraph{Related work}\nThe results here connect closely to the study of linear representations in language models\n \\citep[e.g.,][]{mikolov2013linguistic,pennington2014glove,arora2016latent, elhage2022toy, burns2022discovering, tigges2023linear, nanda2023emergent, moschella2022relative, li2023inference, gurnee2023finding, wang2023concept, jiang2024origins,park2024linear}.\n In particular, \\citet{park2024linear} formalize the linear representation hypothesis by unifying three distinct notions of linearity: word2vec-like embedding differences, logistic probing, and steering vectors. Our work relies on this unification, and just focuses on the steering vector notion. \nOur work also connects to work aimed at theoretically understanding the existence of linear representations. Specifically, \\cite{arora2016latent, arora2018linear, frandsen2019understanding} use RAND-WALK model where the latent vectors are modeled to drift on the unit sphere. \n\\cite{blei2006dynamic, rudolph2016exponential, rudolph2017dynamic} consider a similar dynamic topic modeling. \\citet{Gittens2017SkipGramZ} and subsequent works \\citep{allen2019analogies,allen2019vec} propose a paraphrasing model where a subset of words is semantically equivalent to a single word. \n\\citet{ethayarajh2018towards} try to explain linear representations by decomposing the pointwise mutual information matrix while \\citet{ri2023contrastive} connect it to contrastive loss. \\Citet{jiang2024origins} connect the existence of linear representations to the implicit bias of gradient descent. In this paper, we do not seek to justify the \\emph{existence} of linear representations, but rather to understand their \\emph{structure} if they do exist. \nThough, by empirically estimating vector representations for thousands of concepts, we add to the body of evidence supporting the existence of linear representations. \\citet{elhage2022toy} also empirically observe the formation of polytopes in the representation space of a toy model, and the present work can be viewed in part as giving an explanation for this phenomenon.\n\nThere is also a growing literature studying the representation geometry of natural language \\citep{Mimno2017TheSG, reif2019visualizing, volpi2021natural, Volpi2020EvaluatingNA, li2020sentence, chen2021probing, chang2022geometry, liang2022mind, jiang2023uncovering,wang2023concept, park2024linear, valeriani2024geometry}. \nMuch of this work focuses on connections to hyperbolic geometry \\citep[][]{nickel2017poincare, ganea2018hyperbolic, chen2021probing, he2024language}. We do not find such a connection in existing LLMs, but it is an interesting direction for future work to determine if more efficient LLM representations could be constructed in hyperbolic space. \\Citet{jiang2023uncovering} hypothesize that very general  \"independence structures\" are naturally represented by partial orthogonality in vector spaces \\citep{amini2022non}. The results here confirm and expand on this hypothesis in the case of hierarchical structure in language models.\n\n\\paragraph{Implications and Future Work}\nThe results in this paper are foundational for understanding the structure of representation space in language models.\nOf course, the ultimate purpose of foundations is to build upon them.\nOne immediate direction is to refine the attempts to interpret LLM structure to explicitly account for hierarchical semantics. As a concrete example, there is currently significant interest in using sparse autoencoders to extract interpretable features from LLMs \\Citep[e.g.,][]{cunningham2023sparse, bricken2023monosemanticity, attention_saes, braun2024identifying}.\nThis work searches for representations in terms of distinct binary features. \nConcretely, it hopes to find features for, e.g., \\ConceptName{animal}, \\ConceptName{mammal}, \\ConceptName{bird}, etc. Based on the results here, these representations are strongly co-linear, and potentially difficult to disentangle. \nOn the other hand, a representation in terms of $\\bar\\ell_{\\ConceptValue{animal}}$, $\\bar\\ell_{\\ConceptValue{mammal}} - \\bar\\ell_\\ConceptValue{animal}$, $\\bar\\ell_{\\ConceptValue{bird}} - \\bar\\ell_\\ConceptValue{animal}$, etc., will be cleanly separated and equally interpretable. Fundamentally, semantic meaning has hierarchical structure, so interpretability methods should respect this structure. Understanding the geometric representation makes it possible to design such methods.\n\nIn a separate, foundational, direction: the results in this paper rely on using the canonical representation space. We estimate this using the whitening transformation of the unembedding layer. However, this technique only works for the final layer representation. It is an important open question how to make sense of the geometry of internal layers.\n\n\n\n\n\\clearpage\n\\printbibliography\n\n\n\n\n\\clearpage\n\\appendix\n\n\n\n\\section{Proofs}\\label{sec:proofs}\n\\subsection{Proof of \\Cref{thm:magnitude}}\n\\begin{theorem}[Magnitudes of Linear Representations]\n\\label{thm:magnitude}\n  Suppose there exists a linear representation (normalized direction) $\\bar\\ell_W$ of a binary feature $W$ for an attribute $w$.  \n  Then, there is a constant $b_w>0$ and a choice of unembedding space origin $\\bar{\\gamma}_0^w$ in \\cref{eq:transformation} such that\n  \\begin{equation}\\label{eq:magnitude}\n    \\begin{cases}\n      \\bar\\ell_W^\\top g(y) = b_w & \\text{if } y\\in \\yquad(w)\\\\\n      \\bar\\ell_W^\\top g(y) = 0 & \\text{if } y \\not\\in \\yquad(w).\n    \\end{cases}\n  \\end{equation}\n  Further, if there are $d$ causally separable attributes $\\{w_0, \\dots, w_{d-1}\\}$ with linear representations, we can choose a canonical origin $\\bar\\gamma_0$ in \\cref{eq:transformation} as\n  \\begin{equation}\n    \\bar\\gamma_0 = \\sum_i \\bar{\\gamma}_0^{w_i}.\n  \\end{equation}\n\n\\end{theorem}\n\\begin{proof}\n  For any $y_1, y_0 \\in \\yquad(w)$ or $y_1, y_0 \\not\\in \\yquad(w)$, let $Z$ be a binary concept where $\\yquad(Z = 0) = \\{y_0\\}$ and $\\yquad(Z = 1) = \\{y_1\\}$.\n  Since $Z$ is subordinate to $W$, \\cref{eq:linear_cond2} implies that\n  \\begin{align}\n    & \\logit \\Pr(Y = y_1 \\given Y \\in \\{y_0, y_1\\} , \\ell +  \\bar\\ell_W )= \\logit \\Pr(Y = y_1 \\given Y \\in \\{y_0, y_1\\} , \\ell)\\\\\n    & \\iff  \\bar\\ell_W ^\\top  (g(y_1) - g(y_0))=\\bar\\ell_W ^\\top  A(\\gamma(y_1) - \\gamma(y_0)) = 0\n  \\end{align}\n  where $A$ is the invertible matrix in \\cref{eq:transformation}.\n  This means that $\\bar\\ell_W^\\top A \\gamma(y)$ is the same for all $y\\in \\yquad(w)$, and it is also the same for all $y\\not\\in \\yquad(w)$.\n\n  Furthermore, for any $y_1 \\in \\yquad(w)$ and $y_0 \\not\\in \\yquad(w)$, \\cref{eq:linear_cond1} implies that\n  \\begin{align}\n    & \\logit \\Pr(Y = y_1 \\given Y \\in \\{y_0, y_1\\} , \\ell + \\bar\\ell_W )> \\logit \\Pr(Y = y_1 \\given Y \\in \\{y_0, y_1\\} , \\ell)\\\\\n    & \\iff  \\bar\\ell_W ^\\top  (g(y_1) - g(y_0)) =  \\bar\\ell_W ^\\top  A(\\gamma(y_1) - \\gamma(y_0))> 0.\n  \\end{align}\n\n  Thus, by setting $b_w^0 = \\bar\\ell_W^\\top A \\gamma(y)$ for any $y \\not\\in \\yquad(w)$, and $b_w = \\bar\\ell_W^\\top A \\gamma(y_1) - \\bar\\ell_W^\\top A \\gamma(y_0) > 0$ for any $y_1 \\in \\yquad(w)$ and $y_0 \\not\\in \\yquad(w)$, we get\n  \\begin{equation}\\label{eq:feature_vector}\n    \\begin{cases}\n      \\bar\\ell_W^\\top A \\gamma(y) = b_w^0 + b_w & \\text{if } y\\in \\yquad(w)\\\\\n      \\bar\\ell_W^\\top A \\gamma(y) = b_w^0 & \\text{if } y \\not\\in \\yquad(w).\n    \\end{cases}\n  \\end{equation}\n  Then, we can choose an origin as\n  \\begin{equation}\n    \\bar\\gamma_0^w = b_w^0 A^{-1} \\bar\\ell_W\n  \\end{equation}\n  satisfying \\cref{eq:magnitude}.\n\n  On the other hand, if there exist $\\bar\\ell_W$ and $\\bar\\ell_Z$ for causally separable attributes $w$ and $z$, then $\\bar\\ell_W$ and $\\bar\\ell_Z$ are orthogonal by the property of the causal inner product.\n  If they are not orthogonal, adding $\\bar\\ell_Z$ can change the other concept $W$, and it is a contradiction.\n  Now if there exist the linear representation for $d$ binary features for causally separable attributes $\\{w_0, \\dots, w_{d-1}\\}$, we can choose a canonical $\\bar\\gamma_0$ in \\cref{eq:transformation} as\n  \\begin{equation}\n    \\bar\\gamma_0 = \\sum_i \\bar{\\gamma}_0^{w_i}.\n  \\end{equation}\n  with \\cref{eq:magnitude} satisfied.\n\\end{proof}\n\n\n\n\\subsection{Proof of \\Cref{thm:orthogonality}}\n\\begin{theorem}[Hierarchical Orthogonality]\n\\label{thm:orthogonality}\n  Suppose there exist the vector representations for all the following binary features.\n  Then, we have that\n  \\begin{enumerate}[label=(\\alph*)]\n    \\item $\\bar\\ell_{w_1} - \\bar\\ell_{w_0}$ is a linear representation $\\bar{\\ell}_{\\ConceptDirMath{w_0}{w_1}}$ defined in \\cref{def:linear_representation}; \\label{item:a}\n    \\item $\\bar\\ell_{w} \\perp \\bar\\ell_{z} - \\bar\\ell_w$ for $z \\prec w$; \\label{item:left}\n    \\item $\\bar{\\ell}_{w} \\perp \\bar\\ell_{z_1} - \\bar\\ell_{z_0}$ for $Z \\in_R \\{z_0, z_1\\}$ subordinate to $W \\in_R \\{\\ConceptValue{not\\_w}, \\ConceptValue{is\\_w}\\}$; \\label{item:middle}\n    \\item $\\bar{\\ell}_{w_1} - \\bar{\\ell}_{w_0}  \\perp \\bar\\ell_{z_1} - \\bar\\ell_{z_0}$ for $Z \\in_R \\{z_0, z_1\\}$ subordinate to $W \\in_R \\{w_0, w_1\\}$; and \\label{item:right}\n    \\item $\\bar\\ell_{w_1}-\\bar\\ell_{w_0} \\perp \\bar\\ell_{w_2} - \\bar\\ell_{w_1}$ for $w_2 \\prec w_1 \\prec w_0$.\n  \\end{enumerate}\n\n\\end{theorem}\n\\begin{proof}\n  \\begin{enumerate}[label=(\\alph*)]\n    \\item For $\\bar\\ell_{w_1}$ and $\\bar\\ell_{w_0}$, by \\Cref{thm:magnitude}, we have\n    \\begin{equation}\n      \\begin{cases}\n        (\\bar\\ell_{w_1} - \\bar\\ell_{w_0})^\\top g(y) = 0-b_{w_0} = -b_{w_0} & \\text{if } y\\in \\yquad(w_0)\\\\\n        (\\bar\\ell_{w_1} - \\bar\\ell_{w_0})^\\top g(y) = b_{w_1} - 0 = b_{w_1} & \\text{if } y \\in \\yquad(w_1)\\\\\n        (\\bar\\ell_{w_1} - \\bar\\ell_{w_0})^\\top g(y) = 0 - 0 = 0 & \\text{if } y\\not\\in \\yquad(w_0)\\cup \\yquad(w_1).\n      \\end{cases}\n    \\end{equation}\n    Since $\\bar\\ell_{w_1} - \\bar\\ell_{w_0}$ can change the target concept $\\ConceptDirMath{w_0}{w_1}$ without changing any other concept subordinate or causally separable to the target concept, $\\bar\\ell_{w_1} - \\bar\\ell_{w_0}$ is the linear representation $\\bar{\\ell}_{w_0\\Rightarrow w_1}$.\n\n    \\item For $\\bar\\ell_{w}$ and $\\bar\\ell_{z}$ where $z \\prec w$, by \\Cref{thm:magnitude}, we have\n    \\begin{equation}\n      \\begin{cases}\n        (\\bar\\ell_{z} - \\bar\\ell_{w})^\\top g(y) = b_z - b_w & \\text{if } y\\in \\yquad(z)\\\\\n        (\\bar\\ell_{z} - \\bar\\ell_{w})^\\top g(y) =  0 -b_w =  -b_w & \\text{if } y \\in \\yquad(w)\\setminus \\yquad(z)\\\\\n        (\\bar\\ell_{z} - \\bar\\ell_{w})^\\top g(y) = 0 - 0 = 0 & \\text{if } y\\not\\in \\yquad(w).\n      \\end{cases}\n    \\end{equation}\n    When $w\\setminus z$ denotes an attribute defined by $\\yquad(w)\\setminus \\yquad(z)$, $\\bar\\ell_{z} - \\bar\\ell_{w}$ can change the target concept $\\ConceptDirMath{w\\setminus z}{z}$ without changing any other concept subordinate or causally separable to the target concept.\n    Thus, $\\bar\\ell_{z} - \\bar\\ell_{w}$ is the linear representation $\\bar{\\ell}_{w\\setminus z \\Rightarrow z}$.\n    This concept means $\\ConceptValue{not\\_z}  \\Rightarrow \\ConceptValue{is\\_z}$ conditioned on $w$, and hence it is subordinate to $w$.\n    \n    Therefore, $\\bar\\ell_w$ is orthogonal to the linear representation $\\bar{\\ell}_{w\\setminus z \\Rightarrow z} = \\bar\\ell_z - \\bar\\ell_w$ by the property of the causal inner product.\n    If they are not orthogonal, adding $\\bar\\ell_w$ can change the other concept $\\ConceptDirMath{w\\setminus z}{z}$, and it is a contradiction.\n\n    \\item By the above result \\ref{item:left}, $\\bar\\ell_w^\\top (\\bar\\ell_{z_1} - \\bar\\ell_w) = \\bar\\ell_w^\\top (\\bar\\ell_{z_0} - \\bar\\ell_w) = 0$.\n    Therefore, $\\bar\\ell_w^\\top (\\bar\\ell_{z_1} - \\bar\\ell_{z_0}) = 0$.\n\n    \\item Let's say that $w_1$ is $w_Z$ defined in \\Cref{def:hierarchical_relation}.\n    The binary contrast $\\ConceptDirMath{z_0}{z_1}$ is subordinate to the binary feature for the attribute $w_0$.\n    By the property of the causal inner product, $\\bar\\ell_{w_0}$ is orthogonal to the linear representation $\\bar{\\ell}_{z_0\\Rightarrow z_1} = \\bar\\ell_{z_1} - \\bar\\ell_{z_0}$ (by \\ref{item:a}).\n    Then, with the above result \\ref{item:middle}, we have $(\\bar\\ell_{w_1} - \\bar\\ell_{w_0})^\\top (\\bar\\ell_{z_1} - \\bar\\ell_{z_0})$.\n\n    \\item By the above result \\ref{item:left}, we have\n    \\begin{equation}\n      \\begin{cases}\n        \\|\\bar\\ell_{w_1} - \\bar\\ell_{w_0}\\|_2^2 = \\|\\bar\\ell_{w_1}\\|_2^2  - \\|\\bar\\ell_{w_0}\\|_2^2\\\\\n        \\|\\bar\\ell_{w_2} - \\bar\\ell_{w_1}\\|_2^2 = \\|\\bar\\ell_{w_2}\\|_2^2  - \\|\\bar\\ell_{w_1}\\|_2^2\\\\\n        \\|\\bar\\ell_{w_2} - \\bar\\ell_{w_0}\\|_2^2 = \\|\\bar\\ell_{w_2}\\|_2^2  - \\|\\bar\\ell_{w_0}\\|_2^2.\n      \\end{cases}\n    \\end{equation}\n    Then,\n    \\begin{align}\n      &\\|\\bar\\ell_{w_1} - \\bar\\ell_{w_0}\\|_2^2 + \\|\\bar\\ell_{w_2} - \\bar\\ell_{w_1}\\|_2^2\\\\\n      &= \\|\\bar\\ell_{w_1}\\|_2^2  - \\|\\bar\\ell_{w_0}\\|_2^2 + \\|\\bar\\ell_{w_2}\\|_2^2  - \\|\\bar\\ell_{w_1}\\|_2^2\\\\\n      &= \\|\\bar\\ell_{w_2}\\|_2^2  - \\|\\bar\\ell_{w_0}\\|_2^2\\\\\n      & = \\|\\bar\\ell_{w_2} - \\bar\\ell_{w_0}\\|_2^2.\n    \\end{align}\n    Therefore, $\\bar\\ell_{w_1} - \\bar\\ell_{w_0}$ is orthogonal to $\\bar\\ell_{w_2} - \\bar\\ell_{w_1}$.\n  \\end{enumerate}\n\\end{proof}\n\n\n\\subsection{Proof of \\Cref{thm:simplex}}\n\\begin{theorem}[Categorical Concepts are Represented as Simplices]\n\\label{thm:simplex}\n  Suppose that $\\{w_0, \\dots, w_{k-1}\\}$ is a collection of $k$ mutually exclusive attributes such that for every joint distribution $Q(w_0, \\dots w_{k-1})$ there is some $\\ell_{i}$ such that $\\Pr(W = w_i \\given \\ell_{i}) = Q(W=w_i)$ for every $i$.\n  Then, the vector representations $\\bar\\ell_{w_0}, \\dots, \\bar\\ell_{w_{k-1}}$ form a $(k-1)$-simplex in the representation space. In this case, we take the simplex to be the representation of the categorical concept $W = \\{w_0, \\dots, w_{k-1}\\}$.\n\n\\end{theorem}\n\\begin{proof}\n  If we can represent arbitrary joint distributions, this means, in particular, that we can change the probability of one attribute without changing the relative probability between a pair of other attributes.\n  Consider the case where $k=3$, as illustrated in \\Cref{fig:proof}.\n  If $\\bar\\ell_{w_0}, \\bar\\ell_{w_1}, \\bar\\ell_{w_2}$ are on a line, then there is no direction in that line (to change the value in the categorical concept) such that adding the direction can change the probability of $w_2$ without changing the relative probabilities between $w_0$ and $w_1$.\n  However, if $\\bar\\ell_{w_0}, \\bar\\ell_{w_1}, \\bar\\ell_{w_2}$ are not on a line, they form a triangle.\n  Then, there exists a line that is toward $\\bar\\ell_{w_2}$ and perpendicular to the opposite side of the triangle.\n  Now adding the direction $\\tilde{\\ell}$ can manipulate the probability of $w_2$ without changing the relative probabilities between $w_0$ and $w_1$.\n  That is, for any $\\alpha > 0$ and context embedding $\\ell$,\n  \\begin{equation}\n    \\begin{cases}\n      \\Pr(W = w_2 \\given \\ell + \\alpha \\tilde{\\ell}) > \\Pr(W = w_2 \\given \\ell), \\text{ and}\\\\\n      \\frac{\\Pr(W = w_1 \\given \\ell + \\alpha \\tilde{\\ell})}{\\Pr(W = w_0 \\given \\ell + \\alpha \\tilde{\\ell})}  = \\frac{\\Pr(W = w_1 \\given \\ell )}{\\Pr(W = w_0 \\given \\ell)}.\n    \\end{cases}\n  \\end{equation}\n  Therefore, the vectors $\\bar\\ell_{w_0}, \\bar\\ell_{w_1}, \\bar\\ell_{w_2}$ form a 2-simplex.\n  \n  This argument extends immediately to higher $k$ by induction.\n  For each $i \\in \\{0, \\dots, k-1\\}$, there should exist a direction that is toward $\\bar\\ell_{w_i}$ and orthogonal to the opposite hyperplane ($(k-2)$-simplex) formed by the other $\\bar\\ell_{w_{i'}}$'s.\n  Then, the vectors $\\bar\\ell_{w_0}, \\dots, \\bar\\ell_{w_{k-1}}$ form a $(k-1)$-simplex.\n\\end{proof}\n\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[trim={4cm 8cm 3cm 5cm}, clip, width=0.8\\linewidth]{figures/Theorem.pdf}\n  \\caption{Illustration of the case $k=3$ in the proof of \\Cref{thm:simplex}.}\n  \\label{fig:proof}\n\\end{figure}\n\n\n\n\\section{Experiment Details}\\label{sec:experiment_details}\nWe employ the \\texttt{Gemma-2B} version of the Gemma model \\Citep[][]{team2024gemma}, which is accessible online via the \\texttt{huggingface} library.\nIts two billion parameters are pre-trained on three trillion tokens.\nThis model utilizes 256K tokens and 2,048 dimensions for the representation space.\n\nWe always use tokens that start with a space (`$\\backslash$u2581') in front of the word, as they are used for next-word generation with full meaning.\nAdditionally, like WordNet data we use, we include plural forms, and both capital and lowercase versions of the words in $\\yquad(\\ConceptValue{animal})$ and $\\yquad(\\ConceptValue{plant})$ for visualization in \\Cref{sec:visualization}.\n\nIn the WordNet synset data, each content of the synset \\texttt{mammal.n.01} indicates that \"mammal\" is a word, \"n\" denotes \"noun,\" and \"01\" signifies the first meaning of the word.\nIn the WordNet hierarchy, if a parent has only one child, we combine the two features into one.\nAdditionally, since the WordNet hierarchy is not a perfect tree, a child can have more than one parent.\nWe use one of the parents when computing the $\\bar\\ell_{w} - \\bar\\ell_{\\text{parent of }w}$.\n\n\n\\section{Additional Results}\\label{sec:additional_results}\n\\subsection{Zooming in on a Subtree of Noun Hierarchy}\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=1.0\\linewidth]{figures/sub_graph_tree.pdf}\n  \\caption{Subtree in WordNet noun hierarchy for descendants of \\ConceptValue{animal}.\n  }\n  \\label{fig:sub_tree}\n\\end{figure}\n\n\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=1.0\\linewidth]{figures/sub_tree_three_heatmap_noun_single.pdf}\n  \\caption{Zoomed-in Heatmaps of the subtree for \\ConceptValue{animal} in \\Cref{fig:sub_tree}.\n  }\n  \\label{fig:sub_tree_three_heatmap}\n\\end{figure}\n\nAs it is difficult to understand the entire WordNet hierarchy at once from the heatmaps in \\Cref{fig:three_heatmap}, we present a zoomed-in heatmap for the subtree (\\Cref{fig:sub_tree}) for the feature \\ConceptValue{animal} in \\Cref{fig:sub_tree_three_heatmap}.\nThe left heatmap displays the adjacency matrix of the hierarchical relations between features, aligned with the subtree in \\Cref{fig:sub_tree}.\nThe middle heatmap shows that the cosine similarities between the vector representations $\\bar\\ell_w$ correspond to the adjacency matrix.\nThe final heatmap demonstrates that the child-parent vector $\\bar\\ell_w - \\bar\\ell_{\\text{parent of }w}$ and $\\bar\\ell_{\\text{parent of }w} - \\bar\\ell_{\\text{grandparent of }w}$ are orthogonal, as predicted in \\Cref{thm:orthogonality}.\n\n\n\\subsection{WordNet Verb Hierarchy}\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=0.95\\linewidth]{figures/verb_evaluation.pdf}\n  \\caption{Linear representations exist for most binary features in the WordNet verb hierarchy.  \n  Comparison of projection of test and random words on estimated vector representations for each feature.\n  The values are divided by the norm of the estimated vector representation. The $x$-axis indices denote all features in the verb hierarchy.\n  The thick lines present the mean of the projections for each feature and the error bars indicate the 1.96 $\\times$ standard error.}\n  \\label{fig:verb_evaluation}\n\\end{figure}\n\n\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=1.0\\linewidth]{figures/verb_single_three_heatmap.pdf}\n  \\caption{The adjacency matrix of the hierarchical relations between features in the WordNet verb hierarchy (left), the cosine similarity between the vector representations $\\bar\\ell_w$ for each feature (middle), and the cosine similarity between child-parent vectors $\\bar\\ell_w - \\bar\\ell_{\\text{parent of }w}$ for each feature (right).\n  The features are ordered by the hierarchy.\n  }\n  \\label{fig:verb_three_heatmap}\n\\end{figure}\n\nIn the same way as for the noun hierarchy, we estimate the vector representations for the WordNet verb hierarchy.\nTo evaluate whether vector representations exist, we split $\\yquad(w)$ for each synset $w$ into train words (80\\\n\\Cref{fig:verb_evaluation} shows the mean and standard error of the test projections, divided by the magnitude of each estimated $\\bar{\\ell}_w$. If a vector representation exists for an attribute, we would expect these values to be close to 1. This is indeed the case, providing evidence that vector representations do indeed exist for these features.\n\n\\Cref{fig:verb_three_heatmap} displays the adjacency matrix of the WordNet verb hyponym inclusion graph (left), the cosine similarity between the vector representations $\\bar\\ell_w$ for each feature (middle), and the cosine similarity between child-parent vectors $\\bar\\ell_w - \\bar\\ell_{\\text{parent of }w}$ for each feature (right).\nThe cosine similarity clearly reflects the semantic hierarchy---the adjacency matrix is clearly visible in the middle heatmap.\nBy contrast, as predicted by \\cref{thm:orthogonality}, the child-parent and parent-grandparent vectors are orthogonal. This straightforwardly implies all other theoretical connections between orthogonality and the semantic hierarchy.\n\n\n\\end{document}\n\n\n\n\n\n\n==== END OF /2406.01506/your-paper.tex ====\n==== BEGINNING OF /2406.01506/preamble/preamble.tex ====\n\n\n\n\\usepackage[utf8]{inputenc} \n\\usepackage[T1]{fontenc}    \n\n\n\n\\renewcommand*\\ttdefault{lmvtt}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\usepackage[bitstream-charter]{mathdesign}\n\\usepackage{amsmath}\n\\usepackage[scaled=0.92]{PTSans}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\usepackage[\n  paper  = letterpaper,\n  left   = 1.65in,\n  right  = 1.65in,\n  top    = 1.0in,\n  bottom = 1.0in,\n  ]{geometry}\n\n\n\\usepackage[usenames,dvipsnames,table]{xcolor}\n\n\n\\definecolor{shadecolor}{gray}{0.9}\n\n\n\\usepackage[final,expansion=alltext]{microtype}\n\\usepackage[english]{babel}\n\\usepackage[parfill]{parskip}\n\\usepackage{afterpage}\n\\usepackage{framed}\n\n\n\\renewenvironment{leftbar}[1][\\hsize]\n{\n  \\def\\FrameCommand\n  {\n    {\\color{Gray}\\vrule width 3pt}\n    \\hspace{10pt}\n    \n  }\n  \\MakeFramed{\\hsize#1\\advance\\hsize-\\width\\FrameRestore}\n}\n{\\endMakeFramed}\n\n\n\\DeclareRobustCommand{\\parhead}[1]{\\textbf{#1}~}\n\n\n\n\\usepackage{lineno}\n\\renewcommand\\linenumberfont{\\normalfont\n                             \\footnotesize\n                             \\sffamily\n                             \\color{SkyBlue}}\n\n\\usepackage{ragged2e}\n\\DeclareRobustCommand{\\sidenote}[1]{\\marginpar{\n                                    \\RaggedRight\n                                    \\textcolor{Plum}{\\textsf{#1}}}}\n\n\\newcommand{\\parnum}{\\bfseries\\P\\arabic{parcount}}\n\\newcounter{parcount}\n\\newcommand\\p{\n    \\stepcounter{parcount}\n    \\leavevmode\\marginpar[\\hfill\\parnum]{\\parnum}\n}\n\n\\DeclareRobustCommand{\\PP}{\\textcolor{Plum}{\\P} }\n\n\n\\renewcommand{\\labelenumi}{\\color{black!67}{\\arabic{enumi}.}}\n\\renewcommand{\\labelenumii}{{\\color{black!67}(\\alph{enumii})}}\n\\renewcommand{\\labelitemi}{{\\color{black!67}\\textbullet}}\n\n\n\\usepackage{graphicx}\n\\usepackage{wrapfig}\n\\usepackage[labelfont=bf,font=footnotesize,width=.9\\textwidth]{caption}\n\\usepackage[format=hang]{subcaption}\n\n\n\\usepackage{booktabs,multirow,multicol}       \n\n\n\\usepackage[algoruled,algo2e]{algorithm2e}\n\\usepackage{listings}\n\\usepackage{fancyvrb}\n\\fvset{fontsize=\\normalsize}\n\n\n\n\n\n\\usepackage[colorlinks,linktoc=all]{hyperref}\n\\usepackage[all]{hypcap}\n\\hypersetup{citecolor=MidnightBlue}\n\\hypersetup{linkcolor=MidnightBlue}\n\\hypersetup{urlcolor=MidnightBlue}\n\n\n\\usepackage[nameinlink]{cleveref}\n\n\n\\usepackage[acronym,nowarn]{glossaries}\n\n\n\n\\newcommand{\\red}[1]{\\textcolor{BrickRed}{#1}}\n\\newcommand{\\orange}[1]{\\textcolor{BurntOrange}{#1}}\n\\newcommand{\\green}[1]{\\textcolor{OliveGreen}{#1}}\n\\newcommand{\\blue}[1]{\\textcolor{MidnightBlue}{#1}}\n\\newcommand{\\gray}[1]{\\textcolor{black!60}{#1}}\n\n\n\\lstdefinestyle{mystyle}{\n    commentstyle=\\color{OliveGreen},\n    keywordstyle=\\color{BurntOrange},\n    numberstyle=\\tiny\\color{black!60},\n    stringstyle=\\color{MidnightBlue},\n    basicstyle=\\ttfamily,\n    breakatwhitespace=false,\n    breaklines=true,\n    captionpos=b,\n    keepspaces=true,\n    numbers=left,\n    numbersep=5pt,\n    showspaces=false,\n    showstringspaces=false,\n    showtabs=false,\n    tabsize=2\n}\n\\lstset{style=mystyle}\n==== END OF /2406.01506/preamble/preamble.tex ====\n==== BEGINNING OF /2406.01506/preamble/preamble_math.tex ====\n\n\n\n\n\\usepackage{amsthm}\n\n\n\n\\usepackage{centernot}\n\\usepackage{nicefrac}       \n\\usepackage{mathtools}\n\\usepackage{amsbsy}\n\\usepackage{amstext}\n\\usepackage{thmtools}\n\\usepackage{thm-restate}\n\n\n\\begingroup\n    \\makeatletter\n    \\@for\\theoremstyle:=definition,remark,plain\\do{\n        \\expandafter\\g@addto@macro\\csname th@\\theoremstyle\\endcsname{\n            \\addtolength\\thm@preskip\\parskip\n            }\n        }\n\\endgroup\n\n\\DeclareRobustCommand{\\mb}[1]{\\ensuremath{\\mathbf{\\boldsymbol{#1}}}}\n\n\n\\DeclareRobustCommand{\\KL}[2]{\\ensuremath{\\textrm{KL}\\left(#1\\;\\|\\;#2\\right)}}\n\n\n\n\n\\crefname{lemma}{lemma}{lemmas}\n\\Crefname{lemma}{Lemma}{Lemmas}\n\\crefname{thm}{theorem}{theorems}\n\\Crefname{thm}{Theorem}{Theorems}\n\\crefname{prop}{proposition}{propositions}\n\\Crefname{prop}{Proposition}{Propositions}\n\\crefname{assumption}{assumption}{assumptions}\n\\crefname{assumption}{Assumption}{Assumptions}\n\n\n\n\n\n\n\n\n\n\\newcommand\\independent{\\protect\\mathpalette{\\protect\\independenT}{\\perp}}\n\\def\\independenT#1#2{\\mathrel{\\rlap{$#1#2$}\\mkern2mu{#1#2}}}\n\n\\newcommand{\\grad}{\\nabla}\n\n\\renewcommand{\\mid}{~\\vert~}\n\\newcommand{\\prm}{\\:;\\:}\n\n\\newcommand{\\mbw}{\\mb{w}}\n\\newcommand{\\mbW}{\\mb{W}}\n\n\\newcommand{\\mbx}{\\mb{x}}\n\\newcommand{\\mbX}{\\mb{X}}\n\n\\newcommand{\\mby}{\\mb{y}}\n\\newcommand{\\mbY}{\\mb{Y}}\n\n\\newcommand{\\mbz}{\\mb{z}}\n\\newcommand{\\mbZ}{\\mb{Z}}\n\\newcommand{\\mbT}{\\mb{T}}\n\\newcommand{\\mbA}{\\mb{A}}\n\\newcommand{\\mba}{\\mb{a}}\n\n\\newcommand{\\mbI}{\\mb{I}}\n\\newcommand{\\mbone}{\\mb{1}}\n\n\\newcommand{\\mbL}{\\mb{L}}\n\n\\newcommand{\\mbtheta}{\\mb{\\theta}}\n\\newcommand{\\mbTheta}{\\mb{\\Theta}}\n\\newcommand{\\mbomega}{\\mb{\\omega}}\n\\newcommand{\\mbOmega}{\\mb{\\Omega}}\n\\newcommand{\\mbsigma}{\\mb{\\sigma}}\n\\newcommand{\\mbSigma}{\\mb{\\Sigma}}\n\\newcommand{\\mbphi}{\\mb{\\phi}}\n\\newcommand{\\mbPhi}{\\mb{\\Phi}}\n\n\\newcommand{\\mbalpha}{\\mb{\\alpha}}\n\\newcommand{\\mbbeta}{\\mb{\\beta}}\n\\newcommand{\\mbgamma}{\\mb{\\gamma}}\n\\newcommand{\\mbeta}{\\mb{\\eta}}\n\\newcommand{\\mbmu}{\\mb{\\mu}}\n\\newcommand{\\mbrho}{\\mb{\\rho}}\n\\newcommand{\\mblambda}{\\mb{\\lambda}}\n\\newcommand{\\mbzeta}{\\mb{\\zeta}}\n\n\\newcommand\\dif{\\mathop{}\\!\\mathrm{d}}\n\\newcommand{\\diag}{\\textrm{diag}}\n\\newcommand{\\supp}{\\textrm{supp}}\n\n\\newcommand{\\V}{\\mathbb{V}}\n\\newcommand{\\bbH}{\\mathbb{H}}\n\n\\newcommand{\\bbN}{\\mathbb{N}}\n\\newcommand{\\bbZ}{\\mathbb{Z}}\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\bbS}{\\mathbb{S}}\n\n\\newcommand{\\cL}{\\mathcal{L}}\n\\newcommand{\\cD}{\\mathcal{D}}\n\n\\newcommand{\\cN}{\\mathcal{N}}\n\\newcommand{\\cT}{\\mathcal{T}}\n\\newcommand{\\Gam}{\\textrm{Gam}}\n\\newcommand{\\InvGam}{\\textrm{InvGam}}\n\n\n\n\\newcommand{\\g}{\\, | \\,}\n\\newcommand{\\s}{\\, ; \\,}\n\n\\newcommand{\\indpt}{\\protect\\mathpalette{\\protect\\independenT}{\\perp}}\n\\newcommand{\\E}[2]{\\mathbb{E}_{#1}\\left[#2\\right]}\n\n\\def\\checkmark{\\tikz\\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} \n\n\n\\usepackage{booktabs,arydshln}\n\\makeatletter\n\\def\\adl@drawiv#1#2#3{\n        \\hskip.5\\tabcolsep\n        \\xleaders#3{#2.5\\@tempdimb #1{1}#2.5\\@tempdimb}\n                #2\\z@ plus1fil minus1fil\\relax\n        \\hskip.5\\tabcolsep}\n\\newcommand{\\cdashlinelr}[1]{\n  \\noalign{\\vskip\\aboverulesep\n           \\global\\let\\@dashdrawstore\\adl@draw\n           \\global\\let\\adl@draw\\adl@drawiv}\n  \\cdashline{#1}\n  \\noalign{\\global\\let\\adl@draw\\@dashdrawstore\n           \\vskip\\belowrulesep}}\n\\makeatother\n\n\\newenvironment{proofsk}{\n  \\renewcommand{\\proofname}{Proof sketch}\\proof}{\\endproof}\n\n\\renewcommand{\\epsilon}{\\varepsilon}\n\n\n\n\n\n\\declaretheorem[style=plain,name=Theorem]{theorem}\n\\declaretheorem[style=plain,sibling=theorem,name=Lemma]{lemma}\n\\declaretheorem[style=plain,sibling=theorem,name=Proposition]{proposition}\n\\declaretheorem[style=plain,sibling=theorem,name=Corollary]{cor}\n\\declaretheorem[style=plain,sibling=theorem,name=Claim]{claim}\n\\declaretheorem[style=plain,sibling=theorem,name=Conjecture]{conjecture}\n\\declaretheorem[style=definition,sibling=theorem,name=Definition]{definition}\n\\declaretheorem[style=definition,name=Assumption]{assumption}\n\\declaretheorem[style=definition,sibling=theorem,name=Example]{example}\n\\declaretheorem[style=remark,sibling=theorem,name=Remark]{remark}\n\n\\newenvironment{example*}\n {\\pushQED{\\qed}\\example}\n {\\popQED\\endexample}\n\\numberwithin{equation}{section}\n\n==== END OF /2406.01506/preamble/preamble_math.tex ====\n==== BEGINNING OF /2406.01506/preamble/minimalist_biblatex.tex ====\n\\usepackage{csquotes}\n\\usepackage[\nminnames=1,maxnames=99,maxcitenames=2,\nstyle=alphabetic,\n\ndoi=false,\nurl=false,\ngiveninits=true,\nhyperref,\nnatbib,\nbackend=bibtex,\nsorting=nyt,\nbackref=true\n]{biblatex}\n\\renewbibmacro{in:}{\n  \\ifentrytype{article}{}{\\printtext{\\bibstring{in}\\intitlepunct}}}\n\n\n\\renewbibmacro*{journal}{\n  \\iffieldundef{journaltitle}\n    {}\n    {\\printtext[journaltitle]{\n       \\printfield[noformat]{journaltitle}\n       \\setunit{\\subtitlepunct}\n       \\printfield[noformat]{journalsubtitle}}}}\n\n\n\n\\DeclareFieldFormat{sentencecase}{\\MakeSentenceCase*{#1}}\n\n\\renewbibmacro*{title}{\n  \\ifthenelse{\\iffieldundef{title}\\AND\\iffieldundef{subtitle}}\n    {}\n    {\\ifthenelse{\\ifentrytype{article}\\OR\\ifentrytype{inbook}\n      \\OR\\ifentrytype{incollection}\\OR\\ifentrytype{inproceedings}\n      \\OR\\ifentrytype{inreference}\\OR\\ifentrytype{misc}}\n      {\\printtext[title]{\n        \\printfield[sentencecase]{title}\n        \\setunit{\\subtitlepunct}\n        \\printfield[sentencecase]{subtitle}}}\n      {\\printtext[title]{\n        \\printfield[titlecase]{title}\n        \\setunit{\\subtitlepunct}\n        \\printfield[titlecase]{subtitle}}}\n     \\newunit}\n  \\printfield{titleaddon}}\n\n\n\n\\AtEveryBibitem{\n\\ifentrytype{article}{\n\n    \\clearfield{urldate}\n    \\clearfield{eprint}\n    \\clearfield{eid}\n}{}\n\\ifentrytype{book}{\n    \\clearfield{url}\n    \\clearfield{urldate}\n    \\clearfield{eprint}\n}{}\n\\ifentrytype{collection}{\n    \\clearfield{url}\n    \\clearfield{urldate}\n    \\clearfield{eprint}\n}{}\n\\ifentrytype{incollection}{\n    \\clearfield{url}\n    \\clearfield{urldate}\n    \\clearfield{eprint}\n}{}\n}\n\n\\AtEveryBibitem{\n    \\clearfield{pages}\n    \\clearfield{review}\n    \\clearfield{series}\n    \\clearfield{volume}\n    \\clearfield{month}\n    \n    \\clearfield{isbn}\n    \\clearfield{issn}\n    \\clearlist{location}\n    \\clearfield{series}\n    \\clearlist{publisher}\n    \\clearname{editor}\n}{}\n\n==== END OF /2406.01506/preamble/minimalist_biblatex.tex ====\n==== BEGINNING OF /2406.01506/preamble/definitions_basic.tex ====\n\n\n\n\\newcommand{\\defnphrase}[1]{\\emph{#1}}\n\n\\global\\long\\def\\floor#1{\\lfloor#1\\rfloor}\n\n\n\\newcommand{\\defeq}{\\coloneqq}\n\\newcommand{\\asympeq}{\\ \\sim\\ }\n\n\\newcommand{\\Reals}{\\mathbb{R}}\n\\newcommand{\\Nats}{\\mathbb{N}}\n\\newcommand{\\NNReals}{\\Reals_{+}}\n\n\n\\newcommand{\\exclude}{\\backslash}\n\n\\newcommand{\\eps}{\\varepsilon}\n\n\\renewcommand{\\Re}{\\mathrm{Re}}\n\\renewcommand{\\Im}{\\mathrm{Im}}\n\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\logit}{logit}\n\n\n\n\\newcommand{\\edges}{e}\n\\newcommand{\\vertices}{v}\n\\newcommand{\\loops}{l}\n\n\n\n\\newcommand{\\EE}{\\mathbb{E}}\n\\newcommand{\\var}{\\mathrm{var}}\n\\renewcommand{\\Pr}{\\mathbb{P}}\n\\newcommand{\\convPr}{\\xrightarrow{\\,p\\,}}\n\\newcommand{\\convDist}{\\xrightarrow{\\,d\\,}}\n\\newcommand{\\equaldist}{\\overset{d}{=}}\n\\newcommand{\\upto}{\\!\\uparrow\\!}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\as}{\\textrm{ a.s.}}\n\\newcommand{\\equalas}{\\overset{\\mathrm{a.s.}}{=}}\n\\newcommand{\\abs}[1]{\\left\\lvert#1 \\right\\rvert}\n\\newcommand{\\intd}{\\mathrm{d}}\n\\newcommand{\\dist}{\\ \\sim\\ }\n\\newcommand{\\distiid}{\\overset{\\mathrm{iid}}{\\dist}}\n\\newcommand{\\distind}{\\overset{ind}{\\dist}}\n\\newcommand{\\dtv}[1]{\\|#1\\|_{\\mathrm{TV}}}\n\n\\newcommand{\\PPDist}{\\mathrm{PP}}\n\n\\newcommand{\\Lebesgue}{\\Lambda}\n\\newcommand{\\NatSubs}[1]{\\tilde \\Nats_{#1}}\n\n\n\\newcommand{\\cdo}{\\mathrm{do}} \n\n\n\\newcommand{\\normalDist}{\\mathrm{Normal}}\n\\newcommand{\\diriDist}{\\mathrm{Diri}}\n\\newcommand{\\categDist}{\\mathrm{Cat}}\n\\newcommand{\\betaDist}{\\mathrm{Beta}}\n\\newcommand{\\bernDist}{\\mathrm{Bern}}\n\\newcommand{\\binDist}{\\mathrm{Bin}}\n\\newcommand{\\uniDist}{\\mathrm{Uni}}\n\\newcommand{\\poiDist}{\\mathrm{Poi}}\n\\newcommand{\\gammaDist}{\\mathrm{Gamma}}\n\\newcommand{\\multiDist}{\\mathrm{Multi}}\n\n\n\n\\providecommand\\given{} \n\\newcommand\\SetSymbol[1][]{\n  \\nonscript\\,#1:\\nonscript\\,\\mathopen{}\\allowbreak}\n\\DeclarePairedDelimiterX\\Set[1]{\\lbrace}{\\rbrace}\n{ \\renewcommand\\given{\\SetSymbol[]} #1 }\n\n\n\\newcommand{\\Ind}{\\mathbbm{1}}\n\n\n\n\n\n\n\n==== END OF /2406.01506/preamble/definitions_basic.tex ====",
            "statements": {
                "definitions": [
                    {
                        "statement_id": "988476cc-999a-44a1-9fdf-110c2e4863ea",
                        "paper_id": "9210081a-2305-4894-99c6-c1631ba1cabe",
                        "library_nr": 2,
                        "library_name": "Definition 2",
                        "title": "Linear Concept Separability",
                        "statement_original_tex": "\\begin{definition}\\label{def:linear_representation_orig}\n  A vector $\\bar{\\ell}_W$ is a linear representation of a binary concept $W$ if for all contexts $\\ell$, and all concept variables $Z$ that are causally separable with $W$, we have, for all $\\alpha>0$,\n  \\begin{align}\n    \\Pr(W = 1 \\given \\ell + \\alpha \\bar{\\ell}_W) &> \\Pr(W = 1 \\given \\ell), \\text{ and} \\\\\n    \\Pr(Z \\given \\ell + \\alpha \\bar{\\ell}_W) &= \\Pr(Z\\given \\ell).\n  \\end{align} \n\\end{definition}",
                        "statement_html": "A vector $\\bar{\\ell}_W$ is a linear representation of a binary concept $W$ if for all contexts $\\ell$, and all concept variables $Z$ that are causally separable with $W$, we have, for all $\\alpha>0$,\n\\begin{align}\n  \\Pr(W = 1 \\given \\ell + \\alpha \\bar{\\ell}_W) &> \\Pr(W = 1 \\given \\ell), \\text{ and} \\\\\n  \\Pr(Z \\given \\ell + \\alpha \\bar{\\ell}_W) &= \\Pr(Z\\given \\ell).\n\\end{align}",
                        "statement_type": "definition",
                        "statement_motivation_html": "A linear representation of a binary concept $W$ is useful in the context of causal inference and machine learning. It allows us to isolate the effect of $W$ from other variables $Z$ that are causally separable from $W$. This can be particularly valuable when trying to understand the direct impact of $W$ on an outcome, without interference from other factors.",
                        "html_url": "library/definitions/definition_2/index.html"
                    },
                    {
                        "statement_id": "6adeb11d-05e2-4dd8-b870-82c8b5ab6e27",
                        "paper_id": "9210081a-2305-4894-99c6-c1631ba1cabe",
                        "library_nr": 3,
                        "library_name": "Definition 3",
                        "title": "Subordination Hierarchy",
                        "statement_original_tex": "\\begin{definition}\\label{def:hierarchical_relation}\n  A value $z$ is \\defnphrase{subordinate} to a value $w$ (denoted by $z \\prec w$) if $\\yquad(z) \\subseteq \\yquad(w)$.\n  We say a categorical concept $Z \\in_R \\{z_0, \\dots, z_{k-1}\\}$ is subordinate to a categorical concept $W \\in_R \\{w_0, \\dots, w_{n-1}\\}$ if there exists a value $w_Z$ of $W$ such that each value $z_i$ of $Z$ is subordinate to $w_Z$.\n\\end{definition}",
                        "statement_html": "A value $z$ is $\\texttt{subordinate}$ to a value $w$ (denoted by $z \\prec w$) if $\\yquad(z) \\subseteq \\yquad(w)$. We say a categorical concept $Z \\in_R \\{z_0, \\dots, z_{k-1}\\}$ is subordinate to a categorical concept $W \\in_R \\{w_0, \\dots, w_{n-1}\\}$ if there exists a value $w_Z$ of $W$ such that each value $z_i$ of $Z$ is subordinate to $w_Z$.",
                        "statement_type": "definition",
                        "statement_motivation_html": "The concept of subordination is useful in categorical data analysis and hierarchical classification. It allows us to understand and represent relationships between different categorical concepts. For instance, if we know that a concept $Z$ is subordinate to a concept $W$, we can infer that all values of $Z$ are contained within a specific value of $W$. This can be particularly helpful in organizing data, simplifying complex relationships, and making inferences based on hierarchical structures.",
                        "html_url": "library/definitions/definition_3/index.html"
                    },
                    {
                        "statement_id": "0d4f7b12-8f92-41eb-927a-836b4b2c5dae",
                        "paper_id": "9210081a-2305-4894-99c6-c1631ba1cabe",
                        "library_nr": 4,
                        "library_name": "Definition 4",
                        "title": "Linear Concept Representation",
                        "statement_original_tex": "\\begin{definition}\\label{def:linear_representation}\n    A vector $\\bar{\\ell}_W$ is a linear representation of a binary concept $W$ if\n    \\begin{align}\n      \\Pr(W = 1 \\given \\ell + \\alpha \\bar{\\ell}_W) &> \\Pr(W = 1 \\given \\ell) \\text{, and} \\label{eq:linear_cond1} \\\\\n      \\Pr(Z \\given \\ell + \\alpha\\bar{\\ell}_W) &= \\Pr(Z \\given \\ell), \\label{eq:linear_cond2}\n    \\end{align}   \n    for all contexts $\\ell$, all $\\alpha>0$, and all concept variables $Z$ that are either subordinate or causally separable with $W$.\n    Here, if $W$ is a binary feature for an attribute $w$, $W=1$ denotes $W = \\ConceptValue{is\\_w}$.\n\\end{definition}",
                        "statement_html": "A vector $\\bar{\\ell}_W$ is a linear representation of a binary concept $W$ if\n\\begin{align}\n  \\Pr(W = 1 \\given \\ell + \\alpha \\bar{\\ell}_W) &> \\Pr(W = 1 \\given \\ell) \\text{, and} \\label{eq:linear_cond1} \\\\\n  \\Pr(Z \\given \\ell + \\alpha\\bar{\\ell}_W) &= \\Pr(Z \\given \\ell), \\label{eq:linear_cond2}\n\\end{align}   \nfor all contexts $\\ell$, all $\\alpha>0$, and all concept variables $Z$ that are either subordinate or causally separable with $W$.\nHere, if $W$ is a binary feature for an attribute $w$, $W=1$ denotes $W = \\ConceptValue{is_w}$.",
                        "statement_type": "definition",
                        "statement_motivation_html": "Understanding the linear representation of a binary concept is crucial in fields like machine learning and natural language processing. This definition helps in identifying how a concept $W$ can be represented in a vector space such that the probability of $W$ being true increases with the addition of the vector $\\bar{\\ell}_W$, while the probabilities of other unrelated concepts remain unchanged. This is particularly useful when designing algorithms that need to isolate and manipulate specific features or attributes without affecting others.",
                        "html_url": "library/definitions/definition_4/index.html"
                    },
                    {
                        "statement_id": "2666093f-1d61-4855-9762-11baf044d68f",
                        "paper_id": "9210081a-2305-4894-99c6-c1631ba1cabe",
                        "library_nr": 5,
                        "library_name": "Definition 5",
                        "title": "Vector Magnitude Maximization",
                        "statement_original_tex": "\\begin{definition}\\label{def:vector_representation}\n  We say that binary feature $W$ for an attribute $w$ has a \\emph{vector representation} $\\bar{\\ell}_w \\in \\Reals^d$ if $\\bar{\\ell}_w$ satisfies \\cref{def:linear_representation} and $\\|\\bar{\\ell}_w\\|_2=b_w$ in \\cref{thm:magnitude}.\n  If the vector representation of a binary feature is not unique, we say $\\bar\\ell_w$ is the vector representation that maximizes $b_w$.\n\\end{definition}",
                        "statement_html": "We say that binary feature $W$ for an attribute $w$ has a $\\emph{vector representation}$ $\\bar{\\ell}_w \\in \\Reals^d$ if $\\bar{\\ell}_w$ satisfies <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/definitions/definition_4/index.html#def%3Alinear_representation\">Definition 4</a> and $\\|\\bar{\\ell}_w\\|_2=b_w$ in <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_16/index.html#thm%3Amagnitude\">Theorem 16</a>. If the vector representation of a binary feature is not unique, we say $\\bar\\ell_w$ is the vector representation that maximizes $b_w$.",
                        "statement_type": "definition",
                        "statement_motivation_html": "Understanding the vector representation of a binary feature is crucial in various machine learning and data analysis tasks. It allows us to map categorical data into a continuous vector space, facilitating operations like similarity measurement, clustering, and classification. This representation is particularly useful when dealing with high-dimensional data, as it helps in reducing dimensionality while preserving the essential characteristics of the original data.",
                        "html_url": "library/definitions/definition_5/index.html"
                    },
                    {
                        "statement_id": "375c2009-3ad3-416a-8cdf-6b09708e4d53",
                        "paper_id": "9210081a-2305-4894-99c6-c1631ba1cabe",
                        "library_nr": 6,
                        "library_name": "Definition 6",
                        "title": "Polytope Representation of Categorical Concepts",
                        "statement_original_tex": "\\begin{definition}\n  The \\emph{polytope representation} of a categorical concept $Z = \\{z_0, \\dots, z_{k-1}\\}$ is the convex hull of the vector representations of the elements of the concept.\n\\end{definition}",
                        "statement_html": "The $\\emph{polytope representation}$ of a categorical concept $Z = \\{z_0, \\dots, z_{k-1}\\}$ is the convex hull of the vector representations of the elements of the concept.",
                        "statement_type": "definition",
                        "statement_motivation_html": "The polytope representation is useful in the context of machine learning and data analysis, particularly when dealing with categorical data. By representing a categorical concept as the convex hull of its vector representations, it allows for a more geometric and interpretable way to analyze relationships and similarities between different categories. This approach can be used in clustering, classification, and visualization tasks to better understand the structure and distribution of categorical data.",
                        "html_url": "library/definitions/definition_6/index.html"
                    }
                ],
                "axioms": [],
                "lemmas": [],
                "theorems": [
                    {
                        "statement_id": "e2471c58-2620-4680-bf83-5135232e4045",
                        "paper_id": "9210081a-2305-4894-99c6-c1631ba1cabe",
                        "library_nr": 16,
                        "library_name": "Theorem 16",
                        "title": "Magnitude Consistency Theorem",
                        "statement_original_tex": "\\begin{theorem}[Magnitudes of Linear Representations]\n\\label{thm:magnitude}\n  Suppose there exists a linear representation (normalized direction) $\\bar\\ell_W$ of a binary feature $W$ for an attribute $w$.  \n  Then, there is a constant $b_w>0$ and a choice of unembedding space origin $\\bar{\\gamma}_0^w$ in \\cref{eq:transformation} such that\n  \\begin{equation}\\label{eq:magnitude}\n    \\begin{cases}\n      \\bar\\ell_W^\\top g(y) = b_w & \\text{if } y\\in \\yquad(w)\\\\\n      \\bar\\ell_W^\\top g(y) = 0 & \\text{if } y \\not\\in \\yquad(w).\n    \\end{cases}\n  \\end{equation}\n  Further, if there are $d$ causally separable attributes $\\{w_0, \\dots, w_{d-1}\\}$ with linear representations, we can choose a canonical origin $\\bar\\gamma_0$ in \\cref{eq:transformation} as\n  \\begin{equation}\n    \\bar\\gamma_0 = \\sum_i \\bar{\\gamma}_0^{w_i}.\n  \\end{equation}\n\n\\end{theorem}",
                        "statement_html": "Suppose there exists a linear representation (normalized direction) $\\bar\\ell_W$ of a binary feature $W$ for an attribute $w$.  \nThen, there is a constant $b_w>0$ and a choice of unembedding space origin $\\bar{\\gamma}_0^w$ in (2.2) [in <a href=\"https://arxiv.org/pdf/2406.01506#equation.2.2\">original paper</a>] such that\n\\begin{equation}\\label{eq:magnitude}\n  \\begin{cases}\n    \\bar\\ell_W^\\top g(y) = b_w & \\text{if } y\\in \\yquad(w)\\\\\n    \\bar\\ell_W^\\top g(y) = 0 & \\text{if } y \\not\\in \\yquad(w).\n  \\end{cases}\n\\end{equation}\nFurther, if there are $d$ causally separable attributes $\\{w_0, \\dots, w_{d-1}\\}$ with linear representations, we can choose a canonical origin $\\bar\\gamma_0$ in (2.2) [in <a href=\"https://arxiv.org/pdf/2406.01506#equation.2.2\">original paper</a>] as\n\\begin{equation}\n  \\bar\\gamma_0 = \\sum_i \\bar{\\gamma}_0^{w_i}.\n\\end{equation}",
                        "statement_type": "theorem",
                        "statement_motivation_html": "This statement is useful in the context of understanding and manipulating binary features in a linear representation space. It provides a clear criterion for distinguishing whether a given feature $y$ belongs to a specific attribute $w$ based on the inner product with the normalized direction $\\bar\\ell_W$. This can be particularly useful in machine learning and data analysis, where such representations are often used to interpret and modify features. The ability to choose a canonical origin $\\bar\\gamma_0$ when dealing with multiple causally separable attributes further aids in simplifying and standardizing the representation space, making it easier to work with complex data structures.",
                        "html_url": "library/theorems/theorem_16/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "58c1c3d6-68c4-445d-a42f-e65d251ae99f",
                            "paper_id": "9210081a-2305-4894-99c6-c1631ba1cabe",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\n  For any $y_1, y_0 \\in \\yquad(w)$ or $y_1, y_0 \\not\\in \\yquad(w)$, let $Z$ be a binary concept where $\\yquad(Z = 0) = \\{y_0\\}$ and $\\yquad(Z = 1) = \\{y_1\\}$.\n  Since $Z$ is subordinate to $W$, \\cref{eq:linear_cond2} implies that\n  \\begin{align}\n    & \\logit \\Pr(Y = y_1 \\given Y \\in \\{y_0, y_1\\} , \\ell +  \\bar\\ell_W )= \\logit \\Pr(Y = y_1 \\given Y \\in \\{y_0, y_1\\} , \\ell)\\\\\n    & \\iff  \\bar\\ell_W ^\\top  (g(y_1) - g(y_0))=\\bar\\ell_W ^\\top  A(\\gamma(y_1) - \\gamma(y_0)) = 0\n  \\end{align}\n  where $A$ is the invertible matrix in \\cref{eq:transformation}.\n  This means that $\\bar\\ell_W^\\top A \\gamma(y)$ is the same for all $y\\in \\yquad(w)$, and it is also the same for all $y\\not\\in \\yquad(w)$.\n\n  Furthermore, for any $y_1 \\in \\yquad(w)$ and $y_0 \\not\\in \\yquad(w)$, \\cref{eq:linear_cond1} implies that\n  \\begin{align}\n    & \\logit \\Pr(Y = y_1 \\given Y \\in \\{y_0, y_1\\} , \\ell + \\bar\\ell_W )> \\logit \\Pr(Y = y_1 \\given Y \\in \\{y_0, y_1\\} , \\ell)\\\\\n    & \\iff  \\bar\\ell_W ^\\top  (g(y_1) - g(y_0)) =  \\bar\\ell_W ^\\top  A(\\gamma(y_1) - \\gamma(y_0))> 0.\n  \\end{align}\n\n  Thus, by setting $b_w^0 = \\bar\\ell_W^\\top A \\gamma(y)$ for any $y \\not\\in \\yquad(w)$, and $b_w = \\bar\\ell_W^\\top A \\gamma(y_1) - \\bar\\ell_W^\\top A \\gamma(y_0) > 0$ for any $y_1 \\in \\yquad(w)$ and $y_0 \\not\\in \\yquad(w)$, we get\n  \\begin{equation}\\label{eq:feature_vector}\n    \\begin{cases}\n      \\bar\\ell_W^\\top A \\gamma(y) = b_w^0 + b_w & \\text{if } y\\in \\yquad(w)\\\\\n      \\bar\\ell_W^\\top A \\gamma(y) = b_w^0 & \\text{if } y \\not\\in \\yquad(w).\n    \\end{cases}\n  \\end{equation}\n  Then, we can choose an origin as\n  \\begin{equation}\n    \\bar\\gamma_0^w = b_w^0 A^{-1} \\bar\\ell_W\n  \\end{equation}\n  satisfying \\cref{eq:magnitude}.\n\n  On the other hand, if there exist $\\bar\\ell_W$ and $\\bar\\ell_Z$ for causally separable attributes $w$ and $z$, then $\\bar\\ell_W$ and $\\bar\\ell_Z$ are orthogonal by the property of the causal inner product.\n  If they are not orthogonal, adding $\\bar\\ell_Z$ can change the other concept $W$, and it is a contradiction.\n  Now if there exist the linear representation for $d$ binary features for causally separable attributes $\\{w_0, \\dots, w_{d-1}\\}$, we can choose a canonical $\\bar\\gamma_0$ in \\cref{eq:transformation} as\n  \\begin{equation}\n    \\bar\\gamma_0 = \\sum_i \\bar{\\gamma}_0^{w_i}.\n  \\end{equation}\n  with \\cref{eq:magnitude} satisfied.\n\\end{proof}",
                            "statement_html": "For any $y_1, y_0 \\in \\yquad(w)$ or $y_1, y_0 \\not\\in \\yquad(w)$, let $Z$ be a binary concept where $\\yquad(Z = 0) = \\{y_0\\}$ and $\\yquad(Z = 1) = \\{y_1\\}$. Since $Z$ is subordinate to $W$, <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/definitions/definition_4/index.html#eq%3Alinear_cond2\">Definition 4</a> implies that\n\\begin{align}\n    & \\logit \\Pr(Y = y_1 \\given Y \\in \\{y_0, y_1\\} , \\ell +  \\bar\\ell_W )= \\logit \\Pr(Y = y_1 \\given Y \\in \\{y_0, y_1\\} , \\ell)\\\\\n    & \\iff  \\bar\\ell_W ^\\top  (g(y_1) - g(y_0))=\\bar\\ell_W ^\\top  A(\\gamma(y_1) - \\gamma(y_0)) = 0\n\\end{align}\nwhere $A$ is the invertible matrix in (2.2) [in <a href=\"https://arxiv.org/pdf/2406.01506#equation.2.2\">original paper</a>]. This means that $\\bar\\ell_W^\\top A \\gamma(y)$ is the same for all $y\\in \\yquad(w)$, and it is also the same for all $y\\not\\in \\yquad(w)$.\n\nFurthermore, for any $y_1 \\in \\yquad(w)$ and $y_0 \\not\\in \\yquad(w)$, <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/definitions/definition_4/index.html#eq%3Alinear_cond1\">Definition 4</a> implies that\n\\begin{align}\n    & \\logit \\Pr(Y = y_1 \\given Y \\in \\{y_0, y_1\\} , \\ell + \\bar\\ell_W )> \\logit \\Pr(Y = y_1 \\given Y \\in \\{y_0, y_1\\} , \\ell)\\\\\n    & \\iff  \\bar\\ell_W ^\\top  (g(y_1) - g(y_0)) =  \\bar\\ell_W ^\\top  A(\\gamma(y_1) - \\gamma(y_0))> 0.\n\\end{align}\n\nThus, by setting $b_w^0 = \\bar\\ell_W^\\top A \\gamma(y)$ for any $y \\not\\in \\yquad(w)$, and $b_w = \\bar\\ell_W^\\top A \\gamma(y_1) - \\bar\\ell_W^\\top A \\gamma(y_0) > 0$ for any $y_1 \\in \\yquad(w)$ and $y_0 \\not\\in \\yquad(w)$, we get\n\\begin{equation}\\label{eq:feature_vector}\n    \\begin{cases}\n      \\bar\\ell_W^\\top A \\gamma(y) = b_w^0 + b_w & \\text{if } y\\in \\yquad(w)\\\\\n      \\bar\\ell_W^\\top A \\gamma(y) = b_w^0 & \\text{if } y \\not\\in \\yquad(w).\n    \\end{cases}\n\\end{equation}\nThen, we can choose an origin as\n\\begin{equation}\n    \\bar\\gamma_0^w = b_w^0 A^{-1} \\bar\\ell_W\n\\end{equation}\nsatisfying <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_16/index.html#eq%3Amagnitude\">Theorem 16</a>.\n\nOn the other hand, if there exist $\\bar\\ell_W$ and $\\bar\\ell_Z$ for causally separable attributes $w$ and $z$, then $\\bar\\ell_W$ and $\\bar\\ell_Z$ are orthogonal by the property of the causal inner product. If they are not orthogonal, adding $\\bar\\ell_Z$ can change the other concept $W$, and it is a contradiction. Now if there exist the linear representation for $d$ binary features for causally separable attributes $\\{w_0, \\dots, w_{d-1}\\}$, we can choose a canonical $\\bar\\gamma_0$ in (2.2) [in <a href=\"https://arxiv.org/pdf/2406.01506#equation.2.2\">original paper</a>] as\n\\begin{equation}\n    \\bar\\gamma_0 = \\sum_i \\bar{\\gamma}_0^{w_i}.\n\\end{equation}\nwith <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_16/index.html#eq%3Amagnitude\">Theorem 16</a> satisfied.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "The proof involves several steps to establish the relationship between certain variables and their properties under given conditions. Here is a breakdown of the steps:\n<br>\n<br>1. <i></i>Initial Setup<i></i>:\n<br>   - For any \\( y_1, y_0 \\in \\yquad(w) \\) or \\( y_1, y_0 \\not\\in \\yquad(w) \\), define \\( Z \\) as a binary concept where \\( \\yquad(Z = 0) = \\{y_0\\} \\) and \\( \\yquad(Z = 1) = \\{y_1\\} \\).\n<br>   - Since \\( Z \\) is subordinate to \\( W \\), Definition 4 implies a specific relationship between the logit probabilities.\n<br>\n<br>2. <i></i>Logit Probability Relationship<i></i>:\n<br>   - The logit of the probability \\( \\Pr(Y = y_1 \\given Y \\in \\{y_0, y_1\\}, \\ell + \\bar\\ell_W) \\) is equal to the logit of the probability \\( \\Pr(Y = y_1 \\given Y \\in \\{y_0, y_1\\}, \\ell) \\) if and only if \\( \\bar\\ell_W^\\top (g(y_1) - g(y_0)) = \\bar\\ell_W^\\top A(\\gamma(y_1) - \\gamma(y_0)) = 0 \\).\n<br>\n<br>3. <i></i>Implication for \\( \\bar\\ell_W^\\top A \\gamma(y) \\)<i></i>:\n<br>   - This implies that \\( \\bar\\ell_W^\\top A \\gamma(y) \\) is constant for all \\( y \\in \\yquad(w) \\) and also constant for all \\( y \\not\\in \\yquad(w) \\).\n<br>\n<br>4. <i></i>Different Case for \\( y_1 \\in \\yquad(w) \\) and \\( y_0 \\not\\in \\yquad(w) \\)<i></i>:\n<br>   - For \\( y_1 \\in \\yquad(w) \\) and \\( y_0 \\not\\in \\yquad(w) \\), Definition 4 implies that the logit probability relationship is greater than zero if and only if \\( \\bar\\ell_W^\\top (g(y_1) - g(y_0)) = \\bar\\ell_W^\\top A(\\gamma(y_1) - \\gamma(y_0)) > 0 \\).\n<br>\n<br>5. <i></i>Setting Constants \\( b_w^0 \\) and \\( b_w \\)<i></i>:\n<br>   - Define \\( b_w^0 = \\bar\\ell_W^\\top A \\gamma(y) \\) for any \\( y \\not\\in \\yquad(w) \\).\n<br>   - Define \\( b_w = \\bar\\ell_W^\\top A \\gamma(y_1) - \\bar\\ell_W^\\top A \\gamma(y_0) > 0 \\) for any \\( y_1 \\in \\yquad(w) \\) and \\( y_0 \\not\\in \\yquad(w) \\).\n<br>\n<br>6. <i></i>Feature Vector Representation<i></i>:\n<br>   - The feature vector can be represented as:\n<br>     \\[\n     \\begin{cases}\n       \\bar\\ell_W^\\top A \\gamma(y) = b_w^0 + b_w & \\text{if } y \\in \\yquad(w) \\\\\n       \\bar\\ell_W^\\top A \\gamma(y) = b_w^0 & \\text{if } y \\not\\in \\yquad(w).\n     \\end{cases}\n     \\]\n<br>7. <i></i>Choosing an Origin<i></i>:\n<br>   - Choose an origin as \\( \\bar\\gamma_0^w = b_w^0 A^{-1} \\bar\\ell_W \\) satisfying Theorem 16.\n<br>\n<br>8. <i></i>Orthogonality of \\( \\bar\\ell_W \\) and \\( \\bar\\ell_Z \\)<i></i>:\n<br>   - If there exist \\( \\bar\\ell_W \\) and \\( \\bar\\ell_Z \\) for causally separable attributes \\( w \\) and \\( z \\), they must be orthogonal due to the property of the causal inner product. If not, adding \\( \\bar\\ell_Z \\) would change the other concept \\( W \\), leading to a contradiction.\n<br>\n<br>9. <i></i>Linear Representation for Multiple Attributes<i></i>:\n<br>   - For \\( d \\) binary features for causally separable attributes \\( \\{w_0, \\dots, w_{d-1}\\} \\), choose a canonical \\( \\bar\\gamma_0 \\) as:\n<br>     \\[\n     \\bar\\gamma_0 = \\sum_i \\bar{\\gamma}_0^{w_i}\n     \\]\n     satisfying Theorem 16."
                        }
                    },
                    {
                        "statement_id": "e29a72d7-af5f-4303-846b-19dd4e4114d9",
                        "paper_id": "9210081a-2305-4894-99c6-c1631ba1cabe",
                        "library_nr": 17,
                        "library_name": "Theorem 17",
                        "title": "Hierarchical Orthogonality Theorem",
                        "statement_original_tex": "\\begin{theorem}[Hierarchical Orthogonality]\n\\label{thm:orthogonality}\n  Suppose there exist the vector representations for all the following binary features.\n  Then, we have that\n  \\begin{enumerate}[label=(\\alph*)]\n    \\item $\\bar\\ell_{w_1} - \\bar\\ell_{w_0}$ is a linear representation $\\bar{\\ell}_{\\ConceptDirMath{w_0}{w_1}}$ defined in \\cref{def:linear_representation}; \\label{item:a}\n    \\item $\\bar\\ell_{w} \\perp \\bar\\ell_{z} - \\bar\\ell_w$ for $z \\prec w$; \\label{item:left}\n    \\item $\\bar{\\ell}_{w} \\perp \\bar\\ell_{z_1} - \\bar\\ell_{z_0}$ for $Z \\in_R \\{z_0, z_1\\}$ subordinate to $W \\in_R \\{\\ConceptValue{not\\_w}, \\ConceptValue{is\\_w}\\}$; \\label{item:middle}\n    \\item $\\bar{\\ell}_{w_1} - \\bar{\\ell}_{w_0}  \\perp \\bar\\ell_{z_1} - \\bar\\ell_{z_0}$ for $Z \\in_R \\{z_0, z_1\\}$ subordinate to $W \\in_R \\{w_0, w_1\\}$; and \\label{item:right}\n    \\item $\\bar\\ell_{w_1}-\\bar\\ell_{w_0} \\perp \\bar\\ell_{w_2} - \\bar\\ell_{w_1}$ for $w_2 \\prec w_1 \\prec w_0$.\n  \\end{enumerate}\n\n\\end{theorem}",
                        "statement_html": "Suppose there exist the vector representations for all the following binary features. Then, we have that\n<ol type=\"a\">\n    <li>$\\bar\\ell_{w_1} - \\bar\\ell_{w_0}$ is a linear representation $\\bar{\\ell}_{\\ConceptDirMath{w_0}{w_1}}$ defined in <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/definitions/definition_4/index.html#def%3Alinear_representation\">Definition 4</a>;</li>\n    <li>$\\bar\\ell_{w} \\perp \\bar\\ell_{z} - \\bar\\ell_w$ for $z \\prec w$;</li>\n    <li>$\\bar{\\ell}_{w} \\perp \\bar\\ell_{z_1} - \\bar\\ell_{z_0}$ for $Z \\in_R \\{z_0, z_1\\}$ subordinate to $W \\in_R \\{\\ConceptValue{not_w}, \\ConceptValue{is_w}\\}$;</li>\n    <li>$\\bar{\\ell}_{w_1} - \\bar{\\ell}_{w_0}  \\perp \\bar\\ell_{z_1} - \\bar\\ell_{z_0}$ for $Z \\in_R \\{z_0, z_1\\}$ subordinate to $W \\in_R \\{w_0, w_1\\}$; and</li>\n    <li>$\\bar\\ell_{w_1}-\\bar\\ell_{w_0} \\perp \\bar\\ell_{w_2} - \\bar\\ell_{w_1}$ for $w_2 \\prec w_1 \\prec w_0$.</li>\n</ol>",
                        "statement_type": "theorem",
                        "statement_motivation_html": "This set of conditions is useful in the context of vector representations of binary features. It provides a structured way to understand the relationships and orthogonality between different feature vectors. This can be particularly useful in machine learning and data analysis, where understanding the independence and dependencies between features is crucial for building accurate models.",
                        "html_url": "library/theorems/theorem_17/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "adee1173-1bb2-4c15-b43b-a456380c4ecb",
                            "paper_id": "9210081a-2305-4894-99c6-c1631ba1cabe",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\n  \\begin{enumerate}[label=(\\alph*)]\n    \\item For $\\bar\\ell_{w_1}$ and $\\bar\\ell_{w_0}$, by \\Cref{thm:magnitude}, we have\n    \\begin{equation}\n      \\begin{cases}\n        (\\bar\\ell_{w_1} - \\bar\\ell_{w_0})^\\top g(y) = 0-b_{w_0} = -b_{w_0} & \\text{if } y\\in \\yquad(w_0)\\\\\n        (\\bar\\ell_{w_1} - \\bar\\ell_{w_0})^\\top g(y) = b_{w_1} - 0 = b_{w_1} & \\text{if } y \\in \\yquad(w_1)\\\\\n        (\\bar\\ell_{w_1} - \\bar\\ell_{w_0})^\\top g(y) = 0 - 0 = 0 & \\text{if } y\\not\\in \\yquad(w_0)\\cup \\yquad(w_1).\n      \\end{cases}\n    \\end{equation}\n    Since $\\bar\\ell_{w_1} - \\bar\\ell_{w_0}$ can change the target concept $\\ConceptDirMath{w_0}{w_1}$ without changing any other concept subordinate or causally separable to the target concept, $\\bar\\ell_{w_1} - \\bar\\ell_{w_0}$ is the linear representation $\\bar{\\ell}_{w_0\\Rightarrow w_1}$.\n\n    \\item For $\\bar\\ell_{w}$ and $\\bar\\ell_{z}$ where $z \\prec w$, by \\Cref{thm:magnitude}, we have\n    \\begin{equation}\n      \\begin{cases}\n        (\\bar\\ell_{z} - \\bar\\ell_{w})^\\top g(y) = b_z - b_w & \\text{if } y\\in \\yquad(z)\\\\\n        (\\bar\\ell_{z} - \\bar\\ell_{w})^\\top g(y) =  0 -b_w =  -b_w & \\text{if } y \\in \\yquad(w)\\setminus \\yquad(z)\\\\\n        (\\bar\\ell_{z} - \\bar\\ell_{w})^\\top g(y) = 0 - 0 = 0 & \\text{if } y\\not\\in \\yquad(w).\n      \\end{cases}\n    \\end{equation}\n    When $w\\setminus z$ denotes an attribute defined by $\\yquad(w)\\setminus \\yquad(z)$, $\\bar\\ell_{z} - \\bar\\ell_{w}$ can change the target concept $\\ConceptDirMath{w\\setminus z}{z}$ without changing any other concept subordinate or causally separable to the target concept.\n    Thus, $\\bar\\ell_{z} - \\bar\\ell_{w}$ is the linear representation $\\bar{\\ell}_{w\\setminus z \\Rightarrow z}$.\n    This concept means $\\ConceptValue{not\\_z}  \\Rightarrow \\ConceptValue{is\\_z}$ conditioned on $w$, and hence it is subordinate to $w$.\n    \n    Therefore, $\\bar\\ell_w$ is orthogonal to the linear representation $\\bar{\\ell}_{w\\setminus z \\Rightarrow z} = \\bar\\ell_z - \\bar\\ell_w$ by the property of the causal inner product.\n    If they are not orthogonal, adding $\\bar\\ell_w$ can change the other concept $\\ConceptDirMath{w\\setminus z}{z}$, and it is a contradiction.\n\n    \\item By the above result \\ref{item:left}, $\\bar\\ell_w^\\top (\\bar\\ell_{z_1} - \\bar\\ell_w) = \\bar\\ell_w^\\top (\\bar\\ell_{z_0} - \\bar\\ell_w) = 0$.\n    Therefore, $\\bar\\ell_w^\\top (\\bar\\ell_{z_1} - \\bar\\ell_{z_0}) = 0$.\n\n    \\item Let's say that $w_1$ is $w_Z$ defined in \\Cref{def:hierarchical_relation}.\n    The binary contrast $\\ConceptDirMath{z_0}{z_1}$ is subordinate to the binary feature for the attribute $w_0$.\n    By the property of the causal inner product, $\\bar\\ell_{w_0}$ is orthogonal to the linear representation $\\bar{\\ell}_{z_0\\Rightarrow z_1} = \\bar\\ell_{z_1} - \\bar\\ell_{z_0}$ (by \\ref{item:a}).\n    Then, with the above result \\ref{item:middle}, we have $(\\bar\\ell_{w_1} - \\bar\\ell_{w_0})^\\top (\\bar\\ell_{z_1} - \\bar\\ell_{z_0})$.\n\n    \\item By the above result \\ref{item:left}, we have\n    \\begin{equation}\n      \\begin{cases}\n        \\|\\bar\\ell_{w_1} - \\bar\\ell_{w_0}\\|_2^2 = \\|\\bar\\ell_{w_1}\\|_2^2  - \\|\\bar\\ell_{w_0}\\|_2^2\\\\\n        \\|\\bar\\ell_{w_2} - \\bar\\ell_{w_1}\\|_2^2 = \\|\\bar\\ell_{w_2}\\|_2^2  - \\|\\bar\\ell_{w_1}\\|_2^2\\\\\n        \\|\\bar\\ell_{w_2} - \\bar\\ell_{w_0}\\|_2^2 = \\|\\bar\\ell_{w_2}\\|_2^2  - \\|\\bar\\ell_{w_0}\\|_2^2.\n      \\end{cases}\n    \\end{equation}\n    Then,\n    \\begin{align}\n      &\\|\\bar\\ell_{w_1} - \\bar\\ell_{w_0}\\|_2^2 + \\|\\bar\\ell_{w_2} - \\bar\\ell_{w_1}\\|_2^2\\\\\n      &= \\|\\bar\\ell_{w_1}\\|_2^2  - \\|\\bar\\ell_{w_0}\\|_2^2 + \\|\\bar\\ell_{w_2}\\|_2^2  - \\|\\bar\\ell_{w_1}\\|_2^2\\\\\n      &= \\|\\bar\\ell_{w_2}\\|_2^2  - \\|\\bar\\ell_{w_0}\\|_2^2\\\\\n      & = \\|\\bar\\ell_{w_2} - \\bar\\ell_{w_0}\\|_2^2.\n    \\end{align}\n    Therefore, $\\bar\\ell_{w_1} - \\bar\\ell_{w_0}$ is orthogonal to $\\bar\\ell_{w_2} - \\bar\\ell_{w_1}$.\n  \\end{enumerate}\n\\end{proof}",
                            "statement_html": "<ol type=\"a\">\n    <li>For $\\bar\\ell_{w_1}$ and $\\bar\\ell_{w_0}$, by <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_16/index.html#thm%3Amagnitude\">Theorem 16</a>, we have\n    \\begin{equation}\n      \\begin{cases}\n        (\\bar\\ell_{w_1} - \\bar\\ell_{w_0})^\\top g(y) = 0-b_{w_0} = -b_{w_0} & \\text{if } y\\in \\yquad(w_0)\\\\\n        (\\bar\\ell_{w_1} - \\bar\\ell_{w_0})^\\top g(y) = b_{w_1} - 0 = b_{w_1} & \\text{if } y \\in \\yquad(w_1)\\\\\n        (\\bar\\ell_{w_1} - \\bar\\ell_{w_0})^\\top g(y) = 0 - 0 = 0 & \\text{if } y\\not\\in \\yquad(w_0)\\cup \\yquad(w_1).\n      \\end{cases}\n    \\end{equation}\n    Since $\\bar\\ell_{w_1} - \\bar\\ell_{w_0}$ can change the target concept $\\ConceptDirMath{w_0}{w_1}$ without changing any other concept subordinate or causally separable to the target concept, $\\bar\\ell_{w_1} - \\bar\\ell_{w_0}$ is the linear representation $\\bar{\\ell}_{w_0\\Rightarrow w_1}$.\n\n    <li>For $\\bar\\ell_{w}$ and $\\bar\\ell_{z}$ where $z \\prec w$, by <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_16/index.html#thm%3Amagnitude\">Theorem 16</a>, we have\n    <begin{equation}\n      <begin{cases}\n        (\\bar\\ell_{z} - \\bar\\ell_{w})^\\top g(y) = b_z - b_w & \\text{if } y\\in \\yquad(z)\\\\\n        (\\bar\\ell_{z} - \\bar\\ell_{w})^\\top g(y) =  0 -b_w =  -b_w & \\text{if } y \\in \\yquad(w)\\setminus \\yquad(z)\\\\\n        (\\bar\\ell_{z} - \\bar\\ell_{w})^\\top g(y) = 0 - 0 = 0 & \\text{if } y\\not\\in \\yquad(w).\n      \\end{cases}\n    \\end{equation}\n    When $w\\setminus z$ denotes an attribute defined by $\\yquad(w)\\setminus \\yquad(z)$, $\\bar\\ell_{z} - \\bar\\ell_{w}$ can change the target concept $\\ConceptDirMath{w\\setminus z}{z}$ without changing any other concept subordinate or causally separable to the target concept.\n    Thus, $\\bar\\ell_{z} - \\bar\\ell_{w}$ is the linear representation $\\bar{\\ell}_{w\\setminus z \\Rightarrow z}$.\n    This concept means $\\ConceptValue{not\\_z}  \\Rightarrow \\ConceptValue{is\\_z}$ conditioned on $w$, and hence it is subordinate to $w$.\n    \n    Therefore, $\\bar\\ell_w$ is orthogonal to the linear representation $\\bar{\\ell}_{w\\setminus z \\Rightarrow z} = \\bar\\ell_z - \\bar\\ell_w$ by the property of the causal inner product.\n    If they are not orthogonal, adding $\\bar\\ell_w$ can change the other concept $\\ConceptDirMath{w\\setminus z}{z}$, and it is a contradiction.\n\n    <li>By the above result <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_17/index.html#item%3Aleft\">Theorem 17</a>, $\\bar\\ell_w^\\top (\\bar\\ell_{z_1} - \\bar\\ell_w) = \\bar\\ell_w^\\top (\\bar\\ell_{z_0} - \\bar\\ell_w) = 0$.\n    Therefore, $\\bar\\ell_w^\\top (\\bar\\ell_{z_1} - \\bar\\ell_{z_0}) = 0$.\n\n    <li>Let's say that $w_1$ is $w_Z$ defined in <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/definitions/definition_3/index.html#def%3Ahierarchical_relation\">Definition 3</a>.\n    The binary contrast $\\ConceptDirMath{z_0}{z_1}$ is subordinate to the binary feature for the attribute $w_0$.\n    By the property of the causal inner product, $\\bar\\ell_{w_0}$ is orthogonal to the linear representation $\\bar{\\ell}_{z_0\\Rightarrow z_1} = \\bar\\ell_{z_1} - \\bar\\ell_{z_0}$ (by <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_17/index.html#item%3Aa\">Theorem 17</a>).\n    Then, with the above result <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_17/index.html#item%3Amiddle\">Theorem 17</a>, we have $(\\bar\\ell_{w_1} - \\bar\\ell_{w_0})^\\top (\\bar\\ell_{z_1} - \\bar\\ell_{z_0})$.\n\n    <li>By the above result <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_17/index.html#item%3Aleft\">Theorem 17</a>, we have\n    <begin{equation>\n      <begin{cases}\n        \\|\\bar\\ell_{w_1} - \\bar\\ell_{w_0}\\|_2^2 = \\|\\bar\\ell_{w_1}\\|_2^2  - \\|\\bar\\ell_{w_0}\\|_2^2\\\\\n        \\|\\bar\\ell_{w_2} - \\bar\\ell_{w_1}\\|_2^2 = \\|\\bar\\ell_{w_2}\\|_2^2  - \\|\\bar\\ell_{w_1}\\|_2^2\\\\\n        \\|\\bar\\ell_{w_2} - \\bar\\ell_{w_0}\\|_2^2 = \\|\\bar\\ell_{w_2}\\|_2^2  - \\|\\bar\\ell_{w_0}\\|_2^2.\n      \\end{cases}\n    \\end{equation}\n    Then,\n    <begin{align}\n      &\\|\\bar\\ell_{w_1} - \\bar\\ell_{w_0}\\|_2^2 + \\|\\bar\\ell_{w_2} - \\bar\\ell_{w_1}\\|_2^2\\\\\n      &= \\|\\bar\\ell_{w_1}\\|_2^2  - \\|\\bar\\ell_{w_0}\\|_2^2 + \\|\\bar\\ell_{w_2}\\|_2^2  - \\|\\bar\\ell_{w_1}\\|_2^2\\\\\n      &= \\|\\bar\\ell_{w_2}\\|_2^2  - \\|\\bar\\ell_{w_0}\\|_2^2\\\\\n      & = \\|\\bar\\ell_{w_2} - \\bar\\ell_{w_0}\\|_2^2.\n    \\end{align}\n    Therefore, $\\bar\\ell_{w_1} - \\bar\\ell_{w_0}$ is orthogonal to $\\bar\\ell_{w_2} - \\bar\\ell_{w_1}$.\n</ol>",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "<ol type=\"a\">\n    <li>For $\\bar\\ell_{w_1}$ and $\\bar\\ell_{w_0}$, by <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_16/index.html#thm%3Amagnitude\">Theorem 16</a>, we have\n    \\[\n      \\begin{cases}\n        (\\bar\\ell_{w_1} - \\bar\\ell_{w_0})^\\top g(y) = 0-b_{w_0} = -b_{w_0} & \\text{if } y\\in \\yquad(w_0)\\\\\n        (\\bar\\ell_{w_1} - \\bar\\ell_{w_0})^\\top g(y) = b_{w_1} - 0 = b_{w_1} & \\text{if } y \\in \\yquad(w_1)\\\\\n        (\\bar\\ell_{w_1} - \\bar\\ell_{w_0})^\\top g(y) = 0 - 0 = 0 & \\text{if } y\\not\\in \\yquad(w_0)\\cup \\yquad(w_1).\n      \\end{cases}\n    \\]\n    Since $\\bar\\ell_{w_1} - \\bar\\ell_{w_0}$ can change the target concept $\\ConceptDirMath{w_0}{w_1}$ without changing any other concept subordinate or causally separable to the target concept, $\\bar\\ell_{w_1} - \\bar\\ell_{w_0}$ is the linear representation $\\bar{\\ell}_{w_0\\Rightarrow w_1}$.\n\n    <li>For $\\bar\\ell_{w}$ and $\\bar\\ell_{z}$ where $z \\prec w$, by <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_16/index.html#thm%3Amagnitude\">Theorem 16</a>, we have\n    \\[\n      \\begin{cases}\n        (\\bar\\ell_{z} - \\bar\\ell_{w})^\\top g(y) = b_z - b_w & \\text{if } y\\in \\yquad(z)\\\\\n        (\\bar\\ell_{z} - \\bar\\ell_{w})^\\top g(y) =  0 -b_w =  -b_w & \\text{if } y \\in \\yquad(w)\\setminus \\yquad(z)\\\\\n        (\\bar\\ell_{z} - \\bar\\ell_{w})^\\top g(y) = 0 - 0 = 0 & \\text{if } y\\not\\in \\yquad(w).\n      \\end{cases}\n    \\]\n    When $w\\setminus z$ denotes an attribute defined by $\\yquad(w)\\setminus \\yquad(z)$, $\\bar\\ell_{z} - \\bar\\ell_{w}$ can change the target concept $\\ConceptDirMath{w\\setminus z}{z}$ without changing any other concept subordinate or causally separable to the target concept.\n    Thus, $\\bar\\ell_{z} - \\bar\\ell_{w}$ is the linear representation $\\bar{\\ell}_{w\\setminus z \\Rightarrow z}$.\n    This concept means $\\ConceptValue{not\\_z}  \\Rightarrow \\ConceptValue{is\\_z}$ conditioned on $w$, and hence it is subordinate to $w$.\n    \n    Therefore, $\\bar\\ell_w$ is orthogonal to the linear representation $\\bar{\\ell}_{w\\setminus z \\Rightarrow z} = \\bar\\ell_z - \\bar\\ell_w$ by the property of the causal inner product.\n    If they are not orthogonal, adding $\\bar\\ell_w$ can change the other concept $\\ConceptDirMath{w\\setminus z}{z}$, and it is a contradiction.\n\n    <li>By the above result <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_17/index.html#item%3Aleft\">Theorem 17</a>, $\\bar\\ell_w^\\top (\\bar\\ell_{z_1} - \\bar\\ell_w) = \\bar\\ell_w^\\top (\\bar\\ell_{z_0} - \\bar\\ell_w) = 0$.\n    Therefore, $\\bar\\ell_w^\\top (\\bar\\ell_{z_1} - \\bar\\ell_{z_0}) = 0$.\n\n    <li>Let's say that $w_1$ is $w_Z$ defined in <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/definitions/definition_3/index.html#def%3Ahierarchical_relation\">Definition 3</a>.\n    The binary contrast $\\ConceptDirMath{z_0}{z_1}$ is subordinate to the binary feature for the attribute $w_0$.\n    By the property of the causal inner product, $\\bar\\ell_{w_0}$ is orthogonal to the linear representation $\\bar{\\ell}_{z_0\\Rightarrow z_1} = \\bar\\ell_{z_1} - \\bar\\ell_{z_0}$ (by <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_17/index.html#item%3Aa\">Theorem 17</a>).\n    Then, with the above result <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_17/index.html#item%3Amiddle\">Theorem 17</a>, we have $(\\bar\\ell_{w_1} - \\bar\\ell_{w_0})^\\top (\\bar\\ell_{z_1} - \\bar\\ell_{z_0})$.\n\n    <li>By the above result <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_17/index.html#item%3Aleft\">Theorem 17</a>, we have\n    \\[\n      \\begin{cases}\n        \\|\\bar\\ell_{w_1} - \\bar\\ell_{w_0}\\|_2^2 = \\|\\bar\\ell_{w_1}\\|_2^2  - \\|\\bar\\ell_{w_0}\\|_2^2\\\\\n        \\|\\bar\\ell_{w_2} - \\bar\\ell_{w_1}\\|_2^2 = \\|\\bar\\ell_{w_2}\\|_2^2  - \\|\\bar\\ell_{w_1}\\|_2^2\\\\\n        \\|\\bar\\ell_{w_2} - \\bar\\ell_{w_0}\\|_2^2 = \\|\\bar\\ell_{w_2}\\|_2^2  - \\|\\bar\\ell_{w_0}\\|_2^2.\n      \\end{cases}\n    \\]\n    Then,\n    \\[\n      \\begin{align}\n        &\\|\\bar\\ell_{w_1} - \\bar\\ell_{w_0}\\|_2^2 + \\|\\bar\\ell_{w_2} - \\bar\\ell_{w_1}\\|_2^2\\\\\n        &= \\|\\bar\\ell_{w_1}\\|_2^2  - \\|\\bar\\ell_{w_0}\\|_2^2 + \\|\\bar\\ell_{w_2}\\|_2^2  - \\|\\bar\\ell_{w_1}\\|_2^2\\\\\n        &= \\|\\bar\\ell_{w_2}\\|_2^2  - \\|\\bar\\ell_{w_0}\\|_2^2\\\\\n        & = \\|\\bar\\ell_{w_2} - \\bar\\ell_{w_0}\\|_2^2.\n      \\end{align}\n    \\]\n    Therefore, $\\bar\\ell_{w_1} - \\bar\\ell_{w_0}$ is orthogonal to $\\bar\\ell_{w_2} - \\bar\\ell_{w_1}$.\n</ol>"
                        }
                    },
                    {
                        "statement_id": "677144fb-8d19-4270-85d3-c87be74f011d",
                        "paper_id": "9210081a-2305-4894-99c6-c1631ba1cabe",
                        "library_nr": 18,
                        "library_name": "Theorem 18",
                        "title": "Simplex Representation Theorem",
                        "statement_original_tex": "\\begin{theorem}[Categorical Concepts are Represented as Simplices]\n\\label{thm:simplex}\n  Suppose that $\\{w_0, \\dots, w_{k-1}\\}$ is a collection of $k$ mutually exclusive attributes such that for every joint distribution $Q(w_0, \\dots w_{k-1})$ there is some $\\ell_{i}$ such that $\\Pr(W = w_i \\given \\ell_{i}) = Q(W=w_i)$ for every $i$.\n  Then, the vector representations $\\bar\\ell_{w_0}, \\dots, \\bar\\ell_{w_{k-1}}$ form a $(k-1)$-simplex in the representation space. In this case, we take the simplex to be the representation of the categorical concept $W = \\{w_0, \\dots, w_{k-1}\\}$.\n\n\\end{theorem}",
                        "statement_html": "Suppose that $\\{w_0, \\dots, w_{k-1}\\}$ is a collection of $k$ mutually exclusive attributes such that for every joint distribution $Q(w_0, \\dots w_{k-1})$ there is some $\\ell_{i}$ such that $\\Pr(W = w_i \\given \\ell_{i}) = Q(W=w_i)$ for every $i$. Then, the vector representations $\\bar\\ell_{w_0}, \\dots, \\bar\\ell_{w_{k-1}}$ form a $(k-1)$-simplex in the representation space. In this case, we take the simplex to be the representation of the categorical concept $W = \\{w_0, \\dots, w_{k-1}\\}$.",
                        "statement_type": "theorem",
                        "statement_motivation_html": "This statement is useful in the context of categorical data representation. It ensures that any collection of mutually exclusive attributes can be represented as points on a $(k-1)$-simplex, providing a geometric interpretation of categorical concepts. This is particularly valuable in machine learning and data analysis, where such representations can facilitate the visualization and manipulation of categorical data.",
                        "html_url": "library/theorems/theorem_18/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "c2a0c88e-8248-45bf-a012-d57929f26720",
                            "paper_id": "9210081a-2305-4894-99c6-c1631ba1cabe",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\n  If we can represent arbitrary joint distributions, this means, in particular, that we can change the probability of one attribute without changing the relative probability between a pair of other attributes.\n  Consider the case where $k=3$, as illustrated in \\Cref{fig:proof}.\n  If $\\bar\\ell_{w_0}, \\bar\\ell_{w_1}, \\bar\\ell_{w_2}$ are on a line, then there is no direction in that line (to change the value in the categorical concept) such that adding the direction can change the probability of $w_2$ without changing the relative probabilities between $w_0$ and $w_1$.\n  However, if $\\bar\\ell_{w_0}, \\bar\\ell_{w_1}, \\bar\\ell_{w_2}$ are not on a line, they form a triangle.\n  Then, there exists a line that is toward $\\bar\\ell_{w_2}$ and perpendicular to the opposite side of the triangle.\n  Now adding the direction $\\tilde{\\ell}$ can manipulate the probability of $w_2$ without changing the relative probabilities between $w_0$ and $w_1$.\n  That is, for any $\\alpha > 0$ and context embedding $\\ell$,\n  \\begin{equation}\n    \\begin{cases}\n      \\Pr(W = w_2 \\given \\ell + \\alpha \\tilde{\\ell}) > \\Pr(W = w_2 \\given \\ell), \\text{ and}\\\\\n      \\frac{\\Pr(W = w_1 \\given \\ell + \\alpha \\tilde{\\ell})}{\\Pr(W = w_0 \\given \\ell + \\alpha \\tilde{\\ell})}  = \\frac{\\Pr(W = w_1 \\given \\ell )}{\\Pr(W = w_0 \\given \\ell)}.\n    \\end{cases}\n  \\end{equation}\n  Therefore, the vectors $\\bar\\ell_{w_0}, \\bar\\ell_{w_1}, \\bar\\ell_{w_2}$ form a 2-simplex.\n  \n  This argument extends immediately to higher $k$ by induction.\n  For each $i \\in \\{0, \\dots, k-1\\}$, there should exist a direction that is toward $\\bar\\ell_{w_i}$ and orthogonal to the opposite hyperplane ($(k-2)$-simplex) formed by the other $\\bar\\ell_{w_{i'}}$'s.\n  Then, the vectors $\\bar\\ell_{w_0}, \\dots, \\bar\\ell_{w_{k-1}}$ form a $(k-1)$-simplex.\n\\end{proof}",
                            "statement_html": "If we can represent arbitrary joint distributions, this means, in particular, that we can change the probability of one attribute without changing the relative probability between a pair of other attributes. Consider the case where $k=3$, as illustrated in Figure 6 [in <a href=\"https://arxiv.org/pdf/2406.01506#figure.caption.18\">original paper</a>]. If $\\bar\\ell_{w_0}, \\bar\\ell_{w_1}, \\bar\\ell_{w_2}$ are on a line, then there is no direction in that line (to change the value in the categorical concept) such that adding the direction can change the probability of $w_2$ without changing the relative probabilities between $w_0$ and $w_1$. However, if $\\bar\\ell_{w_0}, \\bar\\ell_{w_1}, \\bar\\ell_{w_2}$ are not on a line, they form a triangle. Then, there exists a line that is toward $\\bar\\ell_{w_2}$ and perpendicular to the opposite side of the triangle. Now adding the direction $\\tilde{\\ell}$ can manipulate the probability of $w_2$ without changing the relative probabilities between $w_0$ and $w_1$. That is, for any $\\alpha > 0$ and context embedding $\\ell$,\n\\begin{equation}\n  \\begin{cases}\n    \\Pr(W = w_2 \\given \\ell + \\alpha \\tilde{\\ell}) > \\Pr(W = w_2 \\given \\ell), \\text{ and}\\\\\n    \\frac{\\Pr(W = w_1 \\given \\ell + \\alpha \\tilde{\\ell})}{\\Pr(W = w_0 \\given \\ell + \\alpha \\tilde{\\ell})}  = \\frac{\\Pr(W = w_1 \\given \\ell )}{\\Pr(W = w_0 \\given \\ell)}.\n  \\end{cases}\n\\end{equation}\nTherefore, the vectors $\\bar\\ell_{w_0}, \\bar\\ell_{w_1}, \\bar\\ell_{w_2}$ form a 2-simplex.\n\nThis argument extends immediately to higher $k$ by induction. For each $i \\in \\{0, \\dots, k-1\\}$, there should exist a direction that is toward $\\bar\\ell_{w_i}$ and orthogonal to the opposite hyperplane ($(k-2)$-simplex) formed by the other $\\bar\\ell_{w_{i'}}$'s. Then, the vectors $\\bar\\ell_{w_0}, \\dots, \\bar\\ell_{w_{k-1}}$ form a $(k-1)$-simplex.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several key steps:\n<br>\n<br>1. <i></i>Arbitrary Joint Distributions<i></i>: The proof starts by assuming that we can represent arbitrary joint distributions. This implies that we can adjust the probability of one attribute independently of the relative probabilities between other attributes.\n<br>\n<br>2. <i></i>Case for $k=3$<i></i>: The proof considers a specific case where there are three attributes ($k=3$). This is illustrated in a figure (referred to as Figure 6).\n<br>\n<br>3. <i></i>Attributes on a Line<i></i>: If the vectors $\\bar\\ell_{w_0}, \\bar\\ell_{w_1}, \\bar\\ell_{w_2}$ lie on a straight line, it is impossible to change the probability of $w_2$ without affecting the relative probabilities between $w_0$ and $w_1$.\n<br>\n<br>4. <i></i>Attributes Forming a Triangle<i></i>: If the vectors $\\bar\\ell_{w_0}, \\bar\\ell_{w_1}, \\bar\\ell_{w_2}$ do not lie on a line, they form a triangle. In this case, there exists a direction towards $\\bar\\ell_{w_2}$ that is perpendicular to the side of the triangle opposite $\\bar\\ell_{w_2}$.\n<br>\n<br>5. <i></i>Manipulating Probabilities<i></i>: By adding a direction $\\tilde{\\ell}$, we can increase the probability of $w_2$ without changing the relative probabilities between $w_0$ and $w_1$. This is expressed mathematically as:\n<br>   \\[\n   \\begin{cases}\n     \\Pr(W = w_2 \\given \\ell + \\alpha \\tilde{\\ell}) > \\Pr(W = w_2 \\given \\ell), \\text{ and}\\\\\n     \\frac{\\Pr(W = w_1 \\given \\ell + \\alpha \\tilde{\\ell})}{\\Pr(W = w_0 \\given \\ell + \\alpha \\tilde{\\ell})}  = \\frac{\\Pr(W = w_1 \\given \\ell )}{\\Pr(W = w_0 \\given \\ell)}.\n   \\end{cases}\n   \\]\n<br>\n<br>6. <i></i>Forming a 2-Simplex<i></i>: The vectors $\\bar\\ell_{w_0}, \\bar\\ell_{w_1}, \\bar\\ell_{w_2}$ thus form a 2-simplex (a triangle in 2-dimensional space).\n<br>\n<br>7. <i></i>Extension to Higher $k$<i></i>: The argument is extended to higher dimensions by induction. For each $i \\in \\{0, \\dots, k-1\\}$, there should exist a direction towards $\\bar\\ell_{w_i}$ that is orthogonal to the opposite hyperplane formed by the other vectors. This means that the vectors $\\bar\\ell_{w_0}, \\dots, \\bar\\ell_{w_{k-1}}$ form a $(k-1)$-simplex.\n<br>\n<br>In summary, the proof demonstrates that the vectors representing the attributes form a simplex, allowing for the independent adjustment of probabilities while maintaining relative probabilities between other attributes."
                        }
                    }
                ],
                "corollaries": []
            },
            "mathjax_macros": [
                "emph: [\"\\\\textit{#1}\", 1]",
                "bm: [\"\\\\boldsymbol{\\\\mathbf{#1}}\", 1]",
                "mathds: [\"\\\\mathbf{#1}\", 1]",
                "textsl: [\"\\\\textit{#1}\", 1]",
                "vspace: [\"\", 1]",
                "xspace: \"\"",
                "repOp: \"\\\\operatorname{Rep}\"",
                "genOp: \"\\\\operatorname{gen}\"",
                "SpanOp: \"\\\\operatorname{Span}\"",
                "ConeOp: \"\\\\operatorname{Cone}\"",
                "argmin: \"\\\\operatorname{argmin}\"",
                "argmax: \"\\\\operatorname{argmax}\"",
                "logit: \"\\\\operatorname{logit}\"",
                "promptSpace: \"\\\\mathcal{X}\"",
                "conceptSpace: \"\\\\mathcal{C}\"",
                "repSpace: \"\\\\mathcal{R}\"",
                "responseSpace: \"\\\\mathcal{Y}\"",
                "defas: \":=\"",
                "nullconcept: \"\\\\phi\"",
                "conceptDistSpace: \"\\\\mathcal{Q}\"",
                "cov: \"\\\\mathrm{Cov}\"",
                "yquad: \"\\\\mathcal{Y}\"",
                "parnum: \"\\\\bfseries\\\\P\\\\arabic{parcount}\"",
                "grad: \"\\\\nabla\"",
                "prm: \"\\\\:;\\\\:\"",
                "mbw: \"\\\\mb{w}\"",
                "mbW: \"\\\\mb{W}\"",
                "mbx: \"\\\\mb{x}\"",
                "mbX: \"\\\\mb{X}\"",
                "mby: \"\\\\mb{y}\"",
                "mbY: \"\\\\mb{Y}\"",
                "mbz: \"\\\\mb{z}\"",
                "mbZ: \"\\\\mb{Z}\"",
                "mbT: \"\\\\mb{T}\"",
                "mbA: \"\\\\mb{A}\"",
                "mba: \"\\\\mb{a}\"",
                "mbI: \"\\\\mb{I}\"",
                "mbone: \"\\\\mb{1}\"",
                "mbL: \"\\\\mb{L}\"",
                "mbtheta: \"\\\\mb{\\\\theta}\"",
                "mbTheta: \"\\\\mb{\\\\Theta}\"",
                "mbomega: \"\\\\mb{\\\\omega}\"",
                "mbOmega: \"\\\\mb{\\\\Omega}\"",
                "mbsigma: \"\\\\mb{\\\\sigma}\"",
                "mbSigma: \"\\\\mb{\\\\Sigma}\"",
                "mbphi: \"\\\\mb{\\\\phi}\"",
                "mbPhi: \"\\\\mb{\\\\Phi}\"",
                "mbalpha: \"\\\\mb{\\\\alpha}\"",
                "mbbeta: \"\\\\mb{\\\\beta}\"",
                "mbgamma: \"\\\\mb{\\\\gamma}\"",
                "mbeta: \"\\\\mb{\\\\eta}\"",
                "mbmu: \"\\\\mb{\\\\mu}\"",
                "mbrho: \"\\\\mb{\\\\rho}\"",
                "mblambda: \"\\\\mb{\\\\lambda}\"",
                "mbzeta: \"\\\\mb{\\\\zeta}\"",
                "diag: \"\\\\textrm{diag}\"",
                "supp: \"\\\\textrm{supp}\"",
                "V: \"\\\\mathbb{V}\"",
                "bbH: \"\\\\mathbb{H}\"",
                "bbN: \"\\\\mathbb{N}\"",
                "bbZ: \"\\\\mathbb{Z}\"",
                "bbR: \"\\\\mathbb{R}\"",
                "bbS: \"\\\\mathbb{S}\"",
                "cL: \"\\\\mathcal{L}\"",
                "cD: \"\\\\mathcal{D}\"",
                "cN: \"\\\\mathcal{N}\"",
                "cT: \"\\\\mathcal{T}\"",
                "Gam: \"\\\\textrm{Gam}\"",
                "InvGam: \"\\\\textrm{InvGam}\"",
                "g: \"\\\\, | \\\\,\"",
                "s: \"\\\\, ; \\\\,\"",
                "indpt: \"\\\\protect\\\\mathpalette{\\\\protect\\\\independenT}{\\\\perp}\"",
                "defeq: \"\\\\coloneqq\"",
                "asympeq: \"\\\\ \\\\sim\\\\ \"",
                "Reals: \"\\\\mathbb{R}\"",
                "Nats: \"\\\\mathbb{N}\"",
                "NNReals: \"\\\\Reals_{+}\"",
                "exclude: \"\\\\backslash\"",
                "eps: \"\\\\varepsilon\"",
                "edges: \"e\"",
                "vertices: \"v\"",
                "loops: \"l\"",
                "EE: \"\\\\mathbb{E}\"",
                "var: \"\\\\mathrm{var}\"",
                "convPr: \"\\\\xrightarrow{\\\\,p\\\\,}\"",
                "convDist: \"\\\\xrightarrow{\\\\,d\\\\,}\"",
                "equaldist: \"\\\\overset{d}{=}\"",
                "upto: \"\\\\!\\\\uparrow\\\\!\"",
                "given: \"\\\\mid\"",
                "as: \"\\\\textrm{ a.s.}\"",
                "equalas: \"\\\\overset{\\\\mathrm{a.s.}}{=}\"",
                "intd: \"\\\\mathrm{d}\"",
                "dist: \"\\\\ \\\\sim\\\\ \"",
                "distiid: \"\\\\overset{\\\\mathrm{iid}}{\\\\dist}\"",
                "distind: \"\\\\overset{ind}{\\\\dist}\"",
                "PPDist: \"\\\\mathrm{PP}\"",
                "Lebesgue: \"\\\\Lambda\"",
                "cdo: \"\\\\mathrm{do}\"",
                "normalDist: \"\\\\mathrm{Normal}\"",
                "diriDist: \"\\\\mathrm{Diri}\"",
                "categDist: \"\\\\mathrm{Cat}\"",
                "betaDist: \"\\\\mathrm{Beta}\"",
                "bernDist: \"\\\\mathrm{Bern}\"",
                "binDist: \"\\\\mathrm{Bin}\"",
                "uniDist: \"\\\\mathrm{Uni}\"",
                "poiDist: \"\\\\mathrm{Poi}\"",
                "gammaDist: \"\\\\mathrm{Gamma}\"",
                "multiDist: \"\\\\mathrm{Multi}\"",
                "Ind: \"\\\\mathbbm{1}\"",
                "p: \"    \\\\stepcounter{parcount}    \\\\leavevmode\\\\marginpar[\\\\hfill\\\\parnum]{\\\\parnum}\"",
                "independent: \"\\\\protect\\\\mathpalette{\\\\protect\\\\independenT}{\\\\perp}\"",
                "dif: \"\\\\mathop{}\\\\!\\\\mathrm{d}\"",
                "lmText: [\"\\\\lmOp(\\\\text{``#1''})\", 1]",
                "lm: [\"\\\\lmOp(#1)\", 1]",
                "repText: [\"\\\\repOp(\\\\text{``#1''})\", 1]",
                "rep: [\"\\\\repOp(#1)\", 1]",
                "ConceptName: [\"$\\\\mathtt{#1}$\", 1]",
                "ConceptValue: [\"\\\\texttt{#1}\", 1]",
                "SpanText: [\"\\\\SpanOp(\\\\text{``#1''})\", 1]",
                "Span: [\"\\\\SpanOp(#1)\", 1]",
                "ConeText: [\"\\\\ConeOp(\\\\text{``#1''})\", 1]",
                "Cone: [\"\\\\ConeOp(#1)\", 1]",
                "red: [\"\\\\textcolor{BrickRed}{#1}\", 1]",
                "orange: [\"\\\\textcolor{BurntOrange}{#1}\", 1]",
                "green: [\"\\\\textcolor{OliveGreen}{#1}\", 1]",
                "blue: [\"\\\\textcolor{MidnightBlue}{#1}\", 1]",
                "gray: [\"\\\\textcolor{black!60}{#1}\", 1]",
                "cdashlinelr: [\"  \\\\noalign{\\\\vskip\\\\aboverulesep           \\\\global\\\\let\\\\@dashdrawstore\\\\adl@draw           \\\\global\\\\let\\\\adl@draw\\\\adl@drawiv}  \\\\cdashline{#1}  \\\\noalign{\\\\global\\\\let\\\\adl@draw\\\\@dashdrawstore           \\\\vskip\\\\belowrulesep}\", 1]",
                "defnphrase: [\"\\\\emph{#1}\", 1]",
                "abs: [\"\\\\left\\\\lvert#1 \\\\right\\\\rvert\", 1]",
                "dtv: [\"\\\\|#1\\\\|_{\\\\mathrm{TV}}\", 1]",
                "NatSubs: [\"\\\\tilde \\\\Nats_{#1}\", 1]",
                "ConceptDirName: [\"\\\\texttt{#1}\\\\Rightarrow\\\\texttt{#2}\", 2]",
                "ConceptDirMath: [\"#1 \\\\Rightarrow #2\", 2]",
                "ip: [\"\\\\langle #1,#2\\\\rangle\", 2]",
                "E: [\"\\\\mathbb{E}_{#1}\\\\left[#2\\\\right]\", 2]",
                "checkmark: \"\\\\tikz\\\\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;\""
            ],
            "mathjax_environments": [
                "subequations: [\"{\", \"}\"]"
            ],
            "label2statementid": {
                "def:linear_representation_orig": "988476cc-999a-44a1-9fdf-110c2e4863ea",
                "def:hierarchical_relation": "6adeb11d-05e2-4dd8-b870-82c8b5ab6e27",
                "def:linear_representation": "0d4f7b12-8f92-41eb-927a-836b4b2c5dae",
                "eq:linear_cond1": "0d4f7b12-8f92-41eb-927a-836b4b2c5dae",
                "eq:linear_cond2": "0d4f7b12-8f92-41eb-927a-836b4b2c5dae",
                "def:vector_representation": "2666093f-1d61-4855-9762-11baf044d68f",
                "thm:magnitude": "e2471c58-2620-4680-bf83-5135232e4045",
                "eq:magnitude": "e2471c58-2620-4680-bf83-5135232e4045",
                "eq:feature_vector": "e2471c58-2620-4680-bf83-5135232e4045",
                "thm:orthogonality": "e29a72d7-af5f-4303-846b-19dd4e4114d9",
                "item:a": "e29a72d7-af5f-4303-846b-19dd4e4114d9",
                "item:left": "e29a72d7-af5f-4303-846b-19dd4e4114d9",
                "item:middle": "e29a72d7-af5f-4303-846b-19dd4e4114d9",
                "item:right": "e29a72d7-af5f-4303-846b-19dd4e4114d9",
                "thm:simplex": "677144fb-8d19-4270-85d3-c87be74f011d"
            }
        },
        {
            "paper_id": "2a384494-8439-4b46-8c5a-7b8d6b3deea1",
            "title": "Signal Propagation in Transformers: Theoretical Perspectives and the Role of Rank Collapse",
            "authors": [
                "Lorenzo Noci",
                "Sotiris Anagnostidis",
                "Luca Biggio",
                "Antonio Orvieto",
                "Sidak Pal Singh",
                "Aurelien Lucchi"
            ],
            "year": 2022,
            "source_url": "https://arxiv.org/abs/2206.03126",
            "html_url": "library/papers/signal_propagation_in_transformers_theoretical_perspectives_and_the_role_of_rank_collapse/index.html",
            "bibtex": "@misc{noci2022signalpropagationtransformerstheoretical,\n\ttitle={Signal Propagation in Transformers: Theoretical Perspectives and the Role of Rank Collapse}, \n\tauthor={Lorenzo Noci and Sotiris Anagnostidis and Luca Biggio and Antonio Orvieto and Sidak Pal Singh and Aurelien Lucchi},\n\tyear={2022},\n\teprint={2206.03126},\n\tarchivePrefix={arXiv},\n\tprimaryClass={cs.LG},\n\turl={https://arxiv.org/abs/2206.03126}}",
            "original_tex": "==== BEGINNING OF /2206.03126/math_commands.tex ====\n\\newcommand{\\mb}{\\mathbf}\n\\newcommand{\\wM}[1]{\\mb W^{#1}}\n\\newcommand{\\wMt}[1]{\\mb W^{#1^\\top}}\n\\DeclareMathOperator{\\soft}{softmax}\n\\newcommand{\\kro}{%\n  \\mathbin{\\mathop{\\otimes}}%\n}\n\\usepackage{xcolor}\n\n\\usepackage{thm-restate}\n\\newtheoremstyle{theoremdd}% name of the style to be used\n  {\\topsep}% measure of space to leave above the theorem. E.g.: 3pt\n  {\\topsep}% measure of space to leave below the theorem. E.g.: 3pt\n  {\\itshape}% name of font to use in the body of the theorem\n  {0pt}% measure of space to indent\n  {\\bfseries}% name of head font\n  {. }% punctuation between head and body\n  { }% space after theorem head; \" \" = normal interword space\n  {\\thmname{#1}\\thmnumber{ #2}\\textnormal{\\thmnote{ (#3)}}}\n\\theoremstyle{theoremdd}\n\\declaretheorem[name=Theorem,numberwithin=section]{thm}\n\\declaretheorem[name=Lemma,numberwithin=section,numberlike=thm]{lem}\n\\declaretheorem[name=Corollary,numberwithin=section,numberlike=thm]{cor}\n\n\\declaretheorem[name=Proposition,numberwithin=section,numberlike=thm]{prop}\n\n\\declaretheorem[name=Assumption,numberlike=thm]{ass}\n\n\n\n\n\\DeclareMathOperator{\\blockdiag}{blockdiag}\n\\DeclareMathOperator{\\diag}{diag}\n\\DeclareMathOperator{\\vect}{vec}\n\\DeclareMathOperator{\\sa}{\\mb {SA}}\n\\DeclareMathOperator{\\rsa}{\\mb {RSA}}\n\\newcommand{\\wQ}{\\mb W^{Q}}\n\\newcommand{\\wQT}{\\mb W^{Q \\top}}\n\\newcommand{\\wK}{\\mb W^{K}}\n\\newcommand{\\wKT}{\\mb W^{K \\top}}\n\\newcommand{\\wV}{\\mb W^{V}}\n\\newcommand{\\wVT}{\\mb W^{V \\top}}\n\\def\\Im{{\\bf I}}\n\\def\\Km{{\\bf K}}\n\\def\\Am{{\\bf A}}\n\\def\\Em{{\\boldsymbol \\epsilon}}\n\\def\\Lm{{\\bf L}}\n\\def\\Bm{{\\bf B}}\n\\def\\Cm{{\\bf C}}\n\\def\\Dm{{\\bf D}}\n\\def\\Sm{{\\bf S}}\n\\def\\Xm{{\\bf X}}\n\\def\\Ym{{\\bf Y}}\n\\def\\Nm{{\\bf N}}\n\\def\\Mm{{\\bf M}}\n\\def\\Tm{{\\bf T}}\n\\def\\Wm{{\\bf W}}\n\\def\\Zm{{\\bf Z}}\n\\def\\Sm{{\\bf S}}\n\\DeclareMathOperator{\\tr}{tr}\n\\newcommand\\Exp{\\mathbb{E}}\n\\DeclareMathOperator{\\rank}{rank}\n\\usepackage{nicematrix}\n\\newcommand{\\p}{\\bm p}\n\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n\\DeclareMathOperator{\\SA}{SA}\n\\newcommand\\R{\\mathbb{R}}\n\\newcommand{\\extra}[1]{{ #1}}\n\n\n==== END OF /2206.03126/math_commands.tex ====\n==== BEGINNING OF /2206.03126/old.tex ====\n\\section{Related work}\n- works on initialization of transformers\n- works on the need of warmup with adam and other on adaptive learning rates\n- works on training stabilization\n- works on enhancing/replacing the softmax\\\\\n\n\\luca{preliminar: just a commented list of papers}\n\\cite{wang2022deepnet} introduces DeepNorm, a simple initialization scheme that enables training of transformer models with more than 1000 layers. The initialization is based on rescaling of the residual connections and on the introduction of a gain parameter for the weights of the feed-forward connections and the value projections. In all the evaluations though the authors still use warmup as well as layer normalization, Adam and gradient clipping.\n\n\\cite{bachlechner2021rezero} introduces ReZero a simple trick consisting in the addition of a trainable parameter (initialized to zero) to the residual term in residual connection. The authors show that transformers equipped with this simple scheme can be quickly trained without LayerNorm, even for very large depths.\n\n\\cite{xiong2020layer} shows that using layer normalization inside the residual connection (Pre-LN) allows training of Transformers without warm-up. However, they claim that SGD cannot achieve performance on par with Adam. Furthermore, parallel works show that Pre-LN can result in sub-optimal performance on neural machine translation tasks. \n\n\\cite{pmlr-v119-huang20f} shows that the need for warm-up comes from the instability in the Adam optimizer combined with gradient vanishing through layer normalization. A new initialization is then introduced that keeps the models update bounded and enables training without warm-up and layer normalization. However, the Adam optimizer is kept and SGD is deemed as sub-optimal for Transformer training.\n\n\n\\cite{liu2020understanding} found that gradient vanishing is not the only issue preventing stable training in transformers. The find that with Pre-LN, vanishing gradient is removed but the resulting performance is sub-optimal as opposed to the cases where Post-LN converges. Interestingly they notice that gradients of all attention modules are unbalanced and they conclude that the only way to optimize Transformer is via adaptive optimizers. Ultimately, they determine that the dependency on residual branches is one of the main factor determining convergence and performance and they introduce a new initialization based on residual rescaling to successfully cope with it.\n\n\n\\cite{zhang2020adaptive} finds that adaptive optimizers are better suited than plain SGD when attention-based models are considered. This is due to the heavy-tailed nature of noise in stochastic gradients found in BERT optimization. The authors find the \\emph{adaptive} gradient clipping is effective in reducing the heavy-tail effect and in closing the gap between SGD and Adam.\n\n\n\\cite{liuadam} provides a theoretical justification for the need for warm-up scheduling when adaptive methods are used. In particular, they show that the convergence issue is due to the undesirably large variance of the adaptive learning rate in the early stage of model training. They thus introduce RAdam, which explicitly rectifies the variance and compares favourably with warm-up.\n\n\n\\cite{shleifer2021normformer} introduces NormFormer, a new model that adds three normalization operations to each layer: a Layer Norm after self attention, head-wise scaling of self-attention outputs, and a Layer Norm after the first fully connected layer. The extra operations incur negligible compute cost (+0.4\\% parameter increase), but improve pretraining perplexity and downstream\ntask performance for both causal and masked language models. \n\n\n\\cite{wang2019learning} modifies the residual connection by introducing multiple links with all previous layers to access lower-level representations in a deep stack. This simple trick allows to train deeper models on several translation tasks.\n\n\\cite{nguyen2019transformers} combines three different types of normalization to cope with training instability in transformer models. PRENORM + FIXNORM + SCALENORM significantly improves NMT on low-resource pairs, with the latter two performing comparably in the high-resource setting, but faster.\n\n\\cite{dufter2020increasing} Use of learnable temperature for the attention weights. They use it for all queries, values and keys and per dimension. They report the temperature has no effect (empirical study).\n\n\n\n\n\n\\paragraph{Training Instability in Attention-Based Models.}\nSuccessful optimization in Transformers is notoriously hard and requires several tailored training tricks and architectural design choices. Among these, Layer Normalization is, arguably, one of the most important -- and debated -- ones.\nIn the original architecture \\citep{vaswani2017attention}, Layer Normalization was used in the Post-Norm setting, that is, it was placed between the residual blocks in order to stabilize the forward pass by reducing the variance of the inputs to the following sublayer. Later works \\citep{chen2018best} found that its inclusion is highly beneficial for the stabilization of the training process. Nevertheless, successful training of Transformers in the Post-Norm setting remains hard without resorting to adaptive methods opportunely combined with strongly hyperparameter-sensitive preliminary warm-up stages \\cite{liuadam,zhang2020adaptive,huang2020improving}. Moreover, training instability in this setting is further exacerbated when the optimization of deeper models is required. These considerations have recently led to the introduction of the so-called Pre-Norm variant \\cite{wang2019learning,nguyen2010estimating,xiong2020layer}, where layer normalization is used inside the residual connection. \\cite{xiong2020layer} show that since Post-LN induces larger gradients in deeper layers than Pre-Norn, networks equipped with the latter can be trained with larger learning rates and without the need of warm-up. Furthermore, Pre-Norm has been found \\cite{wang2019learning,nguyen2010estimating,xiong2020layer} to enable training of deeper models without incurring in training instabilities. However, despite making training easier, the use of Pre-LN can lead to inferior performance than its Post-LN counterpart \\cite{liu2020understanding, nguyen2019transformers}. This observation has motivated research on replacing or complementing layer norm with carefully designed initialization schemes \\cite{zhang2019fixup,liu2020understanding} or additional architectural modifications \\cite{bachlechner2021rezero,wang2022deepnet}. In our work we build upon the above research efforts with the goal of shedding new light on the forward and backward passes of attention-based architectures. In particular, we present, for the first time, a rigorous theoretical analysis of signal propagation in Transformer-based neural networks. In doing so, we provide new insights on the role of Layer Normalization, both in the Post-LN and Pre-LN configurations, and of adaptive optimizers. In relation to this, as also observed by \\citep{liu2020understanding}, we identify in the different scales of the norms of queries and values gradients one of the possible causes for the necessity of adaptive methods in Transformers optimization and, differently from \\citep{liu2020understanding}, we demonstrate that the introduction of a temperature parameter inside the softmax can help SGD in closing the performance gap with Adam. \n\\paragraph{Rank Collapse in Attention-Based Models.}\nA parallel line of research \\citep{dong2021attention} has recently identified an additional key inductive bias characterizing purely attention-based architectures: their input representations lose rank doubly exponentially with depth. The authors identify residual connections as the most effective component to mitigate this phenomenon. However, \\citep{dong2021attention} leave the question on the relation between the emergence of rank collapse and optimization unaddressed. Thanks to our analysis of the backward pass, we are able to demonstrate that rank collapse in Transformer architectures leads to vanishingly small gradients of queries and keys, thereby preventing effective training and completing the analysis started by \\citep{dong2021attention}.\n\n==== END OF /2206.03126/old.tex ====\n==== BEGINNING OF /2206.03126/main.tex ====\n\\documentclass{article}\n\n\n% if you need to pass options to natbib, use, e.g.:\n%     \\PassOptionsToPackage{numbers, compress}{natbib}\n% before loading neurips_2022\n\n\n% ready for submission\n\\usepackage[preprint]{neurips_2022}\n\n\n% to compile a preprint version, e.g., for submission to arXiv, add add the\n% [preprint] option:\n%     \\usepackage[preprint]{neurips_2022}\n\n\n% to compile a camera-ready version, add the [final] option, e.g.:\n%     \\usepackage[final]{neurips_2022}\n\n\n% to avoid loading the natbib package, add option nonatbib:\n%    \\usepackage[nonatbib]{neurips_2022}\n\n\\usepackage[utf8]{inputenc} % allow utf-8 input\n\\usepackage[T1]{fontenc}    % use 8-bit T1 fonts\n\\usepackage{hyperref}       % hyperlinks\n% colors citations\n\\hypersetup{\n    colorlinks=true,\n    linkcolor=blue,\n    filecolor=magenta,      \n    urlcolor=cyan,\n    citecolor=blue,\n}\n\\usepackage{url}            % simple URL typesetting\n\\usepackage{booktabs}       % professional-quality tables\n\\usepackage{amsfonts}       % blackboard math symbols\n\\usepackage{nicefrac}       % compact symbols for 1/2, etc.\n\\usepackage{microtype}      % microtypography\n\\usepackage[dvipsnames]{xcolor}         % colors\n\\newcommand{\\luca}[1]{{\\color{red} Luca: ``#1''}}\n\\newcommand{\\antonio}[1]{{\\color{magenta} Antonio: ``#1''}}\n\\newcommand{\\lorenzo}[1]{{\\color{blue} Lorenzo: ``#1''}}\n\\newcommand{\\sotiris}[1]{{\\color{green} Sotiris: ``#1''}}\n\\newcommand{\\sidak}[1]{{\\color{orange} Sidak: ``#1''}}\n\\newcommand{\\aurelien}[1]{{\\color{ForestGreen} Aurelien: ``#1''}}\n\n\\usepackage{aligned-overset}\n\\usepackage{amsthm}\n\\usepackage{wrapfig}\n\\usepackage{diagbox}\n\\usepackage[toc,page,header]{appendix}\n\\usepackage{minitoc}\n\\renewcommand \\thepart{}\n\\renewcommand \\partname{}\n\n\\usepackage[most]{tcolorbox}\n\\tcbset{\n    boxsep=0mm,\n    boxrule=0pt,\n    colframe=white,\n    arc=0mm,\n    left=0.5mm,\n    right=0.5mm\n}\n\\input{math_commands}\n\\usepackage{bm}\n\\newtheorem{theorem}{Theorem}[section]\n\\newtheorem{lemma}{Lemma}[section]\n\\newtheorem{corollary}{Corollary}[section]\n\n\\newtheorem{assumption}{Assumption}\n\\newcommand\\blfootnote[1]{%\n  \\begingroup\n  \\renewcommand\\thefootnote{}\\footnote{#1}%\n  \\addtocounter{footnote}{-1}%\n  \\endgroup\n}\n\n\n\\usepackage{floatrow}\n\n\\title{\n%Vanishing Gradients in Transformers, and a Simple Trick to Stabilize Them \\\\\n%Stabilizing Transformers without Adam and Layer Normalization \\\\\n%An optimization perspective on Transformers\\\\\n%(Theoretical study/Insights from) Signal propagation in Transformers \\\\\nSignal Propagation in Transformers: Theoretical Perspectives and the Role of Rank Collapse\n}\n\n\\usepackage{array,multirow,graphicx}\n\n\n\\author{Lorenzo Noci$^{*1}$\\\\ \n  \\texttt{\\small lorenzo.noci@inf.ethz.ch} \\\\\n  \\And Sotiris Anagnostidis$^{*1}$\\\\ \n  \\texttt{\\small sotirios.anagnostidis@inf.ethz.ch} \\\\ \\And Luca Biggio$^{*1,2}$ \\\\ \n  \\texttt{\\small luca.biggio@inf.ethz.ch} \\\\ \\And Antonio Orvieto$^{*1}$ \\\\ \n  \\texttt{\\small antonio.orvieto@inf.ethz.ch} \\\\ \\And Sidak Pal Singh$^{*1}$ \\\\ \n  \\texttt{\\small sidak.singh@inf.ethz.ch} \\\\ \\And Aurelien Lucchi$^{3}$ \\\\ \n  \\texttt{\\small aurelien.lucchi@unibas.ch}\n}\n\n\\date{tests}\n\n\n\\begin{document}\n\n\\doparttoc % Tell to minitoc to generate a toc for the parts\n\\faketableofcontents % Run a fake tableofcontents command for the partocs\n\n\\maketitle\n\n\\begin{abstract}\nTransformers have achieved remarkable success in several domains, ranging from natural language processing to computer vision. Nevertheless, it has been recently shown that stacking self-attention layers  the distinctive architectural component of Transformers  can result in rank collapse of the tokens representations at initialization. The question of if and how rank collapse affects training is still largely unanswered, and its investigation is necessary for a more comprehensive understanding of this architecture. In this work, we shed new light on the causes and the effects of this phenomenon. First, we show that rank collapse of the tokens representations hinders training by causing the gradients of the queries and keys to vanish at initialization. Furthermore, we provide a thorough description of the origin of rank collapse and discuss how to prevent it via an appropriate depth-dependent scaling of the residual branches. Finally, our analysis unveils that specific architectural hyperparameters affect the gradients of queries and values differently, leading to disproportionate gradient norms. This suggests an explanation for the widespread use of adaptive methods for Transformers' optimization. \n\\end{abstract}\n\n\\blfootnote{$^{1}$Dept of Computer Science, ETH Z\\\"urich, $^{2}$Robotics \\& ML, CSEM SA, Alpnach, Switzerland, $^{3}$Department of Mathematics and Computer Science, University of Basel}\n\\section{Introduction}\nSince its first appearance in~\\cite{vaswani2017attention}, the Transformer architecture has revolutionized the field of Natural Language Processing (NLP), achieving remarkable success in tasks such as text classification~\\citep{yang2019xlnet}, machine translation~\\citep{mtlample}, reading comprehension~\\citep{brown2020language} and question answering~\\citep{raffel2019exploring} among others. Recent efforts have effectively extended its applicability to computer vision~\\citep{dosovitskiy2020image} and other domains ~\\citep{baevski2020wav2vec, huang2018music, biggio2021neural, polu2022formal}, further popularizing it outside NLP.\n\n% \\vspace{-5px}\nThe Transformer operates on inputs comprising a sequence of tokens. At its core, it relies on stacked attention layers, which compute a measure of relevance for the whole sequence by assigning token-wise importance weights --- obtained by matrix multiplication of the \\textit{queries} and \\textit{keys}, and finally normalized with the softmax function. The output of an attention layer is then a linear combination of the importance weights and the so-called \\textit{values}. Then, the architecture includes fully-connected sub-layers, residual connections \\citep{resnet2016}, and layer normalization (LN), as illustrated in Fig.~\\ref{fig:architecture}.\n\n\n\\begin{figure}[t]\n\\centering\n    \\includegraphics[width=\\textwidth]{figures/adam_correlations.pdf}\n    \\caption{Evolution of the cosine of the angle between tokens for training POST-LN Transformers of increasing depth, with the Adam optimizer, for the IWSLT'14 De-En translation task. Unless adequate residual scaling is used at initialization, increasing depth leads to an increase in the tokens' alignment at initialization, which can inhibit training.}\n    \\label{fig:adam_postln}\n\\end{figure}\n\nIn the absence of residual connections, \\cite{dong2021attention} proved that at initialization the rank of the sequence representation collapses doubly exponentially with depth, and both layer normalization and fully connected layers can only partially alleviate the speed of degeneracy. Under \\textit{rank collapse}, the model does not distinguish between representations of different tokens, which are perfectly aligned in feature space at initialization. Crucially, the precise implications of rank collapse in Transformers are not fully understood. \n\nIn this paper, we show that a high alignment of the tokens' representations at initialization --- corresponding to rank collapse in the extreme case of perfect alignment --- affects training by causing vanishingly small gradients of the queries and keys' parameter matrices. This problem severely diminishes the capabilities of the model to learn meaningful attention weights and is further exacerbated in very deep networks, where the rank deficiency --- and hence the vanishing gradient problem of the queries and keys --- affects several layers (see Fig.~\\ref{fig:adam_postln}). In order to shed light on this problem, we take inspiration from the flourishing literature on signal propagation in random networks and start our analysis by computing the expected gradients of an attention layer with respect to the queries, keys, and values, which leads to Theorem \\ref{thm:vanishing_gradients} on the vanishing gradients for the queries and keys. From here, we pursue two different directions. \n\nFirstly, we investigate under which conditions rank collapse can be avoided by studying the evolution of the input sequence in a Transformer at initialization. Our theory reveals that a depth-dependent scaling of the residual branches, beyond stabilizing the norm of the activations at initialization, also approximately preserves the cosine of the angle between tokens, and hence also stabilizes the rank of the propagating sequence. We show that this holds even in the infinite-depth limit.\n\nSecondly, we illustrate that there are factors, other than the average tokens' correlation, that affect differently the gradient norm of the queries and keys compared to the values. In particular, the propagating sequence's squared norm has a linear dependence in the values, while a cubic one in the queries and keys, justifying the use of layer normalization. We also highlight a different dependence on the embedding dimension and the length of the input sequence, implying that the gradient norm of a subset of parameters can potentially be of different orders of magnitude, as empirically hinted by previous works \\citep{liu2020understanding}. Our analysis brings to light fundamental issues in the signal propagation in Transformers, opening the way for new, well-founded and motivated approaches to improve optimization in these models.\n\n\\section{Background}\n\n\\paragraph{Transformers.}\n\nA Transformer architecture consists of $L$ stacked attention blocks, as show in Fig.~\\ref{fig:architecture}. Layer normalization is usually applied token-wise either after the residual connections or to the inputs of the self-attention and position-wise feed-forward sub-layers, leading to the POST-LN~\\citep{vaswani2017attention} and PRE-LN~\\citep{wang2019learning,xiong2020layer} variants respectively.\n\nFormally, given an input sequence $\\Xm \\in \\mathbb{R}^{n \\times d_{v}}$, with $n$ tokens of dimension $d_{v}$, the single-head unmasked scaled dot-product self-attention\\footnote{Our analysis also easily generalizes to the case of cross-attention.} is defined as:\n\\begin{equation}\n\\label{eq:self_att}\n    \\Sm^{\\ell} := \\Am^\\ell \\Xm^\\ell \\Wm^{V} , \\text{ where } \\Am^\\ell = \\text{softmax}\\left( \\frac{1}{\\sqrt{d_k}}\\Xm^{\\ell}\\Wm^{Q}\\left(\\Xm^{\\ell}\\Wm^{K}\\right)^\\top \\right) ,\n\\end{equation}\nwhere the softmax function is applied independently across each row, and the superscript $\\ell$ indexes the $\\ell$-th layer.\nThe matrices $\\Wm^Q, \\Wm^{K} \\in \\mathbb{R}^{d_v \\times d_k}$ and $\\Wm^{V} \\in \\mathbb{R}^{d_{v} \\times d_v}$ are learnable parameters, and each layer is initialized with an independent set of weights. In the literature, the matrices $\\Xm^{\\ell}\\Wm^{Q}, \\Xm^{\\ell}\\Wm^{K}, \\Xm^{\\ell}\\Wm^{V}$ are referred to as queries, keys and values, respectively. The complete Transformer block, in the absence of layer normalization, can be written recursively as:\n\n\\begin{align}\n    & \\Zm^{\\ell} = \\alpha_1 \\Sm^{\\ell}+ \\Xm^{\\ell} \\\\\n    & \\Ym^{\\ell} = \\sigma(\\Zm^{\\ell} \\Wm^{F_1})\\Wm^{F_2} \\\\\n    & \\Xm^{\\ell+1} = \\alpha_2 \\Ym^\\ell +  \\Zm^\\ell ,\n \\end{align}\nwhere the introduced $\\alpha_1, \\alpha_2$ parameters indicate the strength of the residual block, $\\Wm^{F_1}$, $\\Wm^{F_2} \\in \\mathbb{R}^{d_v \\times d_v}$ \\footnote{In practice, one commonly uses $\\Wm^{F_1} \\in \\mathbb{R}^{d_v \\times d_F}$, $\\Wm^{F_2} \\in \\mathbb{R}^{d_F \\times d_v}$ where $d_F = \\gamma d_v$, with $\\gamma \\in \\{2, 4, 8\\}$. Our results then hold up to a constant factor that depends on $\\gamma$.} are matrices of learnable parameters; we set $\\Xm^0 := \\Xm$, and $\\sigma: \\mathbb{R} \\rightarrow \\mathbb{R}$ is an activation function. In our case, $\\sigma$ is the ReLU function, but we relax this assumption to the linear activation from Section \\ref{sec:forward_pass} on.\n\n\\begin{wrapfigure}{r}{0.25\\textwidth}\n\\vspace{-1.5em}\n\\includegraphics[width=3.5cm]{figures/transformer.pdf}\n\\vspace{-2em}\n\\caption{A single Transformer block.\\vspace{-2.5em}}\n\\label{fig:architecture}\n% \\vspace{-35px}\n\n\\end{wrapfigure} \n% \\vspace{10px}\nAt initialization, each weight is sampled independently from a distribution with zero-mean and variance $\\sigma^2_v = \\frac{1}{d_v}$ for the values and feedforward weights\\footnote{One should explicitly write the layer dependence $\\Wm^{Q,\\ell}, \\Wm^{K,\\ell},\\Wm^{V,\\ell}, \\Wm^{F_1, \\ell},  \\Wm^{F_2, \\ell} $. We at times suppress the $\\ell$ index to improve readability. In case $\\sigma$ is the ReLU function, we set $\\Wm^{F,1}$ to have variance $\\frac{2}{d_v}$.}, and $\\sigma^2_k = \\frac{1}{d_k}$ for the queries and keys. This is the standard ``Xavier''~\\citep{glorot2010understanding} or ``He''~\\citep{he2015delving} initialization, commonly used in deep learning.  \n% layer norm leads to instabilities, cite deepnet and xiong2020layer\n\n\\paragraph{Rank Collapse in Transformers.}\nInterestingly, \\cite{dong2021attention} proved that when the residual branches are omitted, the matrix of the tokens' representations $\\Xm^\\ell$ converges to a rank-1 matrix in which all the representations are the same and equal to a vector $\\bm{x}\\in \\mathbb{R}^{d_v}$, i.e. $\\Xm^\\ell \\to \\bm{1}_{n}\\bm{x}^\\top$, where $\\bm{1}_{d_v}$ is the vector with all ones in $\\mathbb{R}^{d_v}$. Note that this is a slightly stronger notion of a rank-$1$ matrix, as it implies that all the tokens' representations are both perfectly aligned and have the same norm. Indicating the inner product with the usual bracket notations $\\langle \\cdot, \\cdot \\rangle$, and the cosine of the angle between two tokens as $\\theta_{k,k'}$, perfect alignment happens when $\\langle \\Xm^{\\ell}_{k}, \\Xm^{\\ell}_{k'} \\rangle = \\norm{\\Xm^{\\ell}_{k}} \\norm{\\Xm^{\\ell}_{k}} \\cos \\theta_{k,k'}$ with $\\cos \\theta_{k,k'}=1$ for all $k, k' \\in [n]$. Note that perfect alignment together with equal norm between all the tokens implies that all the representations are the same. One of our main contributions is to provide an explanation of how rank collapse affects the gradients of a Transformer at initialization.\n\n\\paragraph{Vanishing Gradient Problem.}\nTraditionally considered one of the core issues that prevents successful training, the vanishing gradient problem has a long and rich history that dates back to before the popularization of deep learning \\citep{hochreiter1991untersuchungen, bengio1994learning}. In its essence, given a loss function  $\\mathcal{L}: \\mathbb{R}^{n \\times d_v} \\to \\mathbb{R}$, vanishing gradients occur when the norm of the gradient of the loss $\\mathcal{L}$ with respect to the parameters of the network $\\Wm$ --- which we indicate as $\\norm{\\frac{\\partial \\mathcal{L}}{\\partial \\Wm}}$ --- is too small to provide enough backpropagating signal, thus hindering gradient-based optimization methods. Despite extensive research toward understanding and overcoming the problem in disparate contexts \\citep{glorot2010init,he2015delving,hanin2018neural,zhang2019fixup}, a formal explanation of its role in relatively new architectures such as Transformers is largely missing in the literature, with a few exceptions ~\\citep{xiong2020layer,wang2022deepnet, huang2020improving}. In our paper (Section \\ref{sec:vanishing_gradients}), we show how vanishing gradient occurs in conjunction with the rank collapse issue identified by \\cite{dong2021attention}. \n\n\n\\paragraph{Signal Propagation in Random Networks at Initialization.}\nAfter addressing the question on the effects of rank collapse, we take a step back and rigorously analyze its causes by looking at how the properties of the input sequence $\\Xm$ are lost/preserved as it propagates through a randomly initialized Transformer. More specifically, we focus on two aspects of the propagating sequence: the expected Frobenius norm $\\Exp \\norm{\\Xm^\\ell}^2$ and the expected inner product between different tokens $\\Exp \\langle\\Xm_k, \\Xm_k'\\rangle$, with $k \\neq k'$. The former is linked to a number of studies on the initialization of neural networks at the \\emph{edge of chaos}  \\citep{poole2016exponential, schoenholz2016deep}, and vanishing/exploding gradients \\citep{hanin2018neural}. The latter quantity describes how the geometry of the feature space changes after applying a Transformer block, and is related to the concept of \\emph{dynamical isometry} \\citep{saxe2013exact}. To understand the evolution of the inner product, we analyze the following measure of correlation \\citep{nachum2021johnson, cho2009kernel}: \n\\begin{equation}\n\\label{eq:tokens_corr}\n    \\rho^\\ell_{kk'} := \\frac{\\Exp\\langle\\Xm^\\ell_{k} , \\Xm^\\ell_{k'}\\rangle}{\\sqrt{\\Exp\\norm{\\Xm_k^\\ell}^2\\Exp\\norm{\\Xm_{k'}^\\ell}^2}}.\n\\end{equation}\nNote that $\\rho^\\ell_{kk'} = 1$ if and only if the $k$-th and $k'$-th tokens are perfectly aligned ($\\cos \\theta_{kk'}=1$). We stress that in our case --- differently from the aforementioned works --- instead of analyzing the relationship between two different data points, we study the relationship between tokens of the same sequence. \n\n% In this paper, we give a possible explanation, showing that rank-1 deficiency with identical rows causes vanishingly small gradients with respect to queries and keys of the self attention layers. \n\n\n\\section{Theoretical Results}\n\\label{sec:theory}\n\n\n\\subsection{Vanishing Gradients for Queries and Keys under Rank Collapse}\n\\label{sec:vanishing_gradients}\nTo investigate the problem of vanishing gradients in the attention layers, we make use of the framework of matrix calculus~\\citep{magnus2019matrix,singh2021analytic}. In particular, we compare the expected Frobenius norm of the gradient of a self-attention layer with respect to its parameters: $\\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\Wm}\\right\\|^2_F$, where here $\\Wm$ indicates one of the keys, queries or values weight matrices. \nDue to the well-known difficulty of computing expectations of the softmax \\citep{daunizeau2017semi, shekhovtsov2018feed}, throughout this manuscript, we make the simplifying assumption that the softmax output is the uniform distribution at initialization, i.e. the $n \\times n$ matrix containing $\\frac{1}{n}$ in each entry.\n\\begin{restatable}[Uniform attention]{ass}{uniformsoftmax}\n\\label{ass:uniform_softmax}\nWe assume that $\\Am^\\ell = \\frac{1}{n} \\bm{1}_{n\\times n}$,\n\\end{restatable}\nwhere $\\bm{1}_{n \\times n}$ is the matrix with all entries equal to $1$.\nCrucially, in Appendix \\ref{app:assumption_unif_soft}, we formally show that \\emph{this assumption holds almost surely} in the limit $d_k\\to\\infty$. There, we also experimentally show that even in the more realistic case where $d_k = d_v \\approx 512$, the empirical simulations provide a surprisingly faithful approximation of the theoretical insights presented in this paper. \n\n\nWe define the mean token $\\bar{\\bm{x}}^{\\ell}$ through its components $\\bar{\\bm{x}}^{\\ell}_i = \\frac{1}{n}\\sum_{k=1}^n \\Xm^{\\ell}_{ki}$, $i \\in [d_v]$. In the following theorem, we compute the expected gradients of an attention layer at initialization, and set the basis for our following analysis. We provide the results only for the queries, as the case for the keys is analogous.\n\n\\begin{tcolorbox}\n\\begin{restatable}[]{lemma}{gradients}\n\\label{lemma:gradients_queries}\nLet $\\Xm^{\\ell}$ be the representations of the input sequence at the $\\ell$-th layer. Under the uniform-attention assumption, we have\n    \\begin{align}\n    \\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\Wm^{V,\\ell}}\\right\\|^2_F &= d_v n \\mathbb{E}\\|\\bar{\\bm{x}}^{\\ell}\\|^2~\\label{eq:jacobian_values};\\\\ \\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\Wm^{Q,\\ell}} \\right\\|^2_F &= \\frac{\\sigma^2_v\\sigma^2_k d_v}{n^2}\\cdot \\Exp \\left[ \\|\\Xm^{\\ell}\\|^2_F \\cdot  \\|(\\Xm^{\\ell})^\\top\\Xm^{\\ell} - n\\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top\\|^2_F\\right]~\\label{eq:jacobian_queries};\\\\ \n    \\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\Xm^{\\ell}}\\right\\|^2_F &\\leq \\frac{8\\sigma^2_q\\sigma^2_k\\sigma^2_v d_kd_v}{n} \\, \\cdot \\mathbb{E} \\norm{(\\Xm^{\\ell})^\\top\\Xm^{\\ell} - n\\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top}^2_F + 2d_v^2\\sigma^2_v \\; .\n\\end{align}\n\\end{restatable}\n\\end{tcolorbox}\n%\\sidak{In order to analyze the gradients concisely, we utilize the approach of matrix derivatives following~\\citep{magnus2019matrix,singh2021analytic} --- and thus employ the notation $\\dfrac{\\partial \\mb Y}{\\partial \\mb X} := \\dfrac{\\partial \\vect_r(\\mb Y)}{\\partial \\vect_r(\\mb X)^\\top}\\,$ where each of the matrix is first row-wise vectorized (denoted by $\\vect_r$). Note, this choice of vectorization is appropriate here as softmax normalizes row-wise as well.}\nWe defer the precise study of the scaling of these quantities as a function of $n$ and $d_v,d_k$, to Section~\\ref{sec:dep_angle}.\nAt this stage, it is crucial to note that $\\frac{1}{n} (\\Xm^{\\ell})^\\top\\Xm^{\\ell} - \\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top$ is the centered empirical covariance matrix of the tokens' representations. It is easy to see that if $\\Xm^{\\ell}$ is a rank-$1$ matrix, then all the rows of $\\Xm^{\\ell}$ are proportional to a fixed $d_{v}$-dimensional vector, and the empirical covariance matrix has all zero entries. Introducing a loss function $\\mathcal{L}: \\mathbb{R}^{n \\times d_v} \\to \\mathbb{R}$, we make the statement on vanishing gradients more formal in the following theorem:\n\\vspace{15px}\n\\begin{tcolorbox}\n\\begin{restatable}[Vanishing gradients under rank collapse]{thm}{vanishinggradients}\n\\label{thm:vanishing_gradients}\n    Suppose that the uniform-attention assumption holds. If additionally $\\Xm^{\\ell}$ for any $l \\in [L]$ has rank-1, and there exists a vector $\\bm{x} \\in \\mathbb{R}^{d}$ such that $\\Xm^\\ell = \\bm{1}_n\\bm{x}^T$, then:\n  \\begin{equation}\n      \\Exp \\norm{\\frac{\\partial \\mathcal{L}}{\\partial \\Wm^{Q,\\ell}}}_F^2 = 0 , \\;\\;\\;\\; \\Exp \\norm{\\frac{\\partial \\mathcal{L}}{\\partial \\Wm^{K,\\ell}}}_F^2 = 0 ,\n  \\end{equation}\n  where the expectation is taken over the weight matrices. This implies that these quantities are vanishing almost surely, due to the non-negativeness of the norm.\n\\end{restatable}\n\\end{tcolorbox}\n\\vspace{15px}\n\nThe proof simply relies on expanding the norm of the gradient of the loss with the aid of the chain rule and then bounding it by the product of the norms of each term of the chain. The final result holds with an application of Lemma~\\ref{lemma:gradients_queries}, in which the rank-$1$ assumption makes $\\mathbb{E}\\norm{\\frac{\\partial \\Sm^{\\ell}}{\\partial \\Wm^{Q,\\ell}}}$ vanish.\n\\vspace{15px}\nIn light of Theorem \\ref{thm:vanishing_gradients}, we can conclude that the rank collapse issue originally identified in \\cite{dong2021attention} corresponds to an initialization in a region of vanishing gradient signal in the subspace of parameters identified by the queries and keys. How can this affect training? One may argue that if rank collapse does not happen in the very first layer, then the corresponding gradients are non-zero, and the rank of the subsequent layers --- affected by rank collapse --- can be increased with the first few steps of gradient descent. In practice, we show empirically in Fig. \\ref{fig:adam_postln} that escaping this pathological landscape is harder in deeper nets.\n\\vspace{15px}\n\\subsection{Forward Signal Propagation and the Importance of Scaling the Residual Branches}\n\\label{sec:forward_pass}\n\\vspace{10px}\nWe now turn our attention to the study of the influence of skip connections in transformers. \\cite{dong2021attention} showed that simply adding skip connections prevents rank collapse. Somewhat surprisingly, we show that while the claim holds for any finite depth, the average angle between different tokens quickly increases with just a few layers, and as $L\\to \\infty$ a Transformer can still lose rank unless the residual branches are adequately initialized. As~\\cite{dong2021attention} showed that layer normalization does not avoid rank collapse, we omit it in our analysis.\nFirstly, we introduce two lemmas on the propagation of inner products (Lemma \\ref{lemma:propagation_of_inner_producets}) and the norm (Lemma \\ref{thm:forward_pass}) of the tokens' representations. \n\\vspace{10px}\n\\begin{tcolorbox}\n\\begin{restatable}[Propagation of inner products]{lemma}{propinnprod}\n\\label{lemma:propagation_of_inner_producets}\n Let $C(\\Xm^\\ell) = \\sum_{k,k'} \\langle \\Xm_{k}^\\ell, \\Xm_{k'}^\\ell \\rangle$ and $\\Xm$ the input sequence. Under the Assumption~\\ref{ass:uniform_softmax} and if $\\sigma$ is the linear activation function, we have that:\n \n \\begin{equation}\n     \\Exp \\left[C(\\Xm^{L})\\right] = (\\alpha_2^2 + 1)^{L}(\\alpha_1^2 + 1)^{L}C(\\Xm)  .\n \\end{equation}\n hence, under the depth scaling for the residual block parameters $\\alpha_1^2 = \\frac{\\tilde{\\alpha}_1}{L}, \\alpha_2^2 = \\frac{\\tilde{\\alpha}_2}{L}$ with $\\tilde{\\alpha}_1, \\tilde{\\alpha}_2 \\in \\mathbb{R}$ independent of $L$, we have that:\n \\begin{equation}\n      \\lim_{L\\to \\infty} \\Exp[C(\\Xm^L)] = \\text{e}^{\\tilde{\\alpha}_1 + \\tilde{\\alpha}_2}C(\\Xm).\n \\end{equation}\n\\end{restatable}\n\\end{tcolorbox}\n\\vspace{15px}\nNote that $C(\\Xm^\\ell) = n^2 \\norm{\\bar{\\bm{x}}^\\ell}^2 $. The lemma on the propagation of the norm is slightly more involved:\n\n\n\\begin{tcolorbox}\n\\begin{restatable}[Propagation of the norm]{lemma}{forwardpass}\n\\label{thm:forward_pass}\n  Let $\\Xm^{L}$ be the representations of the input sequence at the final layer. Under the assumptions of Lemma \\ref{lemma:propagation_of_inner_producets}, we have that:\n \\begin{equation}\n     \\Exp \\norm{\\Xm^{L}}_{F}^2 = n (\\alpha_2^2+1)^{L}\\alpha_1^2 \\sum_{k=0}^{L-1}(\\alpha_1^2+1)^k \\norm{\\bar{\\bm{x}}}^2 + (\\alpha_2^2+1)^{L} ||\\Xm||_F^2  ,\n \\end{equation}\n hence, under the depth scaling for the residual block parameters $\\alpha_1^2 = \\frac{\\tilde{\\alpha}_1}{L}, \\alpha_2^2 = \\frac{\\tilde{\\alpha}_2}{L}$ with $\\tilde{\\alpha_1}, \\tilde{\\alpha_2} \\in \\mathbb{R}$ independent of $L$, we have that:\n \\begin{equation}\n     \\lim_{L\\to \\infty} \\Exp \\norm{\\Xm^{L}}_{F}^2 = n \\text{e}^{\\tilde{\\alpha}_2}(\\text{e}^{\\tilde{\\alpha}_1} - 1)\\norm{\\bar{\\bm{x}}}^2 + \\text{e}^{\\tilde{\\alpha}_2} ||\\Xm||_F^2.\n \\end{equation}\n\\end{restatable}\n\\end{tcolorbox}\nThe proof of Lemma \\ref{thm:forward_pass} consists in expanding $\\Exp \\norm{\\Xm^{L}}_{F}^2$ according to the defining equations for the Transformer, and simplifying the expression by using iterated expectations $\\Exp \\norm{\\Xm^{L}}_{F}^2 = \\mathbb{E}[\\Exp [\\norm{\\Xm^{L}}_{F}^2 | \\Xm^{\\ell}]]$ to exploit the conditional independence between different layers, and then computing the expectations using the independence assumption on the weights. The expression on the right-hand side will then depend on $ \\Xm^{\\ell}$ only through its norm $\\norm{\\Xm^{\\ell}}$ and the norm of the mean token $\\norm{\\bar{\\bm{x}}^{\\ell}}^2$. Using Lemma \\ref{lemma:propagation_of_inner_producets} then allows us to unroll the recursion and get the final result. The complete proof, together with the proof of Lemma \\ref{lemma:propagation_of_inner_producets}, can be found in Appendix \\ref{app:forward_pass}. \n\nThe previous Lemma provides theoretical justification that scaling the residual branches by setting the alpha parameters to be $\\mathcal{O}(1/\\sqrt{L})$ allows both the norm of the propagating input and the inner products between different tokens to be approximately preserved. Hence, the information contained in the input is not lost, even in the infinite depth limit.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/residual_scaling_post.pdf}\n    \\caption{Effect of the residual scaling to the norm of the gradients of the network at initialization with respect to some loss. From left to right: (a) the cosine of the angle between tokens increases with depth. Note how larger values of $\\alpha_1, \\alpha_2$ imply a faster token alignment with depth (Theorem~\\ref{thm:exp_cosine}). Subplots (b) and (c) show the gradients of the queries-keys and values parameters respectively by increasing depth, compared to the corresponding norms of the first layer. Gradients for the queries-keys diminish with depth, while the opposite happens for the values. We use POST-LN to disentangle the effect of the variance of the input.}\n    \\label{fig:residual_scaling}\n\\end{figure}\n\n\\paragraph{Residual Scaling Preserves Correlations.}\nWe now prove that without the depth-dependent residual scaling (i.e. with $\\alpha_1=\\alpha_2=1$) the correlation between the tokens quickly increases, and reaches perfect alignment in the infinite depth limit.\n%which by itself is enough to cause vanishing gradients with depth for the queries and keys, as shown in Fig.~\\ref{fig:residual_scaling}.\nMore specifically, our argument shows that in this limit, the correlation between different tokens $\\rho^\\ell_{k,k'}$ as in Eq.~\\eqref{eq:tokens_corr}  converges to 1, implying rank collapse. Furthermore, we show how setting the residual parameters $\\alpha_1$ and $\\alpha_2$ as dictated by Theorem \\ref{thm:forward_pass}, ensures that the correlation measure is dependent on the input in a non-trivial way even at infinite depth. \nTo this end, we introduce the average correlation at layer $\\ell$: \n\\begin{equation}\n    \\rho^\\ell = \\frac{1}{n(n-1)}\\sum_{k\\neq k'}\\rho^\\ell_{kk'} .\n\\end{equation}\nNote that $\\rho^\\ell = 1$ if and only if every pair of tokens is perfectly aligned.\n\nWe are now ready to formalize the influence of the $1/\\sqrt{L}$-scaling on the correlation between tokens' representations by stating Theorem \\ref{thm:exp_cosine}.\n\\begin{tcolorbox}\n\\begin{restatable}[]{thm}{expectedcosine}\n\\label{thm:exp_cosine} Let the input tokens have the same norm, i.e. $\\norm{\\Xm_k} = \\norm{\\bm{x}} \\; \\forall k \\in [n]$ for some $\\bm{x} \\in \\mathbb{R}^{d_v}$. Under the depth scaling for the residual block parameters $\\alpha_1^2 = \\frac{\\tilde{\\alpha}_1}{L}, \\alpha_2^2 = \\frac{\\tilde{\\alpha}_2}{L}$ with $\\tilde{\\alpha}_1, \\tilde{\\alpha}_2 \\in \\mathbb{R}$ independent of $L$, we have that: \n  \\begin{equation}\n      \\lim_{L\\to \\infty}\\rho^\\ell = \\frac{n \\text{e}^{\\tilde{\\alpha}_1}C(\\Xm) }{(n-1)[(\\text{e}^{\\tilde{\\alpha}_1} - 1)C(\\Xm)  + n\\norm{\\Xm}_{F}^2]} - \\frac{1}{n-1} .\n  \\end{equation}\n On the other hand, if $\\alpha_1, \\alpha_2 \\neq 0$ are some constants independent of $L$, we have that:\n \\begin{equation}\n     \\lim_{L\\to \\infty}\\rho^\\ell = 1.\n \\end{equation}\n\n\\end{restatable}\n\\end{tcolorbox}\n% \\vspace{10px}\nThe proof consists in noting that due to the symmetry of the problem at initialization, for a fixed layer the expected norm of each token is the same. Hence, by our definition of $\\rho^\\ell_{kk'}$, we can write $\\Exp\\langle\\Xm^\\ell_k, \\Xm^\\ell_{k'}\\rangle = \\rho^\\ell_{kk'} \\Exp\\norm{\\bm{x}^\\ell}^2$. By summing over the $k,k'$ indexes, the resulting equation will depend on $\\Exp[C(\\Xm^\\ell)]$ and $\\Exp \\norm{\\Xm^\\ell}^2$, which can be expanded using Lemma \\ref{lemma:propagation_of_inner_producets} and \\ref{thm:forward_pass} respectively. The result is then given by solving for $\\rho^\\ell$. \n\n\\begin{wrapfigure}{r}{0.4\\textwidth}\n\\vspace{-5px}\n\\centering\n\\includegraphics[scale = 0.38]{figures/scalings_correlation.pdf}\n\\caption{Evolution of Correlation in Transformers with (dashed lines) and without (solid lines) $1/\\sqrt{L}$-scaling for PRE-LN, POST-LN and without layer normalization (No-LN).}\n\\label{fig:corr}\n\\vspace{-5px}\n\\end{wrapfigure}\n\n\\vspace{10px}\nNote that under the $1/\\sqrt{L}$-scaling, the correlation term is one if and only if $C(\\Xm) = n\\norm{\\Xm}^2$, which holds in the degenerate case where all the input tokens are perfectly aligned. In Appendix \\ref{app:res_scaling_proofs}, we give precise formulas for the expected correlations at any depth, showing that $\\rho^\\ell$ reaches values close to one even for relatively shallow networks when the $1/\\sqrt{L}$-scaling is not adopted (see also Fig. \\ref{fig:residual_scaling}~(left)). Additionally, in Fig.~\\ref{fig:corr}, we empirically show that in the presence of the $1/\\sqrt{L}$-scaling, layer normalization (either PRE or POST) does not significantly affect the evolution of the correlations. On the other hand, without the residual scaling, PRE-LN seems to alleviate the rate of increase of $\\rho^\\ell_{kk'}$. It is intriguing that most deep Transformer models use this configuration~\\citep{brown2020language}. We provide more extensive empirical results in Appendix~\\ref{app:more_experiments}.\n\n\\vspace{10px}\nNote that the $1/\\sqrt{L}$ scaling for the residual branches has been previously studied in the context of stabilization of residual networks (see Section \\ref{sec:related_work}), here we extend these results to Transformers and provide new insights on its role in the context of rank preservation.\nFinally, note that by setting $\\tilde{\\alpha}_1, \\tilde{\\alpha}_2 = 0$, we recover the so called \"ReZero\" initialization \\citep{bachlechner2021rezero}. In this context, the $1/\\sqrt{L}$ scaling extends this framework as it allows for wider range of values for $\\tilde{\\alpha}_1, \\tilde{\\alpha}_2$ while still guaranteeing stability. \nWe mention here that extending these results from the linear activation to the ReLU case is known to be a hard problem, due to the technical difficulty of propagating the inner products across ReLU layers that are shared among the tokens (this is the case in the position-wise feed-forward layers in Transformers). Exact formulas can be found only in the case of one ReLU layer with Gaussian inputs in \\cite{cho2009kernel}. \n\n\\subsection{Dependence on the Angle between Tokens and the Input Norm}\n\\label{sec:dep_angle}\nIn this section, we drop the superscript $\\ell$ as it is obvious from context and assume for simplicity that $d_k = d_v$. To gain a better intuition on the factors that affect the gradients and provide additional insights, we study the case in which every pair of distinct tokens are  zero-mean Gaussian random variables, correlated in the same way, i.e $\\rho^\\ell_{ii'} = \\rho$ for $i \\neq i'$ or more precisely\n\\begin{equation*}\n    \\Exp\\left[\\Xm_{i,j}\\Xm_{i',j'}\\right] = \\begin{cases}\n    0 & j\\ne j' \\ \\ \\text{(independent dimensions)}\\\\\n    \\sigma^2_x & i=i', j=j'\\\\\n    \\rho\\sigma^2_x & i\\ne i', j=j'\n    \\end{cases}.\n\\end{equation*}\nTo see that this equation satisfies our definition of the correlation metric, note that $\\Exp[\\norm{\\Xm_i}^2] = d\\sigma_x^2$ and $\\Exp\\langle \\Xm_{i}, \\Xm_{i'} \\rangle = d\\sigma_x^2 \\rho$, for $i \\neq i'$.\nThen, the expected norm of the gradients for the values (Eq.~\\eqref{eq:jacobian_values}) simplifies to\n\\begin{equation}\n     \\Exp \\norm{\\frac{\\partial \\Sm}{\\partial \\wV}}_F^2 = \\sigma_x^2 d^2 \\left(1 + \\rho (n-1)  \\right).\n\\label{eq:grad_V}\n\\end{equation}\n\nBy making the additional assumption that the norm and the correlation propagate independently, the respective norm for the queries --- and symmetrically the keys --- (Eq.~\\eqref{eq:jacobian_queries}) reduces to:\n\\begin{equation}\n    \\Exp \\left\\|\\frac{\\partial \\Sm}{\\partial \\wQ}\\right\\|_F^2 = \\sigma_x^6 \\frac{(n-1)}{n} (1 - \\rho)^2 d (n + d).\n\\label{eq:grad_Q}\n\\end{equation}\nIn Appendix~\\ref{sec:constan_cosine_analysis} we provide a rigorous proof, that relies on Isserlis theorem~\\citep{isserlis1918formula} to compute higher-order moments. The above expressions reveal the different dependencies on four main actors, that we inspect separately here. The gradients of the queries depend via a cubic function on the \\emph{variance of the input, $\\sigma_x^2$}, compared to a linear for the values. This provides an additional interpretation of the successful use of layer normalization, as in~\\cite{xiong2020layer}, either in the POST-LN or PRE-LN format, that standardizes the input variance $\\sigma_x^2$ to the value $1$. \n\n\nNext, we emphasize the dependence on the \\emph{correlation between the tokens}, also illustrated in Fig.~\\ref{fig:residual_scaling}. Importantly, note how the queries/keys have opposite monotonic functional dependence with respect to $\\rho$ compared to the values. As revealed by Theorem~\\ref{thm:exp_cosine} and Fig.~\\ref{fig:residual_scaling}~(center), inappropriate scaling of the residual branches can already lead to this phenomenon even in a relatively shallow network. \n\nFinally, Eq.~\\eqref{eq:grad_V} and~\\eqref{eq:grad_Q} reveal a different scaling in terms of the \\emph{embedding size $d$} and the \\emph{sequence length $n$} due to the self-attention operation itself. We hope that the identification of the different dependencies in the gradients of the parameters will inspire a new line of works aimed at solving some of the difficulties in training Transformers. \n\n\n\\subsection{Are Adaptive Methods really needed for training Transformers?}\n\\label{sec:adaptive}\n\n\\begin{wrapfigure}{r}{0.50\\textwidth}\n\\vspace{-15px}\n\\centering\n\\includegraphics[width=6.5cm]{figures/adam_adaptiviy.pdf}\n\\caption{Adaptive learning rates computed by Adam in Transformers.}\n\\label{fig:adam-adap}\n\\vspace{-10px}\n\\end{wrapfigure} \n\nThe existence of the discrepancy in the magnitude of the gradients with respect to the weights $\\Wm^{Q}, \\Wm^{K}$ and $\\Wm^{V}$, might explain the success of adaptive optimization algorithms, as illustrated in Fig.~\\ref{fig:adam-adap}, where we plot the effective learning rate computed by Adam~\\citep{kingma2014adam} in a toy encoder task (more details in Appendix~\\ref{app:experimental_setup}). Hence, we embark on a preliminary exploration to train a Transformer architecture with SGD with the intent of matching Adam's performance. \nBased on our theory, we propose a simple architectural modification, an inverse temperature scaling $\\tau \\in \\mathbb{R}$ inside the softmax:\n\\begin{equation}\n    \\Sm^{\\ell} := \\text{softmax}\\left( \\frac{\\tau}{\\sqrt{d_k}}\\Xm^{\\ell}\\Wm^{Q}\\left(\\Xm^{\\ell}\\Wm^{K}\\right)^\\top \\right) \\Xm^\\ell \\Wm^{V}.\n\\end{equation}\nA direct consequence of our analysis is that $\\tau$ allows controlling the magnitude of the gradients for the queries and keys' parameters.\n\nWe evaluate our proposal, consisting of residual scaling and the aforementioned inverse temperature parameters, on the widely used IWSLT14 German-to-English (De-En) benchmark translation task. All details regarding the experimental setup and the choice of inverse temperature used are provided in Appendix~\\ref{app:experimental_setup}. We train a Transformer encoder-decoder of varying depth with SGD, after removing all normalization layers and adequately initializing the residual connections. For our training with SGD, we avoid using any learning rate warm-up, as commonly done for Adam, and instead use a step-scheduler to decrease the learning rate at 40\\% and 80\\% of training. We compare against the following methods that make use of Adam; POST-LN and PRE-LN refer to the aforementioned alternatives to apply layer normalization. We also compare against other successful techniques that rely on specific initializations to avoid layer normalization, such as ReZero~\\citep{bachlechner2021rezero} and T-Fixup~\\citep{zhang2019fixup}. We report  the average BLEU score~\\citep{papineni2002bleu} across 5 runs in Fig.~\\ref{fig:translation_fig} and Table~\\ref{tab:translation_results}.\n\n\\begin{figure}\n\\CenterFloatBoxes\n\\begin{floatrow}\n\\ffigbox\n  {\\includegraphics[width=0.5\\textwidth]{figures/translation.pdf}}\n  {\\caption{\\small{BLEU scores by increasing the number of transformers blocks. `X' Transformer blocks implies in total `X' encoder self-attention, `X' decoder self-attention and `X' decoder cross-attention layers.}}\\label{fig:translation_fig}}\n\\killfloatstyle\n\\ttabbox\n   {\n   \\small\n   \\renewcommand{\\arraystretch}{1.2}\n   \\begin{tabular}[b]{lc}\\hline\n     Method (6L-Encoder / 6L-Decoder) & BLEU $\\uparrow$ \\\\ \\hline\n     SGD POST-LN & 31.36\\\\\n     SGD res-scale & 32.79\\\\\n     SGD temperature & \\textbf{35.69} \\\\ %impressive. what's the trick? ;)\n     \\hline\n     Adam POST-LN~\\citep{vaswani2017attention} & 35.39\\\\\n     Adam PRE-LN~\\citep{vaswani2017attention} & 35.10 \\\\\n     ReZero~\\citep{bachlechner2021rezero} & 34.55\\\\\n     T-Fixup~\\cite{zhang2019fixup} & 35.59\\\\\n     \\hline\n     \\end{tabular}\n  }\n  {\\caption{\\small{BLEU scores for the IWSLT14 German-to-English translation task. \\textit{SGD res-scale} refers to the training of SGD without layer normalization and initialization of the residual scaling $a_1 = a_2 = \\frac{1}{\\sqrt{L}}$. \\textit{SGD temperature} additionally employs an inverse temperature inside the softmax.}\\label{tab:translation_results}}}\n\\end{floatrow}\n\\end{figure}\n\nOur proposed method considerably improves training with SGD, keeping up and in some cases surpassing any results achieved by the Adam optimizer. We are also able to train deeper networks without the use of layer normalization. We leave for future work to further investigate modifications or alternatives to the self-attention operation.\n\n\\section{Related Work}\n\\label{sec:related_work}\n%\\lorenzo{TODO: reduce this section and make it less focused on the training part}\nOur work builds upon the rich literature on forward and backward signal\npropagation in random neural networks \\citep{poole2016exponential,schoenholz2017deepinformationpropagation,pmlr-v80-xiao18a,NIPS2017_d9fc0cdb,orvieto2021vanishing, noci2021precise, zavatone2021exact}.\n% Similarly to \\cite{poole2016exponential}, but in the context of\n% Transformers, we study how the Frobenius norm of an input signal evolves in expectation with depth, leading us to introduce an appropriate rescaling of the residual connections that stabilizes the forward pass. \nThe $1/\\sqrt{L}$ scaling scheme has been investigated in the literature for the stabilization of residual networks \\citep{hanin2018start, arpit2019initialize, allen2019convergence, hayou2021stable}.\n\nOur work draws inspiration from a series of recent works studying the rank of the representations of random feed-forward neural networks at initialization \\citep{jonas, daneshmand2020batch}. In the context of Transformers, \\cite{dong2021attention} has recently identified the rank collapse issue object of study of the present work. Thanks to our analysis of the backward pass, we are able to demonstrate that rank collapse in Transformer architectures leads to vanishingly small gradients of queries and keys, thereby preventing effective training and allowing us to complete the analysis of \\citep{dong2021attention}.\n\n%Despite their empirical success, successful optimization in Transformers is notoriously hard and heavily depends on a number of training tricks, architectural design choices and on the use of adaptive optimization methods. \nAmong the architectural components in Transformers, layer normalization is, arguably, one of the most important -- and debated -- ones \\citep{chen2018best,wang2019learning,nguyen2010estimating,xiong2020layer}. In the original architecture \\citep{vaswani2017attention}, layer normalization is used to stabilize the forward pass by reducing the variance of the inputs to the following sublayer. Our analysis of the forward pass shows that its inclusion is not strictly necessary for the purpose of controlling the norm of the representations. For a theoretical analysis of signal propagation in the presence of layer norm, we refer the reader to \\cite{xiong2020layer}.\n\nAdditionally, our theoretical study of the backward pass provides a rigorous explanation of the empirically observed discrepancy between the magnitude of the gradients of the queries and the values, which \\cite{liu2020understanding} hypothesize to be one of the causes of the success of adaptive methods in training Transformers \\citep{liuadam,zhang2020adaptive,huang2020improving}.\n\nFinally, properly rescaled residual connections have been found to be beneficial for training Transformers by a number of recent research works \\citep{zhang2019fixup, bachlechner2021rezero, wang2022deepnet}. However, none of these studies characterize the impact of skip connections on rank propagation, while our analysis suggests a theoretically-grounded way to stabilize it.\n\n\\section{Conclusions and Future Work}\nIn this paper, we showed how, at initialization, rank collapse and more generally high correlation in the tokens, causes vanishing gradients of the queries and keys of a Transformer architecture. While residual connections help mitigate rank collapse at finite depth, we showed that they alone cannot prevent high alignments of the tokens' representations --- unless properly scaled by a $1/\\sqrt{L}$-factor. Finally, we have also discovered counter-intuitive dependencies on the variance of the input, embedding size, and sequence length, potentially causing large differences between the gradients of queries/keys compared to the values' parameters. Hence, we conclude that one of the strengths of Transformers lies in their carefully designed architecture together with an adequate initialization. \nFinally, we gave preliminary evidence that one of the factors contributing to the higher efficacy of Adam compared to SGD in training Transformers arises from the disproportionate magnitude of gradients as postulated by our theory.\nNonetheless, other factors might further accentuate the difference between these two algorithms during training, leaving the door open for further research regarding the benefits of adaptive optimization methods with Transformers.\n\n\n\\section*{Acknowledgements}\n\nWe thank our colleague Jonas Kohler who provided insights and expertise that greatly assisted the research.\n\n\n\\bibliographystyle{plainnat}\n\\bibliography{main}\n\n\n\n\n\\newpage\n\n\\appendix\n\\addcontentsline{toc}{section}{Appendix}\n\\part{Appendix} % Start the appendix part\n\\parttoc % Insert the appendix TOC\n\n\n\\section{Proof of Theorems}\nRecall the defining equations of a Transformer:\n\\begin{align*}\n    & \\Zm^{\\ell} = \\alpha_1 \\Sm^{\\ell}+ \\Xm^{\\ell} \\\\\n    & \\Ym^{\\ell} = \\sigma(\\Zm^{\\ell} \\Wm^{F,1})\\Wm^{F,2} \\\\\n    & \\Xm^{\\ell+1} = \\alpha_2 \\Ym^{\\ell} +  \\Zm^{\\ell} ,\n\\end{align*}\nwhere the self attention layers are defined as follows:\n\\begin{equation*}\n    \\Sm^{\\ell} := \\Am^\\ell \\Xm^\\ell \\Wm^{V} , \\text{ where } \\Am^\\ell = \\text{softmax}\\left( \\frac{1}{\\sqrt{d_k}}\\Xm^{\\ell}\\Wm^{Q}\\left(\\Xm^{\\ell}\\Wm^{K}\\right)^\\top \\right).\n\\end{equation*}\nWe remark that when it is clear from the context, we suppress the $\\ell$ index to improve readability.\n\n\\textbf{Initialization}: Recall that we initialize our weights with the so called ``Xavier''~\\citep{glorot2010understanding} or ``He''~\\citep{he2015delving} initialization: each weight is sampled independently from a distribution with zero-mean and variance $\\sigma^2_v = \\frac{1}{d_v}$ for the values and feedforward weights and $\\sigma^2_k = \\frac{1}{d_k}$ for the queries and keys. \n\n\\textbf{Kronecker Delta}: we introduce the Kronecker Delta notation \n\\begin{equation*}\n    \\delta_{ab} = \n    \\begin{cases}\n        1 & a=b \\\\\n        0 & a\\neq b\n    \\end{cases}\n\\end{equation*}\nand, similarly: $\\delta_{a\\neq b} = 1 - \\delta_{ab}$.\n\n\\textbf{Notation}: in this section, we also adopt the following shorthand notation for the argument of the softmax $\\Mm:=\\frac{\\Xm\\wQ{\\wK}^\\top \\Xm^\\top}{\\sqrt{d_k}}$. We will first compute the gradients with respect to values, queries and input (note that the gradients of the keys have the form as the queries, hence we omit the derivation). Recall that for a matrix $\\Xm$, we use $\\Xm_k$ to indicate its $k$-th row. Finally, we indicate with $\\bm{1}_{n\\times n}$ the $n \\times n$ matrix with all ones, with $\\bm{1}_n$ the columns vector with all ones, and with $\\Im_n$ the $n$-dimensional identity matrix. \n\n\\subsection{Backward Pass: Proofs of Lemma \\ref{lemma:gradients_queries} and Theorem \\ref{thm:vanishing_gradients}}\nIn this section, we now look at the proofs of Lemma \\ref{lemma:gradients_queries} and Theorem \\ref{thm:vanishing_gradients}. We will first introduce our notation for the gradients, as well as some useful properties of the Kronecker product.\n\n\\subsubsection{Preliminaries}\nFor the gradients, we avoid directly working with tensors by vectorizing the matrices in a \\textit{row-wise} fashion ($\\vect_r$) and arranging the Jacobian in the numerator layout. More formally,\n\n$$\\dfrac{\\partial \\mb Y}{\\partial \\mb X} := \\dfrac{\\partial \\vect_r(\\mb Y)}{\\partial \\vect_r(\\mb X)^\\top}\\,.$$\n\nAlongside this, we use the following rule ($\\kro$ is the Kronecker product):\n\\begin{align}\n\\label{eq:matrix-derivative}\n \\frac{\\partial \\mb A \\mb W \\mb B}{\\partial \\mb W} = \\mb A \\kro \\; \\mb B^\\top\\,.\n\\end{align}\n\nFor the proof of this rule, we refer to \\cite{singh2021analytic}, and to \\cite{magnus2019matrix} for a complete introduction to matrix calculus. \nWe will also use the following well-known properties of the Kronecker product.\n\\begin{tcolorbox}\n\\begin{lemma}\n\\label{lemma:prop_kro}\nGiven the matrices $\\Am \\in \\mathbb{R}^{n \\times m}$, $\\Bm \\in \\mathbb{R}^{p \\times q}$, $\\Cm \\in \\mathbb{R}^{m \\times r}$, $\\Dm \\in \\mathbb{R}^{q \\times s}$, then the following holds:\n\\begin{equation}\n    \\label{eq:trace_kro}\n    \\tr(\\Am \\kro \\Bm) = \\tr(\\Am)\\tr(\\Bm) ,\n\\end{equation}\nand \n\\begin{equation}\n    \\label{eq:prod_kro}\n    (\\Am \\kro \\Bm)(\\Cm \\kro \\Dm) = (\\Am \\Cm)\\kro(\\Bm \\Dm).\n\\end{equation}\n\\end{lemma}\n\\end{tcolorbox}\n\n\\subsubsection{Proof of Lemma~\\ref{lemma:gradients_queries}}\nIn Lemma \\ref{lemma:grads_SA} and Lemma \\ref{lemma:grads_SA_X} we compute the gradients with respect to the queries, values and $\\Xm$, respectively. Then we use these results to prove Lemma \\ref{lemma:gradients_queries} by computing the expectation of the Frobenius norms. \n\n\\begin{tcolorbox}\n\\begin{lemma}[Gradients of Self Attention for parameter matrices]\n\\label{lemma:grads_SA}\nThe gradients of the self attention layer defined in Eq.~\\eqref{eq:self_att} have the following form:\n\\begin{align*}\n     & \\frac{\\partial \\Sm}{\\partial \\wV} =    \\soft\\left(\\frac{\\Xm\\wQ{\\wK}^\\top \\Xm^\\top}{\\sqrt{d_k}}\\right)\\Xm\\, \\kro \\Im_{d_v} \\, \\\\\n     & \\frac{\\partial \\Sm}{\\partial \\wQ} = \\left(\\Im_n \\kro {\\wV}^\\top\\Xm^\\top\\right) \\frac{\\partial \\Am}{\\partial \\Mm} \\left(\\frac{\\Xm\\kro\\Xm\\wK}{\\sqrt{d_k}}\\right),\n\\end{align*}\nwhere the gradients of the softmax with respect to its inputs are as follows:\n\\begin{equation}\n\\label{eq:grad_soft_complete}\n    \\frac{\\partial \\Am}{\\partial \\Mm} = \\blockdiag\\Bigg( \\dfrac{\\partial \\Am_{i}}{\\partial \\Mm_{i}^\\top}\\Bigg)\n\\end{equation}\nand where $ \\dfrac{\\partial \\Am_{i}}{\\partial \\Mm_{i}^\\top}=\\diag(\\Am_{i}) - \\Am_{i}\\Am_{i}^\\top$ with $\\Am_{i}$ being the $i$-th row of $\\Am$ in column vector format.\\\\\nFinally, note that under the uniform-attention assumption, Eq.~\\eqref{eq:grad_soft_complete} simplifies to:\n\\begin{equation}\n\\label{eq:grad_soft}\n    \\frac{\\partial \\Am}{\\partial \\Mm} = \\frac{1}{n}\\Im_n \\kro \\left(\\Im_n - \\frac{1}{n}\\bm{1}_{n\\times n} \\right).\n\\end{equation}\n\\end{lemma}\n\\end{tcolorbox}\n\n\n\\begin{proof}\nLet's start with the simple case of the values' weights $\\wV$. Using the rule in Eq.~\\eqref{eq:matrix-derivative}, it is immediate that:\n\\begin{equation*}\n \\frac{\\partial \\Sm}{\\partial \\wV} =    \\soft\\left(\\frac{\\Xm\\wQ{\\wK}^\\top \\Xm^\\top}{\\sqrt{d_k}}\\right)\\Xm\\, \\kro \\Im_{d_v} = \\Am\\Xm\\, \\kro \\Im_{d_v} \\,.\n\\end{equation*}\nFor the queries, a simple application of the chain rule and then again Eq.~\\eqref{eq:matrix-derivative} gives:\n\\begin{align*}\n    \\frac{\\partial \\Sm}{\\partial \\wQ} &= \\frac{\\partial \\Sm}{\\partial \\Am} \\frac{\\partial \\Am}{\\partial \\wQ}\n     = \\frac{\\partial \\Sm}{\\partial \\Am} \\frac{\\partial \\Am}{\\partial \\Mm} \\frac{\\partial \\Mm}{\\partial \\wQ} \\\\\n     &= \\left(\\Im_n \\kro {\\wV}^\\top\\Xm^\\top\\right) \\frac{\\partial \\Am}{\\partial \\Mm} \\left(\\frac{\\Xm\\kro\\Xm\\wK}{\\sqrt{d_k}}\\right) \\, ,\n\\end{align*}\nwhich is the desired results. Finally, for the gradients of the softmax note that:\n\\begin{equation*}\n    \\frac{\\partial{\\Am_{pq}}}{\\partial \\Mm_{ij}} =\n    \\frac{\\partial}{\\partial \\Mm_{ij}}\\frac{\\exp(\\Mm_{pq})}{\\sum_k \\exp(\\Mm_{pk})} =  \\delta_{ip}\\delta_{jq} \\Am_{ij} - \\delta_{ip} \\Am_{iq}\\Am_{ij} .\n\\end{equation*}\nBy writing the above expression in the matrix notation described above, we obtain the desired result. More specifically, the block diagonal structure is given from the term $\\delta_{ip}$ which stems from the fact that the softmax is applied row-wise. \n\\end{proof}\n\\begin{tcolorbox}\n\n%Sidak: Added here as a separate lemma not to overcrowd the previous.\n\\begin{lemma}[Gradients of Self Attention with respect to the Embedding matrix]\n\\label{lemma:grads_SA_X}\nThe gradients of the self attention layer with respect to the embedding matrix $\\Xm$ defined in Eq.~\\eqref{eq:self_att} have the following form\n\\small{\n\\begin{align}\n\\label{eq:grad_inp}\n    \\frac{\\partial\\Sm}{\\partial\\Xm} \n    &= \\frac{1}{\\sqrt{d_k}}(\\Im_n\\kro \\wVT\\Xm^\\top)\\,\\frac{\\partial \\Am}{\\partial \\Mm}  \\,\\left(\\Im_n\\kro\\Xm\\wK\\wQT + \\Km_{nn}(\\Im_n\\kro\\Xm\\wQ\\wKT)\\right) \\,+ \\,\\Am\\kro{\\wV}^\\top,\n\\end{align}}\\normalsize\nwhere the gradients of the softmax with respect to its inputs are denoted by $\\frac{\\partial \\Am}{\\partial \\Mm}$ as before.\n\\end{lemma}\n\\end{tcolorbox}\n\\begin{proof}\nRemember that we defined $\\Sm = \\soft(\\extra{\\frac{1}{\\sqrt{d_k}}}\\Xm\\wQ\\wKT\\Xm^\\top)\\Xm\\wV$. Alongside with our previous shorthands $\\Am$, $\\Mm$, let us define the remaining $\\Xm\\wV$ as a matrix $\\Tm$, so that $\\Sm=\\Am\\,\\Tm$. Both $\\Am$ and $\\Tm$ are functions of $\\Xm$. So the matrix differential can be written as:\n\\begin{align}\n    \\frac{\\partial\\Sm}{\\partial\\Xm} &=  \\frac{\\partial\\Sm}{\\partial\\Am}\\frac{\\partial\\Am}{\\partial\\Xm} + \\frac{\\partial\\Sm}{\\partial\\Tm}\\frac{\\partial\\Tm}{\\partial\\Xm}\\\\\n    &=  \\frac{\\partial\\Sm}{\\partial\\Am}\\frac{\\partial\\Am}{\\partial\\Mm} \\frac{\\partial \\Mm}{\\partial\\Xm} + \\frac{\\partial\\Sm}{\\partial\\Tm}\\frac{\\partial\\Tm}{\\partial\\Xm}\\\\\n    &= (\\Im_n\\kro \\wVT\\Xm^\\top)\\,\\frac{\\partial \\Am}{\\partial \\Mm}  \\,\\frac{\\partial \\Mm}{\\partial \\Xm} + (\\Am\\kro\\Im_d)(\\Im_n\\kro\\wVT)\\\\\n    &= (\\Im_n\\kro \\wVT\\Xm^\\top)\\,\\frac{\\partial \\Am}{\\partial \\Mm}  \\,\\frac{\\partial \\Mm}{\\partial \\Xm} + (\\Am\\kro\\wVT)\\label{eq:grad-SA-X}\n\\end{align}\n\n\nNext, we use the matrix differential and then the identification theorem of matrix derivatives to compute the matrix gradient $\\frac{\\partial \\Am}{\\partial \\Xm}$\n\\begin{align*}\n    \\mathrm{d} \\Am &=  \\extra{\\frac{1}{\\sqrt{d_k}}}\\mathrm{d}(\\Xm)\\, \\wQ\\wKT\\Xm^\\top + \\extra{\\frac{1}{\\sqrt{d_k}}}\\Xm\\wQ\\wKT  \\,\\mathrm{d}(\\Xm^\\top).\n\\end{align*}\n\nVectorizing both sides:\n\\begin{align*}\n    \\mathrm{d} \\vect_r(\\Am) &=  \\extra{\\frac{1}{\\sqrt{d_k}}}(\\Im_n\\kro\\Xm\\wK\\wQT)\\mathrm{d}(\\vect_r(\\Xm))\\, + \\,\\extra{\\frac{1}{\\sqrt{d_k}}}(\\Xm\\wQ\\wKT \\kro\\Im_n) \\,\\mathrm{d}(\\vect_r(\\Xm^\\top)) \\\\\n    &=  \\extra{\\frac{1}{\\sqrt{d_k}}}(\\Im_n\\kro\\Xm\\wK\\wQT)\\mathrm{d}(\\vect_r(\\Xm))\\, + \\,\\extra{\\frac{1}{\\sqrt{d_k}}}(\\Xm\\wQ\\wKT \\kro\\Im_n) \\Km_{dn}\\,\\mathrm{d}(\\vect_r(\\Xm)).\n    % &=  (\\Im_n\\kro\\Xm\\wK\\wQT)\\mathrm{d}(\\vect_r(\\Xm))\\, + \\,\\Km_{nn}(\\Im_n\\kro\\Xm\\wQ\\wKT ) \\,\\mathrm{d}(\\vect_r(\\Xm)) \\\\\n\\end{align*}\n\nRecall, for an arbitrary matrix $\\Bm\\in\\mathbb{R}^{m\\times n}$, the commutation matrix $\\Km_{mn}$ transforms columnwise vectorization into rowwise vectorization. More precisely,\n\\begin{align*}\n    \\Km_{mn}\\vect_c(\\Bm) = \\vect_c(\\Bm^\\top)\n\\end{align*}\nand $\\vect_c(\\Bm) = \\vect_r(\\Bm^\\top)$. Therefore, for rowwise vectorization, we have a similar result:\n\\begin{align*}\n    \\Km_{mn}\\vect_r(\\Bm^\\top) &= \\vect_r(\\Bm)\\\\\n    \\vect_r(\\Bm^\\top) &= \\Km_{nm}\\vect_r(\\Bm),\n\\end{align*}\nwhere in the last line we used the fact the commutation is a permutation matrix, so $\\Km_{mn}^{-1}=\\Km_{mn}^\\top=\\Km_{nm}$. Thus, we get the required matrix derivative as follows:\n$$\\frac{\\partial \\Am}{\\partial \\Xm} = \\extra{\\frac{1}{\\sqrt{d_k}}}\\Im_n\\kro\\Xm\\wK\\wQT + \\extra{\\frac{1}{\\sqrt{d_k}}}(\\Xm\\wQ\\wKT\\kro\\Im_n)\\Km_{dn}\\,.$$\nNext, we will use a property of commutation matrix to make things simpler (Theorem 7.9, \\cite{magnus2019matrix}):\n$$\n\\frac{\\partial \\Am}{\\partial \\Xm} = \\extra{\\frac{1}{\\sqrt{d_k}}}\\Im_n\\kro\\Xm\\wK\\wQT + \\extra{\\frac{1}{\\sqrt{d_k}}}\\Km_{nn}(\\Im_n\\kro\\Xm\\wQ\\wKT).\n$$\nPlugging this into the above Eq.~\\eqref{eq:grad-SA-X}, we get:\n\\begin{align*}\n     \\frac{\\partial\\Sm}{\\partial\\Xm} \n    &= \\extra{\\frac{1}{\\sqrt{d_k}}}(\\Im_n\\kro \\wVT\\Xm^\\top)\\,\\frac{\\partial \\Am}{\\partial \\Mm}  \\,\\left(\\Im_n\\kro\\Xm\\wK\\wQT + \\Km_{nn}(\\Im_n\\kro\\Xm\\wQ\\wKT)\\right) \\,+ \\,\\Am\\kro{\\wV}^\\top.\n\\end{align*}\nAs a sanity check, we can calculate if the shapes of the matrices are consistent. LHS should be a $nd\\times nd$ matrix, while the constituent matrices of the first term on RHS: $\\Im_n\\kro {\\wV}^\\top\\Xm^\\top\\in\\mathbb{R}^{nd\\times n^2}$, $\\frac{\\partial \\Am}{\\partial \\Mm} \\in\\mathbb{R}^{n^2\\times n^2}$, the additive term next to it is a $n^2\\times nd$ matrix, and the second term on RHS is a Kronecker product of a $n\\times n$ and a $d\\times d$ matrix. \n\\end{proof}\n\n\\begin{tcolorbox}\n\\gradients*\n\\end{tcolorbox}\n\n\n\\begin{proof}\nHere, we suppress the index $\\ell$.\n\n\\textbf{Gradient with respect to the values matrix.}\n\nRecall that from Lemma \\ref{lemma:grads_SA} we have that:\n\\begin{equation*}\n    \\frac{\\partial \\Sm}{\\partial \\wV} =    \\soft\\left(\\frac{\\Xm\\wQ{\\wK}^\\top \\Xm^\\top}{\\sqrt{d_k}}\\right)\\Xm\\, \\kro \\Im_{d_v} \\overset{\\text{Ass. } \\ref{ass:uniform_softmax}}{=} \\frac{1}{n}\\bm{1}_{n \\times n} \\Xm \\kro \\Im_{d_v}.\n\\end{equation*}\nBy direct computation:\n\\begin{align*}\n    \\norm{\\frac{\\partial \\Sm}{\\partial \\wV}}_F^2 = \\tr\\left(\\frac{\\partial \\Sm}{\\partial \\wV} \\, \\frac{\\partial \\Sm}{\\partial \\wV}^\\top\\right) \\overset{\\eqref{eq:prod_kro}}&{=} \\frac{1}{n^2} \\tr((\\bm{1}_{n\\times n} \\Xm \\Xm^\\top \\bm{1}_{n\\times n}) \\kro \\Im_{d_v})\\\\\n    \\overset{\\eqref{eq:trace_kro}}&{=}\\frac{1}{n^2} \\tr(\\bm{1}_{n\\times n} \\Xm \\Xm^\\top \\bm{1}_{n\\times n})\\tr(\\Im_{d_v})\\\\\n    &= \\frac{d}{n^2}\\tr( \\Xm \\Xm^\\top \\bm{1}_{n\\times n}\\bm{1}_{n\\times n}) \\\\\n    &= \\frac{d_v}{n}\\tr( \\Xm \\Xm^\\top \\bm{1}_{n\\times n}) \\\\\n    &= \\frac{d_v}{n}\\tr(\\Xm\\Xm^\\top \\bm{1}_n \\bm{1}_n^\\top) \\\\\n    &= \\frac{d_v}{n} \\bm{1}_n^\\top\\Xm \\Xm^\\top \\bm{1}_n \\\\\n    &= \\frac{d_v}{n} \\|\\Xm^\\top\\bm{1}_n\\|^2=d_vn\\norm{\\bm{\\bar{x}}}^2 .\n\\end{align*}\n\n\\textbf{Gradients with respect to the queries/keys matrix.}\n\nFirst, recall the expression for the gradient of the softmax under the uniform-attention assumption (Eq.~\\eqref{eq:grad_soft}):\n\\begin{equation*}\n    \\frac{\\partial \\Am}{\\partial \\Mm} = \\frac{1}{ n}\\Im_n \\kro \\left(\\Im_n - \\frac{1}{n}\\bm{1}_{n\\times n} \\right).\n\\end{equation*}\n\nHence, we can rewrite the expression of Lemma \\ref{lemma:grads_SA} for the gradients of the queries as:\n\\begin{align*}\n\\frac{\\partial \\Sm}{\\partial \\wQ} \n     &= \\left(\\Im_n \\kro {\\wV}^\\top\\Xm^\\top\\right) \\frac{\\partial \\Am}{\\partial \\Mm} \\left(\\frac{\\Xm\\kro\\Xm\\wK}{\\sqrt{d_k}}\\right)\\\\ &= \\frac{1}{\\sqrt{d_k}n}\\left(\\Im_n \\kro {\\wV}^\\top\\Xm^\\top\\right) \\left[\\Im_n \\otimes \\left(\\Im_n - \\frac{1}{n}\\bm{1}_{n\\times n} \\right)\\right] \\left(\\Xm\\kro\\Xm\\wK\\right)\\\\\n     &= \\frac{1}{\\sqrt{d_k}n}\\Xm \\kro \\left[ {\\wV}^\\top\\Xm^\\top\\left(\\Im_n - \\frac{1}{n}\\bm{1}_{n\\times n} \\right)\\Xm\\wK\\right],\n\\end{align*}\nwhere in the last step we have used twice the property of the Kronecker product in Eq.~\\eqref{eq:prod_kro} of Lemma~\\ref{lemma:prop_kro}.\n\nHence,\n\\begin{align*}\n\\left\\|\\frac{\\partial \\Sm}{\\partial \\wQ}\\right\\|_F^2 = \\tr \\left(\\frac{\\partial \\Sm}{\\partial \\wQ} \\frac{\\partial \\Sm}{\\partial \\wQ} ^T\\right) \\overset{\\eqref{eq:trace_kro}}{=} \\frac{1}{d_k n^2} \\|\\Xm\\|^2_F \\cdot\\left\\|{\\wV}^\\top\\Xm^\\top\\left(\\Im_n - \\frac{1}{n}\\bm{1}_{n\\times n} \\right)\\Xm\\wK\\right\\|_F^2,\n\\end{align*}\nwhere we have used the property on the trace of the Kronecker product (Lemma \\ref{lemma:prop_kro}, Eq.~\\eqref{eq:trace_kro}). \nNote that if we are conditioning on $\\Xm$, then we only have to take the expectation of the last term with respect to the weights $\\Wm^K$ and $\\Wm^V$. Let us call $\\Lm :=\\Im_n - \\frac{1}{n}\\bm{1}_{n\\times n}$ for notation simplicity.\n\nNote: for a matrix $\\Wm\\in\\mathbb{R}^{d\\times d}$ whose entries $w_{ij}\\sim\\mathcal{N}(0, \\sigma^2)$, then $\\Exp \\Wm\\Wm^\\top=d\\sigma^2 \\, \\Im_d$. Thus, exchanging the order of trace and expectation, we can write: \n\\begin{align*}\n    \\Exp\\|\\wVT\\Xm^\\top\\Lm\\Xm\\wK\\|^2_F &= \\Exp\\tr(\\wVT\\Xm^\\top\\Lm\\Xm\\wK \\, \\cdot\\,\\wKT\\Xm^\\top\\Lm\\Xm\\wV)\\\\\n    &=\\tr(\\Xm^\\top\\Lm\\Xm\\Exp[\\wK\\wKT]\\Xm^\\top\\Lm\\Xm\\Exp[\\wV\\wVT])\\\\\n    &=\\sigma^2_v\\sigma^2_k d_kd_v \\tr(\\Xm^\\top\\Lm\\Xm\\,\\cdot\\,\\Xm^\\top\\Lm\\Xm)\\\\\n    &=\\sigma^2_v\\sigma^2_k d_kd_v\\|\\Xm^\\top\\Lm\\Xm\\|^2_F\\\\ &=\\sigma^2_v\\sigma^2_k d_kd_v\\norm{\\Xm^\\top(\\Im_n - \\frac{1}{n}\\bm{1}_{n}\\bm{1}_{n}^\\top)\\Xm}^2_F \\\\\n    &=\\sigma^2_v\\sigma^2_k d_kd_v\\|\\Xm^\\top\\Xm - n\\bar{\\bm{x}}\\bar{\\bm{x}}^\\top\\|^2_F,\n\\end{align*}\nwhere, $\\bar{\\bm{x}}=\\frac{1}{n}\\Xm^\\top\\bm{1}_n\\in\\mathbb{R}^d$ is the mean embedding. Multiply this by $\\frac{1}{d_kn^2} \\|\\Xm\\|^2_F$ to get the final answer.\n\n\\textbf{Gradient with respect to the input.}\n\nPlugging in the values of $\\frac{\\partial \\Am}{\\partial \\Mm}$ and $\\Am$ under the uniform-attention assumption into Eq.~\\eqref{eq:grad_inp} gives rise to the following:\n\\begin{align*}\n     \\frac{\\partial\\Sm}{\\partial\\Xm} \n    &= \\frac{1}{n\\extra{\\sqrt{d_k}}}\\Im_n\\kro \\wVT\\Xm^\\top\\left(\\Im_n - \\frac{1}{n}\\bm{1}_{n}\\bm{1}_{ n}^\\top\\right)\\Xm\\wK\\wQT  \\\\& + \\frac{1}{n\\extra{\\sqrt{d_k}}}\\left[\\Im_n\\kro \\wVT\\Xm^\\top \\left(\\Im_n - \\frac{1}{n}\\bm{1}_{n}\\bm{1}_{ n}^\\top\\right)\\right] \\Km_{nn}(\\Im_n\\kro\\Xm\\wQ\\wKT) \\, \\\\&+\n    \\frac{1}{n}\\bm{1}_{n\\times n}\\kro\\wVT\n\\end{align*}\nLet's refer to the matrices on the right-hand side as $\\Am_1, \\Am_2, \\Am_3$ respectively. We compute the expected squared Frobenius norm of these as follows:\n\nFor $\\Am_3$:\n\\begin{align*}\n    \\Exp[\\norm{\\Am_3}^2_F] &= \\frac{1}{n^2}\\Exp[\\tr(n \\bm{1}_{n\\times n}\\kro\\wVT\\wV)] \\\\\n    \\overset{\\eqref{eq:trace_kro}}&{=}\\frac{1}{n}\\tr(\\bm{1}_{n\\times n})\\tr(\\Exp[\\wV\\wVT]) = d_v^2\\sigma^2_v.\n\\end{align*}\n\nSimilarly, for $\\Am_1$:\n\\begin{align}\n    \\Exp[\\norm{\\Am_1}^2_F] &= \\frac{\\sigma^2_q\\sigma^2_k\\sigma^2_v d_kd_v}{n^2} \\, \\tr(\\Im_n)\\tr(\\Xm^\\top\\Lm\\Xm \\,\\cdot\\,\\Xm^\\top\\Lm\\Xm)\\\\\n    &=\\frac{\\sigma^2_q\\sigma^2_k\\sigma^2_v d_kd_v}{n} \\, \\norm{\\Xm^\\top\\Xm - n\\bar{\\bm{x}}\\bar{\\bm{x}}^\\top}^2_F\\\\\n    &=\\sigma^2_q\\sigma^2_k\\sigma^2_v d_kd_v n \\, \\norm{\\frac{1}{n}\\Xm^\\top\\Xm - \\bar{\\bm{x}}\\bar{\\bm{x}}^\\top}^2_F .\n\\end{align}\nFinally, for $\\Am_2$:\n\\begin{align}\n    &\\Exp[\\norm{\\Am_2}^2_F]\\\\\n    &= \\frac{1}{n^2 d_k}\\Exp\\left[\\tr( \\left[\\Im_n\\kro \\wVT\\Xm^\\top \\Lm \\right] \\Km_{nn}(\\Im_n\\kro\\Xm\\wQ\\wKT\\wK\\wQT\\Xm^\\top) \\Km_{nn}\\left[\\Im_n\\kro \\Lm\\Xm\\wV \\right])\\right]\\\\\n    &= \\frac{\\sigma^2_q\\sigma^2_k\\sigma^2_v d_kd_v}{n^2}\\,\\tr((\\Im_n\\kro\\Xm\\Xm^\\top)[\\Im_n \\kro \\Lm\\Xm\\Xm^\\top\\Lm]) \\\\\n    &= \\frac{\\sigma^2_q\\sigma^2_k\\sigma^2_v d_kd_v}{n}\\,\\tr(\\Xm^\\top\\Lm\\Xm \\,\\cdot\\,\\Xm^\\top\\Lm\\Xm) =\\Exp[\\norm{\\Am_1}^2_F],\n\\end{align}\nwhere in the second line we have taken the expectation inside and used the fact that $\\Km_{nn}$, being a commutation matrix, is orthogonal. Then, by simple properties of Kronecker product and cyclic property of trace, we have the result, which is the same as that for $\\Am_1$. \\\\\n\nFinally, by the triangle inequality\n\\begin{align}\n    \\Exp\\left\\|\\frac{\\partial\\Sm}{\\partial\\Xm}\\right\\|^2 &\\le 2 \\Exp\\|\\Am_1+\\Am_2\\|^2 + 2 \\Exp\\|\\Am_3\\|^2 \\\\\n    &\\le 4 \\Exp\\|\\Am_1\\|^2 + 4 \\Exp\\|\\Am_2\\|^2+ 2\\Exp\\|\\Am_3\\|^2\\\\\n    &= 8 \\Exp\\|\\Am_1\\|^2 + 2\\Exp\\|\\Am_3\\|^2\\\\\n    &= \\frac{8\\sigma^2_q\\sigma^2_k\\sigma^2_v d_kd_v}{n}\\norm{\\Xm^\\top\\Xm - n\\bar{\\bm{x}}\\bar{\\bm{x}}^\\top}^2_F + 2d_v^2\\sigma_v^2.\n\\end{align}\nThis completes the proof.\n\\end{proof}\n\n\\subsubsection{Proof of Theorem \\ref{thm:vanishing_gradients}}\n\n\\begin{tcolorbox}\n\\vanishinggradients*\n\\end{tcolorbox}\n\nBefore starting the proof, it is interesting to note that, even though the gradients of queries and keys vanish in the rank collapse regime~(i.e. $\\norm{\\Xm^\\top\\Xm - n\\bar{\\bm{x}}\\bar{\\bm{x}}^\\top}=0$), the gradient with respect to the values and the input does not~(see Theorem~\\ref{lemma:gradients_queries}). From this simple remark, we can conclude that, even in the rank collapse regime, information still propagates in the backward pass. In Section~\\ref{sec:adaptive}~(main paper), we show that even if gradients effectively propagate, the phenomenon studied in this theorem still greatly affects training.\n\n\\begin{proof}\nBy using the chain rule and the fact that for two matrixes $\\Am, \\Bm$ we have that $\\norm{\\Am\\Bm}_F^2 \\leq \\norm{\\Am}_F^2\\norm{\\Bm}_F^2$, we can upper bound the gradient as:\n\\begin{align*}\n    \\norm{\\frac{\\partial \\mathcal{L}}{\\partial{\\Wm^{Q,\\ell}}}}_F^2 &\\leq \\prod_{i=\\ell+1}^{L-1} \\norm{\\frac{\\partial \\Xm^{i+1}}{\\partial \\Xm^i}}_{F}^2 \\norm{\\frac{\\partial \\mathcal{L}}{\\partial \\Xm^L}}_F^2 \\norm{\\frac{\\partial \\Xm^{\\ell+1}}{\\partial \\Wm^{Q,\\ell}}}_F^2 \\\\\n    &\\leq \\prod_{i=\\ell+1}^{L-1} \\norm{\\frac{\\partial \\Xm^{i+1}}{\\partial \\Xm^i}}_{F}^2 \\norm{\\frac{\\partial \\mathcal{L}}{\\partial \\Xm^L}}_F^2 \\norm{\\frac{\\partial \\Xm^{\\ell+1}}{\\partial \\Zm^{\\ell}}}_F^2 \\norm{\\frac{\\partial \\Zm^\\ell}{\\partial \\Wm^{Q, \\ell}}}_F^2 \\\\\n    &\\leq \\prod_{i=\\ell+1}^{L-1} \\norm{\\frac{\\partial \\Xm^{i+1}}{\\partial \\Xm^i}}_{F}^2 \\norm{\\frac{\\partial \\mathcal{L}}{\\partial \\Xm^L}}_F^2 \\norm{\\frac{\\partial \\Xm^{\\ell+1}}{\\partial \\Zm^{\\ell}}}_F^2 \\left(\\norm{\\frac{\\partial \\alpha_1 \\Sm^{\\ell}}{\\partial \\Wm^{Q, \\ell}}}_F^2 + \\underbrace{\\norm{\\frac{\\partial \\Xm^{\\ell}}{\\partial \\Wm^{Q, \\ell}}}_F^2}_{=0} \\right) ,\n\\end{align*}\nwhere we recall that $\\Zm^{\\ell} = \\alpha_1 \\Sm^{\\ell}+ \\Xm^{\\ell}$ and in the last step we have used that $\\Xm^\\ell$ does not depend on $\\Wm^{Q,\\ell}$, hence the gradient vanishes. By taking expectation and using the tower property, we have that:\n\\begin{equation*}\n    \\Exp \\norm{\\frac{\\partial \\mathcal{L}}{\\partial{\\Wm^{Q,\\ell}}}}_F^2 \\leq \\Exp\\left[\\underbrace{\\Exp\\left[\\prod_{i=\\ell+1}^{L-1} \\norm{\\frac{\\partial \\Xm^{i+1}}{\\partial \\Xm^i}}_{F}^2 \\norm{\\frac{\\partial \\mathcal{L}}{\\partial \\Xm^L}}_F^2 \\norm{\\frac{\\partial \\Xm^{\\ell+1}}{\\partial \\Zm^{\\ell}}}_F^2\\right]}_{=:G(\\Xm^\\ell)} \\norm{\\frac{\\partial \\alpha_1 \\Sm^{\\ell}}{\\partial \\Wm^{Q, \\ell}}}_F^2   \\right],\n\\end{equation*}\nwhere the expectations are taken with respect to $\\Xm^\\ell$ for the outer one and conditioning on $\\Xm^\\ell$ for inner one. Indeed, the first three terms only depend on the network values after $\\Xm^\\ell$. Now, a repeated application of the tower property in $G(\\Xm^\\ell)$, together with the results on the gradients of Lemma \\ref{lemma:gradients_queries}, easily shows that $G(\\Xm^\\ell)$ stays bounded under our hypothesis. To see this one can also simply note that, since the softmax and its derivatives are almost surely bounded, the boundedness of $G(\\Xm^\\ell)$ is implied by an analogous statement for a vanilla linear MLP~(i.e removing the softmax). In this setting, the random variable inside the expectation in $G(\\Xm^\\ell)$ is a finite linear combination of Gaussian products --- which has bounded expectation.\n\nAll in all, we have that\n\\begin{equation*}\n    \\Exp \\norm{\\frac{\\partial \\mathcal{L}}{\\partial{\\Wm^{Q,\\ell}}}}_F^2\\le \\Exp\\left[ B_{\\Xm^\\ell}\\norm{\\frac{\\partial \\alpha_1 \\Sm^{\\ell}}{\\partial \\Wm^{Q, \\ell}}}_F^2\\right],\n\\end{equation*}\nwhere $B_{\\Xm^\\ell}$ is an almost-surely-bounded function of $\\Xm^{\\ell}$. Hence, to show that $\\Exp \\norm{\\frac{\\partial \\mathcal{L}}{\\partial{\\Wm^{Q,\\ell}}}}_F^2=0$, we now just need to show that:\n\\begin{equation*}\n    \\Exp \\norm{\\frac{\\partial \\alpha_1 \\Sm^{\\ell}}{\\partial \\Wm^{Q, \\ell}}}_F^2 = 0\n\\end{equation*}\nunder the rank-1 hypothesis for $\\Xm^\\ell$.\nLet $\\Xm_{1}^\\ell, \\dots \\Xm_{n}^\\ell \\in \\mathbb{R}^{d_v}$  be the representations for the $n$ tokens. Under the rank-1 assumption, each token can be written as a multiple of a single vector $\\bm{x} \\in \\mathbb{R}^{d_v}$, and hence there exists $a_1, \\dots, a_n \\in \\mathbb{R}$ such that $\\Xm_1 = a_1 \\bm{x}, \\dots, \\Xm_n = a_n \\bm{x}$. From Lemma \\ref{lemma:gradients_queries}, we know that:\n\\begin{equation*}\n    \\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\wQ} \\right\\|^2_F = \\frac{\\sigma^2_v\\sigma^2_k d^2}{n^2}\\cdot \\Exp \\left[ \\|\\Xm^{\\ell}\\|^2_F \\cdot  \\|(\\Xm^{\\ell})^\\top\\Xm^{\\ell} - n\\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top\\|^2_F\\right] .\n\\end{equation*}\nThe mean token simplifies to $\\bar{\\bm{x}}^l = \\frac{\\bm{x}}{n}\\sum_k a_k$ and hence $\\left(\\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top\\right)_{ij} = \\frac{1}{n^2} (\\sum_{k}a_k)^2 x_ix_j$. Similarly, $\\left((\\Xm^{\\ell})^\\top\\Xm^{\\ell}\\right)_{ij} = \\sum_k a_k^2 x_i x_j$. If furthermore all the coefficients $a_i$ are the same (which corresponds to the rank collapse assumption $\\Xm^{\\ell}=\\bm{1}_{n}\\bm{x}^T$ analyzed here), then it is easy to see that $\\left((\\Xm^{\\ell})^\\top\\Xm^{\\ell}\\right)_{ij} - n \\left(\\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top\\right)_{ij} = 0 \\; \\forall i,j$ and hence $\\|(\\Xm^{\\ell})^\\top\\Xm^{\\ell} - n\\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top\\|^2_F = 0$.\n\\end{proof}\n\n\\subsection{Gradient Analysis of Section \\ref{sec:dep_angle}}\n\\label{sec:constan_cosine_analysis}\n\nThroughout this section we assume that between every pair of tokens, the same dimension is a zero-mean Gaussian random variable with the same correlation, meaning that\n\\begin{equation*}\n    \\Exp\\left[\\Xm_{i,j}\\Xm_{i',j'}\\right] = \\begin{cases}\n    0 & j\\ne j' \\ \\ \\text{(independent dimensions)}\\\\\n    \\sigma^2_x & i=i', j=j'\\\\\n    \\rho\\sigma^2_x & i\\ne i', j=j'.\n    \\end{cases}\n\\end{equation*}\n\nAs we will deal with the computation of 4-th order moments of correlated Gaussian random variables, we will make use of Isserlis theorem \\citep{isserlis1918formula}:\n\\begin{tcolorbox}\n    \\begin{theorem}[Isserlis]\n    Let $X_1, \\dots X_m$ be $m$ zero-mean Gaussian random variables. Then:\n    \\begin{equation}\n        \\Exp[X_1 \\cdots X_m] = \\begin{cases}\n    \\sum_{p\\in P_m^2} \\prod_{(i,j) \\in p} \\Exp[X_iX_j] & m \\text{ even} \\\\\n    0 & m \\text{ odd}\n    \\end{cases} \n    \\end{equation}\n    \n    where $P_m^2$ is the set of all the possible pairings of the indexes $1,\\dots, m$. \n    \\end{theorem}\n\\end{tcolorbox}\nIn particular, we will only need the 4-th order term, which reads:\n\\begin{equation*}\n    \\Exp[X_1X_2X_3X_4] = \\Exp[X_1X_2]\\Exp[X_3X_4] + \\Exp[X_1X_3]\\Exp[X_2X_4] + \\Exp[X_1X_4]\\Exp[X_2X_3].\n\\end{equation*}\n\nNow we can prove Eq.~\\eqref{eq:grad_V} , which we re-state here:\n\\begin{tcolorbox}\n\\begin{equation*}\n    \\Exp \\norm{\\frac{\\partial \\Sm}{\\partial \\wV}}_F^2 = \\sigma_x^2 d^2 \\left(1 + \\rho (n-1)  \\right).\n\\end{equation*}\n\\end{tcolorbox}\nAlso, from Eq.~\\eqref{eq:jacobian_values} we have that \n\\begin{equation*}\n    \\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\Wm^{V,\\ell}}\\right\\|^2_F = dn \\mathbb{E}\\|\\bar{\\bm{x}}^{\\ell}\\|^2.\n\\end{equation*}\n\nNow,\n\\begin{equation*}\n    \\mathbb{E}\\|\\bar{\\bm{x}}^{\\ell}\\|^2 = \\mathbb{E} \\left( \\sum_{i=1}^d (\\bar{\\bm{x}}^{\\ell}_i)^2 \\right).\n\\end{equation*}\nEach $\\bar{\\bm{x}}^{\\ell}_i = \\frac{1}{n} \\sum_{k=1}^n \\Xm^\\ell_{ki}$ is equally distributed with mean \n\\begin{equation*}\n    \\mathbb{E} [\\bar{\\bm{x}}^{\\ell}_i] = \\mathbb{E} \\left[\\frac{1}{n} \\sum_{k=1}^n \\Xm^\\ell_{ki}\\right] = 0\n\\end{equation*}\nand variance\n\\begin{equation*}\n    \\text{Var} [\\bar{\\bm{x}}^{\\ell}_i] = \\text{Var} \\left[\\frac{1}{n} \\sum_{k=1}^n \\Xm^\\ell_{ki}\\right] = \\frac{1}{n^2} \\left(n\\sigma_x^2 + n(n - 1) \\rho \\sigma_x^2 \\right) = \\frac{1}{n} \\sigma_x^2 (1 + \\rho (n - 1)).\n\\end{equation*}\nFinally we get\n\\begin{equation*}\n    \\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\Wm^{V,\\ell}}\\right\\|^2_F = \\sigma_x^2 d^2 (1 + \\rho (n - 1)).\n\\end{equation*}\n\nWe know prove Eq.~\\eqref{eq:grad_Q}, which reads:\n\\begin{tcolorbox}\n\\begin{equation*}\n    \\Exp \\left\\|\\frac{\\partial \\Sm}{\\partial \\wQ}\\right\\|_F^2 = \\sigma_x^6 \\frac{(n-1)}{n} (1 - \\rho)^2 d (n + d) .\n\\end{equation*}\n\\end{tcolorbox}\n\nFor the queries (and the keys respectively), recall from Eq.~\\eqref{eq:jacobian_queries} that\n\\begin{equation*}\n    \\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\Wm^{Q,\\ell}} \\right\\|^2_F = \\frac{\\sigma^2_v\\sigma^2_k d^2}{dn^2}\\cdot \\Exp \\left[ \\|\\Xm^{\\ell}\\|^2_F \\cdot  \\|(\\Xm^{\\ell})^\\top\\Xm^{\\ell} - n\\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top\\|^2_F\\right].  \n\\end{equation*}\nTo proceed, we drop the superscript $\\ell$ and we make the additional assumption that $\\|\\Xm\\|^2_F$ is uncorrelated from the correlation magnitude $\\|\\Xm^\\top\\Lm\\Xm\\|^2_F = \\|\\Xm^\\top\\Xm - n\\bar{\\bm{x}}\\bar{\\bm{x}}^\\top\\|^2_F$.\n\nLet us proceed with an expansion:\n\\begin{align*}\n    \\Exp\\left[\\|\\Xm^\\top\\Lm\\Xm\\|^2_F\\right] &= \\sum_{i,j=1}^d\\Exp\\left[\\left(\\sum_{a,b=1}^n \\Xm_{ai} \\Lm_{ab} \\Xm_{bj}\\right)^2\\right]\\\\\n    &= \\sum_{i,j=1}^d\\sum_{a,b, a', b'=1}^n  \\Lm_{ab}\\Lm_{a'b'}\\Exp\\left[\\Xm_{ai}\\Xm_{a'i} \\Xm_{bj}\\Xm_{b'j}\\right].\\\\\n\\end{align*}\nNow, we have 2 cases: if $i\\ne j$, which gives $d(d-1)$ equal terms, we need to compute\n\\begin{equation*}\n    \\text{A} := \\sum_{a,b, a', b'=1}^n  \\Lm_{ab}\\Lm_{a'b'}\\Exp\\left[\\Xm_{ai}\\Xm_{a'i}\\right]\\cdot \\Exp\\left[\\Xm_{bj}\\Xm_{b'j}\\right],\n\\end{equation*}\nwhere $(i,j)$ is any tuple with $i\\ne j$ and we used uncorrelation of different dimensions. Otherwise, we get $d$ each equal to\n\\begin{equation*}\n    \\text{B} := \\sum_{a,b, a', b'=1}^n  \\Lm_{ab}\\Lm_{a'b'}\\Exp\\left[\\Xm_{ai}\\Xm_{a'i}\\Xm_{bi}\\Xm_{b'i}\\right],\n\\end{equation*}\nwhere $i$ is any index.\n\n\\paragraph{Term A.} Note that\n\\begin{equation*}\n    \\Exp\\left[\\Xm_{ai}\\Xm_{a'i}\\right]\\cdot \\Exp\\left[\\Xm_{bj}\\Xm_{b'j}\\right]= \\begin{cases}\n     \\sigma_x^4 & a=a', b=b',\\quad n^2\\  \\text{terms}\\\\\n     \\rho\\sigma_x^4 & a=a', b\\ne b',\\quad  n^2(n-1) \\ \\text{terms}\\\\\n     \\rho\\sigma_x^4 & a\\ne a', b=b',\\quad  n^2(n-1) \\ \\text{terms}\\\\\n     \\rho^2\\sigma_x^4 & a\\ne a', b\\ne b',\\quad  n^2(n-1)^2 \\ \\text{terms}\n    \\end{cases}.\n\\end{equation*}\nSo basically $A$ is the sum of 3 terms:\n\\begin{align*}\n    &A_1 := \\sigma_x^4\\sum_{a,b} \\Lm_{ab}^2 = (n-1)\\sigma_x^4 \\\\\n    &A_2 := 2\\rho\\sigma_x^4\\sum_{a,b}\\sum_{b'\\ne b}  \\Lm_{ab}\\Lm_{ab'} = -2(n-1)\\rho\\sigma_x^4 \\\\\n    &A_3 := \\rho^2\\sigma^4_x \\sum_{a,b}\\sum_{a'\\ne a}\\sum_{b'\\ne b}\\Lm_{ab}\\Lm_{a'b'} = \\rho^2\\sigma^4_x (n-1),\n\\end{align*}\nwhere we leveraged the following direct calculations:\n\\begin{align*}\n    A_1 &= \\sigma_x^4\\sum_{a,b} \\Lm_{ab}^2\\\\\n    &= \\sigma_x^4\\left(n\\left(\\frac{n-1}{n}\\right)^2 + (n-1)n\\frac{1}{n^2}\\right)\\\\\n    &= \\sigma_x^4\\frac{(n-1)^2+(n-1)}{n}\\\\\n    &= \\sigma_x^4(n-1).\n\\end{align*}\nNext, we compute\n\\begin{align*}\n    A_2 &= 2\\rho\\sigma_x^4\\sum_{a,b}\\Lm_{ab}\\sum_{b'\\ne b}  \\Lm_{ab'}\\\\\n    &= 2\\rho\\sigma_x^4\\left(\\sum_{a}\\Lm_{a,a}\\sum_{b'\\ne b}  \\Lm_{ab'} + \\sum_{a}\\sum_{b\\ne a}\\Lm_{ab}\\sum_{b'\\ne b}  \\Lm_{ab'}\\right)\\\\\n    &= 2\\rho\\sigma_x^4\\left(\\sum_{a}\\frac{n-1}{n}\\left[-(n-1)\\frac{1}{n}\\right] + \\sum_{a}\\sum_{b\\ne a}\\Lm_{ab}\\left[\\frac{n-1}{n} - \\frac{n-2}{n}\\right]\\right)\\\\\n    &=2\\rho\\sigma_x^4\\left(-\\frac{(n-1)^2}{n} - \\frac{1}{n}(n-1)n\\frac{1}{n}\\right)\\\\\n    &= -2\\rho\\sigma_x^4(n-1).\n\\end{align*}\nFinally, similar computations also lead to the last term. To follow the calculations, we invite the reader to draw the matrix $\\Lm$ and to hide the columns over which summations are not performed:\n\\begin{align*}\n    A_3 &=\\rho^2\\sigma^4_x \\sum_{a,b}\\Lm_{ab}\\sum_{a'\\ne a}\\sum_{b'\\ne b}\\Lm_{a'b'}\\\\\n    &= \\rho^2\\sigma^4_x \\left(\\sum_{a}\\Lm_{aa}\\sum_{a'\\ne a}\\sum_{b'\\ne a}\\Lm_{a'b'}+\\sum_{a}\\sum_{b\\ne a}\\Lm_{ab}\\sum_{a'\\ne a}\\sum_{b'\\ne b}\\Lm_{a'b'}\\right)\\\\\n    &= \\rho^2\\sigma^4_x \\left(\\sum_{a}\\Lm_{aa} (1 - \\frac{1}{n}+\\sum_{a}\\sum_{b\\ne a}\\Lm_{ab} (-1\\frac{1}{n}) \\right)  \\\\\n    &= \\rho^2\\sigma^4_x \\left(n (1 - \\frac{1}{n}) (1 - \\frac{1}{n}) + n (n - 1) (- \\frac{1}{n}) (- \\frac{1}{n}) \\right) \\\\\n    &=\\rho^2\\sigma^4_x (n-1).\n\\end{align*}\n\nAll in all, we get:\n\\begin{equation*}\n    \\text{A} = (n-1)(1-\\rho)^2\\sigma^4_x.\n\\end{equation*}\n\n\\paragraph{Term B.}\nWe make use of Isserlis theorem, stating that:\n\\begin{equation*}\n    \\Exp\\left[\\Xm_{ai}\\Xm_{a'i}\\Xm_{bi}\\Xm_{b'i}\\right] = \\underbrace{\\Exp \\Xm_{ai} \\Xm_{a'i}\\Exp\\Xm_{bi}\\Xm_{b'i}}_{Q_1} + \\underbrace{\\Exp \\Xm_{ai} \\Xm_{bi}\\Exp\\Xm_{a'i}\\Xm_{b'i}}_{Q_2} + \\underbrace{\\Exp \\Xm_{ai} \\Xm_{b'i}\\Exp\\Xm_{a'i}\\Xm_{bi}}_{Q_3}.\n\\end{equation*}\n\nBy using our independence assumptions, we get:\n\\begin{equation*}\n    Q_1 = \\sigma_x^4(\\delta_{aa'} + \\rho \\delta_{a\\neq a'})(\\delta_{bb'} + \\rho \\delta_{b\\neq b'}) = \\sigma_x^4(\\delta_{aa'}\\delta_{bb'} + \\rho \\delta_{aa'}\\delta_{b\\neq b'} + \\rho \\delta_{a\\neq a'}\\delta_{bb'} + \\rho^2 \\delta_{a\\neq a'}\\delta_{b\\neq b'}).\n\\end{equation*}\nSimilarly for $Q_2$ and $Q_3$:\n\\begin{equation*}\n    Q_2 =\\sigma_x^4(\\delta_{ab}\\delta_{a'b'} + \\rho \\delta_{ab}\\delta_{a'\\neq b'} + \\rho \\delta_{a\\neq b}\\delta_{a'b'} + \\rho^2 \\delta_{a\\neq b}\\delta_{a'\\neq b'})\n\\end{equation*}\nand\n\\begin{equation*}\n    Q_3 = \\sigma_x^4(\\delta_{ab'}\\delta_{a'b} + \\rho \\delta_{ab'}\\delta_{a'\\neq b} + \\rho \\delta_{a\\neq b'}\\delta_{a'b} + \\rho^2 \\delta_{a\\neq b'}\\delta_{a'\\neq b}).\n\\end{equation*}\n\nHence,\n\\begin{equation*}\n    B = \\sum_{a,b, a', b'=1}^n  \\Lm_{ab}\\Lm_{a'b'}\\Exp\\left[\\Xm_{ai}\\Xm_{a'i}\\Xm_{bi}\\Xm_{b'i}\\right] = \\sum_{a,b, a', b'=1}^n  \\Lm_{ab}\\Lm_{a'b'}(Q_1 + Q_2 + Q_3).\n\\end{equation*} \n\nLet's study it term by term. We will also use $\\Lm_{ab} = (\\delta_{ab} - \\frac{1}{n})$, and so $\\Lm_{ab}\\Lm_{a'b'} = (\\delta_{ab}\\delta_{a'b'} - \\frac{\\delta_{ab}}{n} - \\frac{\\delta_{a'b'}}{n} + \\frac{1}{n^2})$.\n\n\\textbf{First term}: we have that $\\sigma_x^4 \\sum_{aa'b'b'} \\Lm_{ab}\\Lm_{a'b'} Q_1 $ which is equal to (omitting the constant $\\sigma_x^4$):\n\\begin{align*}\n    &=\\sum_{a,a',b,b'} (\\delta_{ab}\\delta_{a'b'} - \\frac{\\delta_{ab}}{n} - \\frac{\\delta_{a'b'}}{n} + \\frac{1}{n^2}) (\\delta_{aa'}\\delta_{bb'} + \\rho \\delta_{aa'}\\delta_{b\\neq b'} + \\rho \\delta_{a\\neq a'}\\delta_{bb'} + \\rho^2 \\delta_{a\\neq a'}\\delta_{b\\neq b'}) \\\\\n    &= \\rho^2\\left(n(n-1) - 2(n-1)(n-1) + (n-1)^2\\right) + \\rho (- 4(n-1) + 2(n-1) ) + n - 2 + 1 \\\\\n    &= \\rho^2(n-1)\\left(n - 2(n-1) + (n-1)\\right) - 2\\rho(n-1) + (n - 1) \\\\\n    &= \\rho^2(n-1) - 2\\rho(n-1) + (n-1).\n\\end{align*}\n\n\\textbf{Second term}: we have that $\\sigma_x^4 \\sum_{aa'b'b'} \\Lm_{ab}\\Lm_{a'b'} Q_2 $ which is equal to (omitting the constant $\\sigma_x^4$):\n\\begin{align*}\n    &=\\sum_{a,a',b,b'}(\\delta_{ab}\\delta_{a'b'} - \\frac{\\delta_{ab}}{n} - \\frac{\\delta_{a'b'}}{n} + \\frac{1}{n^2})(\\delta_{ab}\\delta_{a'b'} + \\rho \\delta_{ab}\\delta_{a'\\neq b'} + \\rho \\delta_{a\\neq b}\\delta_{a'b'} + \\rho^2 \\delta_{a\\neq b}\\delta_{a'\\neq b'}) \\\\\n    &= \\rho^2(n-1)^2 + \\rho(-2n(n-1) + 2(n-1)) + n^2 - 2n + 1 \\\\\n    &= \\rho^2 (n-1)^2 - 2\\rho(n-1)^2 + (n-1)^2.\n\\end{align*}\n\n\\textbf{Third term:}\nwe have that $\\sigma_x^4 \\sum_{aa'b'b'} \\Lm_{ab}\\Lm_{a'b'} Q_3 $ which is equal to (omitting the constant $\\sigma_x^4$):\n\\begin{align*}\n    &=\\sum_{a,a',b,b'}(\\delta_{ab}\\delta_{a'b'} - \\frac{\\delta_{ab}}{n} - \\frac{\\delta_{a'b'}}{n} + \\frac{1}{n^2})(\\delta_{ab'}\\delta_{a'b} + \\rho \\delta_{ab'}\\delta_{a'\\neq b} + \\rho \\delta_{a\\neq b'}\\delta_{a'b} + \\rho^2 \\delta_{a\\neq b'}\\delta_{a'\\neq b}) \\\\\n    &= \\rho^2\\left( n(n-1) - 2(n-1)(n-1) + (n-1)^2\\right) + \\rho(-4(n-1) + 2(n-1)) + n - 2 + 1 \\\\\n    &= \\rho^2(n-1) - 2\\rho(n-1) + (n - 1).\n\\end{align*}\n\nSumming all the three terms, we get:\n\\begin{align*}\n    B = \\sigma_x^4(n-1)\\left[ \\rho^2 (n+1) - 2(n+1)\\rho + (n+1) \\right] = \\sigma_x^4(n-1)(n + 1)(1 - \\rho)^2.\n\\end{align*}\n\n\\textbf{Plugging in the values of A and B} we get:\n\\begin{equation*}\n    \\Exp[\\|\\Xm^\\top \\Lm\\Xm\\|^2_F] = d\\cdot B + d(d-1)\\cdot A = \\sigma^4_x (1 - \\rho)^2 d (n - 1) (n + d),\n\\end{equation*}\nand finally assuming Xavier initialization\n\\begin{align*}\n    \\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\Wm^{Q,\\ell}} \\right\\|^2_F &= \\frac{\\sigma^2_v\\sigma^2_k d^2}{dn^2}\\cdot \\Exp \\left[ \\|\\Xm^{\\ell}\\|^2_F \\cdot  \\|(\\Xm^{\\ell})^\\top\\Xm^{\\ell} - n\\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top\\|^2_F\\right] \\\\\n    &= \\sigma_x^6 \\frac{n - 1}{n} (1 - \\rho)^2 d (n + d).\n\\end{align*}\n\n\\subsection{Forward Pass: Proofs of Lemma \\ref{lemma:propagation_of_inner_producets} and \\ref{thm:forward_pass} }\n\\label{app:forward_pass}\nFirst, we characterize the evolution of the correlations between tokens $\\Xm_{k}, \\Xm_{k'}$ with depth, under the assumptions of Theorem \\ref{thm:forward_pass}, namely uniform-attention assumption, and the adoption of a linear activation.\n\n% \\begin{tcolorbox}\n% \\begin{lemma}[Expectation of matrix product]\n% \\label{lemma:exp_matrix_prod}\n% For a matrix $\\Am\\in\\mathbb{R}^{p\\times q}$ containing i.i.d random entries with variance $\\sigma^2$ and some fixed matrix $\\Bm\\in\\mathbb{R}^{p\\times p}$, we have that \n% \\begin{equation}\n%     \\Exp[\\Am^\\top \\Bm \\Am] = \\sigma^2 \\tr(\\Bm) \\Im_q\n% \\end{equation}\n% \\end{lemma}\n% \\end{tcolorbox}\n\n% \\begin{proof}\n% $\\Exp(\\Am^\\top\\Bm\\Am)_{ij} = 0$ for $i\\neq j$ and the diagonal elements are given by $(\\Am^\\top\\Bm\\Am)_{ii} = \\Am_{:i}^\\top\\Bm\\Am_{:i} = \\tr(\\Bm\\Am_{:i}\\,\\Am_{:i}^\\top)$. Now taking the expectation over $\\Am$ gives the desired result.\n% \\end{proof}\n\\begin{tcolorbox}\n\\begin{lemma}[Expectation of Linear Layers]\n\\label{lemma:exp_linear}\nLet $\\Dm = \\Xm \\Wm$, where $\\Wm\\in\\mathbb{R}^{d\\times d}$ is a random matrix with i.i.d random entries with variance $\\sigma^2 = \\frac{1}{d}$ and $\\Xm\\in\\mathbb{R}^{n\\times d}$ is a fixed matrix:\n\\begin{equation*}\n    \\Exp[\\Dm_{kj}\\Dm_{k'j}] = \\frac{1}{d}\\langle \\Xm_k, \\Xm_{k'} \\rangle\n\\end{equation*}\n\\end{lemma}\n\\end{tcolorbox}\nNote that by summing over the indexes, Lemma \\ref{lemma:exp_linear} implies:\n\\begin{align*}\n    &\\Exp \\norm{\\Dm}_F^2 = \\Exp \\norm{\\Xm}_{F}^2 \\\\\n    &\\Exp C(\\Dm) = \\Exp C(\\Xm).\n\\end{align*}\n\n\\begin{proof}\n\\begin{equation*}\n     \\Exp [\\Dm_{kj} \\Dm_{k'j}] = \\sum_{zz'} \\Xm_{kz} \\Xm_{k'z'} \\Exp[\\Wm_{zj}\\Wm_{z'j}] = \\sigma^2 \\sum_{z}\\Xm_{kz}\\Xm_{k'z} = \\frac{1}{d}\\langle \\Xm_k, \\Xm_{k'} \\rangle.\n\\end{equation*}\n   \n\\end{proof}\n\n\\begin{tcolorbox}\n\\begin{lemma}[Expectation of skip connection]\n\\label{lemma:exp_skip}\n    Let $\\Am, \\Bm \\in \\mathbb{R}^{p \\times q}$. Let $\\Dm := \\alpha\\Am + \\Bm$ with $\\Exp[\\Am | \\Bm] = \\bm{0}$ and $\\alpha \\in \\mathbb{R}$. Then:\n    \\begin{equation}\n        \\Exp\\left[\\Dm_{ij}\\Dm_{i'j}\\right] = \\alpha^2 \\mathbb{E}[\\Am_{ij}\\Am_{i'j}] + \\mathbb{E}[\\Bm_{ij}\\Bm_{ij'}] \n    \\end{equation}\n    holds for all $i,i' \\in [p], j \\in [q]$.\n\\end{lemma}\n\\end{tcolorbox}\nNote that by summing over the indexes, Lemma \\ref{lemma:exp_skip} implies:\n\\begin{align*}\n    &\\Exp \\norm{\\Dm}_F^2 = \\alpha^2\\Exp \\norm{\\Am}_{F}^2 + \\Exp \\norm{\\Bm}_F^2 \\\\\n    &\\Exp C(\\Dm) = \\alpha^2 \\Exp C(\\Am) + \\Exp C(\\Bm).\n\\end{align*}\n\n\\begin{proof}\n\\begin{align*}\n    \\mathbb{E}[\\Dm_{ij}\\Dm_{i'j}] &= \\mathbb{E}\\left[(\\alpha \\Am_{ij} + \\Bm_{ij})(\\alpha \\Am_{i'j} + \\Bm_{i'j})\\right]\\\\\n    &= \\mathbb{E}\\left[\\alpha^2 \\Am_{ij}\\Am_{i'j} + \\alpha \\Am_{ij}\\Bm_{i'j} + \\alpha \\Am_{i'j}\\Bm_{ij} + \\Bm_{ij}\\Bm_{i'j} \\right] \\\\\n    &= \\alpha^2\\mathbb{E}\\left[ \\Am_{kj}\\Am_{i'j}\\right] + \\mathbb{E}\\left[ \\Bm_{ij}\\Bm_{i',j} \\right],\n\\end{align*}\nwhere using iterated expectations $\\alpha \\Exp[\\Am_{i'j}\\Bm_{ij}] = \\alpha \\Exp[\\Exp [\\Am_{i'j} | \\Bm] \\Bm_{ij}]] = 0$ and identically $\\alpha \\Exp [\\Am_{ij}\\Bm_{i'j}] = 0$.\n\\end{proof}\n\n\\begin{tcolorbox}\n\\begin{lemma}[Expectation of Attention Layers]\n\\label{lemma:exp_softmax}\nUnder the uniform-attention assumption:\n\\begin{equation*}\n    \\Exp[\\Sm_{kj}\\Sm_{k'j}] = \\frac{1}{d_vn^2}\\Exp C(\\Xm).\n\\end{equation*}\n\\end{lemma}\n\\end{tcolorbox}\nIn this case, by summing over the indexes we have that:\n\\begin{align*}\n    &\\Exp\\norm{\\Sm}_F^2 = \\frac{\\Exp C(\\Xm)}{n} \\\\\n    & \\Exp C(\\Sm) = \\Exp C(\\Xm).\n\\end{align*}\n\\begin{proof}\nNote that under the uniform-attention assumption:\n\\begin{equation*}\n    \\Sm_{kj} = \\frac{1}{n}\\left(\\bm{1}_{n\\times n}\\Xm \\Wm^V\\right)_{kj} = \\frac{1}{n}\\sum_{zi}\\Xm_{zi}\\Wm^V_{ij}.\n\\end{equation*}\nHence, using the fact that the weights are i.i.d with variance $\\sigma_v^2=\\frac{1}{d_v}$:\n\\begin{align*}\n    \\Exp[\\Sm_{kj}\\Sm_{k'j}] = \\frac{\\sigma_{v}^2}{n^2}\\sum_{z,z'}\\sum_i\\Exp[\\Xm_{zi} \\Xm_{z'i}] = \\frac{1}{d_vn^2}\\sum_{k,k'}\\langle\\Xm_z, \\Xm_{z'}\\rangle = \\frac{1}{d_vn^2}\\Exp C(\\Xm).\n\\end{align*}\n\\end{proof}\n\\begin{tcolorbox}\n\\propinnprod*\n\\end{tcolorbox}\n\n\\begin{proof}\nFirst, note that for the residual blocks we have that $\\Exp [Y^{\\ell}_{kj} | Z^{\\ell}_{k'j}] = 0$ due to the independence assumption on the feedforward weights, and similarly $\\Exp [S^{\\ell}_{kj} | X^{\\ell}_{k'j}] = 0$. Hence, we can use Lemma \\ref{lemma:exp_skip} in both the skip connections of the Transformer architecture.\nTherefore, using Lemma \\ref{lemma:exp_skip} (skip), Lemma \\ref{lemma:exp_linear} (linear) and Lemma \\ref{lemma:exp_softmax} (attention):\n\\begin{align*}\n        &\\mathbb{E}[C(\\Xm^{\\ell+1})] \\\\\n        \\overset{\\text{skip}}&{=} \\alpha_2^2\\mathbb{E}C(\\Ym^\\ell) + \\mathbb{E}C(\\Zm^{\\ell}) \\\\ \n        \\overset{\\text{linear}}&{=} \\alpha_2^2\\mathbb{E}C(\\Zm^{\\ell}) + \\mathbb{E}C(\\Zm^{\\ell}) \\\\ \n        &= (\\alpha_2^2 + 1)\\mathbb{E}C(\\Zm^{\\ell}) \\\\\n        \\overset{\\text{skip}}&{=} (\\alpha_2^2 + 1)\\left(\\alpha_1^2\\mathbb{E}C(\\Sm^{\\ell}) + \\mathbb{E}C(\\Xm^{\\ell})\\right) \\\\ \n        \\overset{\\text{attention}}&{=}  (\\alpha_2^2 + 1)(\\alpha_1^2 + 1) \\mathbb{E}[C(\\Xm^{\\ell})] \\\\ \n        \\overset{\\text{unroll recurs.}}&{=} (\\alpha_2^2 + 1)^{\\ell+1}(\\alpha_1^2 + 1)^{\\ell+1}C(\\Xm) ,\n    \\end{align*}\n    where in the last step we have unrolled the recursion until the input layer.\n    \n    For the limit as $L\\to \\infty$, simply note that:\n    $$\\lim_{L\\to \\infty}\\left(\\frac{\\tilde{\\alpha}_i}{L}+1\\right)^{L} = \\text{e}^{\\tilde{\\alpha}_i} ,$$ \n    with $i \\in \\{1, 2\\}$.\n\\end{proof}\n\nNow we are ready to re-state and prove Lemma \\ref{thm:forward_pass}.\n\\begin{tcolorbox}\n\\forwardpass*\n\\end{tcolorbox}\n\n\\begin{proof}\nThe proof is in the same spirit as Lemma \\ref{lemma:propagation_of_inner_producets} but slightly more involved. Again, using Lemma \\ref{lemma:exp_skip} in both the skip connections of the Transfomer architecture.\nTherefore, using Lemma \\ref{lemma:exp_skip} (skip), Lemma \\ref{lemma:exp_linear} (linear) and Lemma \\ref{lemma:exp_softmax} (attention):\n\\begin{align*}\n    \\mathbb{E}[||\\Xm^{\\ell+1}||_F^2] \\overset{\\text{skip}}&{=} \\alpha_2^2 \\mathbb{E}||\\Ym^{\\ell}||_F^2 + \\mathbb{E}||\\Zm^{\\ell}||_F^2 \\\\\n    \\overset{\\text{linear}}&{=} (\\alpha_2^2+1)\\mathbb{E}||\\Zm^{\\ell}||_F^2 \\\\\n    \\overset{\\text{skip}}&{=} (\\alpha_2^2+1)\\left(\\alpha_1^2 \\mathbb{E}[||\\Sm^{\\ell}||_F^2] + \\mathbb{E}[||\\Xm^{\\ell}||_F^2] \\right) \\\\\n    \\overset{\\text{softmax}}&{=} (\\alpha_2^2+1)\\left(\\frac{\\alpha_1^2}{n} \\mathbb{E}[C(\\Xm^{\\ell})] + \\mathbb{E}[||\\Xm^{\\ell}||_F^2] \\right) \\\\\n    &= (\\alpha_2^2+1)\\frac{\\alpha_1^2}{n} \\mathbb{E}[C(\\Xm^{\\ell})] + (\\alpha_2^2+1) \\mathbb{E}[||\\Xm^{\\ell}||_F^2] \\\\\n    \\overset{\\text{unroll }C(\\Xm^\\ell)}&{=} (\\alpha_2^2+1)^{\\ell+1}\\alpha_1^2(\\alpha_1^2+1)^{\\ell}\\frac{C(\\Xm) }{n} + (\\alpha_2^2+1) \\mathbb{E}[||\\Xm^{\\ell}||_F^2] \\\\\n    \\overset{\\text{unroll }\\norm{\\Xm^\\ell}_F^2}&{=} (\\alpha_2^2+1)^{\\ell+1}\\alpha_1^2\\frac{C(\\Xm) }{n} \\sum_{k=0}^{\\ell}(\\alpha_1^2+1)^k + (\\alpha_2^2+1)^{\\ell+1} ||\\Xm||_F^2,\n\\end{align*}\n\nwhere in the second to last step we have used Lemma \\ref{lemma:propagation_of_inner_producets} and in the last step we have unrolled the recursion for $\\norm{\\Xm^\\ell}_F^2$ until the input layer. \n\nFor the second part, we now show that for a network of $L$ layers, the choice $\\alpha_1^2 =  \\frac{\\tilde{\\alpha}_1}{L}$ and $\\alpha_2^2 = \\frac{\\tilde{\\alpha}_2}{L}$ stabilizes the norm of the activations in the forward pass. Using the product law for the limits, we can study the converges of $(\\alpha_2^2+1)^{\\ell}$ and $\\alpha_1\\sum_{k=0}^{\\ell}(\\alpha_1^2+1)^k$ separately. \n\nLet $i \\in \\{1, 2\\}$. For the latter term we have that: \n\\begin{align*}\n    \\lim_{L\\to \\infty} \\frac{\\tilde{\\alpha}_i}{L}\\sum_{l=0}^{L-1} \\left(1 + \\frac{\\tilde{\\alpha}_i}{L} \\right)^{\\ell} &= \\lim_{L\\to \\infty} \\frac{\\tilde{\\alpha}_i}{L}\\frac{1-\\left(1 + \\frac{\\tilde{\\alpha}_i}{L} \\right)^{\\ell}}{1-1-\\frac{\\tilde{\\alpha}_i}{L}} \\\\\n    &= \\lim_{L\\to \\infty} -1+\\left(1 + \\frac{\\tilde{\\alpha}_i}{L} \\right)^{\\ell}\\\\\n    &= \\text{e}^{\\tilde{\\alpha}_i} - 1  ,\n\\end{align*}\nwhile for the former term we have that $\\lim_{L\\to \\infty}(\\frac{\\tilde{\\alpha}_i}{L}+1)^{\\ell} = \\text{e}^{\\tilde{\\alpha}_i}$. Hence, the norm of the representations converges to:\n\\begin{equation*}\n    \\lim_{L\\to \\infty}\\mathbb{E}[||\\Xm^{\\ell}||_F^2] = \\text{e}^{\\tilde{\\alpha}_2}(\\text{e}^{\\tilde{\\alpha}_1} - 1)\\frac{C(\\Xm) }{n} + \\text{e}^{\\tilde{\\alpha}_2} ||\\Xm||_F^2.\n\\end{equation*}\n\nThe final results as stated in the theorem hold because of the following:\n\n\\textbf{Remark}: note that $C(\\Xm)  = \\sum_{k,k'}\\sum_{j} \\Xm_{kj}\\Xm_{kj'} = \\sum_{j} (\\sum_{k,k'} \\Xm_{kj}\\Xm_{k'j}) = \\sum_{j} (\\sum_{k} \\Xm_{kj})^2 = n^2 \\norm{\\bar{\\bm{x}}}^2$.\n\\end{proof}\n\n\\subsection{Proof of Theorem \\ref{thm:exp_cosine}: Correlations are Preserved under Residual Scaling}\n\\label{app:res_scaling_proofs}\n\n\\begin{tcolorbox}\n\\expectedcosine*\n\\end{tcolorbox}\n\\begin{proof}\nDue to the rotational symmetries of the Gaussian random matrices, if the input  tokens have the same norm, then the expected norm at layer $\\ell \\in [L]$ is also the same across the token's representations. Hence, we can write $\\Exp\\norm{\\Xm^\\ell}_F^2 = n \\Exp\\norm{\\bm{x}^\\ell}^2$, where $\\norm{\\bm{x}^\\ell}^2$ is the norm of every token at layer $\\ell$. Furthermore, by definition of our correlation coefficient $\\rho^l_{kk'}$, we have that $\\Exp\\langle\\Xm^\\ell_k, \\Xm^\\ell_{k'}\\rangle = \\rho^\\ell_{kk'} \\Exp\\norm{\\bm{x}^\\ell}^2$. By summing over the indexes $k,k'$, we can expand the relation as:\n\\begin{equation*}\n    \\underbrace{\\sum_{k,k'}\\Exp\\langle\\Xm^\\ell_k, \\Xm^\\ell_{k'}\\rangle}_{\\Exp C(\\Xm)} = \\sum_{k,k'} \\rho^\\ell_{kk'} \\Exp\\norm{\\bm{x}^\\ell}^2 = (n + \\sum_{k\\neq k'}\\rho^\\ell_{k,k'})\\Exp\\norm{\\bm{x}^\\ell}^2 = \\underbrace{n\\Exp\\norm{\\bm{x}^\\ell}^2}_{\\Exp \\norm{\\Xm^\\ell}_F^2}(1 + (n-1) \\rho^\\ell).\n\\end{equation*}\nBy solving for $\\rho^\\ell$, we have that:\n\\begin{equation*}\n    \\rho^\\ell = \\frac{\\Exp C(\\Xm^\\ell)}{(n-1)\\Exp \\norm{\\Xm^\\ell}^2 } - \\frac{1}{n-1} .\n\\end{equation*}\nNow we plug in the expressions for $\\Exp C(\\Xm^\\ell)$ and $\\Exp \\norm{\\Xm^\\ell}^2 $ with the aid of Lemma \\ref{lemma:propagation_of_inner_producets} and Lemma \\ref{thm:forward_pass}, respectively. Finally, by taking the limits with respect to $L$, we get the desired result.\n\\end{proof}\n\n\n\n\\subsection{Motivation for Assumption~\\ref{ass:uniform_softmax}}\n\\label{app:assumption_unif_soft}\nWe motivate here the following assumption, stated in the main paper. This assumption is crucial to compute expectations involving the softmax function.\n\n\\uniformsoftmax*\n\n\\paragraph{Theoretical analysis.}\nWe first show that this assumption holds when taking $d_k$ to infinity, keeping $d_{v}$ fixed.\n\\begin{tcolorbox}\n\\begin{lemma}\n\\label{app:convergence_A}\nConsider initializing each entry of $\\wQ\\in\\mathbb{R}^{d_{v}\\times d_k}$ and $\\wK\\in\\mathbb{R}^{d_{v}\\times d_k}$ independently with variance $\\sigma^2_k = 2/(d_{v}+d_k)$ --- i.e. Glorot initialization~\\citep{glorot2010understanding}. Let $\\Mm = \\frac{1}{\\sqrt{d_k}}\\Xm^\\ell\\Wm^{Q,\\ell}{\\Wm^{K,\\ell}}^\\top{\\Xm^\\ell}^\\top$; for any $(i,j)\\in[n]\\times[n]$ we have\n\\begin{equation}\n    \\Exp[\\Mm_{i,j} \\ | \\ \\Xm] = 0,\\qquad \\Exp[\\Mm_{i,j}^2 \\ | \\ \\Xm] = \\sigma_k^4 \\cdot \\|\\Xm_{i,:}\\|^2 \\cdot \\|\\Xm_{j,:}\\|^2.\n\\end{equation}\nWhile keeping $d_v<\\infty$ fixed, taking $d_k$ to infinity yields\n\\begin{equation}\n    \\Exp[\\Mm_{i,j}^2 \\ | \\ \\Xm] = \\mathcal{O}\\left(\\frac{1}{d_k^2}\\right).\n\\end{equation}\nIn other words, $\\Mm$ converges to $\\bm{0}_{n\\times n}$ in $L^2$ as $d_k\\to\\infty$. \n\\end{lemma}\n\\end{tcolorbox}\n\\begin{proof}\nFirst, note that\n\\begin{align*}\n    \\Mm_{i,j} = \\frac{1}{\\sqrt{d_k}}\\sum_{a,c=1}^{d_{v}}\\sum_{b=1}^{d_k} \\Xm_{i,a}\\wQ_{a,b}\\wK_{c,b} \\Xm_{j,c}.\n\\end{align*}\nSince $\\wQ$ is independent from $\\wK$ at initialization, $\\Exp[\\Mm_{i,j} \\ | \\ \\Xm] = 0$. Next, we compute\n\\begin{align*}\n    \\Exp[\\Mm_{i,j}^2] &= \\frac{1}{d_k}\\sum_{a,c,a',c'=1}^{d_{v}}\\sum_{b,b'=1}^{d_{k}} \\Xm_{i,a} \\Xm_{i,a'} \\Xm_{j,c} \\Xm_{j,c'} \\Exp\\left[\\wQ_{a,b}\\wQ_{a',b'}\\wK_{c,b} \\wK_{c',b'}\\right] \\\\\n    &=\\frac{1}{d_k}\\sum_{a,c,a',c'=1}^{d_{v}}\\sum_{b,b'=1}^{d_{k}} \\Xm_{i,a} \\Xm_{i,a'} \\Xm_{j,c} \\Xm_{j,c'} \\Exp\\left[\\wQ_{a,b}\\wQ_{a',b'}\\right]\\Exp\\left[\\wK_{c,b} \\wK_{c',b'}\\right] \\\\\n    &= \\frac{\\sigma_k^4}{d_k}\\sum_{a,c=1}^{d_{v}}\\sum_{b=1}^{d_{k}}  \\Xm_{i,a}^2 \\Xm_{j,c}^2\\\\\n    &= \\sigma_k^4 \\|\\Xm_{i,:}\\|^2 \\|\\Xm_{j,:}\\|^2.\n\\end{align*}\nThis concludes the proof.\n\\end{proof}\n\nThe following classical result implies almost sure convergence of the softmax matrix as $d_k\\to\\infty$.\n\n\\begin{lemma}[Borel-Cantelli]\nLet $(X_i)$ be a sequence of random variables. If for any $\\epsilon>0$\n\\begin{equation*}\n    \\sum_{i=0}^\\infty \\mathbb{P}[|X_i-X|>\\epsilon]<\\infty,\n\\end{equation*}\nthen $X_i$ converges to $X$ almost surely\\footnote{That is, $\\lim_{i\\to\\infty} X_i(\\omega) = X(\\omega)$ for almost every $\\omega\\in \\Omega$~(i.e. with probability one).}.\n\\end{lemma}\n\n\\begin{tcolorbox}\n\\begin{theorem}[Almost-sure convergence]\n\\label{thm:soft_assumption_proof}\nConsider initializing each entry of $\\wQ\\in\\mathbb{R}^{d_v\\times d_k}$ and $\\wK\\in\\mathbb{R}^{d_v\\times d_k}$ independently with variance $\\sigma^2_k = 2/(d_v+d_k)$ --- i.e. Glorot initialization~\\citep{glorot2010understanding}. Let $d_v<\\infty$ be fixed, as $d_k\\to\\infty$ we have that, for any $\\Xm$,\n\\begin{equation*}\n    \\Am := \\soft\\left(\\frac{1}{\\sqrt{d_k}}\\Xm\\Wm^{Q}{\\Wm^{K}}^\\top{\\Xm}^\\top\\right)\\stackrel{a.s.}{\\to} \\frac{1}{n}\\bm{1}_{n\\times n}\n\\end{equation*}\nand\n\\begin{equation*}\n    \\frac{\\partial\\Am}{\\partial\\Mm} \\stackrel{a.s.}{\\to} \\frac{1}{n}\\Im_n \\otimes \\left(\\Im_n - \\frac{1}{n}\\bm{1}_{n\\times n} \\right).\n\\end{equation*}\n\\end{theorem}\n\\end{tcolorbox}\n\\begin{proof}\nThanks to Lemma~\\ref{app:convergence_A} and Markov Inequality, we have fast convergence in probability: for any fixed $\\Xm$,\n\\begin{equation*}\n    \\mathbb{P}[|\\Mm_{i,j}|>\\epsilon]\\le\\frac{\\Exp[\\Mm^2_{i,j}]}{\\epsilon^2} \\le \\frac{C_\\epsilon}{d_k^2}.\n\\end{equation*}\nBorel Cantelli then directly yields almost sure convergence of $\\Mm$ to $\\bm{0}_{n\\times n}$ as $d_k\\to\\infty$. Next, note that both $\\Am$ and $\\frac{\\partial\\Am}{\\partial\\Mm}$ are continuous functions of $\\Am$, hence we can apply standard continuity event-per-event. For almost every $\\omega\\in\\Omega$,\n\\begin{equation*}\n    \\lim_{d_k\\to\\infty} \\Am(\\Am(\\omega)) = \\Am\\left(\\lim_{d_k\\to\\infty}\\Am(\\omega)\\right) = \\Am( \\bm{0}_{n\\times n}) = \\frac{1}{n}\\bm{1}_{n\\times n}.\n\\end{equation*}\nHence $\\Am\\to \\frac{1}{n}\\bm{1}_{n\\times n}$ almost surely. This can also be seen as a simple application of the continuous mapping theorem. The same reasoning yields almost sure convergence of\n\\begin{equation*}\n    \\frac{\\partial \\Am}{\\partial \\Mm} = \\blockdiag\\Bigg(\\diag(\\Am_{i:}) - \\Am_{i:}\\Am_{i:}^\\top\\Bigg),\n\\end{equation*}\nto the corresponding limiting quantity.\n\\end{proof}\n\n\n\\vspace{10px}\n\\paragraph{Empirical analysis.}\n\\begin{wrapfigure}{r}{0.45\\textwidth}\n\\vspace{-10px}\n\\includegraphics[scale = 0.4]{figures/assumption_uniform_softmax.pdf}\n\\caption{\\small{Evolution of $\\frac{1}{n^2}\\vert\\vert\\Am^{\\ell}-\\frac{1}{n}{\\bf{1}}_{n\\times n}\\vert\\vert_{F}^{2}$ as a function of $d_k$ for $d_v$ fixed at 100.}}\\label{fig:softmaxuniform}\n\\vspace{-10px}\n\\end{wrapfigure} \nWe empirically assess the validity of\nAssumption~\\ref{ass:uniform_softmax} and of its theoretical justification by performing the following experiments: for a range of increasing values of $d_k$, we compute $\\Am$ and we calculate $\\frac{1}{n^2}\\vert\\vert\\Am^{\\ell}-\\frac{1}{n} {\\bf{1}}_{n\\times n}\\vert\\vert_{F}^{2}$, i.e. its average (entry-wise) distance from a uniform matrix with entries all equal to $1/n$. \nFor each value of $d_k$, we repeat this calculation 200 times, each time with different random weight matrices. Fig.~\\ref{fig:softmaxuniform} displays how the $\\vert\\vert\\Am-\\frac{1}{n}{\\bf{1}}_{n\\times n}\\vert\\vert_{F}^{2}$ averaged over 200 runs, tends to zero with a trend inversely proportional to $d_k^2$, as predicted by our theoretical analysis.\n\n\\vspace{20px}\n\\section{Additional Results}\n\\label{app:more_experiments}\n\n\\subsection{On the Roles of the $1/\\sqrt{L}$-Scaling of the Residuals and Layer Normalization}\nWe present some additional results on the propagation of the norm and the correlations in Figure~\\ref{fig:norm_corr_prop_1}. In particular, we empirically show that, with an adequate depth-dependent residual scaling, the norm and the correlation are stabilized, even for very deep networks. Furthermore, we demonstrate the propagation of the correlation and the gradient norms for the PRE-LN configuration in Figure~\\ref{fig:pre_ln_correlations}. As also hinted in the main text, in Figure~\\ref{fig:corr}, the increase in correlation with depth for PRE-LN is much less wild. This also results in better stabilized gradients for the queries and keys' parameters. We also observe the opposite trend for the gradients of the values, in relation to the POST-LN case in Figure~\\ref{fig:residual_scaling}. We speculate that this different dependence, along with the better preserved correlation, is the main reason PRE-LN configured Transformers have been shown to scale better with depth. We plan to investigate this dependence more in future work.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[scale=0.28]{figures/corr_norm.pdf}\n    \\caption{(Left) Propagation of the Frobenius norm of the input sequence; (Right) Propagation of the average token correlation.}\n    \\label{fig:norm_corr_prop_1}\n\\end{figure}\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1\\linewidth]{figures/residual_scaling_pre.pdf}\n    \\caption{Same figure as~\\ref{fig:residual_scaling} but with a PRE-LN architecture. The correlation at depth 0 originates from the correlations in the randomly initialized tokens' embeddings and positional encodings.}\n    \\label{fig:pre_ln_correlations}\n\\end{figure}\n\n\\subsection{Further Empirical Assessment of Assumption \\ref{ass:uniform_softmax}}\nHere, we empirically test the accuracy and limitations of the uniform-attention assumption.\n\nFor the empirical verification of Assumption \\ref{ass:uniform_softmax} in the forward pass analysis, we plot the density of the norm of the representations for only-encoder Transformers of increasing depth. The results are shown in Fig \\ref{fig:norm_std_init_vs_uniform_att}. Note that when the standard deviation of the input is set to $1/\\sqrt{d}$, then the uniform-attention assumption provide an excellent approximation to the common Xavier-initialization. On the contrary, we observe a deviation when the standard deviation of the input is increased. Also, note how as the depth increases, the distribution becomes more heavy-tailed. This heavy-tailedness was recently formally shown for standard MLPs with and without ReLU activation \\citep{noci2021precise, zavatone2021exact}.   \n\n\\begin{figure}\n    \\centering\n    \\includegraphics[scale=0.33]{figures/norm_distribution_uniform_att.png}\n    \\includegraphics[scale=0.33]{figures/norm_distribution_std_init.png}\n    \\includegraphics[scale=0.33]{figures/norm_distribution_std_init_std_1.png}\n    \\caption{Density plots for $\\norm{\\Xm^\\ell}_F^2$ for Transformers of depths $L$ from $1$ to $10$. The input $\\Xm$ contains i.i.d Gaussian entries, simulating an embedding layer. We set $d:=d_v=d_q=30$. The empirical mean at $L=10$ is highlighted in a vertical dashed red line, while the theoretical mean (Lemma \\ref{thm:forward_pass}) is a dashed blue line. The densities are estimated by sampling 1000 times the weights of the network. (Left): we adopt the uniform-attention. The standard deviation of the input is set to $1/\\sqrt{d}$. (Center): Same, but removing the uniform-attention assumption. (Right): We remove the uniform-attention assumption, and set the standard deviation of the input to $1$.}\n    \\label{fig:norm_std_init_vs_uniform_att}\n\\end{figure}\n\n\nFor the verification of the assumption in the backward pass, we additionally show in Fig. \\ref{fig:empirical_verification} how the norm of the gradients w.r.t queries and keys depends on the hidden dimension, the sequence length, the input correlation and the input variance. \\emph{Ground-truth} gradients are calculated with automatic differentiation, and they are compared with our theoretical results based on Assumption \\ref{ass:uniform_softmax}. As shown in  Fig.\\ref{fig:empirical_verification}, our theoretical predictions show a very good agreement with the true gradients. Again, we notice that the smaller the values of the input standard deviation the tighter the agreement of the theory with the simulations. Intuitively, a higher input variance causes the argument of the softmax to have a large range of values. This in turn causes a deviation from the uniform distribution (i.e. maximum entropy), towards the distribution of minimum entropy (a Delta Dirac, corresponding to attending to only one token). \n\n\n\\subsection{Empirical Verification of the Gradient Analysis of Section \\ref{sec:dep_angle}}\nFinally, in Figures~\\ref{fig:constant_correlation_factors} and~\\ref{fig:constant_correlation_factors_theory} we show the dependence of the norm of the gradients for the keys and values based on the parameters of the architecture and the task-specific parameters. Figure~\\ref{fig:constant_correlation_factors} illustrates the true dependence and Figure~\\ref{fig:constant_correlation_factors_theory} the one expected by the theory based on our assumptions. In short, the main takeaways are the following.\n\\begin{itemize}\n    \\item As the correlation between the tokens increases ($x$-axis in the global plot), the norm of the gradients of the queries quickly diminishes compared to the one of the values.\n    \\item The dependence on the variance of the input $\\sigma_x^2$ is different ($y$-axis in the global plot), being linear for the values and cubic for the queries. This highlights the importance of a stabilized forward pass and provides another explanation regarding the successful use of layer norm in Transformers.\n    \\item The dependence on $n$ ($x$-axis in each subplot) and $d$ ($y$-axis in each subplot) is more complicated, also being a function of the correlation $\\rho$ (compare the first column where $\\rho = 0$ to the rest). \n\\end{itemize}\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[scale = 0.39]{figures/check_assumption/n_Q_True_.pdf}\n    \\includegraphics[scale = 0.39]{figures/check_assumption/n_V_True_.pdf}\n    \\includegraphics[scale = 0.39]{figures/check_assumption/d_Q_True_.pdf}\n    \\includegraphics[scale = 0.39]{figures/check_assumption/d_V_True_.pdf}\n    \\includegraphics[scale = 0.39]{figures/check_assumption/sigma_Q_True_.pdf}\n    \\includegraphics[scale = 0.39]{figures/check_assumption/sigma_V_True_.pdf}\n    \\includegraphics[scale = 0.39]{figures/check_assumption/rho_Q_True_.pdf}\n    \\includegraphics[scale = 0.39]{figures/check_assumption/rho_V_True_.pdf}\n    \\caption{Empirical comparison of our theoretical findings. We sample, as aforementioned, the tokens according to a zero-mean Gaussian distribution, while varying the hidden dimension, sequence length, input correlation and input variance. Results are averaged over 20 runs.}\n    \\label{fig:empirical_verification}\n\\end{figure}\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1\\linewidth]{figures/constant_corr.pdf}\n    \\caption{Log ratio of the norm of the gradients for the queries compared to those of the values for varying values of embedding dimension, sequence length, cosine of the tokens angle and standard deviation.}\n    \\label{fig:constant_correlation_factors}\n\\end{figure}\n\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1\\linewidth]{figures/constant_corr_theory.pdf}\n    \\caption{Log ratio of the norm of the gradients of the queries as expected by the theory, compared to those of the values for varying values of embedding dimension, sequence length, cosine of the tokens angle and standard deviation. We use Equations~\\eqref{eq:grad_V} and~\\eqref{eq:grad_Q}.}\n    \\label{fig:constant_correlation_factors_theory}\n\\end{figure}\n\n\n\\section{Experimental Setup}\n\\label{app:experimental_setup}\n\nHere we provide more details regarding the experimental setup.\n\n\\subsection{Toy Example}\n\nIn Figure~\\ref{fig:adam-adap}, we focus on a toy example where the task is to reverse a sequence of tokens. More specifically, given a sequence of $20$ numbers in the range $0-9$, we predict the same tokens in the inverted order. We use an embedding layer of size 16, initializes with variance 1, and sinusoidal positional encodings to initially embed the input. We use a 5-layer POST-LN Transformer encoder model, with a single head attention operation and a two-layer feed-forward layer with a ReLU nonlinearity. We use residual scaling in this case equal to $\\alpha_1 = \\alpha_2 = 1$. We train using Adam with betas parameters $(0.9, 0.999)$, learning rate $0.01$ and weight decay 0.\n\n\\subsection{Translation Task}\n\nIntroducing an inverse temperature scaling $\\tau$ inside the softmax, modifies the attention operation to\n\\begin{equation}\n    \\Sm^{\\ell} :=  \\text{softmax}\\left( \\frac{\\tau}{\\sqrt{d_k}}\\Xm^{\\ell}\\Wm^{Q}\\left(\\Xm^{\\ell}\\Wm^{K}\\right)^\\top \\right) \\Xm^\\ell \\Wm^{V}.\n\\end{equation}\n\nThen the gradient of the queries and keys parameters are directly scaled by this temperature value, following the same proof as for Eq.~\\eqref{eq:jacobian_queries}. We choose a temperature value of $\\tau_{\\text{final}} = 8.5$ to match the gradient norms of the values and queries as in Equations.~\\eqref{eq:grad_V} and~\\eqref{eq:grad_Q}. Doing so, we assume a constant small correlation between tokens (also empirically verified in Fig.~\\ref{fig:our_training}) and set the sequence length $n$ to the average found in our training dataset. Due to instabilities in training, we use warm-up on this temperature value. In short:\n$$\n\\tau = \\tau_{\\text{final}} \\cdot \\text{max}(1, \\frac{\\text{step}}{\\text{steps}_{\\text{warmup}}}),\n$$\nwith `$\\text{steps}_{\\text{warmup}}=1000$' and `step' the current training step.\n\nWe base our implementation on fairseq~\\citep{ott2019fairseq}. For the hyperparameter configuration, we mostly rely on the extensive search already done in fairseq~\\citep{ott2019fairseq} and~\\cite{liu2020understanding}. The final used parameters are exhibited in Table~\\ref{tab:hyperparameters}. For the final evaluation, we use the best-performing model on the left-out validation set. We apply weight decay as in~\\cite{loshchilov2017decoupled} for both SGD and Adam.\n\n \\begin{table}[h]\n \\centering\n \\begin{tabular}{cl|c}\n \\hline\n & \\multicolumn{1}{c|}{\\textbf{Hyperparameters}} & \\multicolumn{1}{c|}{\\textbf{Value}}\\\\\n \\hline\n \\parbox[t]{2mm}{\\multirow{9}{*}{\\rotatebox[origin=c]{90}{}}} & Max tokens & 4096 \\\\\n  & Label smoothing & 0.1 \\\\\n  & clip-norm & 0.0 \\\\\n  & General Dropout & 0.3 \\\\\n  & Attention Dropout & 0.1 \\\\\n  & ReLU Dropout & 0.1 \\\\\n  & Hidden size & 512 \\\\\n  & FFN inner hidden size & 2048 \\\\\n  & Attention Heads & 4 \\\\\n  \\hline\n  \\parbox[t]{2mm}{\\multirow{7}{*}{\\rotatebox[origin=c]{90}{Adam}}} & Learning rate & $7\\epsilon^{-4}$ \\\\\n  & Learning rate scheduler & inverse sqrt \\\\\n  & Warm-up updates & 6000 \\\\\n  & Warm-up init learning rate & 1e-7 \\\\\n  & Adam $(\\beta_1, \\beta_2)$ & (0.9, 0.98) \\\\\n  & Training updates & 100K \\\\\n  & Weight decay & 0.0001 \\\\\n  \\hline\n  \\parbox[t]{2mm}{\\multirow{6}{*}{\\rotatebox[origin=c]{90}{SGD}}} & Learning rate & $2\\epsilon^{-2}$ \\\\\n  & Learning rate scheduler & step \\\\\n  & Step scheduler $\\gamma$ & 0.1 \\\\\n  & Step scheduler update steps & [100K, 200K] \\\\\n  & Training updates & 250K \\\\\n  & Weight decay & 0.001 \\\\\n \\hline\n\\end{tabular}\n\\caption{Hyperparameters for the IWSLT'14 De-En translation task.}\n\\label{tab:hyperparameters}\n\\end{table}\n\nFinally, in Figure~\\ref{fig:our_training} we display the evolution of correlations, residual scaling, and norm of the activations, with depth, for our best trained model. The residual scaling $\\alpha_1, \\alpha_2$ are trainable parameters. This enables them to weight differently the residual branches if deemed necessary. Although these values increase during training, the correlation between the tokens does not significantly increase, which as implied by our main results, allows efficient propagation of the gradients. The norm of the propagated forward signal tends to slightly increase with depth.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1\\linewidth]{figures/our_training.pdf}\n    \\caption{Evolution of the cosine of the angles, the trained residual $\\alpha_1, \\alpha_2$ and the activation norm throughout our training.}\n    \\label{fig:our_training}\n\\end{figure}\n\n\n\n\\end{document}\n\n==== END OF /2206.03126/main.tex ====",
            "processed_original_tex": "==== BEGINNING OF /2206.03126/math_commands.tex ====\n\\newcommand{\\mb}{\\mathbf}\n\\newcommand{\\wM}[1]{\\mb W^{#1}}\n\\newcommand{\\wMt}[1]{\\mb W^{#1^\\top}}\n\\DeclareMathOperator{\\soft}{softmax}\n\\newcommand{\\kro}{\n  \\mathbin{\\mathop{\\otimes}}\n}\n\\usepackage{xcolor}\n\n\\usepackage{thm-restate}\n\\newtheoremstyle{theoremdd}\n  {\\topsep}\n  {\\topsep}\n  {\\itshape}\n  {0pt}\n  {\\bfseries}\n  {. }\n  { }\n  {\\thmname{#1}\\thmnumber{ #2}\\textnormal{\\thmnote{ (#3)}}}\n\\theoremstyle{theoremdd}\n\\declaretheorem[name=Theorem,numberwithin=section]{thm}\n\\declaretheorem[name=Lemma,numberwithin=section,numberlike=thm]{lem}\n\\declaretheorem[name=Corollary,numberwithin=section,numberlike=thm]{cor}\n\n\\declaretheorem[name=Proposition,numberwithin=section,numberlike=thm]{prop}\n\n\\declaretheorem[name=Assumption,numberlike=thm]{ass}\n\n\n\n\n\\DeclareMathOperator{\\blockdiag}{blockdiag}\n\\DeclareMathOperator{\\diag}{diag}\n\\DeclareMathOperator{\\vect}{vec}\n\\DeclareMathOperator{\\sa}{\\mb {SA}}\n\\DeclareMathOperator{\\rsa}{\\mb {RSA}}\n\\newcommand{\\wQ}{\\mb W^{Q}}\n\\newcommand{\\wQT}{\\mb W^{Q \\top}}\n\\newcommand{\\wK}{\\mb W^{K}}\n\\newcommand{\\wKT}{\\mb W^{K \\top}}\n\\newcommand{\\wV}{\\mb W^{V}}\n\\newcommand{\\wVT}{\\mb W^{V \\top}}\n\\def\\Im{{\\bf I}}\n\\def\\Km{{\\bf K}}\n\\def\\Am{{\\bf A}}\n\\def\\Em{{\\boldsymbol \\epsilon}}\n\\def\\Lm{{\\bf L}}\n\\def\\Bm{{\\bf B}}\n\\def\\Cm{{\\bf C}}\n\\def\\Dm{{\\bf D}}\n\\def\\Sm{{\\bf S}}\n\\def\\Xm{{\\bf X}}\n\\def\\Ym{{\\bf Y}}\n\\def\\Nm{{\\bf N}}\n\\def\\Mm{{\\bf M}}\n\\def\\Tm{{\\bf T}}\n\\def\\Wm{{\\bf W}}\n\\def\\Zm{{\\bf Z}}\n\\def\\Sm{{\\bf S}}\n\\DeclareMathOperator{\\tr}{tr}\n\\newcommand\\Exp{\\mathbb{E}}\n\\DeclareMathOperator{\\rank}{rank}\n\\usepackage{nicematrix}\n\\newcommand{\\p}{\\bm p}\n\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n\\DeclareMathOperator{\\SA}{SA}\n\\newcommand\\R{\\mathbb{R}}\n\\newcommand{\\extra}[1]{{ #1}}\n\n\n==== END OF /2206.03126/math_commands.tex ====\n==== BEGINNING OF /2206.03126/old.tex ====\n\\section{Related work}\n- works on initialization of transformers\n- works on the need of warmup with adam and other on adaptive learning rates\n- works on training stabilization\n- works on enhancing/replacing the softmax\\\\\n\n\\luca{preliminar: just a commented list of papers}\n\\cite{wang2022deepnet} introduces DeepNorm, a simple initialization scheme that enables training of transformer models with more than 1000 layers. The initialization is based on rescaling of the residual connections and on the introduction of a gain parameter for the weights of the feed-forward connections and the value projections. In all the evaluations though the authors still use warmup as well as layer normalization, Adam and gradient clipping.\n\n\\cite{bachlechner2021rezero} introduces ReZero a simple trick consisting in the addition of a trainable parameter (initialized to zero) to the residual term in residual connection. The authors show that transformers equipped with this simple scheme can be quickly trained without LayerNorm, even for very large depths.\n\n\\cite{xiong2020layer} shows that using layer normalization inside the residual connection (Pre-LN) allows training of Transformers without warm-up. However, they claim that SGD cannot achieve performance on par with Adam. Furthermore, parallel works show that Pre-LN can result in sub-optimal performance on neural machine translation tasks. \n\n\\cite{pmlr-v119-huang20f} shows that the need for warm-up comes from the instability in the Adam optimizer combined with gradient vanishing through layer normalization. A new initialization is then introduced that keeps the models update bounded and enables training without warm-up and layer normalization. However, the Adam optimizer is kept and SGD is deemed as sub-optimal for Transformer training.\n\n\n\\cite{liu2020understanding} found that gradient vanishing is not the only issue preventing stable training in transformers. The find that with Pre-LN, vanishing gradient is removed but the resulting performance is sub-optimal as opposed to the cases where Post-LN converges. Interestingly they notice that gradients of all attention modules are unbalanced and they conclude that the only way to optimize Transformer is via adaptive optimizers. Ultimately, they determine that the dependency on residual branches is one of the main factor determining convergence and performance and they introduce a new initialization based on residual rescaling to successfully cope with it.\n\n\n\\cite{zhang2020adaptive} finds that adaptive optimizers are better suited than plain SGD when attention-based models are considered. This is due to the heavy-tailed nature of noise in stochastic gradients found in BERT optimization. The authors find the \\emph{adaptive} gradient clipping is effective in reducing the heavy-tail effect and in closing the gap between SGD and Adam.\n\n\n\\cite{liuadam} provides a theoretical justification for the need for warm-up scheduling when adaptive methods are used. In particular, they show that the convergence issue is due to the undesirably large variance of the adaptive learning rate in the early stage of model training. They thus introduce RAdam, which explicitly rectifies the variance and compares favourably with warm-up.\n\n\n\\cite{shleifer2021normformer} introduces NormFormer, a new model that adds three normalization operations to each layer: a Layer Norm after self attention, head-wise scaling of self-attention outputs, and a Layer Norm after the first fully connected layer. The extra operations incur negligible compute cost (+0.4\\\ntask performance for both causal and masked language models. \n\n\n\\cite{wang2019learning} modifies the residual connection by introducing multiple links with all previous layers to access lower-level representations in a deep stack. This simple trick allows to train deeper models on several translation tasks.\n\n\\cite{nguyen2019transformers} combines three different types of normalization to cope with training instability in transformer models. PRENORM + FIXNORM + SCALENORM significantly improves NMT on low-resource pairs, with the latter two performing comparably in the high-resource setting, but faster.\n\n\\cite{dufter2020increasing} Use of learnable temperature for the attention weights. They use it for all queries, values and keys and per dimension. They report the temperature has no effect (empirical study).\n\n\n\n\n\n\\paragraph{Training Instability in Attention-Based Models.}\nSuccessful optimization in Transformers is notoriously hard and requires several tailored training tricks and architectural design choices. Among these, Layer Normalization is, arguably, one of the most important -- and debated -- ones.\nIn the original architecture \\citep{vaswani2017attention}, Layer Normalization was used in the Post-Norm setting, that is, it was placed between the residual blocks in order to stabilize the forward pass by reducing the variance of the inputs to the following sublayer. Later works \\citep{chen2018best} found that its inclusion is highly beneficial for the stabilization of the training process. Nevertheless, successful training of Transformers in the Post-Norm setting remains hard without resorting to adaptive methods opportunely combined with strongly hyperparameter-sensitive preliminary warm-up stages \\cite{liuadam,zhang2020adaptive,huang2020improving}. Moreover, training instability in this setting is further exacerbated when the optimization of deeper models is required. These considerations have recently led to the introduction of the so-called Pre-Norm variant \\cite{wang2019learning,nguyen2010estimating,xiong2020layer}, where layer normalization is used inside the residual connection. \\cite{xiong2020layer} show that since Post-LN induces larger gradients in deeper layers than Pre-Norn, networks equipped with the latter can be trained with larger learning rates and without the need of warm-up. Furthermore, Pre-Norm has been found \\cite{wang2019learning,nguyen2010estimating,xiong2020layer} to enable training of deeper models without incurring in training instabilities. However, despite making training easier, the use of Pre-LN can lead to inferior performance than its Post-LN counterpart \\cite{liu2020understanding, nguyen2019transformers}. This observation has motivated research on replacing or complementing layer norm with carefully designed initialization schemes \\cite{zhang2019fixup,liu2020understanding} or additional architectural modifications \\cite{bachlechner2021rezero,wang2022deepnet}. In our work we build upon the above research efforts with the goal of shedding new light on the forward and backward passes of attention-based architectures. In particular, we present, for the first time, a rigorous theoretical analysis of signal propagation in Transformer-based neural networks. In doing so, we provide new insights on the role of Layer Normalization, both in the Post-LN and Pre-LN configurations, and of adaptive optimizers. In relation to this, as also observed by \\citep{liu2020understanding}, we identify in the different scales of the norms of queries and values gradients one of the possible causes for the necessity of adaptive methods in Transformers optimization and, differently from \\citep{liu2020understanding}, we demonstrate that the introduction of a temperature parameter inside the softmax can help SGD in closing the performance gap with Adam. \n\\paragraph{Rank Collapse in Attention-Based Models.}\nA parallel line of research \\citep{dong2021attention} has recently identified an additional key inductive bias characterizing purely attention-based architectures: their input representations lose rank doubly exponentially with depth. The authors identify residual connections as the most effective component to mitigate this phenomenon. However, \\citep{dong2021attention} leave the question on the relation between the emergence of rank collapse and optimization unaddressed. Thanks to our analysis of the backward pass, we are able to demonstrate that rank collapse in Transformer architectures leads to vanishingly small gradients of queries and keys, thereby preventing effective training and completing the analysis started by \\citep{dong2021attention}.\n\n==== END OF /2206.03126/old.tex ====\n==== BEGINNING OF /2206.03126/main.tex ====\n\\documentclass{article}\n\n\n\n\n\n\n\n\n\\usepackage[preprint]{neurips_2022}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\usepackage[utf8]{inputenc} \n\\usepackage[T1]{fontenc}    \n\\usepackage{hyperref}       \n\n\\hypersetup{\n    colorlinks=true,\n    linkcolor=blue,\n    filecolor=magenta,      \n    urlcolor=cyan,\n    citecolor=blue,\n}\n\\usepackage{url}            \n\\usepackage{booktabs}       \n\\usepackage{amsfonts}       \n\\usepackage{nicefrac}       \n\\usepackage{microtype}      \n\\usepackage[dvipsnames]{xcolor}         \n\\newcommand{\\luca}[1]{{\\color{red} Luca: ``#1''}}\n\\newcommand{\\antonio}[1]{{\\color{magenta} Antonio: ``#1''}}\n\\newcommand{\\lorenzo}[1]{{\\color{blue} Lorenzo: ``#1''}}\n\\newcommand{\\sotiris}[1]{{\\color{green} Sotiris: ``#1''}}\n\\newcommand{\\sidak}[1]{{\\color{orange} Sidak: ``#1''}}\n\\newcommand{\\aurelien}[1]{{\\color{ForestGreen} Aurelien: ``#1''}}\n\n\\usepackage{aligned-overset}\n\\usepackage{amsthm}\n\\usepackage{wrapfig}\n\\usepackage{diagbox}\n\\usepackage[toc,page,header]{appendix}\n\\usepackage{minitoc}\n\\renewcommand \\thepart{}\n\\renewcommand \\partname{}\n\n\\usepackage[most]{tcolorbox}\n\\tcbset{\n    boxsep=0mm,\n    boxrule=0pt,\n    colframe=white,\n    arc=0mm,\n    left=0.5mm,\n    right=0.5mm\n}\n\\input{math_commands}\n\\usepackage{bm}\n\\newtheorem{theorem}{Theorem}[section]\n\\newtheorem{lemma}{Lemma}[section]\n\\newtheorem{corollary}{Corollary}[section]\n\n\\newtheorem{assumption}{Assumption}\n\\newcommand\\blfootnote[1]{\n  \\begingroup\n  \\renewcommand\\thefootnote{}\\footnote{#1}\n  \\addtocounter{footnote}{-1}\n  \\endgroup\n}\n\n\n\\usepackage{floatrow}\n\n\\title{\n\n\n\n\nSignal Propagation in Transformers: Theoretical Perspectives and the Role of Rank Collapse\n}\n\n\\usepackage{array,multirow,graphicx}\n\n\n\\author{Lorenzo Noci$^{*1}$\\\\ \n  \\texttt{\\small lorenzo.noci@inf.ethz.ch} \\\\\n  \\And Sotiris Anagnostidis$^{*1}$\\\\ \n  \\texttt{\\small sotirios.anagnostidis@inf.ethz.ch} \\\\ \\And Luca Biggio$^{*1,2}$ \\\\ \n  \\texttt{\\small luca.biggio@inf.ethz.ch} \\\\ \\And Antonio Orvieto$^{*1}$ \\\\ \n  \\texttt{\\small antonio.orvieto@inf.ethz.ch} \\\\ \\And Sidak Pal Singh$^{*1}$ \\\\ \n  \\texttt{\\small sidak.singh@inf.ethz.ch} \\\\ \\And Aurelien Lucchi$^{3}$ \\\\ \n  \\texttt{\\small aurelien.lucchi@unibas.ch}\n}\n\n\\date{tests}\n\n\n\\begin{document}\n\n\\doparttoc \n\\faketableofcontents \n\n\\maketitle\n\n\\begin{abstract}\nTransformers have achieved remarkable success in several domains, ranging from natural language processing to computer vision. Nevertheless, it has been recently shown that stacking self-attention layers  the distinctive architectural component of Transformers  can result in rank collapse of the tokens representations at initialization. The question of if and how rank collapse affects training is still largely unanswered, and its investigation is necessary for a more comprehensive understanding of this architecture. In this work, we shed new light on the causes and the effects of this phenomenon. First, we show that rank collapse of the tokens representations hinders training by causing the gradients of the queries and keys to vanish at initialization. Furthermore, we provide a thorough description of the origin of rank collapse and discuss how to prevent it via an appropriate depth-dependent scaling of the residual branches. Finally, our analysis unveils that specific architectural hyperparameters affect the gradients of queries and values differently, leading to disproportionate gradient norms. This suggests an explanation for the widespread use of adaptive methods for Transformers' optimization. \n\\end{abstract}\n\n\\blfootnote{$^{1}$Dept of Computer Science, ETH Z\\\"urich, $^{2}$Robotics \\& ML, CSEM SA, Alpnach, Switzerland, $^{3}$Department of Mathematics and Computer Science, University of Basel}\n\\section{Introduction}\nSince its first appearance in~\\cite{vaswani2017attention}, the Transformer architecture has revolutionized the field of Natural Language Processing (NLP), achieving remarkable success in tasks such as text classification~\\citep{yang2019xlnet}, machine translation~\\citep{mtlample}, reading comprehension~\\citep{brown2020language} and question answering~\\citep{raffel2019exploring} among others. Recent efforts have effectively extended its applicability to computer vision~\\citep{dosovitskiy2020image} and other domains ~\\citep{baevski2020wav2vec, huang2018music, biggio2021neural, polu2022formal}, further popularizing it outside NLP.\n\n\nThe Transformer operates on inputs comprising a sequence of tokens. At its core, it relies on stacked attention layers, which compute a measure of relevance for the whole sequence by assigning token-wise importance weights --- obtained by matrix multiplication of the \\textit{queries} and \\textit{keys}, and finally normalized with the softmax function. The output of an attention layer is then a linear combination of the importance weights and the so-called \\textit{values}. Then, the architecture includes fully-connected sub-layers, residual connections \\citep{resnet2016}, and layer normalization (LN), as illustrated in Fig.~\\ref{fig:architecture}.\n\n\n\\begin{figure}[t]\n\\centering\n    \\includegraphics[width=\\textwidth]{figures/adam_correlations.pdf}\n    \\caption{Evolution of the cosine of the angle between tokens for training POST-LN Transformers of increasing depth, with the Adam optimizer, for the IWSLT'14 De-En translation task. Unless adequate residual scaling is used at initialization, increasing depth leads to an increase in the tokens' alignment at initialization, which can inhibit training.}\n    \\label{fig:adam_postln}\n\\end{figure}\n\nIn the absence of residual connections, \\cite{dong2021attention} proved that at initialization the rank of the sequence representation collapses doubly exponentially with depth, and both layer normalization and fully connected layers can only partially alleviate the speed of degeneracy. Under \\textit{rank collapse}, the model does not distinguish between representations of different tokens, which are perfectly aligned in feature space at initialization. Crucially, the precise implications of rank collapse in Transformers are not fully understood. \n\nIn this paper, we show that a high alignment of the tokens' representations at initialization --- corresponding to rank collapse in the extreme case of perfect alignment --- affects training by causing vanishingly small gradients of the queries and keys' parameter matrices. This problem severely diminishes the capabilities of the model to learn meaningful attention weights and is further exacerbated in very deep networks, where the rank deficiency --- and hence the vanishing gradient problem of the queries and keys --- affects several layers (see Fig.~\\ref{fig:adam_postln}). In order to shed light on this problem, we take inspiration from the flourishing literature on signal propagation in random networks and start our analysis by computing the expected gradients of an attention layer with respect to the queries, keys, and values, which leads to Theorem \\ref{thm:vanishing_gradients} on the vanishing gradients for the queries and keys. From here, we pursue two different directions. \n\nFirstly, we investigate under which conditions rank collapse can be avoided by studying the evolution of the input sequence in a Transformer at initialization. Our theory reveals that a depth-dependent scaling of the residual branches, beyond stabilizing the norm of the activations at initialization, also approximately preserves the cosine of the angle between tokens, and hence also stabilizes the rank of the propagating sequence. We show that this holds even in the infinite-depth limit.\n\nSecondly, we illustrate that there are factors, other than the average tokens' correlation, that affect differently the gradient norm of the queries and keys compared to the values. In particular, the propagating sequence's squared norm has a linear dependence in the values, while a cubic one in the queries and keys, justifying the use of layer normalization. We also highlight a different dependence on the embedding dimension and the length of the input sequence, implying that the gradient norm of a subset of parameters can potentially be of different orders of magnitude, as empirically hinted by previous works \\citep{liu2020understanding}. Our analysis brings to light fundamental issues in the signal propagation in Transformers, opening the way for new, well-founded and motivated approaches to improve optimization in these models.\n\n\\section{Background}\n\n\\paragraph{Transformers.}\n\nA Transformer architecture consists of $L$ stacked attention blocks, as show in Fig.~\\ref{fig:architecture}. Layer normalization is usually applied token-wise either after the residual connections or to the inputs of the self-attention and position-wise feed-forward sub-layers, leading to the POST-LN~\\citep{vaswani2017attention} and PRE-LN~\\citep{wang2019learning,xiong2020layer} variants respectively.\n\nFormally, given an input sequence $\\Xm \\in \\mathbb{R}^{n \\times d_{v}}$, with $n$ tokens of dimension $d_{v}$, the single-head unmasked scaled dot-product self-attention\\footnote{Our analysis also easily generalizes to the case of cross-attention.} is defined as:\n\\begin{equation}\n\\label{eq:self_att}\n    \\Sm^{\\ell} := \\Am^\\ell \\Xm^\\ell \\Wm^{V} , \\text{ where } \\Am^\\ell = \\text{softmax}\\left( \\frac{1}{\\sqrt{d_k}}\\Xm^{\\ell}\\Wm^{Q}\\left(\\Xm^{\\ell}\\Wm^{K}\\right)^\\top \\right) ,\n\\end{equation}\nwhere the softmax function is applied independently across each row, and the superscript $\\ell$ indexes the $\\ell$-th layer.\nThe matrices $\\Wm^Q, \\Wm^{K} \\in \\mathbb{R}^{d_v \\times d_k}$ and $\\Wm^{V} \\in \\mathbb{R}^{d_{v} \\times d_v}$ are learnable parameters, and each layer is initialized with an independent set of weights. In the literature, the matrices $\\Xm^{\\ell}\\Wm^{Q}, \\Xm^{\\ell}\\Wm^{K}, \\Xm^{\\ell}\\Wm^{V}$ are referred to as queries, keys and values, respectively. The complete Transformer block, in the absence of layer normalization, can be written recursively as:\n\n\\begin{align}\n    & \\Zm^{\\ell} = \\alpha_1 \\Sm^{\\ell}+ \\Xm^{\\ell} \\\\\n    & \\Ym^{\\ell} = \\sigma(\\Zm^{\\ell} \\Wm^{F_1})\\Wm^{F_2} \\\\\n    & \\Xm^{\\ell+1} = \\alpha_2 \\Ym^\\ell +  \\Zm^\\ell ,\n \\end{align}\nwhere the introduced $\\alpha_1, \\alpha_2$ parameters indicate the strength of the residual block, $\\Wm^{F_1}$, $\\Wm^{F_2} \\in \\mathbb{R}^{d_v \\times d_v}$ \\footnote{In practice, one commonly uses $\\Wm^{F_1} \\in \\mathbb{R}^{d_v \\times d_F}$, $\\Wm^{F_2} \\in \\mathbb{R}^{d_F \\times d_v}$ where $d_F = \\gamma d_v$, with $\\gamma \\in \\{2, 4, 8\\}$. Our results then hold up to a constant factor that depends on $\\gamma$.} are matrices of learnable parameters; we set $\\Xm^0 := \\Xm$, and $\\sigma: \\mathbb{R} \\rightarrow \\mathbb{R}$ is an activation function. In our case, $\\sigma$ is the ReLU function, but we relax this assumption to the linear activation from Section \\ref{sec:forward_pass} on.\n\n\\begin{wrapfigure}{r}{0.25\\textwidth}\n\\vspace{-1.5em}\n\\includegraphics[width=3.5cm]{figures/transformer.pdf}\n\\vspace{-2em}\n\\caption{A single Transformer block.\\vspace{-2.5em}}\n\\label{fig:architecture}\n\n\n\\end{wrapfigure} \n\nAt initialization, each weight is sampled independently from a distribution with zero-mean and variance $\\sigma^2_v = \\frac{1}{d_v}$ for the values and feedforward weights\\footnote{One should explicitly write the layer dependence $\\Wm^{Q,\\ell}, \\Wm^{K,\\ell},\\Wm^{V,\\ell}, \\Wm^{F_1, \\ell},  \\Wm^{F_2, \\ell} $. We at times suppress the $\\ell$ index to improve readability. In case $\\sigma$ is the ReLU function, we set $\\Wm^{F,1}$ to have variance $\\frac{2}{d_v}$.}, and $\\sigma^2_k = \\frac{1}{d_k}$ for the queries and keys. This is the standard ``Xavier''~\\citep{glorot2010understanding} or ``He''~\\citep{he2015delving} initialization, commonly used in deep learning.  \n\n\n\\paragraph{Rank Collapse in Transformers.}\nInterestingly, \\cite{dong2021attention} proved that when the residual branches are omitted, the matrix of the tokens' representations $\\Xm^\\ell$ converges to a rank-1 matrix in which all the representations are the same and equal to a vector $\\bm{x}\\in \\mathbb{R}^{d_v}$, i.e. $\\Xm^\\ell \\to \\bm{1}_{n}\\bm{x}^\\top$, where $\\bm{1}_{d_v}$ is the vector with all ones in $\\mathbb{R}^{d_v}$. Note that this is a slightly stronger notion of a rank-$1$ matrix, as it implies that all the tokens' representations are both perfectly aligned and have the same norm. Indicating the inner product with the usual bracket notations $\\langle \\cdot, \\cdot \\rangle$, and the cosine of the angle between two tokens as $\\theta_{k,k'}$, perfect alignment happens when $\\langle \\Xm^{\\ell}_{k}, \\Xm^{\\ell}_{k'} \\rangle = \\norm{\\Xm^{\\ell}_{k}} \\norm{\\Xm^{\\ell}_{k}} \\cos \\theta_{k,k'}$ with $\\cos \\theta_{k,k'}=1$ for all $k, k' \\in [n]$. Note that perfect alignment together with equal norm between all the tokens implies that all the representations are the same. One of our main contributions is to provide an explanation of how rank collapse affects the gradients of a Transformer at initialization.\n\n\\paragraph{Vanishing Gradient Problem.}\nTraditionally considered one of the core issues that prevents successful training, the vanishing gradient problem has a long and rich history that dates back to before the popularization of deep learning \\citep{hochreiter1991untersuchungen, bengio1994learning}. In its essence, given a loss function  $\\mathcal{L}: \\mathbb{R}^{n \\times d_v} \\to \\mathbb{R}$, vanishing gradients occur when the norm of the gradient of the loss $\\mathcal{L}$ with respect to the parameters of the network $\\Wm$ --- which we indicate as $\\norm{\\frac{\\partial \\mathcal{L}}{\\partial \\Wm}}$ --- is too small to provide enough backpropagating signal, thus hindering gradient-based optimization methods. Despite extensive research toward understanding and overcoming the problem in disparate contexts \\citep{glorot2010init,he2015delving,hanin2018neural,zhang2019fixup}, a formal explanation of its role in relatively new architectures such as Transformers is largely missing in the literature, with a few exceptions ~\\citep{xiong2020layer,wang2022deepnet, huang2020improving}. In our paper (Section \\ref{sec:vanishing_gradients}), we show how vanishing gradient occurs in conjunction with the rank collapse issue identified by \\cite{dong2021attention}. \n\n\n\\paragraph{Signal Propagation in Random Networks at Initialization.}\nAfter addressing the question on the effects of rank collapse, we take a step back and rigorously analyze its causes by looking at how the properties of the input sequence $\\Xm$ are lost/preserved as it propagates through a randomly initialized Transformer. More specifically, we focus on two aspects of the propagating sequence: the expected Frobenius norm $\\Exp \\norm{\\Xm^\\ell}^2$ and the expected inner product between different tokens $\\Exp \\langle\\Xm_k, \\Xm_k'\\rangle$, with $k \\neq k'$. The former is linked to a number of studies on the initialization of neural networks at the \\emph{edge of chaos}  \\citep{poole2016exponential, schoenholz2016deep}, and vanishing/exploding gradients \\citep{hanin2018neural}. The latter quantity describes how the geometry of the feature space changes after applying a Transformer block, and is related to the concept of \\emph{dynamical isometry} \\citep{saxe2013exact}. To understand the evolution of the inner product, we analyze the following measure of correlation \\citep{nachum2021johnson, cho2009kernel}: \n\\begin{equation}\n\\label{eq:tokens_corr}\n    \\rho^\\ell_{kk'} := \\frac{\\Exp\\langle\\Xm^\\ell_{k} , \\Xm^\\ell_{k'}\\rangle}{\\sqrt{\\Exp\\norm{\\Xm_k^\\ell}^2\\Exp\\norm{\\Xm_{k'}^\\ell}^2}}.\n\\end{equation}\nNote that $\\rho^\\ell_{kk'} = 1$ if and only if the $k$-th and $k'$-th tokens are perfectly aligned ($\\cos \\theta_{kk'}=1$). We stress that in our case --- differently from the aforementioned works --- instead of analyzing the relationship between two different data points, we study the relationship between tokens of the same sequence. \n\n\n\n\n\\section{Theoretical Results}\n\\label{sec:theory}\n\n\n\\subsection{Vanishing Gradients for Queries and Keys under Rank Collapse}\n\\label{sec:vanishing_gradients}\nTo investigate the problem of vanishing gradients in the attention layers, we make use of the framework of matrix calculus~\\citep{magnus2019matrix,singh2021analytic}. In particular, we compare the expected Frobenius norm of the gradient of a self-attention layer with respect to its parameters: $\\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\Wm}\\right\\|^2_F$, where here $\\Wm$ indicates one of the keys, queries or values weight matrices. \nDue to the well-known difficulty of computing expectations of the softmax \\citep{daunizeau2017semi, shekhovtsov2018feed}, throughout this manuscript, we make the simplifying assumption that the softmax output is the uniform distribution at initialization, i.e. the $n \\times n$ matrix containing $\\frac{1}{n}$ in each entry.\n\nwhere $\\bm{1}_{n \\times n}$ is the matrix with all entries equal to $1$.\nCrucially, in Appendix \\ref{app:assumption_unif_soft}, we formally show that \\emph{this assumption holds almost surely} in the limit $d_k\\to\\infty$. There, we also experimentally show that even in the more realistic case where $d_k = d_v \\approx 512$, the empirical simulations provide a surprisingly faithful approximation of the theoretical insights presented in this paper. \n\n\nWe define the mean token $\\bar{\\bm{x}}^{\\ell}$ through its components $\\bar{\\bm{x}}^{\\ell}_i = \\frac{1}{n}\\sum_{k=1}^n \\Xm^{\\ell}_{ki}$, $i \\in [d_v]$. In the following theorem, we compute the expected gradients of an attention layer at initialization, and set the basis for our following analysis. We provide the results only for the queries, as the case for the keys is analogous.\n\n\\begin{tcolorbox}\n\n\\end{tcolorbox}\n\nWe defer the precise study of the scaling of these quantities as a function of $n$ and $d_v,d_k$, to Section~\\ref{sec:dep_angle}.\nAt this stage, it is crucial to note that $\\frac{1}{n} (\\Xm^{\\ell})^\\top\\Xm^{\\ell} - \\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top$ is the centered empirical covariance matrix of the tokens' representations. It is easy to see that if $\\Xm^{\\ell}$ is a rank-$1$ matrix, then all the rows of $\\Xm^{\\ell}$ are proportional to a fixed $d_{v}$-dimensional vector, and the empirical covariance matrix has all zero entries. Introducing a loss function $\\mathcal{L}: \\mathbb{R}^{n \\times d_v} \\to \\mathbb{R}$, we make the statement on vanishing gradients more formal in the following theorem:\n\\vspace{15px}\n\\begin{tcolorbox}\n\n\\end{tcolorbox}\n\\vspace{15px}\n\nThe proof simply relies on expanding the norm of the gradient of the loss with the aid of the chain rule and then bounding it by the product of the norms of each term of the chain. The final result holds with an application of Lemma~\\ref{lemma:gradients_queries}, in which the rank-$1$ assumption makes $\\mathbb{E}\\norm{\\frac{\\partial \\Sm^{\\ell}}{\\partial \\Wm^{Q,\\ell}}}$ vanish.\n\\vspace{15px}\nIn light of Theorem \\ref{thm:vanishing_gradients}, we can conclude that the rank collapse issue originally identified in \\cite{dong2021attention} corresponds to an initialization in a region of vanishing gradient signal in the subspace of parameters identified by the queries and keys. How can this affect training? One may argue that if rank collapse does not happen in the very first layer, then the corresponding gradients are non-zero, and the rank of the subsequent layers --- affected by rank collapse --- can be increased with the first few steps of gradient descent. In practice, we show empirically in Fig. \\ref{fig:adam_postln} that escaping this pathological landscape is harder in deeper nets.\n\\vspace{15px}\n\\subsection{Forward Signal Propagation and the Importance of Scaling the Residual Branches}\n\\label{sec:forward_pass}\n\\vspace{10px}\nWe now turn our attention to the study of the influence of skip connections in transformers. \\cite{dong2021attention} showed that simply adding skip connections prevents rank collapse. Somewhat surprisingly, we show that while the claim holds for any finite depth, the average angle between different tokens quickly increases with just a few layers, and as $L\\to \\infty$ a Transformer can still lose rank unless the residual branches are adequately initialized. As~\\cite{dong2021attention} showed that layer normalization does not avoid rank collapse, we omit it in our analysis.\nFirstly, we introduce two lemmas on the propagation of inner products (Lemma \\ref{lemma:propagation_of_inner_producets}) and the norm (Lemma \\ref{thm:forward_pass}) of the tokens' representations. \n\\vspace{10px}\n\\begin{tcolorbox}\n\n\\end{tcolorbox}\n\\vspace{15px}\nNote that $C(\\Xm^\\ell) = n^2 \\norm{\\bar{\\bm{x}}^\\ell}^2 $. The lemma on the propagation of the norm is slightly more involved:\n\n\n\\begin{tcolorbox}\n\n\\end{tcolorbox}\nThe proof of Lemma \\ref{thm:forward_pass} consists in expanding $\\Exp \\norm{\\Xm^{L}}_{F}^2$ according to the defining equations for the Transformer, and simplifying the expression by using iterated expectations $\\Exp \\norm{\\Xm^{L}}_{F}^2 = \\mathbb{E}[\\Exp [\\norm{\\Xm^{L}}_{F}^2 | \\Xm^{\\ell}]]$ to exploit the conditional independence between different layers, and then computing the expectations using the independence assumption on the weights. The expression on the right-hand side will then depend on $ \\Xm^{\\ell}$ only through its norm $\\norm{\\Xm^{\\ell}}$ and the norm of the mean token $\\norm{\\bar{\\bm{x}}^{\\ell}}^2$. Using Lemma \\ref{lemma:propagation_of_inner_producets} then allows us to unroll the recursion and get the final result. The complete proof, together with the proof of Lemma \\ref{lemma:propagation_of_inner_producets}, can be found in Appendix \\ref{app:forward_pass}. \n\nThe previous Lemma provides theoretical justification that scaling the residual branches by setting the alpha parameters to be $\\mathcal{O}(1/\\sqrt{L})$ allows both the norm of the propagating input and the inner products between different tokens to be approximately preserved. Hence, the information contained in the input is not lost, even in the infinite depth limit.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/residual_scaling_post.pdf}\n    \\caption{Effect of the residual scaling to the norm of the gradients of the network at initialization with respect to some loss. From left to right: (a) the cosine of the angle between tokens increases with depth. Note how larger values of $\\alpha_1, \\alpha_2$ imply a faster token alignment with depth (Theorem~\\ref{thm:exp_cosine}). Subplots (b) and (c) show the gradients of the queries-keys and values parameters respectively by increasing depth, compared to the corresponding norms of the first layer. Gradients for the queries-keys diminish with depth, while the opposite happens for the values. We use POST-LN to disentangle the effect of the variance of the input.}\n    \\label{fig:residual_scaling}\n\\end{figure}\n\n\\paragraph{Residual Scaling Preserves Correlations.}\nWe now prove that without the depth-dependent residual scaling (i.e. with $\\alpha_1=\\alpha_2=1$) the correlation between the tokens quickly increases, and reaches perfect alignment in the infinite depth limit.\n\nMore specifically, our argument shows that in this limit, the correlation between different tokens $\\rho^\\ell_{k,k'}$ as in Eq.~\\eqref{eq:tokens_corr}  converges to 1, implying rank collapse. Furthermore, we show how setting the residual parameters $\\alpha_1$ and $\\alpha_2$ as dictated by Theorem \\ref{thm:forward_pass}, ensures that the correlation measure is dependent on the input in a non-trivial way even at infinite depth. \nTo this end, we introduce the average correlation at layer $\\ell$: \n\\begin{equation}\n    \\rho^\\ell = \\frac{1}{n(n-1)}\\sum_{k\\neq k'}\\rho^\\ell_{kk'} .\n\\end{equation}\nNote that $\\rho^\\ell = 1$ if and only if every pair of tokens is perfectly aligned.\n\nWe are now ready to formalize the influence of the $1/\\sqrt{L}$-scaling on the correlation between tokens' representations by stating Theorem \\ref{thm:exp_cosine}.\n\\begin{tcolorbox}\n\n\\end{tcolorbox}\n\nThe proof consists in noting that due to the symmetry of the problem at initialization, for a fixed layer the expected norm of each token is the same. Hence, by our definition of $\\rho^\\ell_{kk'}$, we can write $\\Exp\\langle\\Xm^\\ell_k, \\Xm^\\ell_{k'}\\rangle = \\rho^\\ell_{kk'} \\Exp\\norm{\\bm{x}^\\ell}^2$. By summing over the $k,k'$ indexes, the resulting equation will depend on $\\Exp[C(\\Xm^\\ell)]$ and $\\Exp \\norm{\\Xm^\\ell}^2$, which can be expanded using Lemma \\ref{lemma:propagation_of_inner_producets} and \\ref{thm:forward_pass} respectively. The result is then given by solving for $\\rho^\\ell$. \n\n\\begin{wrapfigure}{r}{0.4\\textwidth}\n\\vspace{-5px}\n\\centering\n\\includegraphics[scale = 0.38]{figures/scalings_correlation.pdf}\n\\caption{Evolution of Correlation in Transformers with (dashed lines) and without (solid lines) $1/\\sqrt{L}$-scaling for PRE-LN, POST-LN and without layer normalization (No-LN).}\n\\label{fig:corr}\n\\vspace{-5px}\n\\end{wrapfigure}\n\n\\vspace{10px}\nNote that under the $1/\\sqrt{L}$-scaling, the correlation term is one if and only if $C(\\Xm) = n\\norm{\\Xm}^2$, which holds in the degenerate case where all the input tokens are perfectly aligned. In Appendix \\ref{app:res_scaling_proofs}, we give precise formulas for the expected correlations at any depth, showing that $\\rho^\\ell$ reaches values close to one even for relatively shallow networks when the $1/\\sqrt{L}$-scaling is not adopted (see also Fig. \\ref{fig:residual_scaling}~(left)). Additionally, in Fig.~\\ref{fig:corr}, we empirically show that in the presence of the $1/\\sqrt{L}$-scaling, layer normalization (either PRE or POST) does not significantly affect the evolution of the correlations. On the other hand, without the residual scaling, PRE-LN seems to alleviate the rate of increase of $\\rho^\\ell_{kk'}$. It is intriguing that most deep Transformer models use this configuration~\\citep{brown2020language}. We provide more extensive empirical results in Appendix~\\ref{app:more_experiments}.\n\n\\vspace{10px}\nNote that the $1/\\sqrt{L}$ scaling for the residual branches has been previously studied in the context of stabilization of residual networks (see Section \\ref{sec:related_work}), here we extend these results to Transformers and provide new insights on its role in the context of rank preservation.\nFinally, note that by setting $\\tilde{\\alpha}_1, \\tilde{\\alpha}_2 = 0$, we recover the so called \"ReZero\" initialization \\citep{bachlechner2021rezero}. In this context, the $1/\\sqrt{L}$ scaling extends this framework as it allows for wider range of values for $\\tilde{\\alpha}_1, \\tilde{\\alpha}_2$ while still guaranteeing stability. \nWe mention here that extending these results from the linear activation to the ReLU case is known to be a hard problem, due to the technical difficulty of propagating the inner products across ReLU layers that are shared among the tokens (this is the case in the position-wise feed-forward layers in Transformers). Exact formulas can be found only in the case of one ReLU layer with Gaussian inputs in \\cite{cho2009kernel}. \n\n\\subsection{Dependence on the Angle between Tokens and the Input Norm}\n\\label{sec:dep_angle}\nIn this section, we drop the superscript $\\ell$ as it is obvious from context and assume for simplicity that $d_k = d_v$. To gain a better intuition on the factors that affect the gradients and provide additional insights, we study the case in which every pair of distinct tokens are  zero-mean Gaussian random variables, correlated in the same way, i.e $\\rho^\\ell_{ii'} = \\rho$ for $i \\neq i'$ or more precisely\n\\begin{equation*}\n    \\Exp\\left[\\Xm_{i,j}\\Xm_{i',j'}\\right] = \\begin{cases}\n    0 & j\\ne j' \\ \\ \\text{(independent dimensions)}\\\\\n    \\sigma^2_x & i=i', j=j'\\\\\n    \\rho\\sigma^2_x & i\\ne i', j=j'\n    \\end{cases}.\n\\end{equation*}\nTo see that this equation satisfies our definition of the correlation metric, note that $\\Exp[\\norm{\\Xm_i}^2] = d\\sigma_x^2$ and $\\Exp\\langle \\Xm_{i}, \\Xm_{i'} \\rangle = d\\sigma_x^2 \\rho$, for $i \\neq i'$.\nThen, the expected norm of the gradients for the values (Eq.~\\eqref{eq:jacobian_values}) simplifies to\n\\begin{equation}\n     \\Exp \\norm{\\frac{\\partial \\Sm}{\\partial \\wV}}_F^2 = \\sigma_x^2 d^2 \\left(1 + \\rho (n-1)  \\right).\n\\label{eq:grad_V}\n\\end{equation}\n\nBy making the additional assumption that the norm and the correlation propagate independently, the respective norm for the queries --- and symmetrically the keys --- (Eq.~\\eqref{eq:jacobian_queries}) reduces to:\n\\begin{equation}\n    \\Exp \\left\\|\\frac{\\partial \\Sm}{\\partial \\wQ}\\right\\|_F^2 = \\sigma_x^6 \\frac{(n-1)}{n} (1 - \\rho)^2 d (n + d).\n\\label{eq:grad_Q}\n\\end{equation}\nIn Appendix~\\ref{sec:constan_cosine_analysis} we provide a rigorous proof, that relies on Isserlis theorem~\\citep{isserlis1918formula} to compute higher-order moments. The above expressions reveal the different dependencies on four main actors, that we inspect separately here. The gradients of the queries depend via a cubic function on the \\emph{variance of the input, $\\sigma_x^2$}, compared to a linear for the values. This provides an additional interpretation of the successful use of layer normalization, as in~\\cite{xiong2020layer}, either in the POST-LN or PRE-LN format, that standardizes the input variance $\\sigma_x^2$ to the value $1$. \n\n\nNext, we emphasize the dependence on the \\emph{correlation between the tokens}, also illustrated in Fig.~\\ref{fig:residual_scaling}. Importantly, note how the queries/keys have opposite monotonic functional dependence with respect to $\\rho$ compared to the values. As revealed by Theorem~\\ref{thm:exp_cosine} and Fig.~\\ref{fig:residual_scaling}~(center), inappropriate scaling of the residual branches can already lead to this phenomenon even in a relatively shallow network. \n\nFinally, Eq.~\\eqref{eq:grad_V} and~\\eqref{eq:grad_Q} reveal a different scaling in terms of the \\emph{embedding size $d$} and the \\emph{sequence length $n$} due to the self-attention operation itself. We hope that the identification of the different dependencies in the gradients of the parameters will inspire a new line of works aimed at solving some of the difficulties in training Transformers. \n\n\n\\subsection{Are Adaptive Methods really needed for training Transformers?}\n\\label{sec:adaptive}\n\n\\begin{wrapfigure}{r}{0.50\\textwidth}\n\\vspace{-15px}\n\\centering\n\\includegraphics[width=6.5cm]{figures/adam_adaptiviy.pdf}\n\\caption{Adaptive learning rates computed by Adam in Transformers.}\n\\label{fig:adam-adap}\n\\vspace{-10px}\n\\end{wrapfigure} \n\nThe existence of the discrepancy in the magnitude of the gradients with respect to the weights $\\Wm^{Q}, \\Wm^{K}$ and $\\Wm^{V}$, might explain the success of adaptive optimization algorithms, as illustrated in Fig.~\\ref{fig:adam-adap}, where we plot the effective learning rate computed by Adam~\\citep{kingma2014adam} in a toy encoder task (more details in Appendix~\\ref{app:experimental_setup}). Hence, we embark on a preliminary exploration to train a Transformer architecture with SGD with the intent of matching Adam's performance. \nBased on our theory, we propose a simple architectural modification, an inverse temperature scaling $\\tau \\in \\mathbb{R}$ inside the softmax:\n\\begin{equation}\n    \\Sm^{\\ell} := \\text{softmax}\\left( \\frac{\\tau}{\\sqrt{d_k}}\\Xm^{\\ell}\\Wm^{Q}\\left(\\Xm^{\\ell}\\Wm^{K}\\right)^\\top \\right) \\Xm^\\ell \\Wm^{V}.\n\\end{equation}\nA direct consequence of our analysis is that $\\tau$ allows controlling the magnitude of the gradients for the queries and keys' parameters.\n\nWe evaluate our proposal, consisting of residual scaling and the aforementioned inverse temperature parameters, on the widely used IWSLT14 German-to-English (De-En) benchmark translation task. All details regarding the experimental setup and the choice of inverse temperature used are provided in Appendix~\\ref{app:experimental_setup}. We train a Transformer encoder-decoder of varying depth with SGD, after removing all normalization layers and adequately initializing the residual connections. For our training with SGD, we avoid using any learning rate warm-up, as commonly done for Adam, and instead use a step-scheduler to decrease the learning rate at 40\\\n\n\\begin{figure}\n\\CenterFloatBoxes\n\\begin{floatrow}\n\\ffigbox\n  {\\includegraphics[width=0.5\\textwidth]{figures/translation.pdf}}\n  {\\caption{\\small{BLEU scores by increasing the number of transformers blocks. `X' Transformer blocks implies in total `X' encoder self-attention, `X' decoder self-attention and `X' decoder cross-attention layers.}}\\label{fig:translation_fig}}\n\\killfloatstyle\n\\ttabbox\n   {\n   \\small\n   \\renewcommand{\\arraystretch}{1.2}\n   \\begin{tabular}[b]{lc}\\hline\n     Method (6L-Encoder / 6L-Decoder) & BLEU $\\uparrow$ \\\\ \\hline\n     SGD POST-LN & 31.36\\\\\n     SGD res-scale & 32.79\\\\\n     SGD temperature & \\textbf{35.69} \\\\ \n     \\hline\n     Adam POST-LN~\\citep{vaswani2017attention} & 35.39\\\\\n     Adam PRE-LN~\\citep{vaswani2017attention} & 35.10 \\\\\n     ReZero~\\citep{bachlechner2021rezero} & 34.55\\\\\n     T-Fixup~\\cite{zhang2019fixup} & 35.59\\\\\n     \\hline\n     \\end{tabular}\n  }\n  {\\caption{\\small{BLEU scores for the IWSLT14 German-to-English translation task. \\textit{SGD res-scale} refers to the training of SGD without layer normalization and initialization of the residual scaling $a_1 = a_2 = \\frac{1}{\\sqrt{L}}$. \\textit{SGD temperature} additionally employs an inverse temperature inside the softmax.}\\label{tab:translation_results}}}\n\\end{floatrow}\n\\end{figure}\n\nOur proposed method considerably improves training with SGD, keeping up and in some cases surpassing any results achieved by the Adam optimizer. We are also able to train deeper networks without the use of layer normalization. We leave for future work to further investigate modifications or alternatives to the self-attention operation.\n\n\\section{Related Work}\n\\label{sec:related_work}\n\nOur work builds upon the rich literature on forward and backward signal\npropagation in random neural networks \\citep{poole2016exponential,schoenholz2017deepinformationpropagation,pmlr-v80-xiao18a,NIPS2017_d9fc0cdb,orvieto2021vanishing, noci2021precise, zavatone2021exact}.\n\n\nThe $1/\\sqrt{L}$ scaling scheme has been investigated in the literature for the stabilization of residual networks \\citep{hanin2018start, arpit2019initialize, allen2019convergence, hayou2021stable}.\n\nOur work draws inspiration from a series of recent works studying the rank of the representations of random feed-forward neural networks at initialization \\citep{jonas, daneshmand2020batch}. In the context of Transformers, \\cite{dong2021attention} has recently identified the rank collapse issue object of study of the present work. Thanks to our analysis of the backward pass, we are able to demonstrate that rank collapse in Transformer architectures leads to vanishingly small gradients of queries and keys, thereby preventing effective training and allowing us to complete the analysis of \\citep{dong2021attention}.\n\n\nAmong the architectural components in Transformers, layer normalization is, arguably, one of the most important -- and debated -- ones \\citep{chen2018best,wang2019learning,nguyen2010estimating,xiong2020layer}. In the original architecture \\citep{vaswani2017attention}, layer normalization is used to stabilize the forward pass by reducing the variance of the inputs to the following sublayer. Our analysis of the forward pass shows that its inclusion is not strictly necessary for the purpose of controlling the norm of the representations. For a theoretical analysis of signal propagation in the presence of layer norm, we refer the reader to \\cite{xiong2020layer}.\n\nAdditionally, our theoretical study of the backward pass provides a rigorous explanation of the empirically observed discrepancy between the magnitude of the gradients of the queries and the values, which \\cite{liu2020understanding} hypothesize to be one of the causes of the success of adaptive methods in training Transformers \\citep{liuadam,zhang2020adaptive,huang2020improving}.\n\nFinally, properly rescaled residual connections have been found to be beneficial for training Transformers by a number of recent research works \\citep{zhang2019fixup, bachlechner2021rezero, wang2022deepnet}. However, none of these studies characterize the impact of skip connections on rank propagation, while our analysis suggests a theoretically-grounded way to stabilize it.\n\n\\section{Conclusions and Future Work}\nIn this paper, we showed how, at initialization, rank collapse and more generally high correlation in the tokens, causes vanishing gradients of the queries and keys of a Transformer architecture. While residual connections help mitigate rank collapse at finite depth, we showed that they alone cannot prevent high alignments of the tokens' representations --- unless properly scaled by a $1/\\sqrt{L}$-factor. Finally, we have also discovered counter-intuitive dependencies on the variance of the input, embedding size, and sequence length, potentially causing large differences between the gradients of queries/keys compared to the values' parameters. Hence, we conclude that one of the strengths of Transformers lies in their carefully designed architecture together with an adequate initialization. \nFinally, we gave preliminary evidence that one of the factors contributing to the higher efficacy of Adam compared to SGD in training Transformers arises from the disproportionate magnitude of gradients as postulated by our theory.\nNonetheless, other factors might further accentuate the difference between these two algorithms during training, leaving the door open for further research regarding the benefits of adaptive optimization methods with Transformers.\n\n\n\\section*{Acknowledgements}\n\nWe thank our colleague Jonas Kohler who provided insights and expertise that greatly assisted the research.\n\n\n\\bibliographystyle{plainnat}\n\\bibliography{main}\n\n\n\n\n\\newpage\n\n\\appendix\n\\addcontentsline{toc}{section}{Appendix}\n\\part{Appendix} \n\\parttoc \n\n\n\\section{Proof of Theorems}\nRecall the defining equations of a Transformer:\n\\begin{align*}\n    & \\Zm^{\\ell} = \\alpha_1 \\Sm^{\\ell}+ \\Xm^{\\ell} \\\\\n    & \\Ym^{\\ell} = \\sigma(\\Zm^{\\ell} \\Wm^{F,1})\\Wm^{F,2} \\\\\n    & \\Xm^{\\ell+1} = \\alpha_2 \\Ym^{\\ell} +  \\Zm^{\\ell} ,\n\\end{align*}\nwhere the self attention layers are defined as follows:\n\\begin{equation*}\n    \\Sm^{\\ell} := \\Am^\\ell \\Xm^\\ell \\Wm^{V} , \\text{ where } \\Am^\\ell = \\text{softmax}\\left( \\frac{1}{\\sqrt{d_k}}\\Xm^{\\ell}\\Wm^{Q}\\left(\\Xm^{\\ell}\\Wm^{K}\\right)^\\top \\right).\n\\end{equation*}\nWe remark that when it is clear from the context, we suppress the $\\ell$ index to improve readability.\n\n\\textbf{Initialization}: Recall that we initialize our weights with the so called ``Xavier''~\\citep{glorot2010understanding} or ``He''~\\citep{he2015delving} initialization: each weight is sampled independently from a distribution with zero-mean and variance $\\sigma^2_v = \\frac{1}{d_v}$ for the values and feedforward weights and $\\sigma^2_k = \\frac{1}{d_k}$ for the queries and keys. \n\n\\textbf{Kronecker Delta}: we introduce the Kronecker Delta notation \n\\begin{equation*}\n    \\delta_{ab} = \n    \\begin{cases}\n        1 & a=b \\\\\n        0 & a\\neq b\n    \\end{cases}\n\\end{equation*}\nand, similarly: $\\delta_{a\\neq b} = 1 - \\delta_{ab}$.\n\n\\textbf{Notation}: in this section, we also adopt the following shorthand notation for the argument of the softmax $\\Mm:=\\frac{\\Xm\\wQ{\\wK}^\\top \\Xm^\\top}{\\sqrt{d_k}}$. We will first compute the gradients with respect to values, queries and input (note that the gradients of the keys have the form as the queries, hence we omit the derivation). Recall that for a matrix $\\Xm$, we use $\\Xm_k$ to indicate its $k$-th row. Finally, we indicate with $\\bm{1}_{n\\times n}$ the $n \\times n$ matrix with all ones, with $\\bm{1}_n$ the columns vector with all ones, and with $\\Im_n$ the $n$-dimensional identity matrix. \n\n\\subsection{Backward Pass: Proofs of Lemma \\ref{lemma:gradients_queries} and Theorem \\ref{thm:vanishing_gradients}}\nIn this section, we now look at the proofs of Lemma \\ref{lemma:gradients_queries} and Theorem \\ref{thm:vanishing_gradients}. We will first introduce our notation for the gradients, as well as some useful properties of the Kronecker product.\n\n\\subsubsection{Preliminaries}\nFor the gradients, we avoid directly working with tensors by vectorizing the matrices in a \\textit{row-wise} fashion ($\\vect_r$) and arranging the Jacobian in the numerator layout. More formally,\n\n$$\\dfrac{\\partial \\mb Y}{\\partial \\mb X} := \\dfrac{\\partial \\vect_r(\\mb Y)}{\\partial \\vect_r(\\mb X)^\\top}\\,.$$\n\nAlongside this, we use the following rule ($\\kro$ is the Kronecker product):\n\\begin{align}\n\\label{eq:matrix-derivative}\n \\frac{\\partial \\mb A \\mb W \\mb B}{\\partial \\mb W} = \\mb A \\kro \\; \\mb B^\\top\\,.\n\\end{align}\n\nFor the proof of this rule, we refer to \\cite{singh2021analytic}, and to \\cite{magnus2019matrix} for a complete introduction to matrix calculus. \nWe will also use the following well-known properties of the Kronecker product.\n\\begin{tcolorbox}\n\\begin{lemma}\n\\label{lemma:prop_kro}\nGiven the matrices $\\Am \\in \\mathbb{R}^{n \\times m}$, $\\Bm \\in \\mathbb{R}^{p \\times q}$, $\\Cm \\in \\mathbb{R}^{m \\times r}$, $\\Dm \\in \\mathbb{R}^{q \\times s}$, then the following holds:\n\\begin{equation}\n    \\label{eq:trace_kro}\n    \\tr(\\Am \\kro \\Bm) = \\tr(\\Am)\\tr(\\Bm) ,\n\\end{equation}\nand \n\\begin{equation}\n    \\label{eq:prod_kro}\n    (\\Am \\kro \\Bm)(\\Cm \\kro \\Dm) = (\\Am \\Cm)\\kro(\\Bm \\Dm).\n\\end{equation}\n\\end{lemma}\n\\end{tcolorbox}\n\n\\subsubsection{Proof of Lemma~\\ref{lemma:gradients_queries}}\nIn Lemma \\ref{lemma:grads_SA} and Lemma \\ref{lemma:grads_SA_X} we compute the gradients with respect to the queries, values and $\\Xm$, respectively. Then we use these results to prove Lemma \\ref{lemma:gradients_queries} by computing the expectation of the Frobenius norms. \n\n\\begin{tcolorbox}\n\\begin{lemma}[Gradients of Self Attention for parameter matrices]\n\\label{lemma:grads_SA}\nThe gradients of the self attention layer defined in Eq.~\\eqref{eq:self_att} have the following form:\n\\begin{align*}\n     & \\frac{\\partial \\Sm}{\\partial \\wV} =    \\soft\\left(\\frac{\\Xm\\wQ{\\wK}^\\top \\Xm^\\top}{\\sqrt{d_k}}\\right)\\Xm\\, \\kro \\Im_{d_v} \\, \\\\\n     & \\frac{\\partial \\Sm}{\\partial \\wQ} = \\left(\\Im_n \\kro {\\wV}^\\top\\Xm^\\top\\right) \\frac{\\partial \\Am}{\\partial \\Mm} \\left(\\frac{\\Xm\\kro\\Xm\\wK}{\\sqrt{d_k}}\\right),\n\\end{align*}\nwhere the gradients of the softmax with respect to its inputs are as follows:\n\\begin{equation}\n\\label{eq:grad_soft_complete}\n    \\frac{\\partial \\Am}{\\partial \\Mm} = \\blockdiag\\Bigg( \\dfrac{\\partial \\Am_{i}}{\\partial \\Mm_{i}^\\top}\\Bigg)\n\\end{equation}\nand where $ \\dfrac{\\partial \\Am_{i}}{\\partial \\Mm_{i}^\\top}=\\diag(\\Am_{i}) - \\Am_{i}\\Am_{i}^\\top$ with $\\Am_{i}$ being the $i$-th row of $\\Am$ in column vector format.\\\\\nFinally, note that under the uniform-attention assumption, Eq.~\\eqref{eq:grad_soft_complete} simplifies to:\n\\begin{equation}\n\\label{eq:grad_soft}\n    \\frac{\\partial \\Am}{\\partial \\Mm} = \\frac{1}{n}\\Im_n \\kro \\left(\\Im_n - \\frac{1}{n}\\bm{1}_{n\\times n} \\right).\n\\end{equation}\n\\end{lemma}\n\\end{tcolorbox}\n\n\n\\begin{proof}\nLet's start with the simple case of the values' weights $\\wV$. Using the rule in Eq.~\\eqref{eq:matrix-derivative}, it is immediate that:\n\\begin{equation*}\n \\frac{\\partial \\Sm}{\\partial \\wV} =    \\soft\\left(\\frac{\\Xm\\wQ{\\wK}^\\top \\Xm^\\top}{\\sqrt{d_k}}\\right)\\Xm\\, \\kro \\Im_{d_v} = \\Am\\Xm\\, \\kro \\Im_{d_v} \\,.\n\\end{equation*}\nFor the queries, a simple application of the chain rule and then again Eq.~\\eqref{eq:matrix-derivative} gives:\n\\begin{align*}\n    \\frac{\\partial \\Sm}{\\partial \\wQ} &= \\frac{\\partial \\Sm}{\\partial \\Am} \\frac{\\partial \\Am}{\\partial \\wQ}\n     = \\frac{\\partial \\Sm}{\\partial \\Am} \\frac{\\partial \\Am}{\\partial \\Mm} \\frac{\\partial \\Mm}{\\partial \\wQ} \\\\\n     &= \\left(\\Im_n \\kro {\\wV}^\\top\\Xm^\\top\\right) \\frac{\\partial \\Am}{\\partial \\Mm} \\left(\\frac{\\Xm\\kro\\Xm\\wK}{\\sqrt{d_k}}\\right) \\, ,\n\\end{align*}\nwhich is the desired results. Finally, for the gradients of the softmax note that:\n\\begin{equation*}\n    \\frac{\\partial{\\Am_{pq}}}{\\partial \\Mm_{ij}} =\n    \\frac{\\partial}{\\partial \\Mm_{ij}}\\frac{\\exp(\\Mm_{pq})}{\\sum_k \\exp(\\Mm_{pk})} =  \\delta_{ip}\\delta_{jq} \\Am_{ij} - \\delta_{ip} \\Am_{iq}\\Am_{ij} .\n\\end{equation*}\nBy writing the above expression in the matrix notation described above, we obtain the desired result. More specifically, the block diagonal structure is given from the term $\\delta_{ip}$ which stems from the fact that the softmax is applied row-wise. \n\\end{proof}\n\\begin{tcolorbox}\n\n\n\\begin{lemma}[Gradients of Self Attention with respect to the Embedding matrix]\n\\label{lemma:grads_SA_X}\nThe gradients of the self attention layer with respect to the embedding matrix $\\Xm$ defined in Eq.~\\eqref{eq:self_att} have the following form\n\\small{\n\\begin{align}\n\\label{eq:grad_inp}\n    \\frac{\\partial\\Sm}{\\partial\\Xm} \n    &= \\frac{1}{\\sqrt{d_k}}(\\Im_n\\kro \\wVT\\Xm^\\top)\\,\\frac{\\partial \\Am}{\\partial \\Mm}  \\,\\left(\\Im_n\\kro\\Xm\\wK\\wQT + \\Km_{nn}(\\Im_n\\kro\\Xm\\wQ\\wKT)\\right) \\,+ \\,\\Am\\kro{\\wV}^\\top,\n\\end{align}}\\normalsize\nwhere the gradients of the softmax with respect to its inputs are denoted by $\\frac{\\partial \\Am}{\\partial \\Mm}$ as before.\n\\end{lemma}\n\\end{tcolorbox}\n\\begin{proof}\nRemember that we defined $\\Sm = \\soft(\\extra{\\frac{1}{\\sqrt{d_k}}}\\Xm\\wQ\\wKT\\Xm^\\top)\\Xm\\wV$. Alongside with our previous shorthands $\\Am$, $\\Mm$, let us define the remaining $\\Xm\\wV$ as a matrix $\\Tm$, so that $\\Sm=\\Am\\,\\Tm$. Both $\\Am$ and $\\Tm$ are functions of $\\Xm$. So the matrix differential can be written as:\n\\begin{align}\n    \\frac{\\partial\\Sm}{\\partial\\Xm} &=  \\frac{\\partial\\Sm}{\\partial\\Am}\\frac{\\partial\\Am}{\\partial\\Xm} + \\frac{\\partial\\Sm}{\\partial\\Tm}\\frac{\\partial\\Tm}{\\partial\\Xm}\\\\\n    &=  \\frac{\\partial\\Sm}{\\partial\\Am}\\frac{\\partial\\Am}{\\partial\\Mm} \\frac{\\partial \\Mm}{\\partial\\Xm} + \\frac{\\partial\\Sm}{\\partial\\Tm}\\frac{\\partial\\Tm}{\\partial\\Xm}\\\\\n    &= (\\Im_n\\kro \\wVT\\Xm^\\top)\\,\\frac{\\partial \\Am}{\\partial \\Mm}  \\,\\frac{\\partial \\Mm}{\\partial \\Xm} + (\\Am\\kro\\Im_d)(\\Im_n\\kro\\wVT)\\\\\n    &= (\\Im_n\\kro \\wVT\\Xm^\\top)\\,\\frac{\\partial \\Am}{\\partial \\Mm}  \\,\\frac{\\partial \\Mm}{\\partial \\Xm} + (\\Am\\kro\\wVT)\\label{eq:grad-SA-X}\n\\end{align}\n\n\nNext, we use the matrix differential and then the identification theorem of matrix derivatives to compute the matrix gradient $\\frac{\\partial \\Am}{\\partial \\Xm}$\n\\begin{align*}\n    \\mathrm{d} \\Am &=  \\extra{\\frac{1}{\\sqrt{d_k}}}\\mathrm{d}(\\Xm)\\, \\wQ\\wKT\\Xm^\\top + \\extra{\\frac{1}{\\sqrt{d_k}}}\\Xm\\wQ\\wKT  \\,\\mathrm{d}(\\Xm^\\top).\n\\end{align*}\n\nVectorizing both sides:\n\\begin{align*}\n    \\mathrm{d} \\vect_r(\\Am) &=  \\extra{\\frac{1}{\\sqrt{d_k}}}(\\Im_n\\kro\\Xm\\wK\\wQT)\\mathrm{d}(\\vect_r(\\Xm))\\, + \\,\\extra{\\frac{1}{\\sqrt{d_k}}}(\\Xm\\wQ\\wKT \\kro\\Im_n) \\,\\mathrm{d}(\\vect_r(\\Xm^\\top)) \\\\\n    &=  \\extra{\\frac{1}{\\sqrt{d_k}}}(\\Im_n\\kro\\Xm\\wK\\wQT)\\mathrm{d}(\\vect_r(\\Xm))\\, + \\,\\extra{\\frac{1}{\\sqrt{d_k}}}(\\Xm\\wQ\\wKT \\kro\\Im_n) \\Km_{dn}\\,\\mathrm{d}(\\vect_r(\\Xm)).\n    \n\\end{align*}\n\nRecall, for an arbitrary matrix $\\Bm\\in\\mathbb{R}^{m\\times n}$, the commutation matrix $\\Km_{mn}$ transforms columnwise vectorization into rowwise vectorization. More precisely,\n\\begin{align*}\n    \\Km_{mn}\\vect_c(\\Bm) = \\vect_c(\\Bm^\\top)\n\\end{align*}\nand $\\vect_c(\\Bm) = \\vect_r(\\Bm^\\top)$. Therefore, for rowwise vectorization, we have a similar result:\n\\begin{align*}\n    \\Km_{mn}\\vect_r(\\Bm^\\top) &= \\vect_r(\\Bm)\\\\\n    \\vect_r(\\Bm^\\top) &= \\Km_{nm}\\vect_r(\\Bm),\n\\end{align*}\nwhere in the last line we used the fact the commutation is a permutation matrix, so $\\Km_{mn}^{-1}=\\Km_{mn}^\\top=\\Km_{nm}$. Thus, we get the required matrix derivative as follows:\n$$\\frac{\\partial \\Am}{\\partial \\Xm} = \\extra{\\frac{1}{\\sqrt{d_k}}}\\Im_n\\kro\\Xm\\wK\\wQT + \\extra{\\frac{1}{\\sqrt{d_k}}}(\\Xm\\wQ\\wKT\\kro\\Im_n)\\Km_{dn}\\,.$$\nNext, we will use a property of commutation matrix to make things simpler (Theorem 7.9, \\cite{magnus2019matrix}):\n$$\n\\frac{\\partial \\Am}{\\partial \\Xm} = \\extra{\\frac{1}{\\sqrt{d_k}}}\\Im_n\\kro\\Xm\\wK\\wQT + \\extra{\\frac{1}{\\sqrt{d_k}}}\\Km_{nn}(\\Im_n\\kro\\Xm\\wQ\\wKT).\n$$\nPlugging this into the above Eq.~\\eqref{eq:grad-SA-X}, we get:\n\\begin{align*}\n     \\frac{\\partial\\Sm}{\\partial\\Xm} \n    &= \\extra{\\frac{1}{\\sqrt{d_k}}}(\\Im_n\\kro \\wVT\\Xm^\\top)\\,\\frac{\\partial \\Am}{\\partial \\Mm}  \\,\\left(\\Im_n\\kro\\Xm\\wK\\wQT + \\Km_{nn}(\\Im_n\\kro\\Xm\\wQ\\wKT)\\right) \\,+ \\,\\Am\\kro{\\wV}^\\top.\n\\end{align*}\nAs a sanity check, we can calculate if the shapes of the matrices are consistent. LHS should be a $nd\\times nd$ matrix, while the constituent matrices of the first term on RHS: $\\Im_n\\kro {\\wV}^\\top\\Xm^\\top\\in\\mathbb{R}^{nd\\times n^2}$, $\\frac{\\partial \\Am}{\\partial \\Mm} \\in\\mathbb{R}^{n^2\\times n^2}$, the additive term next to it is a $n^2\\times nd$ matrix, and the second term on RHS is a Kronecker product of a $n\\times n$ and a $d\\times d$ matrix. \n\\end{proof}\n\n\\begin{tcolorbox}\n\\begin{lemma}[]\n\n\\label{lemma:gradients_queries}\nLet $\\Xm^{\\ell}$ be the representations of the input sequence at the $\\ell$-th layer. Under the uniform-attention assumption, we have\n    \\begin{align}\n    \\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\Wm^{V,\\ell}}\\right\\|^2_F &= d_v n \\mathbb{E}\\|\\bar{\\bm{x}}^{\\ell}\\|^2~\\label{eq:jacobian_values};\\\\ \\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\Wm^{Q,\\ell}} \\right\\|^2_F &= \\frac{\\sigma^2_v\\sigma^2_k d_v}{n^2}\\cdot \\Exp \\left[ \\|\\Xm^{\\ell}\\|^2_F \\cdot  \\|(\\Xm^{\\ell})^\\top\\Xm^{\\ell} - n\\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top\\|^2_F\\right]~\\label{eq:jacobian_queries};\\\\ \n    \\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\Xm^{\\ell}}\\right\\|^2_F &\\leq \\frac{8\\sigma^2_q\\sigma^2_k\\sigma^2_v d_kd_v}{n} \\, \\cdot \\mathbb{E} \\norm{(\\Xm^{\\ell})^\\top\\Xm^{\\ell} - n\\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top}^2_F + 2d_v^2\\sigma^2_v \\; .\n\\end{align}\n\n\\end{lemma}\n\\end{tcolorbox}\n\n\n\\begin{proof}\nHere, we suppress the index $\\ell$.\n\n\\textbf{Gradient with respect to the values matrix.}\n\nRecall that from Lemma \\ref{lemma:grads_SA} we have that:\n\\begin{equation*}\n    \\frac{\\partial \\Sm}{\\partial \\wV} =    \\soft\\left(\\frac{\\Xm\\wQ{\\wK}^\\top \\Xm^\\top}{\\sqrt{d_k}}\\right)\\Xm\\, \\kro \\Im_{d_v} \\overset{\\text{Ass. } \\ref{ass:uniform_softmax}}{=} \\frac{1}{n}\\bm{1}_{n \\times n} \\Xm \\kro \\Im_{d_v}.\n\\end{equation*}\nBy direct computation:\n\\begin{align*}\n    \\norm{\\frac{\\partial \\Sm}{\\partial \\wV}}_F^2 = \\tr\\left(\\frac{\\partial \\Sm}{\\partial \\wV} \\, \\frac{\\partial \\Sm}{\\partial \\wV}^\\top\\right) \\overset{\\eqref{eq:prod_kro}}&{=} \\frac{1}{n^2} \\tr((\\bm{1}_{n\\times n} \\Xm \\Xm^\\top \\bm{1}_{n\\times n}) \\kro \\Im_{d_v})\\\\\n    \\overset{\\eqref{eq:trace_kro}}&{=}\\frac{1}{n^2} \\tr(\\bm{1}_{n\\times n} \\Xm \\Xm^\\top \\bm{1}_{n\\times n})\\tr(\\Im_{d_v})\\\\\n    &= \\frac{d}{n^2}\\tr( \\Xm \\Xm^\\top \\bm{1}_{n\\times n}\\bm{1}_{n\\times n}) \\\\\n    &= \\frac{d_v}{n}\\tr( \\Xm \\Xm^\\top \\bm{1}_{n\\times n}) \\\\\n    &= \\frac{d_v}{n}\\tr(\\Xm\\Xm^\\top \\bm{1}_n \\bm{1}_n^\\top) \\\\\n    &= \\frac{d_v}{n} \\bm{1}_n^\\top\\Xm \\Xm^\\top \\bm{1}_n \\\\\n    &= \\frac{d_v}{n} \\|\\Xm^\\top\\bm{1}_n\\|^2=d_vn\\norm{\\bm{\\bar{x}}}^2 .\n\\end{align*}\n\n\\textbf{Gradients with respect to the queries/keys matrix.}\n\nFirst, recall the expression for the gradient of the softmax under the uniform-attention assumption (Eq.~\\eqref{eq:grad_soft}):\n\\begin{equation*}\n    \\frac{\\partial \\Am}{\\partial \\Mm} = \\frac{1}{ n}\\Im_n \\kro \\left(\\Im_n - \\frac{1}{n}\\bm{1}_{n\\times n} \\right).\n\\end{equation*}\n\nHence, we can rewrite the expression of Lemma \\ref{lemma:grads_SA} for the gradients of the queries as:\n\\begin{align*}\n\\frac{\\partial \\Sm}{\\partial \\wQ} \n     &= \\left(\\Im_n \\kro {\\wV}^\\top\\Xm^\\top\\right) \\frac{\\partial \\Am}{\\partial \\Mm} \\left(\\frac{\\Xm\\kro\\Xm\\wK}{\\sqrt{d_k}}\\right)\\\\ &= \\frac{1}{\\sqrt{d_k}n}\\left(\\Im_n \\kro {\\wV}^\\top\\Xm^\\top\\right) \\left[\\Im_n \\otimes \\left(\\Im_n - \\frac{1}{n}\\bm{1}_{n\\times n} \\right)\\right] \\left(\\Xm\\kro\\Xm\\wK\\right)\\\\\n     &= \\frac{1}{\\sqrt{d_k}n}\\Xm \\kro \\left[ {\\wV}^\\top\\Xm^\\top\\left(\\Im_n - \\frac{1}{n}\\bm{1}_{n\\times n} \\right)\\Xm\\wK\\right],\n\\end{align*}\nwhere in the last step we have used twice the property of the Kronecker product in Eq.~\\eqref{eq:prod_kro} of Lemma~\\ref{lemma:prop_kro}.\n\nHence,\n\\begin{align*}\n\\left\\|\\frac{\\partial \\Sm}{\\partial \\wQ}\\right\\|_F^2 = \\tr \\left(\\frac{\\partial \\Sm}{\\partial \\wQ} \\frac{\\partial \\Sm}{\\partial \\wQ} ^T\\right) \\overset{\\eqref{eq:trace_kro}}{=} \\frac{1}{d_k n^2} \\|\\Xm\\|^2_F \\cdot\\left\\|{\\wV}^\\top\\Xm^\\top\\left(\\Im_n - \\frac{1}{n}\\bm{1}_{n\\times n} \\right)\\Xm\\wK\\right\\|_F^2,\n\\end{align*}\nwhere we have used the property on the trace of the Kronecker product (Lemma \\ref{lemma:prop_kro}, Eq.~\\eqref{eq:trace_kro}). \nNote that if we are conditioning on $\\Xm$, then we only have to take the expectation of the last term with respect to the weights $\\Wm^K$ and $\\Wm^V$. Let us call $\\Lm :=\\Im_n - \\frac{1}{n}\\bm{1}_{n\\times n}$ for notation simplicity.\n\nNote: for a matrix $\\Wm\\in\\mathbb{R}^{d\\times d}$ whose entries $w_{ij}\\sim\\mathcal{N}(0, \\sigma^2)$, then $\\Exp \\Wm\\Wm^\\top=d\\sigma^2 \\, \\Im_d$. Thus, exchanging the order of trace and expectation, we can write: \n\\begin{align*}\n    \\Exp\\|\\wVT\\Xm^\\top\\Lm\\Xm\\wK\\|^2_F &= \\Exp\\tr(\\wVT\\Xm^\\top\\Lm\\Xm\\wK \\, \\cdot\\,\\wKT\\Xm^\\top\\Lm\\Xm\\wV)\\\\\n    &=\\tr(\\Xm^\\top\\Lm\\Xm\\Exp[\\wK\\wKT]\\Xm^\\top\\Lm\\Xm\\Exp[\\wV\\wVT])\\\\\n    &=\\sigma^2_v\\sigma^2_k d_kd_v \\tr(\\Xm^\\top\\Lm\\Xm\\,\\cdot\\,\\Xm^\\top\\Lm\\Xm)\\\\\n    &=\\sigma^2_v\\sigma^2_k d_kd_v\\|\\Xm^\\top\\Lm\\Xm\\|^2_F\\\\ &=\\sigma^2_v\\sigma^2_k d_kd_v\\norm{\\Xm^\\top(\\Im_n - \\frac{1}{n}\\bm{1}_{n}\\bm{1}_{n}^\\top)\\Xm}^2_F \\\\\n    &=\\sigma^2_v\\sigma^2_k d_kd_v\\|\\Xm^\\top\\Xm - n\\bar{\\bm{x}}\\bar{\\bm{x}}^\\top\\|^2_F,\n\\end{align*}\nwhere, $\\bar{\\bm{x}}=\\frac{1}{n}\\Xm^\\top\\bm{1}_n\\in\\mathbb{R}^d$ is the mean embedding. Multiply this by $\\frac{1}{d_kn^2} \\|\\Xm\\|^2_F$ to get the final answer.\n\n\\textbf{Gradient with respect to the input.}\n\nPlugging in the values of $\\frac{\\partial \\Am}{\\partial \\Mm}$ and $\\Am$ under the uniform-attention assumption into Eq.~\\eqref{eq:grad_inp} gives rise to the following:\n\\begin{align*}\n     \\frac{\\partial\\Sm}{\\partial\\Xm} \n    &= \\frac{1}{n\\extra{\\sqrt{d_k}}}\\Im_n\\kro \\wVT\\Xm^\\top\\left(\\Im_n - \\frac{1}{n}\\bm{1}_{n}\\bm{1}_{ n}^\\top\\right)\\Xm\\wK\\wQT  \\\\& + \\frac{1}{n\\extra{\\sqrt{d_k}}}\\left[\\Im_n\\kro \\wVT\\Xm^\\top \\left(\\Im_n - \\frac{1}{n}\\bm{1}_{n}\\bm{1}_{ n}^\\top\\right)\\right] \\Km_{nn}(\\Im_n\\kro\\Xm\\wQ\\wKT) \\, \\\\&+\n    \\frac{1}{n}\\bm{1}_{n\\times n}\\kro\\wVT\n\\end{align*}\nLet's refer to the matrices on the right-hand side as $\\Am_1, \\Am_2, \\Am_3$ respectively. We compute the expected squared Frobenius norm of these as follows:\n\nFor $\\Am_3$:\n\\begin{align*}\n    \\Exp[\\norm{\\Am_3}^2_F] &= \\frac{1}{n^2}\\Exp[\\tr(n \\bm{1}_{n\\times n}\\kro\\wVT\\wV)] \\\\\n    \\overset{\\eqref{eq:trace_kro}}&{=}\\frac{1}{n}\\tr(\\bm{1}_{n\\times n})\\tr(\\Exp[\\wV\\wVT]) = d_v^2\\sigma^2_v.\n\\end{align*}\n\nSimilarly, for $\\Am_1$:\n\\begin{align}\n    \\Exp[\\norm{\\Am_1}^2_F] &= \\frac{\\sigma^2_q\\sigma^2_k\\sigma^2_v d_kd_v}{n^2} \\, \\tr(\\Im_n)\\tr(\\Xm^\\top\\Lm\\Xm \\,\\cdot\\,\\Xm^\\top\\Lm\\Xm)\\\\\n    &=\\frac{\\sigma^2_q\\sigma^2_k\\sigma^2_v d_kd_v}{n} \\, \\norm{\\Xm^\\top\\Xm - n\\bar{\\bm{x}}\\bar{\\bm{x}}^\\top}^2_F\\\\\n    &=\\sigma^2_q\\sigma^2_k\\sigma^2_v d_kd_v n \\, \\norm{\\frac{1}{n}\\Xm^\\top\\Xm - \\bar{\\bm{x}}\\bar{\\bm{x}}^\\top}^2_F .\n\\end{align}\nFinally, for $\\Am_2$:\n\\begin{align}\n    &\\Exp[\\norm{\\Am_2}^2_F]\\\\\n    &= \\frac{1}{n^2 d_k}\\Exp\\left[\\tr( \\left[\\Im_n\\kro \\wVT\\Xm^\\top \\Lm \\right] \\Km_{nn}(\\Im_n\\kro\\Xm\\wQ\\wKT\\wK\\wQT\\Xm^\\top) \\Km_{nn}\\left[\\Im_n\\kro \\Lm\\Xm\\wV \\right])\\right]\\\\\n    &= \\frac{\\sigma^2_q\\sigma^2_k\\sigma^2_v d_kd_v}{n^2}\\,\\tr((\\Im_n\\kro\\Xm\\Xm^\\top)[\\Im_n \\kro \\Lm\\Xm\\Xm^\\top\\Lm]) \\\\\n    &= \\frac{\\sigma^2_q\\sigma^2_k\\sigma^2_v d_kd_v}{n}\\,\\tr(\\Xm^\\top\\Lm\\Xm \\,\\cdot\\,\\Xm^\\top\\Lm\\Xm) =\\Exp[\\norm{\\Am_1}^2_F],\n\\end{align}\nwhere in the second line we have taken the expectation inside and used the fact that $\\Km_{nn}$, being a commutation matrix, is orthogonal. Then, by simple properties of Kronecker product and cyclic property of trace, we have the result, which is the same as that for $\\Am_1$. \\\\\n\nFinally, by the triangle inequality\n\\begin{align}\n    \\Exp\\left\\|\\frac{\\partial\\Sm}{\\partial\\Xm}\\right\\|^2 &\\le 2 \\Exp\\|\\Am_1+\\Am_2\\|^2 + 2 \\Exp\\|\\Am_3\\|^2 \\\\\n    &\\le 4 \\Exp\\|\\Am_1\\|^2 + 4 \\Exp\\|\\Am_2\\|^2+ 2\\Exp\\|\\Am_3\\|^2\\\\\n    &= 8 \\Exp\\|\\Am_1\\|^2 + 2\\Exp\\|\\Am_3\\|^2\\\\\n    &= \\frac{8\\sigma^2_q\\sigma^2_k\\sigma^2_v d_kd_v}{n}\\norm{\\Xm^\\top\\Xm - n\\bar{\\bm{x}}\\bar{\\bm{x}}^\\top}^2_F + 2d_v^2\\sigma_v^2.\n\\end{align}\nThis completes the proof.\n\\end{proof}\n\n\\subsubsection{Proof of Theorem \\ref{thm:vanishing_gradients}}\n\n\\begin{tcolorbox}\n\\begin{thm}[Vanishing gradients under rank collapse]\n\n\\label{thm:vanishing_gradients}\n    Suppose that the uniform-attention assumption holds. If additionally $\\Xm^{\\ell}$ for any $l \\in [L]$ has rank-1, and there exists a vector $\\bm{x} \\in \\mathbb{R}^{d}$ such that $\\Xm^\\ell = \\bm{1}_n\\bm{x}^T$, then:\n  \\begin{equation}\n      \\Exp \\norm{\\frac{\\partial \\mathcal{L}}{\\partial \\Wm^{Q,\\ell}}}_F^2 = 0 , \\;\\;\\;\\; \\Exp \\norm{\\frac{\\partial \\mathcal{L}}{\\partial \\Wm^{K,\\ell}}}_F^2 = 0 ,\n  \\end{equation}\n  where the expectation is taken over the weight matrices. This implies that these quantities are vanishing almost surely, due to the non-negativeness of the norm.\n\n\\end{thm}\n\\end{tcolorbox}\n\nBefore starting the proof, it is interesting to note that, even though the gradients of queries and keys vanish in the rank collapse regime~(i.e. $\\norm{\\Xm^\\top\\Xm - n\\bar{\\bm{x}}\\bar{\\bm{x}}^\\top}=0$), the gradient with respect to the values and the input does not~(see Theorem~\\ref{lemma:gradients_queries}). From this simple remark, we can conclude that, even in the rank collapse regime, information still propagates in the backward pass. In Section~\\ref{sec:adaptive}~(main paper), we show that even if gradients effectively propagate, the phenomenon studied in this theorem still greatly affects training.\n\n\\begin{proof}\nBy using the chain rule and the fact that for two matrixes $\\Am, \\Bm$ we have that $\\norm{\\Am\\Bm}_F^2 \\leq \\norm{\\Am}_F^2\\norm{\\Bm}_F^2$, we can upper bound the gradient as:\n\\begin{align*}\n    \\norm{\\frac{\\partial \\mathcal{L}}{\\partial{\\Wm^{Q,\\ell}}}}_F^2 &\\leq \\prod_{i=\\ell+1}^{L-1} \\norm{\\frac{\\partial \\Xm^{i+1}}{\\partial \\Xm^i}}_{F}^2 \\norm{\\frac{\\partial \\mathcal{L}}{\\partial \\Xm^L}}_F^2 \\norm{\\frac{\\partial \\Xm^{\\ell+1}}{\\partial \\Wm^{Q,\\ell}}}_F^2 \\\\\n    &\\leq \\prod_{i=\\ell+1}^{L-1} \\norm{\\frac{\\partial \\Xm^{i+1}}{\\partial \\Xm^i}}_{F}^2 \\norm{\\frac{\\partial \\mathcal{L}}{\\partial \\Xm^L}}_F^2 \\norm{\\frac{\\partial \\Xm^{\\ell+1}}{\\partial \\Zm^{\\ell}}}_F^2 \\norm{\\frac{\\partial \\Zm^\\ell}{\\partial \\Wm^{Q, \\ell}}}_F^2 \\\\\n    &\\leq \\prod_{i=\\ell+1}^{L-1} \\norm{\\frac{\\partial \\Xm^{i+1}}{\\partial \\Xm^i}}_{F}^2 \\norm{\\frac{\\partial \\mathcal{L}}{\\partial \\Xm^L}}_F^2 \\norm{\\frac{\\partial \\Xm^{\\ell+1}}{\\partial \\Zm^{\\ell}}}_F^2 \\left(\\norm{\\frac{\\partial \\alpha_1 \\Sm^{\\ell}}{\\partial \\Wm^{Q, \\ell}}}_F^2 + \\underbrace{\\norm{\\frac{\\partial \\Xm^{\\ell}}{\\partial \\Wm^{Q, \\ell}}}_F^2}_{=0} \\right) ,\n\\end{align*}\nwhere we recall that $\\Zm^{\\ell} = \\alpha_1 \\Sm^{\\ell}+ \\Xm^{\\ell}$ and in the last step we have used that $\\Xm^\\ell$ does not depend on $\\Wm^{Q,\\ell}$, hence the gradient vanishes. By taking expectation and using the tower property, we have that:\n\\begin{equation*}\n    \\Exp \\norm{\\frac{\\partial \\mathcal{L}}{\\partial{\\Wm^{Q,\\ell}}}}_F^2 \\leq \\Exp\\left[\\underbrace{\\Exp\\left[\\prod_{i=\\ell+1}^{L-1} \\norm{\\frac{\\partial \\Xm^{i+1}}{\\partial \\Xm^i}}_{F}^2 \\norm{\\frac{\\partial \\mathcal{L}}{\\partial \\Xm^L}}_F^2 \\norm{\\frac{\\partial \\Xm^{\\ell+1}}{\\partial \\Zm^{\\ell}}}_F^2\\right]}_{=:G(\\Xm^\\ell)} \\norm{\\frac{\\partial \\alpha_1 \\Sm^{\\ell}}{\\partial \\Wm^{Q, \\ell}}}_F^2   \\right],\n\\end{equation*}\nwhere the expectations are taken with respect to $\\Xm^\\ell$ for the outer one and conditioning on $\\Xm^\\ell$ for inner one. Indeed, the first three terms only depend on the network values after $\\Xm^\\ell$. Now, a repeated application of the tower property in $G(\\Xm^\\ell)$, together with the results on the gradients of Lemma \\ref{lemma:gradients_queries}, easily shows that $G(\\Xm^\\ell)$ stays bounded under our hypothesis. To see this one can also simply note that, since the softmax and its derivatives are almost surely bounded, the boundedness of $G(\\Xm^\\ell)$ is implied by an analogous statement for a vanilla linear MLP~(i.e removing the softmax). In this setting, the random variable inside the expectation in $G(\\Xm^\\ell)$ is a finite linear combination of Gaussian products --- which has bounded expectation.\n\nAll in all, we have that\n\\begin{equation*}\n    \\Exp \\norm{\\frac{\\partial \\mathcal{L}}{\\partial{\\Wm^{Q,\\ell}}}}_F^2\\le \\Exp\\left[ B_{\\Xm^\\ell}\\norm{\\frac{\\partial \\alpha_1 \\Sm^{\\ell}}{\\partial \\Wm^{Q, \\ell}}}_F^2\\right],\n\\end{equation*}\nwhere $B_{\\Xm^\\ell}$ is an almost-surely-bounded function of $\\Xm^{\\ell}$. Hence, to show that $\\Exp \\norm{\\frac{\\partial \\mathcal{L}}{\\partial{\\Wm^{Q,\\ell}}}}_F^2=0$, we now just need to show that:\n\\begin{equation*}\n    \\Exp \\norm{\\frac{\\partial \\alpha_1 \\Sm^{\\ell}}{\\partial \\Wm^{Q, \\ell}}}_F^2 = 0\n\\end{equation*}\nunder the rank-1 hypothesis for $\\Xm^\\ell$.\nLet $\\Xm_{1}^\\ell, \\dots \\Xm_{n}^\\ell \\in \\mathbb{R}^{d_v}$  be the representations for the $n$ tokens. Under the rank-1 assumption, each token can be written as a multiple of a single vector $\\bm{x} \\in \\mathbb{R}^{d_v}$, and hence there exists $a_1, \\dots, a_n \\in \\mathbb{R}$ such that $\\Xm_1 = a_1 \\bm{x}, \\dots, \\Xm_n = a_n \\bm{x}$. From Lemma \\ref{lemma:gradients_queries}, we know that:\n\\begin{equation*}\n    \\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\wQ} \\right\\|^2_F = \\frac{\\sigma^2_v\\sigma^2_k d^2}{n^2}\\cdot \\Exp \\left[ \\|\\Xm^{\\ell}\\|^2_F \\cdot  \\|(\\Xm^{\\ell})^\\top\\Xm^{\\ell} - n\\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top\\|^2_F\\right] .\n\\end{equation*}\nThe mean token simplifies to $\\bar{\\bm{x}}^l = \\frac{\\bm{x}}{n}\\sum_k a_k$ and hence $\\left(\\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top\\right)_{ij} = \\frac{1}{n^2} (\\sum_{k}a_k)^2 x_ix_j$. Similarly, $\\left((\\Xm^{\\ell})^\\top\\Xm^{\\ell}\\right)_{ij} = \\sum_k a_k^2 x_i x_j$. If furthermore all the coefficients $a_i$ are the same (which corresponds to the rank collapse assumption $\\Xm^{\\ell}=\\bm{1}_{n}\\bm{x}^T$ analyzed here), then it is easy to see that $\\left((\\Xm^{\\ell})^\\top\\Xm^{\\ell}\\right)_{ij} - n \\left(\\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top\\right)_{ij} = 0 \\; \\forall i,j$ and hence $\\|(\\Xm^{\\ell})^\\top\\Xm^{\\ell} - n\\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top\\|^2_F = 0$.\n\\end{proof}\n\n\\subsection{Gradient Analysis of Section \\ref{sec:dep_angle}}\n\\label{sec:constan_cosine_analysis}\n\nThroughout this section we assume that between every pair of tokens, the same dimension is a zero-mean Gaussian random variable with the same correlation, meaning that\n\\begin{equation*}\n    \\Exp\\left[\\Xm_{i,j}\\Xm_{i',j'}\\right] = \\begin{cases}\n    0 & j\\ne j' \\ \\ \\text{(independent dimensions)}\\\\\n    \\sigma^2_x & i=i', j=j'\\\\\n    \\rho\\sigma^2_x & i\\ne i', j=j'.\n    \\end{cases}\n\\end{equation*}\n\nAs we will deal with the computation of 4-th order moments of correlated Gaussian random variables, we will make use of Isserlis theorem \\citep{isserlis1918formula}:\n\\begin{tcolorbox}\n    \\begin{theorem}[Isserlis]\n    Let $X_1, \\dots X_m$ be $m$ zero-mean Gaussian random variables. Then:\n    \\begin{equation}\n        \\Exp[X_1 \\cdots X_m] = \\begin{cases}\n    \\sum_{p\\in P_m^2} \\prod_{(i,j) \\in p} \\Exp[X_iX_j] & m \\text{ even} \\\\\n    0 & m \\text{ odd}\n    \\end{cases} \n    \\end{equation}\n    \n    where $P_m^2$ is the set of all the possible pairings of the indexes $1,\\dots, m$. \n    \\end{theorem}\n\\end{tcolorbox}\nIn particular, we will only need the 4-th order term, which reads:\n\\begin{equation*}\n    \\Exp[X_1X_2X_3X_4] = \\Exp[X_1X_2]\\Exp[X_3X_4] + \\Exp[X_1X_3]\\Exp[X_2X_4] + \\Exp[X_1X_4]\\Exp[X_2X_3].\n\\end{equation*}\n\nNow we can prove Eq.~\\eqref{eq:grad_V} , which we re-state here:\n\\begin{tcolorbox}\n\\begin{equation*}\n    \\Exp \\norm{\\frac{\\partial \\Sm}{\\partial \\wV}}_F^2 = \\sigma_x^2 d^2 \\left(1 + \\rho (n-1)  \\right).\n\\end{equation*}\n\\end{tcolorbox}\nAlso, from Eq.~\\eqref{eq:jacobian_values} we have that \n\\begin{equation*}\n    \\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\Wm^{V,\\ell}}\\right\\|^2_F = dn \\mathbb{E}\\|\\bar{\\bm{x}}^{\\ell}\\|^2.\n\\end{equation*}\n\nNow,\n\\begin{equation*}\n    \\mathbb{E}\\|\\bar{\\bm{x}}^{\\ell}\\|^2 = \\mathbb{E} \\left( \\sum_{i=1}^d (\\bar{\\bm{x}}^{\\ell}_i)^2 \\right).\n\\end{equation*}\nEach $\\bar{\\bm{x}}^{\\ell}_i = \\frac{1}{n} \\sum_{k=1}^n \\Xm^\\ell_{ki}$ is equally distributed with mean \n\\begin{equation*}\n    \\mathbb{E} [\\bar{\\bm{x}}^{\\ell}_i] = \\mathbb{E} \\left[\\frac{1}{n} \\sum_{k=1}^n \\Xm^\\ell_{ki}\\right] = 0\n\\end{equation*}\nand variance\n\\begin{equation*}\n    \\text{Var} [\\bar{\\bm{x}}^{\\ell}_i] = \\text{Var} \\left[\\frac{1}{n} \\sum_{k=1}^n \\Xm^\\ell_{ki}\\right] = \\frac{1}{n^2} \\left(n\\sigma_x^2 + n(n - 1) \\rho \\sigma_x^2 \\right) = \\frac{1}{n} \\sigma_x^2 (1 + \\rho (n - 1)).\n\\end{equation*}\nFinally we get\n\\begin{equation*}\n    \\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\Wm^{V,\\ell}}\\right\\|^2_F = \\sigma_x^2 d^2 (1 + \\rho (n - 1)).\n\\end{equation*}\n\nWe know prove Eq.~\\eqref{eq:grad_Q}, which reads:\n\\begin{tcolorbox}\n\\begin{equation*}\n    \\Exp \\left\\|\\frac{\\partial \\Sm}{\\partial \\wQ}\\right\\|_F^2 = \\sigma_x^6 \\frac{(n-1)}{n} (1 - \\rho)^2 d (n + d) .\n\\end{equation*}\n\\end{tcolorbox}\n\nFor the queries (and the keys respectively), recall from Eq.~\\eqref{eq:jacobian_queries} that\n\\begin{equation*}\n    \\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\Wm^{Q,\\ell}} \\right\\|^2_F = \\frac{\\sigma^2_v\\sigma^2_k d^2}{dn^2}\\cdot \\Exp \\left[ \\|\\Xm^{\\ell}\\|^2_F \\cdot  \\|(\\Xm^{\\ell})^\\top\\Xm^{\\ell} - n\\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top\\|^2_F\\right].  \n\\end{equation*}\nTo proceed, we drop the superscript $\\ell$ and we make the additional assumption that $\\|\\Xm\\|^2_F$ is uncorrelated from the correlation magnitude $\\|\\Xm^\\top\\Lm\\Xm\\|^2_F = \\|\\Xm^\\top\\Xm - n\\bar{\\bm{x}}\\bar{\\bm{x}}^\\top\\|^2_F$.\n\nLet us proceed with an expansion:\n\\begin{align*}\n    \\Exp\\left[\\|\\Xm^\\top\\Lm\\Xm\\|^2_F\\right] &= \\sum_{i,j=1}^d\\Exp\\left[\\left(\\sum_{a,b=1}^n \\Xm_{ai} \\Lm_{ab} \\Xm_{bj}\\right)^2\\right]\\\\\n    &= \\sum_{i,j=1}^d\\sum_{a,b, a', b'=1}^n  \\Lm_{ab}\\Lm_{a'b'}\\Exp\\left[\\Xm_{ai}\\Xm_{a'i} \\Xm_{bj}\\Xm_{b'j}\\right].\\\\\n\\end{align*}\nNow, we have 2 cases: if $i\\ne j$, which gives $d(d-1)$ equal terms, we need to compute\n\\begin{equation*}\n    \\text{A} := \\sum_{a,b, a', b'=1}^n  \\Lm_{ab}\\Lm_{a'b'}\\Exp\\left[\\Xm_{ai}\\Xm_{a'i}\\right]\\cdot \\Exp\\left[\\Xm_{bj}\\Xm_{b'j}\\right],\n\\end{equation*}\nwhere $(i,j)$ is any tuple with $i\\ne j$ and we used uncorrelation of different dimensions. Otherwise, we get $d$ each equal to\n\\begin{equation*}\n    \\text{B} := \\sum_{a,b, a', b'=1}^n  \\Lm_{ab}\\Lm_{a'b'}\\Exp\\left[\\Xm_{ai}\\Xm_{a'i}\\Xm_{bi}\\Xm_{b'i}\\right],\n\\end{equation*}\nwhere $i$ is any index.\n\n\\paragraph{Term A.} Note that\n\\begin{equation*}\n    \\Exp\\left[\\Xm_{ai}\\Xm_{a'i}\\right]\\cdot \\Exp\\left[\\Xm_{bj}\\Xm_{b'j}\\right]= \\begin{cases}\n     \\sigma_x^4 & a=a', b=b',\\quad n^2\\  \\text{terms}\\\\\n     \\rho\\sigma_x^4 & a=a', b\\ne b',\\quad  n^2(n-1) \\ \\text{terms}\\\\\n     \\rho\\sigma_x^4 & a\\ne a', b=b',\\quad  n^2(n-1) \\ \\text{terms}\\\\\n     \\rho^2\\sigma_x^4 & a\\ne a', b\\ne b',\\quad  n^2(n-1)^2 \\ \\text{terms}\n    \\end{cases}.\n\\end{equation*}\nSo basically $A$ is the sum of 3 terms:\n\\begin{align*}\n    &A_1 := \\sigma_x^4\\sum_{a,b} \\Lm_{ab}^2 = (n-1)\\sigma_x^4 \\\\\n    &A_2 := 2\\rho\\sigma_x^4\\sum_{a,b}\\sum_{b'\\ne b}  \\Lm_{ab}\\Lm_{ab'} = -2(n-1)\\rho\\sigma_x^4 \\\\\n    &A_3 := \\rho^2\\sigma^4_x \\sum_{a,b}\\sum_{a'\\ne a}\\sum_{b'\\ne b}\\Lm_{ab}\\Lm_{a'b'} = \\rho^2\\sigma^4_x (n-1),\n\\end{align*}\nwhere we leveraged the following direct calculations:\n\\begin{align*}\n    A_1 &= \\sigma_x^4\\sum_{a,b} \\Lm_{ab}^2\\\\\n    &= \\sigma_x^4\\left(n\\left(\\frac{n-1}{n}\\right)^2 + (n-1)n\\frac{1}{n^2}\\right)\\\\\n    &= \\sigma_x^4\\frac{(n-1)^2+(n-1)}{n}\\\\\n    &= \\sigma_x^4(n-1).\n\\end{align*}\nNext, we compute\n\\begin{align*}\n    A_2 &= 2\\rho\\sigma_x^4\\sum_{a,b}\\Lm_{ab}\\sum_{b'\\ne b}  \\Lm_{ab'}\\\\\n    &= 2\\rho\\sigma_x^4\\left(\\sum_{a}\\Lm_{a,a}\\sum_{b'\\ne b}  \\Lm_{ab'} + \\sum_{a}\\sum_{b\\ne a}\\Lm_{ab}\\sum_{b'\\ne b}  \\Lm_{ab'}\\right)\\\\\n    &= 2\\rho\\sigma_x^4\\left(\\sum_{a}\\frac{n-1}{n}\\left[-(n-1)\\frac{1}{n}\\right] + \\sum_{a}\\sum_{b\\ne a}\\Lm_{ab}\\left[\\frac{n-1}{n} - \\frac{n-2}{n}\\right]\\right)\\\\\n    &=2\\rho\\sigma_x^4\\left(-\\frac{(n-1)^2}{n} - \\frac{1}{n}(n-1)n\\frac{1}{n}\\right)\\\\\n    &= -2\\rho\\sigma_x^4(n-1).\n\\end{align*}\nFinally, similar computations also lead to the last term. To follow the calculations, we invite the reader to draw the matrix $\\Lm$ and to hide the columns over which summations are not performed:\n\\begin{align*}\n    A_3 &=\\rho^2\\sigma^4_x \\sum_{a,b}\\Lm_{ab}\\sum_{a'\\ne a}\\sum_{b'\\ne b}\\Lm_{a'b'}\\\\\n    &= \\rho^2\\sigma^4_x \\left(\\sum_{a}\\Lm_{aa}\\sum_{a'\\ne a}\\sum_{b'\\ne a}\\Lm_{a'b'}+\\sum_{a}\\sum_{b\\ne a}\\Lm_{ab}\\sum_{a'\\ne a}\\sum_{b'\\ne b}\\Lm_{a'b'}\\right)\\\\\n    &= \\rho^2\\sigma^4_x \\left(\\sum_{a}\\Lm_{aa} (1 - \\frac{1}{n}+\\sum_{a}\\sum_{b\\ne a}\\Lm_{ab} (-1\\frac{1}{n}) \\right)  \\\\\n    &= \\rho^2\\sigma^4_x \\left(n (1 - \\frac{1}{n}) (1 - \\frac{1}{n}) + n (n - 1) (- \\frac{1}{n}) (- \\frac{1}{n}) \\right) \\\\\n    &=\\rho^2\\sigma^4_x (n-1).\n\\end{align*}\n\nAll in all, we get:\n\\begin{equation*}\n    \\text{A} = (n-1)(1-\\rho)^2\\sigma^4_x.\n\\end{equation*}\n\n\\paragraph{Term B.}\nWe make use of Isserlis theorem, stating that:\n\\begin{equation*}\n    \\Exp\\left[\\Xm_{ai}\\Xm_{a'i}\\Xm_{bi}\\Xm_{b'i}\\right] = \\underbrace{\\Exp \\Xm_{ai} \\Xm_{a'i}\\Exp\\Xm_{bi}\\Xm_{b'i}}_{Q_1} + \\underbrace{\\Exp \\Xm_{ai} \\Xm_{bi}\\Exp\\Xm_{a'i}\\Xm_{b'i}}_{Q_2} + \\underbrace{\\Exp \\Xm_{ai} \\Xm_{b'i}\\Exp\\Xm_{a'i}\\Xm_{bi}}_{Q_3}.\n\\end{equation*}\n\nBy using our independence assumptions, we get:\n\\begin{equation*}\n    Q_1 = \\sigma_x^4(\\delta_{aa'} + \\rho \\delta_{a\\neq a'})(\\delta_{bb'} + \\rho \\delta_{b\\neq b'}) = \\sigma_x^4(\\delta_{aa'}\\delta_{bb'} + \\rho \\delta_{aa'}\\delta_{b\\neq b'} + \\rho \\delta_{a\\neq a'}\\delta_{bb'} + \\rho^2 \\delta_{a\\neq a'}\\delta_{b\\neq b'}).\n\\end{equation*}\nSimilarly for $Q_2$ and $Q_3$:\n\\begin{equation*}\n    Q_2 =\\sigma_x^4(\\delta_{ab}\\delta_{a'b'} + \\rho \\delta_{ab}\\delta_{a'\\neq b'} + \\rho \\delta_{a\\neq b}\\delta_{a'b'} + \\rho^2 \\delta_{a\\neq b}\\delta_{a'\\neq b'})\n\\end{equation*}\nand\n\\begin{equation*}\n    Q_3 = \\sigma_x^4(\\delta_{ab'}\\delta_{a'b} + \\rho \\delta_{ab'}\\delta_{a'\\neq b} + \\rho \\delta_{a\\neq b'}\\delta_{a'b} + \\rho^2 \\delta_{a\\neq b'}\\delta_{a'\\neq b}).\n\\end{equation*}\n\nHence,\n\\begin{equation*}\n    B = \\sum_{a,b, a', b'=1}^n  \\Lm_{ab}\\Lm_{a'b'}\\Exp\\left[\\Xm_{ai}\\Xm_{a'i}\\Xm_{bi}\\Xm_{b'i}\\right] = \\sum_{a,b, a', b'=1}^n  \\Lm_{ab}\\Lm_{a'b'}(Q_1 + Q_2 + Q_3).\n\\end{equation*} \n\nLet's study it term by term. We will also use $\\Lm_{ab} = (\\delta_{ab} - \\frac{1}{n})$, and so $\\Lm_{ab}\\Lm_{a'b'} = (\\delta_{ab}\\delta_{a'b'} - \\frac{\\delta_{ab}}{n} - \\frac{\\delta_{a'b'}}{n} + \\frac{1}{n^2})$.\n\n\\textbf{First term}: we have that $\\sigma_x^4 \\sum_{aa'b'b'} \\Lm_{ab}\\Lm_{a'b'} Q_1 $ which is equal to (omitting the constant $\\sigma_x^4$):\n\\begin{align*}\n    &=\\sum_{a,a',b,b'} (\\delta_{ab}\\delta_{a'b'} - \\frac{\\delta_{ab}}{n} - \\frac{\\delta_{a'b'}}{n} + \\frac{1}{n^2}) (\\delta_{aa'}\\delta_{bb'} + \\rho \\delta_{aa'}\\delta_{b\\neq b'} + \\rho \\delta_{a\\neq a'}\\delta_{bb'} + \\rho^2 \\delta_{a\\neq a'}\\delta_{b\\neq b'}) \\\\\n    &= \\rho^2\\left(n(n-1) - 2(n-1)(n-1) + (n-1)^2\\right) + \\rho (- 4(n-1) + 2(n-1) ) + n - 2 + 1 \\\\\n    &= \\rho^2(n-1)\\left(n - 2(n-1) + (n-1)\\right) - 2\\rho(n-1) + (n - 1) \\\\\n    &= \\rho^2(n-1) - 2\\rho(n-1) + (n-1).\n\\end{align*}\n\n\\textbf{Second term}: we have that $\\sigma_x^4 \\sum_{aa'b'b'} \\Lm_{ab}\\Lm_{a'b'} Q_2 $ which is equal to (omitting the constant $\\sigma_x^4$):\n\\begin{align*}\n    &=\\sum_{a,a',b,b'}(\\delta_{ab}\\delta_{a'b'} - \\frac{\\delta_{ab}}{n} - \\frac{\\delta_{a'b'}}{n} + \\frac{1}{n^2})(\\delta_{ab}\\delta_{a'b'} + \\rho \\delta_{ab}\\delta_{a'\\neq b'} + \\rho \\delta_{a\\neq b}\\delta_{a'b'} + \\rho^2 \\delta_{a\\neq b}\\delta_{a'\\neq b'}) \\\\\n    &= \\rho^2(n-1)^2 + \\rho(-2n(n-1) + 2(n-1)) + n^2 - 2n + 1 \\\\\n    &= \\rho^2 (n-1)^2 - 2\\rho(n-1)^2 + (n-1)^2.\n\\end{align*}\n\n\\textbf{Third term:}\nwe have that $\\sigma_x^4 \\sum_{aa'b'b'} \\Lm_{ab}\\Lm_{a'b'} Q_3 $ which is equal to (omitting the constant $\\sigma_x^4$):\n\\begin{align*}\n    &=\\sum_{a,a',b,b'}(\\delta_{ab}\\delta_{a'b'} - \\frac{\\delta_{ab}}{n} - \\frac{\\delta_{a'b'}}{n} + \\frac{1}{n^2})(\\delta_{ab'}\\delta_{a'b} + \\rho \\delta_{ab'}\\delta_{a'\\neq b} + \\rho \\delta_{a\\neq b'}\\delta_{a'b} + \\rho^2 \\delta_{a\\neq b'}\\delta_{a'\\neq b}) \\\\\n    &= \\rho^2\\left( n(n-1) - 2(n-1)(n-1) + (n-1)^2\\right) + \\rho(-4(n-1) + 2(n-1)) + n - 2 + 1 \\\\\n    &= \\rho^2(n-1) - 2\\rho(n-1) + (n - 1).\n\\end{align*}\n\nSumming all the three terms, we get:\n\\begin{align*}\n    B = \\sigma_x^4(n-1)\\left[ \\rho^2 (n+1) - 2(n+1)\\rho + (n+1) \\right] = \\sigma_x^4(n-1)(n + 1)(1 - \\rho)^2.\n\\end{align*}\n\n\\textbf{Plugging in the values of A and B} we get:\n\\begin{equation*}\n    \\Exp[\\|\\Xm^\\top \\Lm\\Xm\\|^2_F] = d\\cdot B + d(d-1)\\cdot A = \\sigma^4_x (1 - \\rho)^2 d (n - 1) (n + d),\n\\end{equation*}\nand finally assuming Xavier initialization\n\\begin{align*}\n    \\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\Wm^{Q,\\ell}} \\right\\|^2_F &= \\frac{\\sigma^2_v\\sigma^2_k d^2}{dn^2}\\cdot \\Exp \\left[ \\|\\Xm^{\\ell}\\|^2_F \\cdot  \\|(\\Xm^{\\ell})^\\top\\Xm^{\\ell} - n\\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top\\|^2_F\\right] \\\\\n    &= \\sigma_x^6 \\frac{n - 1}{n} (1 - \\rho)^2 d (n + d).\n\\end{align*}\n\n\\subsection{Forward Pass: Proofs of Lemma \\ref{lemma:propagation_of_inner_producets} and \\ref{thm:forward_pass} }\n\\label{app:forward_pass}\nFirst, we characterize the evolution of the correlations between tokens $\\Xm_{k}, \\Xm_{k'}$ with depth, under the assumptions of Theorem \\ref{thm:forward_pass}, namely uniform-attention assumption, and the adoption of a linear activation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{tcolorbox}\n\\begin{lemma}[Expectation of Linear Layers]\n\\label{lemma:exp_linear}\nLet $\\Dm = \\Xm \\Wm$, where $\\Wm\\in\\mathbb{R}^{d\\times d}$ is a random matrix with i.i.d random entries with variance $\\sigma^2 = \\frac{1}{d}$ and $\\Xm\\in\\mathbb{R}^{n\\times d}$ is a fixed matrix:\n\\begin{equation*}\n    \\Exp[\\Dm_{kj}\\Dm_{k'j}] = \\frac{1}{d}\\langle \\Xm_k, \\Xm_{k'} \\rangle\n\\end{equation*}\n\\end{lemma}\n\\end{tcolorbox}\nNote that by summing over the indexes, Lemma \\ref{lemma:exp_linear} implies:\n\\begin{align*}\n    &\\Exp \\norm{\\Dm}_F^2 = \\Exp \\norm{\\Xm}_{F}^2 \\\\\n    &\\Exp C(\\Dm) = \\Exp C(\\Xm).\n\\end{align*}\n\n\\begin{proof}\n\\begin{equation*}\n     \\Exp [\\Dm_{kj} \\Dm_{k'j}] = \\sum_{zz'} \\Xm_{kz} \\Xm_{k'z'} \\Exp[\\Wm_{zj}\\Wm_{z'j}] = \\sigma^2 \\sum_{z}\\Xm_{kz}\\Xm_{k'z} = \\frac{1}{d}\\langle \\Xm_k, \\Xm_{k'} \\rangle.\n\\end{equation*}\n   \n\\end{proof}\n\n\\begin{tcolorbox}\n\\begin{lemma}[Expectation of skip connection]\n\\label{lemma:exp_skip}\n    Let $\\Am, \\Bm \\in \\mathbb{R}^{p \\times q}$. Let $\\Dm := \\alpha\\Am + \\Bm$ with $\\Exp[\\Am | \\Bm] = \\bm{0}$ and $\\alpha \\in \\mathbb{R}$. Then:\n    \\begin{equation}\n        \\Exp\\left[\\Dm_{ij}\\Dm_{i'j}\\right] = \\alpha^2 \\mathbb{E}[\\Am_{ij}\\Am_{i'j}] + \\mathbb{E}[\\Bm_{ij}\\Bm_{ij'}] \n    \\end{equation}\n    holds for all $i,i' \\in [p], j \\in [q]$.\n\\end{lemma}\n\\end{tcolorbox}\nNote that by summing over the indexes, Lemma \\ref{lemma:exp_skip} implies:\n\\begin{align*}\n    &\\Exp \\norm{\\Dm}_F^2 = \\alpha^2\\Exp \\norm{\\Am}_{F}^2 + \\Exp \\norm{\\Bm}_F^2 \\\\\n    &\\Exp C(\\Dm) = \\alpha^2 \\Exp C(\\Am) + \\Exp C(\\Bm).\n\\end{align*}\n\n\\begin{proof}\n\\begin{align*}\n    \\mathbb{E}[\\Dm_{ij}\\Dm_{i'j}] &= \\mathbb{E}\\left[(\\alpha \\Am_{ij} + \\Bm_{ij})(\\alpha \\Am_{i'j} + \\Bm_{i'j})\\right]\\\\\n    &= \\mathbb{E}\\left[\\alpha^2 \\Am_{ij}\\Am_{i'j} + \\alpha \\Am_{ij}\\Bm_{i'j} + \\alpha \\Am_{i'j}\\Bm_{ij} + \\Bm_{ij}\\Bm_{i'j} \\right] \\\\\n    &= \\alpha^2\\mathbb{E}\\left[ \\Am_{kj}\\Am_{i'j}\\right] + \\mathbb{E}\\left[ \\Bm_{ij}\\Bm_{i',j} \\right],\n\\end{align*}\nwhere using iterated expectations $\\alpha \\Exp[\\Am_{i'j}\\Bm_{ij}] = \\alpha \\Exp[\\Exp [\\Am_{i'j} | \\Bm] \\Bm_{ij}]] = 0$ and identically $\\alpha \\Exp [\\Am_{ij}\\Bm_{i'j}] = 0$.\n\\end{proof}\n\n\\begin{tcolorbox}\n\\begin{lemma}[Expectation of Attention Layers]\n\\label{lemma:exp_softmax}\nUnder the uniform-attention assumption:\n\\begin{equation*}\n    \\Exp[\\Sm_{kj}\\Sm_{k'j}] = \\frac{1}{d_vn^2}\\Exp C(\\Xm).\n\\end{equation*}\n\\end{lemma}\n\\end{tcolorbox}\nIn this case, by summing over the indexes we have that:\n\\begin{align*}\n    &\\Exp\\norm{\\Sm}_F^2 = \\frac{\\Exp C(\\Xm)}{n} \\\\\n    & \\Exp C(\\Sm) = \\Exp C(\\Xm).\n\\end{align*}\n\\begin{proof}\nNote that under the uniform-attention assumption:\n\\begin{equation*}\n    \\Sm_{kj} = \\frac{1}{n}\\left(\\bm{1}_{n\\times n}\\Xm \\Wm^V\\right)_{kj} = \\frac{1}{n}\\sum_{zi}\\Xm_{zi}\\Wm^V_{ij}.\n\\end{equation*}\nHence, using the fact that the weights are i.i.d with variance $\\sigma_v^2=\\frac{1}{d_v}$:\n\\begin{align*}\n    \\Exp[\\Sm_{kj}\\Sm_{k'j}] = \\frac{\\sigma_{v}^2}{n^2}\\sum_{z,z'}\\sum_i\\Exp[\\Xm_{zi} \\Xm_{z'i}] = \\frac{1}{d_vn^2}\\sum_{k,k'}\\langle\\Xm_z, \\Xm_{z'}\\rangle = \\frac{1}{d_vn^2}\\Exp C(\\Xm).\n\\end{align*}\n\\end{proof}\n\\begin{tcolorbox}\n\\begin{lemma}[Propagation of inner products]\n\n\\label{lemma:propagation_of_inner_producets}\n Let $C(\\Xm^\\ell) = \\sum_{k,k'} \\langle \\Xm_{k}^\\ell, \\Xm_{k'}^\\ell \\rangle$ and $\\Xm$ the input sequence. Under the Assumption~\\ref{ass:uniform_softmax} and if $\\sigma$ is the linear activation function, we have that:\n \n \\begin{equation}\n     \\Exp \\left[C(\\Xm^{L})\\right] = (\\alpha_2^2 + 1)^{L}(\\alpha_1^2 + 1)^{L}C(\\Xm)  .\n \\end{equation}\n hence, under the depth scaling for the residual block parameters $\\alpha_1^2 = \\frac{\\tilde{\\alpha}_1}{L}, \\alpha_2^2 = \\frac{\\tilde{\\alpha}_2}{L}$ with $\\tilde{\\alpha}_1, \\tilde{\\alpha}_2 \\in \\mathbb{R}$ independent of $L$, we have that:\n \\begin{equation}\n      \\lim_{L\\to \\infty} \\Exp[C(\\Xm^L)] = \\text{e}^{\\tilde{\\alpha}_1 + \\tilde{\\alpha}_2}C(\\Xm).\n \\end{equation}\n\n\\end{lemma}\n\\end{tcolorbox}\n\n\\begin{proof}\nFirst, note that for the residual blocks we have that $\\Exp [Y^{\\ell}_{kj} | Z^{\\ell}_{k'j}] = 0$ due to the independence assumption on the feedforward weights, and similarly $\\Exp [S^{\\ell}_{kj} | X^{\\ell}_{k'j}] = 0$. Hence, we can use Lemma \\ref{lemma:exp_skip} in both the skip connections of the Transformer architecture.\nTherefore, using Lemma \\ref{lemma:exp_skip} (skip), Lemma \\ref{lemma:exp_linear} (linear) and Lemma \\ref{lemma:exp_softmax} (attention):\n\\begin{align*}\n        &\\mathbb{E}[C(\\Xm^{\\ell+1})] \\\\\n        \\overset{\\text{skip}}&{=} \\alpha_2^2\\mathbb{E}C(\\Ym^\\ell) + \\mathbb{E}C(\\Zm^{\\ell}) \\\\ \n        \\overset{\\text{linear}}&{=} \\alpha_2^2\\mathbb{E}C(\\Zm^{\\ell}) + \\mathbb{E}C(\\Zm^{\\ell}) \\\\ \n        &= (\\alpha_2^2 + 1)\\mathbb{E}C(\\Zm^{\\ell}) \\\\\n        \\overset{\\text{skip}}&{=} (\\alpha_2^2 + 1)\\left(\\alpha_1^2\\mathbb{E}C(\\Sm^{\\ell}) + \\mathbb{E}C(\\Xm^{\\ell})\\right) \\\\ \n        \\overset{\\text{attention}}&{=}  (\\alpha_2^2 + 1)(\\alpha_1^2 + 1) \\mathbb{E}[C(\\Xm^{\\ell})] \\\\ \n        \\overset{\\text{unroll recurs.}}&{=} (\\alpha_2^2 + 1)^{\\ell+1}(\\alpha_1^2 + 1)^{\\ell+1}C(\\Xm) ,\n    \\end{align*}\n    where in the last step we have unrolled the recursion until the input layer.\n    \n    For the limit as $L\\to \\infty$, simply note that:\n    $$\\lim_{L\\to \\infty}\\left(\\frac{\\tilde{\\alpha}_i}{L}+1\\right)^{L} = \\text{e}^{\\tilde{\\alpha}_i} ,$$ \n    with $i \\in \\{1, 2\\}$.\n\\end{proof}\n\nNow we are ready to re-state and prove Lemma \\ref{thm:forward_pass}.\n\\begin{tcolorbox}\n\\begin{lemma}[Propagation of the norm]\n\n\\label{thm:forward_pass}\n  Let $\\Xm^{L}$ be the representations of the input sequence at the final layer. Under the assumptions of Lemma \\ref{lemma:propagation_of_inner_producets}, we have that:\n \\begin{equation}\n     \\Exp \\norm{\\Xm^{L}}_{F}^2 = n (\\alpha_2^2+1)^{L}\\alpha_1^2 \\sum_{k=0}^{L-1}(\\alpha_1^2+1)^k \\norm{\\bar{\\bm{x}}}^2 + (\\alpha_2^2+1)^{L} ||\\Xm||_F^2  ,\n \\end{equation}\n hence, under the depth scaling for the residual block parameters $\\alpha_1^2 = \\frac{\\tilde{\\alpha}_1}{L}, \\alpha_2^2 = \\frac{\\tilde{\\alpha}_2}{L}$ with $\\tilde{\\alpha_1}, \\tilde{\\alpha_2} \\in \\mathbb{R}$ independent of $L$, we have that:\n \\begin{equation}\n     \\lim_{L\\to \\infty} \\Exp \\norm{\\Xm^{L}}_{F}^2 = n \\text{e}^{\\tilde{\\alpha}_2}(\\text{e}^{\\tilde{\\alpha}_1} - 1)\\norm{\\bar{\\bm{x}}}^2 + \\text{e}^{\\tilde{\\alpha}_2} ||\\Xm||_F^2.\n \\end{equation}\n\n\\end{lemma}\n\\end{tcolorbox}\n\n\\begin{proof}\nThe proof is in the same spirit as Lemma \\ref{lemma:propagation_of_inner_producets} but slightly more involved. Again, using Lemma \\ref{lemma:exp_skip} in both the skip connections of the Transfomer architecture.\nTherefore, using Lemma \\ref{lemma:exp_skip} (skip), Lemma \\ref{lemma:exp_linear} (linear) and Lemma \\ref{lemma:exp_softmax} (attention):\n\\begin{align*}\n    \\mathbb{E}[||\\Xm^{\\ell+1}||_F^2] \\overset{\\text{skip}}&{=} \\alpha_2^2 \\mathbb{E}||\\Ym^{\\ell}||_F^2 + \\mathbb{E}||\\Zm^{\\ell}||_F^2 \\\\\n    \\overset{\\text{linear}}&{=} (\\alpha_2^2+1)\\mathbb{E}||\\Zm^{\\ell}||_F^2 \\\\\n    \\overset{\\text{skip}}&{=} (\\alpha_2^2+1)\\left(\\alpha_1^2 \\mathbb{E}[||\\Sm^{\\ell}||_F^2] + \\mathbb{E}[||\\Xm^{\\ell}||_F^2] \\right) \\\\\n    \\overset{\\text{softmax}}&{=} (\\alpha_2^2+1)\\left(\\frac{\\alpha_1^2}{n} \\mathbb{E}[C(\\Xm^{\\ell})] + \\mathbb{E}[||\\Xm^{\\ell}||_F^2] \\right) \\\\\n    &= (\\alpha_2^2+1)\\frac{\\alpha_1^2}{n} \\mathbb{E}[C(\\Xm^{\\ell})] + (\\alpha_2^2+1) \\mathbb{E}[||\\Xm^{\\ell}||_F^2] \\\\\n    \\overset{\\text{unroll }C(\\Xm^\\ell)}&{=} (\\alpha_2^2+1)^{\\ell+1}\\alpha_1^2(\\alpha_1^2+1)^{\\ell}\\frac{C(\\Xm) }{n} + (\\alpha_2^2+1) \\mathbb{E}[||\\Xm^{\\ell}||_F^2] \\\\\n    \\overset{\\text{unroll }\\norm{\\Xm^\\ell}_F^2}&{=} (\\alpha_2^2+1)^{\\ell+1}\\alpha_1^2\\frac{C(\\Xm) }{n} \\sum_{k=0}^{\\ell}(\\alpha_1^2+1)^k + (\\alpha_2^2+1)^{\\ell+1} ||\\Xm||_F^2,\n\\end{align*}\n\nwhere in the second to last step we have used Lemma \\ref{lemma:propagation_of_inner_producets} and in the last step we have unrolled the recursion for $\\norm{\\Xm^\\ell}_F^2$ until the input layer. \n\nFor the second part, we now show that for a network of $L$ layers, the choice $\\alpha_1^2 =  \\frac{\\tilde{\\alpha}_1}{L}$ and $\\alpha_2^2 = \\frac{\\tilde{\\alpha}_2}{L}$ stabilizes the norm of the activations in the forward pass. Using the product law for the limits, we can study the converges of $(\\alpha_2^2+1)^{\\ell}$ and $\\alpha_1\\sum_{k=0}^{\\ell}(\\alpha_1^2+1)^k$ separately. \n\nLet $i \\in \\{1, 2\\}$. For the latter term we have that: \n\\begin{align*}\n    \\lim_{L\\to \\infty} \\frac{\\tilde{\\alpha}_i}{L}\\sum_{l=0}^{L-1} \\left(1 + \\frac{\\tilde{\\alpha}_i}{L} \\right)^{\\ell} &= \\lim_{L\\to \\infty} \\frac{\\tilde{\\alpha}_i}{L}\\frac{1-\\left(1 + \\frac{\\tilde{\\alpha}_i}{L} \\right)^{\\ell}}{1-1-\\frac{\\tilde{\\alpha}_i}{L}} \\\\\n    &= \\lim_{L\\to \\infty} -1+\\left(1 + \\frac{\\tilde{\\alpha}_i}{L} \\right)^{\\ell}\\\\\n    &= \\text{e}^{\\tilde{\\alpha}_i} - 1  ,\n\\end{align*}\nwhile for the former term we have that $\\lim_{L\\to \\infty}(\\frac{\\tilde{\\alpha}_i}{L}+1)^{\\ell} = \\text{e}^{\\tilde{\\alpha}_i}$. Hence, the norm of the representations converges to:\n\\begin{equation*}\n    \\lim_{L\\to \\infty}\\mathbb{E}[||\\Xm^{\\ell}||_F^2] = \\text{e}^{\\tilde{\\alpha}_2}(\\text{e}^{\\tilde{\\alpha}_1} - 1)\\frac{C(\\Xm) }{n} + \\text{e}^{\\tilde{\\alpha}_2} ||\\Xm||_F^2.\n\\end{equation*}\n\nThe final results as stated in the theorem hold because of the following:\n\n\\textbf{Remark}: note that $C(\\Xm)  = \\sum_{k,k'}\\sum_{j} \\Xm_{kj}\\Xm_{kj'} = \\sum_{j} (\\sum_{k,k'} \\Xm_{kj}\\Xm_{k'j}) = \\sum_{j} (\\sum_{k} \\Xm_{kj})^2 = n^2 \\norm{\\bar{\\bm{x}}}^2$.\n\\end{proof}\n\n\\subsection{Proof of Theorem \\ref{thm:exp_cosine}: Correlations are Preserved under Residual Scaling}\n\\label{app:res_scaling_proofs}\n\n\\begin{tcolorbox}\n\\begin{thm}[]\n\n\\label{thm:exp_cosine} Let the input tokens have the same norm, i.e. $\\norm{\\Xm_k} = \\norm{\\bm{x}} \\; \\forall k \\in [n]$ for some $\\bm{x} \\in \\mathbb{R}^{d_v}$. Under the depth scaling for the residual block parameters $\\alpha_1^2 = \\frac{\\tilde{\\alpha}_1}{L}, \\alpha_2^2 = \\frac{\\tilde{\\alpha}_2}{L}$ with $\\tilde{\\alpha}_1, \\tilde{\\alpha}_2 \\in \\mathbb{R}$ independent of $L$, we have that: \n  \\begin{equation}\n      \\lim_{L\\to \\infty}\\rho^\\ell = \\frac{n \\text{e}^{\\tilde{\\alpha}_1}C(\\Xm) }{(n-1)[(\\text{e}^{\\tilde{\\alpha}_1} - 1)C(\\Xm)  + n\\norm{\\Xm}_{F}^2]} - \\frac{1}{n-1} .\n  \\end{equation}\n On the other hand, if $\\alpha_1, \\alpha_2 \\neq 0$ are some constants independent of $L$, we have that:\n \\begin{equation}\n     \\lim_{L\\to \\infty}\\rho^\\ell = 1.\n \\end{equation}\n\n\n\\end{thm}\n\\end{tcolorbox}\n\\begin{proof}\nDue to the rotational symmetries of the Gaussian random matrices, if the input  tokens have the same norm, then the expected norm at layer $\\ell \\in [L]$ is also the same across the token's representations. Hence, we can write $\\Exp\\norm{\\Xm^\\ell}_F^2 = n \\Exp\\norm{\\bm{x}^\\ell}^2$, where $\\norm{\\bm{x}^\\ell}^2$ is the norm of every token at layer $\\ell$. Furthermore, by definition of our correlation coefficient $\\rho^l_{kk'}$, we have that $\\Exp\\langle\\Xm^\\ell_k, \\Xm^\\ell_{k'}\\rangle = \\rho^\\ell_{kk'} \\Exp\\norm{\\bm{x}^\\ell}^2$. By summing over the indexes $k,k'$, we can expand the relation as:\n\\begin{equation*}\n    \\underbrace{\\sum_{k,k'}\\Exp\\langle\\Xm^\\ell_k, \\Xm^\\ell_{k'}\\rangle}_{\\Exp C(\\Xm)} = \\sum_{k,k'} \\rho^\\ell_{kk'} \\Exp\\norm{\\bm{x}^\\ell}^2 = (n + \\sum_{k\\neq k'}\\rho^\\ell_{k,k'})\\Exp\\norm{\\bm{x}^\\ell}^2 = \\underbrace{n\\Exp\\norm{\\bm{x}^\\ell}^2}_{\\Exp \\norm{\\Xm^\\ell}_F^2}(1 + (n-1) \\rho^\\ell).\n\\end{equation*}\nBy solving for $\\rho^\\ell$, we have that:\n\\begin{equation*}\n    \\rho^\\ell = \\frac{\\Exp C(\\Xm^\\ell)}{(n-1)\\Exp \\norm{\\Xm^\\ell}^2 } - \\frac{1}{n-1} .\n\\end{equation*}\nNow we plug in the expressions for $\\Exp C(\\Xm^\\ell)$ and $\\Exp \\norm{\\Xm^\\ell}^2 $ with the aid of Lemma \\ref{lemma:propagation_of_inner_producets} and Lemma \\ref{thm:forward_pass}, respectively. Finally, by taking the limits with respect to $L$, we get the desired result.\n\\end{proof}\n\n\n\n\\subsection{Motivation for Assumption~\\ref{ass:uniform_softmax}}\n\\label{app:assumption_unif_soft}\nWe motivate here the following assumption, stated in the main paper. This assumption is crucial to compute expectations involving the softmax function.\n\n\\begin{ass}[Uniform attention]\n\n\\label{ass:uniform_softmax}\nWe assume that $\\Am^\\ell = \\frac{1}{n} \\bm{1}_{n\\times n}$,\n\n\\end{ass}\n\n\\paragraph{Theoretical analysis.}\nWe first show that this assumption holds when taking $d_k$ to infinity, keeping $d_{v}$ fixed.\n\\begin{tcolorbox}\n\\begin{lemma}\n\\label{app:convergence_A}\nConsider initializing each entry of $\\wQ\\in\\mathbb{R}^{d_{v}\\times d_k}$ and $\\wK\\in\\mathbb{R}^{d_{v}\\times d_k}$ independently with variance $\\sigma^2_k = 2/(d_{v}+d_k)$ --- i.e. Glorot initialization~\\citep{glorot2010understanding}. Let $\\Mm = \\frac{1}{\\sqrt{d_k}}\\Xm^\\ell\\Wm^{Q,\\ell}{\\Wm^{K,\\ell}}^\\top{\\Xm^\\ell}^\\top$; for any $(i,j)\\in[n]\\times[n]$ we have\n\\begin{equation}\n    \\Exp[\\Mm_{i,j} \\ | \\ \\Xm] = 0,\\qquad \\Exp[\\Mm_{i,j}^2 \\ | \\ \\Xm] = \\sigma_k^4 \\cdot \\|\\Xm_{i,:}\\|^2 \\cdot \\|\\Xm_{j,:}\\|^2.\n\\end{equation}\nWhile keeping $d_v<\\infty$ fixed, taking $d_k$ to infinity yields\n\\begin{equation}\n    \\Exp[\\Mm_{i,j}^2 \\ | \\ \\Xm] = \\mathcal{O}\\left(\\frac{1}{d_k^2}\\right).\n\\end{equation}\nIn other words, $\\Mm$ converges to $\\bm{0}_{n\\times n}$ in $L^2$ as $d_k\\to\\infty$. \n\\end{lemma}\n\\end{tcolorbox}\n\\begin{proof}\nFirst, note that\n\\begin{align*}\n    \\Mm_{i,j} = \\frac{1}{\\sqrt{d_k}}\\sum_{a,c=1}^{d_{v}}\\sum_{b=1}^{d_k} \\Xm_{i,a}\\wQ_{a,b}\\wK_{c,b} \\Xm_{j,c}.\n\\end{align*}\nSince $\\wQ$ is independent from $\\wK$ at initialization, $\\Exp[\\Mm_{i,j} \\ | \\ \\Xm] = 0$. Next, we compute\n\\begin{align*}\n    \\Exp[\\Mm_{i,j}^2] &= \\frac{1}{d_k}\\sum_{a,c,a',c'=1}^{d_{v}}\\sum_{b,b'=1}^{d_{k}} \\Xm_{i,a} \\Xm_{i,a'} \\Xm_{j,c} \\Xm_{j,c'} \\Exp\\left[\\wQ_{a,b}\\wQ_{a',b'}\\wK_{c,b} \\wK_{c',b'}\\right] \\\\\n    &=\\frac{1}{d_k}\\sum_{a,c,a',c'=1}^{d_{v}}\\sum_{b,b'=1}^{d_{k}} \\Xm_{i,a} \\Xm_{i,a'} \\Xm_{j,c} \\Xm_{j,c'} \\Exp\\left[\\wQ_{a,b}\\wQ_{a',b'}\\right]\\Exp\\left[\\wK_{c,b} \\wK_{c',b'}\\right] \\\\\n    &= \\frac{\\sigma_k^4}{d_k}\\sum_{a,c=1}^{d_{v}}\\sum_{b=1}^{d_{k}}  \\Xm_{i,a}^2 \\Xm_{j,c}^2\\\\\n    &= \\sigma_k^4 \\|\\Xm_{i,:}\\|^2 \\|\\Xm_{j,:}\\|^2.\n\\end{align*}\nThis concludes the proof.\n\\end{proof}\n\nThe following classical result implies almost sure convergence of the softmax matrix as $d_k\\to\\infty$.\n\n\\begin{lemma}[Borel-Cantelli]\nLet $(X_i)$ be a sequence of random variables. If for any $\\epsilon>0$\n\\begin{equation*}\n    \\sum_{i=0}^\\infty \\mathbb{P}[|X_i-X|>\\epsilon]<\\infty,\n\\end{equation*}\nthen $X_i$ converges to $X$ almost surely\\footnote{That is, $\\lim_{i\\to\\infty} X_i(\\omega) = X(\\omega)$ for almost every $\\omega\\in \\Omega$~(i.e. with probability one).}.\n\\end{lemma}\n\n\\begin{tcolorbox}\n\\begin{theorem}[Almost-sure convergence]\n\\label{thm:soft_assumption_proof}\nConsider initializing each entry of $\\wQ\\in\\mathbb{R}^{d_v\\times d_k}$ and $\\wK\\in\\mathbb{R}^{d_v\\times d_k}$ independently with variance $\\sigma^2_k = 2/(d_v+d_k)$ --- i.e. Glorot initialization~\\citep{glorot2010understanding}. Let $d_v<\\infty$ be fixed, as $d_k\\to\\infty$ we have that, for any $\\Xm$,\n\\begin{equation*}\n    \\Am := \\soft\\left(\\frac{1}{\\sqrt{d_k}}\\Xm\\Wm^{Q}{\\Wm^{K}}^\\top{\\Xm}^\\top\\right)\\stackrel{a.s.}{\\to} \\frac{1}{n}\\bm{1}_{n\\times n}\n\\end{equation*}\nand\n\\begin{equation*}\n    \\frac{\\partial\\Am}{\\partial\\Mm} \\stackrel{a.s.}{\\to} \\frac{1}{n}\\Im_n \\otimes \\left(\\Im_n - \\frac{1}{n}\\bm{1}_{n\\times n} \\right).\n\\end{equation*}\n\\end{theorem}\n\\end{tcolorbox}\n\\begin{proof}\nThanks to Lemma~\\ref{app:convergence_A} and Markov Inequality, we have fast convergence in probability: for any fixed $\\Xm$,\n\\begin{equation*}\n    \\mathbb{P}[|\\Mm_{i,j}|>\\epsilon]\\le\\frac{\\Exp[\\Mm^2_{i,j}]}{\\epsilon^2} \\le \\frac{C_\\epsilon}{d_k^2}.\n\\end{equation*}\nBorel Cantelli then directly yields almost sure convergence of $\\Mm$ to $\\bm{0}_{n\\times n}$ as $d_k\\to\\infty$. Next, note that both $\\Am$ and $\\frac{\\partial\\Am}{\\partial\\Mm}$ are continuous functions of $\\Am$, hence we can apply standard continuity event-per-event. For almost every $\\omega\\in\\Omega$,\n\\begin{equation*}\n    \\lim_{d_k\\to\\infty} \\Am(\\Am(\\omega)) = \\Am\\left(\\lim_{d_k\\to\\infty}\\Am(\\omega)\\right) = \\Am( \\bm{0}_{n\\times n}) = \\frac{1}{n}\\bm{1}_{n\\times n}.\n\\end{equation*}\nHence $\\Am\\to \\frac{1}{n}\\bm{1}_{n\\times n}$ almost surely. This can also be seen as a simple application of the continuous mapping theorem. The same reasoning yields almost sure convergence of\n\\begin{equation*}\n    \\frac{\\partial \\Am}{\\partial \\Mm} = \\blockdiag\\Bigg(\\diag(\\Am_{i:}) - \\Am_{i:}\\Am_{i:}^\\top\\Bigg),\n\\end{equation*}\nto the corresponding limiting quantity.\n\\end{proof}\n\n\n\\vspace{10px}\n\\paragraph{Empirical analysis.}\n\\begin{wrapfigure}{r}{0.45\\textwidth}\n\\vspace{-10px}\n\\includegraphics[scale = 0.4]{figures/assumption_uniform_softmax.pdf}\n\\caption{\\small{Evolution of $\\frac{1}{n^2}\\vert\\vert\\Am^{\\ell}-\\frac{1}{n}{\\bf{1}}_{n\\times n}\\vert\\vert_{F}^{2}$ as a function of $d_k$ for $d_v$ fixed at 100.}}\\label{fig:softmaxuniform}\n\\vspace{-10px}\n\\end{wrapfigure} \nWe empirically assess the validity of\nAssumption~\\ref{ass:uniform_softmax} and of its theoretical justification by performing the following experiments: for a range of increasing values of $d_k$, we compute $\\Am$ and we calculate $\\frac{1}{n^2}\\vert\\vert\\Am^{\\ell}-\\frac{1}{n} {\\bf{1}}_{n\\times n}\\vert\\vert_{F}^{2}$, i.e. its average (entry-wise) distance from a uniform matrix with entries all equal to $1/n$. \nFor each value of $d_k$, we repeat this calculation 200 times, each time with different random weight matrices. Fig.~\\ref{fig:softmaxuniform} displays how the $\\vert\\vert\\Am-\\frac{1}{n}{\\bf{1}}_{n\\times n}\\vert\\vert_{F}^{2}$ averaged over 200 runs, tends to zero with a trend inversely proportional to $d_k^2$, as predicted by our theoretical analysis.\n\n\\vspace{20px}\n\\section{Additional Results}\n\\label{app:more_experiments}\n\n\\subsection{On the Roles of the $1/\\sqrt{L}$-Scaling of the Residuals and Layer Normalization}\nWe present some additional results on the propagation of the norm and the correlations in Figure~\\ref{fig:norm_corr_prop_1}. In particular, we empirically show that, with an adequate depth-dependent residual scaling, the norm and the correlation are stabilized, even for very deep networks. Furthermore, we demonstrate the propagation of the correlation and the gradient norms for the PRE-LN configuration in Figure~\\ref{fig:pre_ln_correlations}. As also hinted in the main text, in Figure~\\ref{fig:corr}, the increase in correlation with depth for PRE-LN is much less wild. This also results in better stabilized gradients for the queries and keys' parameters. We also observe the opposite trend for the gradients of the values, in relation to the POST-LN case in Figure~\\ref{fig:residual_scaling}. We speculate that this different dependence, along with the better preserved correlation, is the main reason PRE-LN configured Transformers have been shown to scale better with depth. We plan to investigate this dependence more in future work.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[scale=0.28]{figures/corr_norm.pdf}\n    \\caption{(Left) Propagation of the Frobenius norm of the input sequence; (Right) Propagation of the average token correlation.}\n    \\label{fig:norm_corr_prop_1}\n\\end{figure}\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1\\linewidth]{figures/residual_scaling_pre.pdf}\n    \\caption{Same figure as~\\ref{fig:residual_scaling} but with a PRE-LN architecture. The correlation at depth 0 originates from the correlations in the randomly initialized tokens' embeddings and positional encodings.}\n    \\label{fig:pre_ln_correlations}\n\\end{figure}\n\n\\subsection{Further Empirical Assessment of Assumption \\ref{ass:uniform_softmax}}\nHere, we empirically test the accuracy and limitations of the uniform-attention assumption.\n\nFor the empirical verification of Assumption \\ref{ass:uniform_softmax} in the forward pass analysis, we plot the density of the norm of the representations for only-encoder Transformers of increasing depth. The results are shown in Fig \\ref{fig:norm_std_init_vs_uniform_att}. Note that when the standard deviation of the input is set to $1/\\sqrt{d}$, then the uniform-attention assumption provide an excellent approximation to the common Xavier-initialization. On the contrary, we observe a deviation when the standard deviation of the input is increased. Also, note how as the depth increases, the distribution becomes more heavy-tailed. This heavy-tailedness was recently formally shown for standard MLPs with and without ReLU activation \\citep{noci2021precise, zavatone2021exact}.   \n\n\\begin{figure}\n    \\centering\n    \\includegraphics[scale=0.33]{figures/norm_distribution_uniform_att.png}\n    \\includegraphics[scale=0.33]{figures/norm_distribution_std_init.png}\n    \\includegraphics[scale=0.33]{figures/norm_distribution_std_init_std_1.png}\n    \\caption{Density plots for $\\norm{\\Xm^\\ell}_F^2$ for Transformers of depths $L$ from $1$ to $10$. The input $\\Xm$ contains i.i.d Gaussian entries, simulating an embedding layer. We set $d:=d_v=d_q=30$. The empirical mean at $L=10$ is highlighted in a vertical dashed red line, while the theoretical mean (Lemma \\ref{thm:forward_pass}) is a dashed blue line. The densities are estimated by sampling 1000 times the weights of the network. (Left): we adopt the uniform-attention. The standard deviation of the input is set to $1/\\sqrt{d}$. (Center): Same, but removing the uniform-attention assumption. (Right): We remove the uniform-attention assumption, and set the standard deviation of the input to $1$.}\n    \\label{fig:norm_std_init_vs_uniform_att}\n\\end{figure}\n\n\nFor the verification of the assumption in the backward pass, we additionally show in Fig. \\ref{fig:empirical_verification} how the norm of the gradients w.r.t queries and keys depends on the hidden dimension, the sequence length, the input correlation and the input variance. \\emph{Ground-truth} gradients are calculated with automatic differentiation, and they are compared with our theoretical results based on Assumption \\ref{ass:uniform_softmax}. As shown in  Fig.\\ref{fig:empirical_verification}, our theoretical predictions show a very good agreement with the true gradients. Again, we notice that the smaller the values of the input standard deviation the tighter the agreement of the theory with the simulations. Intuitively, a higher input variance causes the argument of the softmax to have a large range of values. This in turn causes a deviation from the uniform distribution (i.e. maximum entropy), towards the distribution of minimum entropy (a Delta Dirac, corresponding to attending to only one token). \n\n\n\\subsection{Empirical Verification of the Gradient Analysis of Section \\ref{sec:dep_angle}}\nFinally, in Figures~\\ref{fig:constant_correlation_factors} and~\\ref{fig:constant_correlation_factors_theory} we show the dependence of the norm of the gradients for the keys and values based on the parameters of the architecture and the task-specific parameters. Figure~\\ref{fig:constant_correlation_factors} illustrates the true dependence and Figure~\\ref{fig:constant_correlation_factors_theory} the one expected by the theory based on our assumptions. In short, the main takeaways are the following.\n\\begin{itemize}\n    \\item As the correlation between the tokens increases ($x$-axis in the global plot), the norm of the gradients of the queries quickly diminishes compared to the one of the values.\n    \\item The dependence on the variance of the input $\\sigma_x^2$ is different ($y$-axis in the global plot), being linear for the values and cubic for the queries. This highlights the importance of a stabilized forward pass and provides another explanation regarding the successful use of layer norm in Transformers.\n    \\item The dependence on $n$ ($x$-axis in each subplot) and $d$ ($y$-axis in each subplot) is more complicated, also being a function of the correlation $\\rho$ (compare the first column where $\\rho = 0$ to the rest). \n\\end{itemize}\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[scale = 0.39]{figures/check_assumption/n_Q_True_.pdf}\n    \\includegraphics[scale = 0.39]{figures/check_assumption/n_V_True_.pdf}\n    \\includegraphics[scale = 0.39]{figures/check_assumption/d_Q_True_.pdf}\n    \\includegraphics[scale = 0.39]{figures/check_assumption/d_V_True_.pdf}\n    \\includegraphics[scale = 0.39]{figures/check_assumption/sigma_Q_True_.pdf}\n    \\includegraphics[scale = 0.39]{figures/check_assumption/sigma_V_True_.pdf}\n    \\includegraphics[scale = 0.39]{figures/check_assumption/rho_Q_True_.pdf}\n    \\includegraphics[scale = 0.39]{figures/check_assumption/rho_V_True_.pdf}\n    \\caption{Empirical comparison of our theoretical findings. We sample, as aforementioned, the tokens according to a zero-mean Gaussian distribution, while varying the hidden dimension, sequence length, input correlation and input variance. Results are averaged over 20 runs.}\n    \\label{fig:empirical_verification}\n\\end{figure}\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1\\linewidth]{figures/constant_corr.pdf}\n    \\caption{Log ratio of the norm of the gradients for the queries compared to those of the values for varying values of embedding dimension, sequence length, cosine of the tokens angle and standard deviation.}\n    \\label{fig:constant_correlation_factors}\n\\end{figure}\n\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1\\linewidth]{figures/constant_corr_theory.pdf}\n    \\caption{Log ratio of the norm of the gradients of the queries as expected by the theory, compared to those of the values for varying values of embedding dimension, sequence length, cosine of the tokens angle and standard deviation. We use Equations~\\eqref{eq:grad_V} and~\\eqref{eq:grad_Q}.}\n    \\label{fig:constant_correlation_factors_theory}\n\\end{figure}\n\n\n\\section{Experimental Setup}\n\\label{app:experimental_setup}\n\nHere we provide more details regarding the experimental setup.\n\n\\subsection{Toy Example}\n\nIn Figure~\\ref{fig:adam-adap}, we focus on a toy example where the task is to reverse a sequence of tokens. More specifically, given a sequence of $20$ numbers in the range $0-9$, we predict the same tokens in the inverted order. We use an embedding layer of size 16, initializes with variance 1, and sinusoidal positional encodings to initially embed the input. We use a 5-layer POST-LN Transformer encoder model, with a single head attention operation and a two-layer feed-forward layer with a ReLU nonlinearity. We use residual scaling in this case equal to $\\alpha_1 = \\alpha_2 = 1$. We train using Adam with betas parameters $(0.9, 0.999)$, learning rate $0.01$ and weight decay 0.\n\n\\subsection{Translation Task}\n\nIntroducing an inverse temperature scaling $\\tau$ inside the softmax, modifies the attention operation to\n\\begin{equation}\n    \\Sm^{\\ell} :=  \\text{softmax}\\left( \\frac{\\tau}{\\sqrt{d_k}}\\Xm^{\\ell}\\Wm^{Q}\\left(\\Xm^{\\ell}\\Wm^{K}\\right)^\\top \\right) \\Xm^\\ell \\Wm^{V}.\n\\end{equation}\n\nThen the gradient of the queries and keys parameters are directly scaled by this temperature value, following the same proof as for Eq.~\\eqref{eq:jacobian_queries}. We choose a temperature value of $\\tau_{\\text{final}} = 8.5$ to match the gradient norms of the values and queries as in Equations.~\\eqref{eq:grad_V} and~\\eqref{eq:grad_Q}. Doing so, we assume a constant small correlation between tokens (also empirically verified in Fig.~\\ref{fig:our_training}) and set the sequence length $n$ to the average found in our training dataset. Due to instabilities in training, we use warm-up on this temperature value. In short:\n$$\n\\tau = \\tau_{\\text{final}} \\cdot \\text{max}(1, \\frac{\\text{step}}{\\text{steps}_{\\text{warmup}}}),\n$$\nwith `$\\text{steps}_{\\text{warmup}}=1000$' and `step' the current training step.\n\nWe base our implementation on fairseq~\\citep{ott2019fairseq}. For the hyperparameter configuration, we mostly rely on the extensive search already done in fairseq~\\citep{ott2019fairseq} and~\\cite{liu2020understanding}. The final used parameters are exhibited in Table~\\ref{tab:hyperparameters}. For the final evaluation, we use the best-performing model on the left-out validation set. We apply weight decay as in~\\cite{loshchilov2017decoupled} for both SGD and Adam.\n\n \\begin{table}[h]\n \\centering\n \\begin{tabular}{cl|c}\n \\hline\n & \\multicolumn{1}{c|}{\\textbf{Hyperparameters}} & \\multicolumn{1}{c|}{\\textbf{Value}}\\\\\n \\hline\n \\parbox[t]{2mm}{\\multirow{9}{*}{\\rotatebox[origin=c]{90}{}}} & Max tokens & 4096 \\\\\n  & Label smoothing & 0.1 \\\\\n  & clip-norm & 0.0 \\\\\n  & General Dropout & 0.3 \\\\\n  & Attention Dropout & 0.1 \\\\\n  & ReLU Dropout & 0.1 \\\\\n  & Hidden size & 512 \\\\\n  & FFN inner hidden size & 2048 \\\\\n  & Attention Heads & 4 \\\\\n  \\hline\n  \\parbox[t]{2mm}{\\multirow{7}{*}{\\rotatebox[origin=c]{90}{Adam}}} & Learning rate & $7\\epsilon^{-4}$ \\\\\n  & Learning rate scheduler & inverse sqrt \\\\\n  & Warm-up updates & 6000 \\\\\n  & Warm-up init learning rate & 1e-7 \\\\\n  & Adam $(\\beta_1, \\beta_2)$ & (0.9, 0.98) \\\\\n  & Training updates & 100K \\\\\n  & Weight decay & 0.0001 \\\\\n  \\hline\n  \\parbox[t]{2mm}{\\multirow{6}{*}{\\rotatebox[origin=c]{90}{SGD}}} & Learning rate & $2\\epsilon^{-2}$ \\\\\n  & Learning rate scheduler & step \\\\\n  & Step scheduler $\\gamma$ & 0.1 \\\\\n  & Step scheduler update steps & [100K, 200K] \\\\\n  & Training updates & 250K \\\\\n  & Weight decay & 0.001 \\\\\n \\hline\n\\end{tabular}\n\\caption{Hyperparameters for the IWSLT'14 De-En translation task.}\n\\label{tab:hyperparameters}\n\\end{table}\n\nFinally, in Figure~\\ref{fig:our_training} we display the evolution of correlations, residual scaling, and norm of the activations, with depth, for our best trained model. The residual scaling $\\alpha_1, \\alpha_2$ are trainable parameters. This enables them to weight differently the residual branches if deemed necessary. Although these values increase during training, the correlation between the tokens does not significantly increase, which as implied by our main results, allows efficient propagation of the gradients. The norm of the propagated forward signal tends to slightly increase with depth.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1\\linewidth]{figures/our_training.pdf}\n    \\caption{Evolution of the cosine of the angles, the trained residual $\\alpha_1, \\alpha_2$ and the activation norm throughout our training.}\n    \\label{fig:our_training}\n\\end{figure}\n\n\n\n\\end{document}\n\n==== END OF /2206.03126/main.tex ====",
            "statements": {
                "definitions": [],
                "axioms": [],
                "lemmas": [
                    {
                        "statement_id": "ad659961-5f5f-4afa-b748-82ab4f9c289c",
                        "paper_id": "2a384494-8439-4b46-8c5a-7b8d6b3deea1",
                        "library_nr": 10,
                        "library_name": "Lemma 10",
                        "title": "Properties of the Kronecker Product",
                        "statement_original_tex": "\\begin{lemma}\n\\label{lemma:prop_kro}\nGiven the matrices $\\Am \\in \\mathbb{R}^{n \\times m}$, $\\Bm \\in \\mathbb{R}^{p \\times q}$, $\\Cm \\in \\mathbb{R}^{m \\times r}$, $\\Dm \\in \\mathbb{R}^{q \\times s}$, then the following holds:\n\\begin{equation}\n    \\label{eq:trace_kro}\n    \\tr(\\Am \\kro \\Bm) = \\tr(\\Am)\\tr(\\Bm) ,\n\\end{equation}\nand \n\\begin{equation}\n    \\label{eq:prod_kro}\n    (\\Am \\kro \\Bm)(\\Cm \\kro \\Dm) = (\\Am \\Cm)\\kro(\\Bm \\Dm).\n\\end{equation}\n\\end{lemma}",
                        "statement_html": "Given the matrices $\\Am \\in \\mathbb{R}^{n \\times m}$, $\\Bm \\in \\mathbb{R}^{p \\times q}$, $\\Cm \\in \\mathbb{R}^{m \\times r}$, $\\Dm \\in \\mathbb{R}^{q \\times s}$, then the following holds:\n\\begin{equation}\n    \\label{eq:trace_kro}\n    \\tr(\\Am \\kro \\Bm) = \\tr(\\Am)\\tr(\\Bm) ,\n\\end{equation}\nand \n\\begin{equation}\n    \\label{eq:prod_kro}\n    (\\Am \\kro \\Bm)(\\Cm \\kro \\Dm) = (\\Am \\Cm)\\kro(\\Bm \\Dm).\n\\end{equation}",
                        "statement_type": "lemma",
                        "statement_motivation_html": "The properties of the Kronecker product are extremely useful in various fields such as signal processing, control theory, and quantum computing. The trace property \\(\\tr(\\Am \\kro \\Bm) = \\tr(\\Am)\\tr(\\Bm)\\) simplifies the computation of the trace of Kronecker products, which can be particularly helpful in simplifying expressions in multivariate statistics and matrix calculus. The product property \\((\\Am \\kro \\Bm)(\\Cm \\kro \\Dm) = (\\Am \\Cm)\\kro(\\Bm \\Dm)\\) is essential when dealing with large-scale linear systems and tensor operations, as it allows for the decomposition of complex matrix operations into simpler, more manageable parts.",
                        "html_url": "library/lemmas/lemma_10/index.html",
                        "corollary_ids": [],
                        "proof": null
                    },
                    {
                        "statement_id": "6024845f-e810-41c6-bdf9-60e2c45d03d2",
                        "paper_id": "2a384494-8439-4b46-8c5a-7b8d6b3deea1",
                        "library_nr": 11,
                        "library_name": "Lemma 11",
                        "title": "Gradients of Self Attention",
                        "statement_original_tex": "\\begin{lemma}[Gradients of Self Attention for parameter matrices]\n\\label{lemma:grads_SA}\nThe gradients of the self attention layer defined in Eq.~\\eqref{eq:self_att} have the following form:\n\\begin{align*}\n     & \\frac{\\partial \\Sm}{\\partial \\wV} =    \\soft\\left(\\frac{\\Xm\\wQ{\\wK}^\\top \\Xm^\\top}{\\sqrt{d_k}}\\right)\\Xm\\, \\kro \\Im_{d_v} \\, \\\\\n     & \\frac{\\partial \\Sm}{\\partial \\wQ} = \\left(\\Im_n \\kro {\\wV}^\\top\\Xm^\\top\\right) \\frac{\\partial \\Am}{\\partial \\Mm} \\left(\\frac{\\Xm\\kro\\Xm\\wK}{\\sqrt{d_k}}\\right),\n\\end{align*}\nwhere the gradients of the softmax with respect to its inputs are as follows:\n\\begin{equation}\n\\label{eq:grad_soft_complete}\n    \\frac{\\partial \\Am}{\\partial \\Mm} = \\blockdiag\\Bigg( \\dfrac{\\partial \\Am_{i}}{\\partial \\Mm_{i}^\\top}\\Bigg)\n\\end{equation}\nand where $ \\dfrac{\\partial \\Am_{i}}{\\partial \\Mm_{i}^\\top}=\\diag(\\Am_{i}) - \\Am_{i}\\Am_{i}^\\top$ with $\\Am_{i}$ being the $i$-th row of $\\Am$ in column vector format.\\\\\nFinally, note that under the uniform-attention assumption, Eq.~\\eqref{eq:grad_soft_complete} simplifies to:\n\\begin{equation}\n\\label{eq:grad_soft}\n    \\frac{\\partial \\Am}{\\partial \\Mm} = \\frac{1}{n}\\Im_n \\kro \\left(\\Im_n - \\frac{1}{n}\\bm{1}_{n\\times n} \\right).\n\\end{equation}\n\\end{lemma}",
                        "statement_html": "The gradients of the self attention layer defined in Eq. (1) [in <a href=\"https://arxiv.org/pdf/2206.03126#equation.2.1\">original paper</a>] have the following form:\n\\begin{align*}\n     & \\frac{\\partial \\Sm}{\\partial \\wV} =    \\soft\\left(\\frac{\\Xm\\wQ{\\wK}^\\top \\Xm^\\top}{\\sqrt{d_k}}\\right)\\Xm\\, \\kro \\Im_{d_v} \\, \\\\\n     & \\frac{\\partial \\Sm}{\\partial \\wQ} = \\left(\\Im_n \\kro {\\wV}^\\top\\Xm^\\top\\right) \\frac{\\partial \\Am}{\\partial \\Mm} \\left(\\frac{\\Xm\\kro\\Xm\\wK}{\\sqrt{d_k}}\\right),\n\\end{align*}\nwhere the gradients of the softmax with respect to its inputs are as follows:\n\\begin{equation}\n\\label{eq:grad_soft_complete}\n    \\frac{\\partial \\Am}{\\partial \\Mm} = \\blockdiag\\Bigg( \\dfrac{\\partial \\Am_{i}}{\\partial \\Mm_{i}^\\top}\\Bigg)\n\\end{equation}\nand where $ \\dfrac{\\partial \\Am_{i}}{\\partial \\Mm_{i}^\\top}=\\diag(\\Am_{i}) - \\Am_{i}\\Am_{i}^\\top$ with $\\Am_{i}$ being the $i$-th row of $\\Am$ in column vector format.\\\\\nFinally, note that under the uniform-attention assumption, Eq. (23) [in <a href=\"https://arxiv.org/pdf/2206.03126#equation.A.23\">original paper</a>] simplifies to:\n\\begin{equation}\n\\label{eq:grad_soft}\n    \\frac{\\partial \\Am}{\\partial \\Mm} = \\frac{1}{n}\\Im_n \\kro \\left(\\Im_n - \\frac{1}{n}\\mathbf{1}_{n\\times n} \\right).\n\\end{equation}",
                        "statement_type": "lemma",
                        "statement_motivation_html": "Understanding the gradients of the self-attention layer is crucial for optimizing transformer models, which are widely used in natural language processing and other machine learning tasks. The equations provided give explicit forms for these gradients, allowing for efficient backpropagation during training. This is particularly useful when implementing custom attention mechanisms or debugging gradient-related issues in deep learning models.",
                        "html_url": "library/lemmas/lemma_11/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "7493b8e2-9de7-494d-88e7-c21b53f6c9ed",
                            "paper_id": "2a384494-8439-4b46-8c5a-7b8d6b3deea1",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\nLet's start with the simple case of the values' weights $\\wV$. Using the rule in Eq.~\\eqref{eq:matrix-derivative}, it is immediate that:\n\\begin{equation*}\n \\frac{\\partial \\Sm}{\\partial \\wV} =    \\soft\\left(\\frac{\\Xm\\wQ{\\wK}^\\top \\Xm^\\top}{\\sqrt{d_k}}\\right)\\Xm\\, \\kro \\Im_{d_v} = \\Am\\Xm\\, \\kro \\Im_{d_v} \\,.\n\\end{equation*}\nFor the queries, a simple application of the chain rule and then again Eq.~\\eqref{eq:matrix-derivative} gives:\n\\begin{align*}\n    \\frac{\\partial \\Sm}{\\partial \\wQ} &= \\frac{\\partial \\Sm}{\\partial \\Am} \\frac{\\partial \\Am}{\\partial \\wQ}\n     = \\frac{\\partial \\Sm}{\\partial \\Am} \\frac{\\partial \\Am}{\\partial \\Mm} \\frac{\\partial \\Mm}{\\partial \\wQ} \\\\\n     &= \\left(\\Im_n \\kro {\\wV}^\\top\\Xm^\\top\\right) \\frac{\\partial \\Am}{\\partial \\Mm} \\left(\\frac{\\Xm\\kro\\Xm\\wK}{\\sqrt{d_k}}\\right) \\, ,\n\\end{align*}\nwhich is the desired results. Finally, for the gradients of the softmax note that:\n\\begin{equation*}\n    \\frac{\\partial{\\Am_{pq}}}{\\partial \\Mm_{ij}} =\n    \\frac{\\partial}{\\partial \\Mm_{ij}}\\frac{\\exp(\\Mm_{pq})}{\\sum_k \\exp(\\Mm_{pk})} =  \\delta_{ip}\\delta_{jq} \\Am_{ij} - \\delta_{ip} \\Am_{iq}\\Am_{ij} .\n\\end{equation*}\nBy writing the above expression in the matrix notation described above, we obtain the desired result. More specifically, the block diagonal structure is given from the term $\\delta_{ip}$ which stems from the fact that the softmax is applied row-wise. \n\\end{proof}",
                            "statement_html": "Let's start with the simple case of the values' weights $\\wV$. Using the rule in Eq. (20) [in <a href=\"https://arxiv.org/pdf/2206.03126#equation.A.20\">original paper</a>], it is immediate that:\n\\begin{equation*}\n \\frac{\\partial \\Sm}{\\partial \\wV} =    \\soft\\left(\\frac{\\Xm\\wQ{\\wK}^\\top \\Xm^\\top}{\\sqrt{d_k}}\\right)\\Xm\\, \\kro \\Im_{d_v} = \\Am\\Xm\\, \\kro \\Im_{d_v} \\,.\n\\end{equation*}\nFor the queries, a simple application of the chain rule and then again Eq. (20) [in <a href=\"https://arxiv.org/pdf/2206.03126#equation.A.20\">original paper</a>] gives:\n\\begin{align*}\n    \\frac{\\partial \\Sm}{\\partial \\wQ} &= \\frac{\\partial \\Sm}{\\partial \\Am} \\frac{\\partial \\Am}{\\partial \\wQ}\n     = \\frac{\\partial \\Sm}{\\partial \\Am} \\frac{\\partial \\Am}{\\partial \\Mm} \\frac{\\partial \\Mm}{\\partial \\wQ} \\\\\n     &= \\left(\\Im_n \\kro {\\wV}^\\top\\Xm^\\top\\right) \\frac{\\partial \\Am}{\\partial \\Mm} \\left(\\frac{\\Xm\\kro\\Xm\\wK}{\\sqrt{d_k}}\\right) \\, ,\n\\end{align*}\nwhich is the desired results. Finally, for the gradients of the softmax note that:\n\\begin{equation*}\n    \\frac{\\partial{\\Am_{pq}}}{\\partial \\Mm_{ij}} =\n    \\frac{\\partial}{\\partial \\Mm_{ij}}\\frac{\\exp(\\Mm_{pq})}{\\sum_k \\exp(\\Mm_{pk})} =  \\delta_{ip}\\delta_{jq} \\Am_{ij} - \\delta_{ip} \\Am_{iq}\\Am_{ij} .\n\\end{equation*}\nBy writing the above expression in the matrix notation described above, we obtain the desired result. More specifically, the block diagonal structure is given from the term $\\delta_{ip}$ which stems from the fact that the softmax is applied row-wise.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n<br>\n<br>1. <i>Initial Expression</i>: The proof begins by considering the partial derivative of \\(\\Sm\\) with respect to the weights \\(\\wV\\). Using the rule from Eq. (20) [in <a href=\"https://arxiv.org/pdf/2206.03126#equation.A.20\">original paper</a>], we have:\n<br>   \\[\n   \\frac{\\partial \\Sm}{\\partial \\wV} = \\soft\\left(\\frac{\\Xm\\wQ{\\wK}^\\top \\Xm^\\top}{\\sqrt{d_k}}\\right)\\Xm\\, \\kro \\Im_{d_v} = \\Am\\Xm\\, \\kro \\Im_{d_v} \\,.\n   \\]\n<br>\n<br>2. <i>Chain Rule for Queries</i>: Next, the proof applies the chain rule to find the partial derivative of \\(\\Sm\\) with respect to the queries \\(\\wQ\\). This involves multiple steps:\n<br>   \\[\n   \\frac{\\partial \\Sm}{\\partial \\wQ} = \\frac{\\partial \\Sm}{\\partial \\Am} \\frac{\\partial \\Am}{\\partial \\wQ}\n   \\]\n<br>   Breaking it down further using the chain rule again:\n<br>   \\[\n   \\frac{\\partial \\Sm}{\\partial \\Am} \\frac{\\partial \\Am}{\\partial \\Mm} \\frac{\\partial \\Mm}{\\partial \\wQ}\n   \\]\n<br>   This results in:\n<br>   \\[\n   \\left(\\Im_n \\kro {\\wV}^\\top\\Xm^\\top\\right) \\frac{\\partial \\Am}{\\partial \\Mm} \\left(\\frac{\\Xm\\kro\\Xm\\wK}{\\sqrt{d_k}}\\right) \\, ,\n   \\]\n<br>   which is the desired result.\n<br>\n<br>3. <i>Gradients of the Softmax</i>: Finally, the proof considers the gradients of the softmax function. The partial derivative of \\(\\Am_{pq}\\) with respect to \\(\\Mm_{ij}\\) is given by:\n<br>   \\[\n   \\frac{\\partial{\\Am_{pq}}}{\\partial \\Mm_{ij}} = \\frac{\\partial}{\\partial \\Mm_{ij}}\\frac{\\exp(\\Mm_{pq})}{\\sum_k \\exp(\\Mm_{pk})} =  \\delta_{ip}\\delta_{jq} \\Am_{ij} - \\delta_{ip} \\Am_{iq}\\Am_{ij} .\n   \\]\n<br>   By expressing this in matrix notation, we obtain the desired result. The block diagonal structure arises from the term \\(\\delta_{ip}\\), which indicates that the softmax is applied row-wise.\n<br>\n<br>Thus, the proof shows the step-by-step derivation of the partial derivatives and the structure of the gradients."
                        }
                    },
                    {
                        "statement_id": "0c204a91-22dc-4547-93a3-10c7b9c98988",
                        "paper_id": "2a384494-8439-4b46-8c5a-7b8d6b3deea1",
                        "library_nr": 12,
                        "library_name": "Lemma 12",
                        "title": "Gradients of Self Attention with respect to Embedding Matrix",
                        "statement_original_tex": "\\begin{lemma}[Gradients of Self Attention with respect to the Embedding matrix]\n\\label{lemma:grads_SA_X}\nThe gradients of the self attention layer with respect to the embedding matrix $\\Xm$ defined in Eq.~\\eqref{eq:self_att} have the following form\n\\small{\n\\begin{align}\n\\label{eq:grad_inp}\n    \\frac{\\partial\\Sm}{\\partial\\Xm} \n    &= \\frac{1}{\\sqrt{d_k}}(\\Im_n\\kro \\wVT\\Xm^\\top)\\,\\frac{\\partial \\Am}{\\partial \\Mm}  \\,\\left(\\Im_n\\kro\\Xm\\wK\\wQT + \\Km_{nn}(\\Im_n\\kro\\Xm\\wQ\\wKT)\\right) \\,+ \\,\\Am\\kro{\\wV}^\\top,\n\\end{align}}\\normalsize\nwhere the gradients of the softmax with respect to its inputs are denoted by $\\frac{\\partial \\Am}{\\partial \\Mm}$ as before.\n\\end{lemma}",
                        "statement_html": "The gradients of the self attention layer with respect to the embedding matrix $\\Xm$ defined in Eq. (1) [in <a href=\"https://arxiv.org/pdf/2206.03126#equation.2.1\">original paper</a>] have the following form\n\n\\begin{align}\n\\label{eq:grad_inp}\n    \\frac{\\partial\\Sm}{\\partial\\Xm} \n    &= \\frac{1}{\\sqrt{d_k}}(\\Im_n\\kro \\wVT\\Xm^\\top)\\,\\frac{\\partial \\Am}{\\partial \\Mm}  \\,\\left(\\Im_n\\kro\\Xm\\wK\\wQT + \\Km_{nn}(\\Im_n\\kro\\Xm\\wQ\\wKT)\\right) \\,+ \\,\\Am\\kro{\\wV}^\\top,\n\\end{align}\nwhere the gradients of the softmax with respect to its inputs are denoted by $\\frac{\\partial \\Am}{\\partial \\Mm}$ as before.",
                        "statement_type": "lemma",
                        "statement_motivation_html": "Understanding the gradients of the self-attention layer with respect to the embedding matrix $\\Xm$ is crucial for training transformer models. This expression allows us to backpropagate errors through the self-attention mechanism, enabling the model to learn meaningful representations from the input data. It is particularly useful in natural language processing tasks, where transformers have become the state-of-the-art architecture for tasks such as translation, summarization, and question answering.",
                        "html_url": "library/lemmas/lemma_12/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "9a473c9d-186d-416d-9748-249bc72169e3",
                            "paper_id": "2a384494-8439-4b46-8c5a-7b8d6b3deea1",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\nRemember that we defined $\\Sm = \\soft(\\extra{\\frac{1}{\\sqrt{d_k}}}\\Xm\\wQ\\wKT\\Xm^\\top)\\Xm\\wV$. Alongside with our previous shorthands $\\Am$, $\\Mm$, let us define the remaining $\\Xm\\wV$ as a matrix $\\Tm$, so that $\\Sm=\\Am\\,\\Tm$. Both $\\Am$ and $\\Tm$ are functions of $\\Xm$. So the matrix differential can be written as:\n\\begin{align}\n    \\frac{\\partial\\Sm}{\\partial\\Xm} &=  \\frac{\\partial\\Sm}{\\partial\\Am}\\frac{\\partial\\Am}{\\partial\\Xm} + \\frac{\\partial\\Sm}{\\partial\\Tm}\\frac{\\partial\\Tm}{\\partial\\Xm}\\\\\n    &=  \\frac{\\partial\\Sm}{\\partial\\Am}\\frac{\\partial\\Am}{\\partial\\Mm} \\frac{\\partial \\Mm}{\\partial\\Xm} + \\frac{\\partial\\Sm}{\\partial\\Tm}\\frac{\\partial\\Tm}{\\partial\\Xm}\\\\\n    &= (\\Im_n\\kro \\wVT\\Xm^\\top)\\,\\frac{\\partial \\Am}{\\partial \\Mm}  \\,\\frac{\\partial \\Mm}{\\partial \\Xm} + (\\Am\\kro\\Im_d)(\\Im_n\\kro\\wVT)\\\\\n    &= (\\Im_n\\kro \\wVT\\Xm^\\top)\\,\\frac{\\partial \\Am}{\\partial \\Mm}  \\,\\frac{\\partial \\Mm}{\\partial \\Xm} + (\\Am\\kro\\wVT)\\label{eq:grad-SA-X}\n\\end{align}\n\n\nNext, we use the matrix differential and then the identification theorem of matrix derivatives to compute the matrix gradient $\\frac{\\partial \\Am}{\\partial \\Xm}$\n\\begin{align*}\n    \\mathrm{d} \\Am &=  \\extra{\\frac{1}{\\sqrt{d_k}}}\\mathrm{d}(\\Xm)\\, \\wQ\\wKT\\Xm^\\top + \\extra{\\frac{1}{\\sqrt{d_k}}}\\Xm\\wQ\\wKT  \\,\\mathrm{d}(\\Xm^\\top).\n\\end{align*}\n\nVectorizing both sides:\n\\begin{align*}\n    \\mathrm{d} \\vect_r(\\Am) &=  \\extra{\\frac{1}{\\sqrt{d_k}}}(\\Im_n\\kro\\Xm\\wK\\wQT)\\mathrm{d}(\\vect_r(\\Xm))\\, + \\,\\extra{\\frac{1}{\\sqrt{d_k}}}(\\Xm\\wQ\\wKT \\kro\\Im_n) \\,\\mathrm{d}(\\vect_r(\\Xm^\\top)) \\\\\n    &=  \\extra{\\frac{1}{\\sqrt{d_k}}}(\\Im_n\\kro\\Xm\\wK\\wQT)\\mathrm{d}(\\vect_r(\\Xm))\\, + \\,\\extra{\\frac{1}{\\sqrt{d_k}}}(\\Xm\\wQ\\wKT \\kro\\Im_n) \\Km_{dn}\\,\\mathrm{d}(\\vect_r(\\Xm)).\n    \n\\end{align*}\n\nRecall, for an arbitrary matrix $\\Bm\\in\\mathbb{R}^{m\\times n}$, the commutation matrix $\\Km_{mn}$ transforms columnwise vectorization into rowwise vectorization. More precisely,\n\\begin{align*}\n    \\Km_{mn}\\vect_c(\\Bm) = \\vect_c(\\Bm^\\top)\n\\end{align*}\nand $\\vect_c(\\Bm) = \\vect_r(\\Bm^\\top)$. Therefore, for rowwise vectorization, we have a similar result:\n\\begin{align*}\n    \\Km_{mn}\\vect_r(\\Bm^\\top) &= \\vect_r(\\Bm)\\\\\n    \\vect_r(\\Bm^\\top) &= \\Km_{nm}\\vect_r(\\Bm),\n\\end{align*}\nwhere in the last line we used the fact the commutation is a permutation matrix, so $\\Km_{mn}^{-1}=\\Km_{mn}^\\top=\\Km_{nm}$. Thus, we get the required matrix derivative as follows:\n$$\\frac{\\partial \\Am}{\\partial \\Xm} = \\extra{\\frac{1}{\\sqrt{d_k}}}\\Im_n\\kro\\Xm\\wK\\wQT + \\extra{\\frac{1}{\\sqrt{d_k}}}(\\Xm\\wQ\\wKT\\kro\\Im_n)\\Km_{dn}\\,.$$\nNext, we will use a property of commutation matrix to make things simpler (Theorem 7.9, \\cite{magnus2019matrix}):\n$$\n\\frac{\\partial \\Am}{\\partial \\Xm} = \\extra{\\frac{1}{\\sqrt{d_k}}}\\Im_n\\kro\\Xm\\wK\\wQT + \\extra{\\frac{1}{\\sqrt{d_k}}}\\Km_{nn}(\\Im_n\\kro\\Xm\\wQ\\wKT).\n$$\nPlugging this into the above Eq.~\\eqref{eq:grad-SA-X}, we get:\n\\begin{align*}\n     \\frac{\\partial\\Sm}{\\partial\\Xm} \n    &= \\extra{\\frac{1}{\\sqrt{d_k}}}(\\Im_n\\kro \\wVT\\Xm^\\top)\\,\\frac{\\partial \\Am}{\\partial \\Mm}  \\,\\left(\\Im_n\\kro\\Xm\\wK\\wQT + \\Km_{nn}(\\Im_n\\kro\\Xm\\wQ\\wKT)\\right) \\,+ \\,\\Am\\kro{\\wV}^\\top.\n\\end{align*}\nAs a sanity check, we can calculate if the shapes of the matrices are consistent. LHS should be a $nd\\times nd$ matrix, while the constituent matrices of the first term on RHS: $\\Im_n\\kro {\\wV}^\\top\\Xm^\\top\\in\\mathbb{R}^{nd\\times n^2}$, $\\frac{\\partial \\Am}{\\partial \\Mm} \\in\\mathbb{R}^{n^2\\times n^2}$, the additive term next to it is a $n^2\\times nd$ matrix, and the second term on RHS is a Kronecker product of a $n\\times n$ and a $d\\times d$ matrix. \n\\end{proof}",
                            "statement_html": "Remember that we defined $\\Sm = \\soft(\\extra{\\frac{1}{\\sqrt{d_k}}}\\Xm\\wQ\\wKT\\Xm^\\top)\\Xm\\wV$. Alongside with our previous shorthands $\\Am$, $\\Mm$, let us define the remaining $\\Xm\\wV$ as a matrix $\\Tm$, so that $\\Sm=\\Am\\,\\Tm$. Both $\\Am$ and $\\Tm$ are functions of $\\Xm$. So the matrix differential can be written as:\n\\begin{align}\n    \\frac{\\partial\\Sm}{\\partial\\Xm} &=  \\frac{\\partial\\Sm}{\\partial\\Am}\\frac{\\partial\\Am}{\\partial\\Xm} + \\frac{\\partial\\Sm}{\\partial\\Tm}\\frac{\\partial\\Tm}{\\partial\\Xm}\\\\\n    &=  \\frac{\\partial\\Sm}{\\partial\\Am}\\frac{\\partial\\Am}{\\partial\\Mm} \\frac{\\partial \\Mm}{\\partial\\Xm} + \\frac{\\partial\\Sm}{\\partial\\Tm}\\frac{\\partial\\Tm}{\\partial\\Xm}\\\\\n    &= (\\Im_n\\kro \\wVT\\Xm^\\top)\\,\\frac{\\partial \\Am}{\\partial \\Mm}  \\,\\frac{\\partial \\Mm}{\\partial \\Xm} + (\\Am\\kro\\Im_d)(\\Im_n\\kro\\wVT)\\\\\n    &= (\\Im_n\\kro \\wVT\\Xm^\\top)\\,\\frac{\\partial \\Am}{\\partial \\Mm}  \\,\\frac{\\partial \\Mm}{\\partial \\Xm} + (\\Am\\kro\\wVT)\\label{eq:grad-SA-X}\n\\end{align}\n\nNext, we use the matrix differential and then the identification theorem of matrix derivatives to compute the matrix gradient $\\frac{\\partial \\Am}{\\partial \\Xm}$\n\\begin{align*}\n    \\mathrm{d} \\Am &=  \\extra{\\frac{1}{\\sqrt{d_k}}}\\mathrm{d}(\\Xm)\\, \\wQ\\wKT\\Xm^\\top + \\extra{\\frac{1}{\\sqrt{d_k}}}\\Xm\\wQ\\wKT  \\,\\mathrm{d}(\\Xm^\\top).\n\\end{align*}\n\nVectorizing both sides:\n\\begin{align*}\n    \\mathrm{d} \\vect_r(\\Am) &=  \\extra{\\frac{1}{\\sqrt{d_k}}}(\\Im_n\\kro\\Xm\\wK\\wQT)\\mathrm{d}(\\vect_r(\\Xm))\\, + \\,\\extra{\\frac{1}{\\sqrt{d_k}}}(\\Xm\\wQ\\wKT \\kro\\Im_n) \\,\\mathrm{d}(\\vect_r(\\Xm^\\top)) \\\\\n    &=  \\extra{\\frac{1}{\\sqrt{d_k}}}(\\Im_n\\kro\\Xm\\wK\\wQT)\\mathrm{d}(\\vect_r(\\Xm))\\, + \\,\\extra{\\frac{1}{\\sqrt{d_k}}}(\\Xm\\wQ\\wKT \\kro\\Im_n) \\Km_{dn}\\,\\mathrm{d}(\\vect_r(\\Xm)).\n\\end{align*}\n\nRecall, for an arbitrary matrix $\\Bm\\in\\mathbb{R}^{m\\times n}$, the commutation matrix $\\Km_{mn}$ transforms columnwise vectorization into rowwise vectorization. More precisely,\n\\begin{align*}\n    \\Km_{mn}\\vect_c(\\Bm) = \\vect_c(\\Bm^\\top)\n\\end{align*}\nand $\\vect_c(\\Bm) = \\vect_r(\\Bm^\\top)$. Therefore, for rowwise vectorization, we have a similar result:\n\\begin{align*}\n    \\Km_{mn}\\vect_r(\\Bm^\\top) &= \\vect_r(\\Bm)\\\\\n    \\vect_r(\\Bm^\\top) &= \\Km_{nm}\\vect_r(\\Bm),\n\\end{align*}\nwhere in the last line we used the fact the commutation is a permutation matrix, so $\\Km_{mn}^{-1}=\\Km_{mn}^\\top=\\Km_{nm}$. Thus, we get the required matrix derivative as follows:\n$$\\frac{\\partial \\Am}{\\partial \\Xm} = \\extra{\\frac{1}{\\sqrt{d_k}}}\\Im_n\\kro\\Xm\\wK\\wQT + \\extra{\\frac{1}{\\sqrt{d_k}}}(\\Xm\\wQ\\wKT\\kro\\Im_n)\\Km_{dn}\\,.$$\nNext, we will use a property of commutation matrix to make things simpler (Theorem 7.9, <a href=\"https://arxiv.org/pdf/2206.03126#cite.magnus2019matrix\"> Magnus and Neudecker [2019]</a>):\n$$\n\\frac{\\partial \\Am}{\\partial \\Xm} = \\extra{\\frac{1}{\\sqrt{d_k}}}\\Im_n\\kro\\Xm\\wK\\wQT + \\extra{\\frac{1}{\\sqrt{d_k}}}\\Km_{nn}(\\Im_n\\kro\\Xm\\wQ\\wKT).\n$$\nPlugging this into the above Eq. \\eqref{eq:grad-SA-X}, we get:\n\\begin{align*}\n     \\frac{\\partial\\Sm}{\\partial\\Xm} \n    &= \\extra{\\frac{1}{\\sqrt{d_k}}}(\\Im_n\\kro \\wVT\\Xm^\\top)\\,\\frac{\\partial \\Am}{\\partial \\Mm}  \\,\\left(\\Im_n\\kro\\Xm\\wK\\wQT + \\Km_{nn}(\\Im_n\\kro\\Xm\\wQ\\wKT)\\right) \\,+ \\,\\Am\\kro{\\wV}^\\top.\n\\end{align*}\nAs a sanity check, we can calculate if the shapes of the matrices are consistent. LHS should be a $nd\\times nd$ matrix, while the constituent matrices of the first term on RHS: $\\Im_n\\kro {\\wV}^\\top\\Xm^\\top\\in\\mathbb{R}^{nd\\times n^2}$, $\\frac{\\partial \\Am}{\\partial \\Mm} \\in\\mathbb{R}^{n^2\\times n^2}$, the additive term next to it is a $n^2\\times nd$ matrix, and the second term on RHS is a Kronecker product of a $n\\times n$ and a $d\\times d$ matrix.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "In this proof, we aim to compute the matrix gradient \\(\\frac{\\partial \\Sm}{\\partial \\Xm}\\) for the given definition of \\(\\Sm\\). Here are the steps involved:\n<br>\n<br>1. <i></i>Definition and Shorthands<i></i>:\n<br>   We start by defining \\(\\Sm = \\soft\\left(\\frac{1}{\\sqrt{d_k}}\\Xm\\wQ\\wKT\\Xm^\\top\\right)\\Xm\\wV\\). We introduce shorthands \\(\\Am\\) and \\(\\Mm\\), and define \\(\\Tm = \\Xm\\wV\\) so that \\(\\Sm = \\Am\\,\\Tm\\).\n<br>\n<br>2. <i></i>Matrix Differential<i></i>:\n<br>   We express the matrix differential of \\(\\Sm\\) in terms of \\(\\Am\\) and \\(\\Tm\\):\n<br>   \\[\n   \\frac{\\partial\\Sm}{\\partial\\Xm} = \\frac{\\partial\\Sm}{\\partial\\Am}\\frac{\\partial\\Am}{\\partial\\Xm} + \\frac{\\partial\\Sm}{\\partial\\Tm}\\frac{\\partial\\Tm}{\\partial\\Xm}\n   \\]\n<br>   Breaking it down further using the chain rule:\n<br>   \\[\n   \\frac{\\partial\\Sm}{\\partial\\Xm} = \\frac{\\partial\\Sm}{\\partial\\Am}\\frac{\\partial\\Am}{\\partial\\Mm} \\frac{\\partial \\Mm}{\\partial\\Xm} + \\frac{\\partial\\Sm}{\\partial\\Tm}\\frac{\\partial\\Tm}{\\partial\\Xm}\n   \\]\n<br>\n<br>3. <i></i>Vectorization and Commutation Matrix<i></i>:\n<br>   We use vectorization and the commutation matrix \\(\\Km_{mn}\\) to transform the differentials:\n<br>   \\[\n   \\mathrm{d} \\vect_r(\\Am) = \\frac{1}{\\sqrt{d_k}}(\\Im_n\\kro\\Xm\\wK\\wQT)\\mathrm{d}(\\vect_r(\\Xm)) + \\frac{1}{\\sqrt{d_k}}(\\Xm\\wQ\\wKT \\kro\\Im_n) \\Km_{dn}\\,\\mathrm{d}(\\vect_r(\\Xm))\n   \\]\n<br>\n<br>4. <i></i>Matrix Derivative<i></i>:\n<br>   Using properties of the commutation matrix, we derive the matrix derivative:\n<br>   \\[\n   \\frac{\\partial \\Am}{\\partial \\Xm} = \\frac{1}{\\sqrt{d_k}}\\Im_n\\kro\\Xm\\wK\\wQT + \\frac{1}{\\sqrt{d_k}}(\\Xm\\wQ\\wKT\\kro\\Im_n)\\Km_{dn}\n   \\]\n<br>   Simplifying further:\n<br>   \\[\n   \\frac{\\partial \\Am}{\\partial \\Xm} = \\frac{1}{\\sqrt{d_k}}\\Im_n\\kro\\Xm\\wK\\wQT + \\frac{1}{\\sqrt{d_k}}\\Km_{nn}(\\Im_n\\kro\\Xm\\wQ\\wKT)\n   \\]\n<br>\n<br>5. <i></i>Final Gradient Expression<i></i>:\n<br>   Plugging the matrix derivative back into the expression for \\(\\frac{\\partial\\Sm}{\\partial\\Xm}\\):\n<br>   \\[\n   \\frac{\\partial\\Sm}{\\partial\\Xm} = \\frac{1}{\\sqrt{d_k}}(\\Im_n\\kro \\wVT\\Xm^\\top)\\,\\frac{\\partial \\Am}{\\partial \\Mm}  \\,\\left(\\Im_n\\kro\\Xm\\wK\\wQT + \\Km_{nn}(\\Im_n\\kro\\Xm\\wQ\\wKT)\\right) + \\Am\\kro{\\wV}^\\top\n   \\]\n<br>\n<br>6. <i></i>Sanity Check<i></i>:\n<br>   We verify the consistency of matrix dimensions to ensure correctness. The left-hand side (LHS) should be a \\(nd \\times nd\\) matrix, and the right-hand side (RHS) components are checked for matching dimensions.\n<br>\n<br>This step-by-step breakdown helps in understanding the derivation of the matrix gradient \\(\\frac{\\partial \\Sm}{\\partial \\Xm}\\) using matrix calculus and properties of the commutation matrix."
                        }
                    },
                    {
                        "statement_id": "b3f81e84-9174-4452-ba0a-008c842507dc",
                        "paper_id": "2a384494-8439-4b46-8c5a-7b8d6b3deea1",
                        "library_nr": 13,
                        "library_name": "Lemma 13",
                        "title": "Gradients of Uniform-Attention Representations",
                        "statement_original_tex": "\\begin{lemma}[]\n\n\\label{lemma:gradients_queries}\nLet $\\Xm^{\\ell}$ be the representations of the input sequence at the $\\ell$-th layer. Under the uniform-attention assumption, we have\n    \\begin{align}\n    \\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\Wm^{V,\\ell}}\\right\\|^2_F &= d_v n \\mathbb{E}\\|\\bar{\\bm{x}}^{\\ell}\\|^2~\\label{eq:jacobian_values};\\\\ \\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\Wm^{Q,\\ell}} \\right\\|^2_F &= \\frac{\\sigma^2_v\\sigma^2_k d_v}{n^2}\\cdot \\Exp \\left[ \\|\\Xm^{\\ell}\\|^2_F \\cdot  \\|(\\Xm^{\\ell})^\\top\\Xm^{\\ell} - n\\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top\\|^2_F\\right]~\\label{eq:jacobian_queries};\\\\ \n    \\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\Xm^{\\ell}}\\right\\|^2_F &\\leq \\frac{8\\sigma^2_q\\sigma^2_k\\sigma^2_v d_kd_v}{n} \\, \\cdot \\mathbb{E} \\norm{(\\Xm^{\\ell})^\\top\\Xm^{\\ell} - n\\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top}^2_F + 2d_v^2\\sigma^2_v \\; .\n\\end{align}\n\n\\end{lemma}",
                        "statement_html": "Let $\\Xm^{\\ell}$ be the representations of the input sequence at the $\\ell$-th layer. Under the uniform-attention assumption, we have\n\\begin{align}\n\\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\Wm^{V,\\ell}}\\right\\|^2_F &= d_v n \\mathbb{E}\\|\\bar{\\mathbf{x}}^{\\ell}\\|^2~\\label{eq:jacobian_values};\\\\ \n\\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\Wm^{Q,\\ell}} \\right\\|^2_F &= \\frac{\\sigma^2_v\\sigma^2_k d_v}{n^2}\\cdot \\Exp \\left[ \\|\\Xm^{\\ell}\\|^2_F \\cdot  \\|(\\Xm^{\\ell})^\\top\\Xm^{\\ell} - n\\bar{\\mathbf{x}}^{\\ell}(\\bar{\\mathbf{x}}^{\\ell})^\\top\\|^2_F\\right]~\\label{eq:jacobian_queries};\\\\ \n\\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\Xm^{\\ell}}\\right\\|^2_F &\\leq \\frac{8\\sigma^2_q\\sigma^2_k\\sigma^2_v d_kd_v}{n} \\, \\cdot \\mathbb{E} \\norm{(\\Xm^{\\ell})^\\top\\Xm^{\\ell} - n\\bar{\\mathbf{x}}^{\\ell}(\\bar{\\mathbf{x}}^{\\ell})^\\top}^2_F + 2d_v^2\\sigma^2_v \\; .\n\\end{align}",
                        "statement_type": "lemma",
                        "statement_motivation_html": "Understanding the gradients of the output sequence with respect to the value, query, and input matrices in a transformer model is crucial for analyzing the model's training dynamics and stability. These equations provide insights into how changes in the model parameters affect the output, which can help in diagnosing issues such as vanishing or exploding gradients. Use these results to ensure that the model is learning effectively and to guide adjustments in the architecture or training process.",
                        "html_url": "library/lemmas/lemma_13/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "36933d25-94dc-453a-8c07-21b3e27c8b91",
                            "paper_id": "2a384494-8439-4b46-8c5a-7b8d6b3deea1",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\nBy using the chain rule and the fact that for two matrixes $\\Am, \\Bm$ we have that $\\norm{\\Am\\Bm}_F^2 \\leq \\norm{\\Am}_F^2\\norm{\\Bm}_F^2$, we can upper bound the gradient as:\n\\begin{align*}\n    \\norm{\\frac{\\partial \\mathcal{L}}{\\partial{\\Wm^{Q,\\ell}}}}_F^2 &\\leq \\prod_{i=\\ell+1}^{L-1} \\norm{\\frac{\\partial \\Xm^{i+1}}{\\partial \\Xm^i}}_{F}^2 \\norm{\\frac{\\partial \\mathcal{L}}{\\partial \\Xm^L}}_F^2 \\norm{\\frac{\\partial \\Xm^{\\ell+1}}{\\partial \\Wm^{Q,\\ell}}}_F^2 \\\\\n    &\\leq \\prod_{i=\\ell+1}^{L-1} \\norm{\\frac{\\partial \\Xm^{i+1}}{\\partial \\Xm^i}}_{F}^2 \\norm{\\frac{\\partial \\mathcal{L}}{\\partial \\Xm^L}}_F^2 \\norm{\\frac{\\partial \\Xm^{\\ell+1}}{\\partial \\Zm^{\\ell}}}_F^2 \\norm{\\frac{\\partial \\Zm^\\ell}{\\partial \\Wm^{Q, \\ell}}}_F^2 \\\\\n    &\\leq \\prod_{i=\\ell+1}^{L-1} \\norm{\\frac{\\partial \\Xm^{i+1}}{\\partial \\Xm^i}}_{F}^2 \\norm{\\frac{\\partial \\mathcal{L}}{\\partial \\Xm^L}}_F^2 \\norm{\\frac{\\partial \\Xm^{\\ell+1}}{\\partial \\Zm^{\\ell}}}_F^2 \\left(\\norm{\\frac{\\partial \\alpha_1 \\Sm^{\\ell}}{\\partial \\Wm^{Q, \\ell}}}_F^2 + \\underbrace{\\norm{\\frac{\\partial \\Xm^{\\ell}}{\\partial \\Wm^{Q, \\ell}}}_F^2}_{=0} \\right) ,\n\\end{align*}\nwhere we recall that $\\Zm^{\\ell} = \\alpha_1 \\Sm^{\\ell}+ \\Xm^{\\ell}$ and in the last step we have used that $\\Xm^\\ell$ does not depend on $\\Wm^{Q,\\ell}$, hence the gradient vanishes. By taking expectation and using the tower property, we have that:\n\\begin{equation*}\n    \\Exp \\norm{\\frac{\\partial \\mathcal{L}}{\\partial{\\Wm^{Q,\\ell}}}}_F^2 \\leq \\Exp\\left[\\underbrace{\\Exp\\left[\\prod_{i=\\ell+1}^{L-1} \\norm{\\frac{\\partial \\Xm^{i+1}}{\\partial \\Xm^i}}_{F}^2 \\norm{\\frac{\\partial \\mathcal{L}}{\\partial \\Xm^L}}_F^2 \\norm{\\frac{\\partial \\Xm^{\\ell+1}}{\\partial \\Zm^{\\ell}}}_F^2\\right]}_{=:G(\\Xm^\\ell)} \\norm{\\frac{\\partial \\alpha_1 \\Sm^{\\ell}}{\\partial \\Wm^{Q, \\ell}}}_F^2   \\right],\n\\end{equation*}\nwhere the expectations are taken with respect to $\\Xm^\\ell$ for the outer one and conditioning on $\\Xm^\\ell$ for inner one. Indeed, the first three terms only depend on the network values after $\\Xm^\\ell$. Now, a repeated application of the tower property in $G(\\Xm^\\ell)$, together with the results on the gradients of Lemma \\ref{lemma:gradients_queries}, easily shows that $G(\\Xm^\\ell)$ stays bounded under our hypothesis. To see this one can also simply note that, since the softmax and its derivatives are almost surely bounded, the boundedness of $G(\\Xm^\\ell)$ is implied by an analogous statement for a vanilla linear MLP~(i.e removing the softmax). In this setting, the random variable inside the expectation in $G(\\Xm^\\ell)$ is a finite linear combination of Gaussian products --- which has bounded expectation.\n\nAll in all, we have that\n\\begin{equation*}\n    \\Exp \\norm{\\frac{\\partial \\mathcal{L}}{\\partial{\\Wm^{Q,\\ell}}}}_F^2\\le \\Exp\\left[ B_{\\Xm^\\ell}\\norm{\\frac{\\partial \\alpha_1 \\Sm^{\\ell}}{\\partial \\Wm^{Q, \\ell}}}_F^2\\right],\n\\end{equation*}\nwhere $B_{\\Xm^\\ell}$ is an almost-surely-bounded function of $\\Xm^{\\ell}$. Hence, to show that $\\Exp \\norm{\\frac{\\partial \\mathcal{L}}{\\partial{\\Wm^{Q,\\ell}}}}_F^2=0$, we now just need to show that:\n\\begin{equation*}\n    \\Exp \\norm{\\frac{\\partial \\alpha_1 \\Sm^{\\ell}}{\\partial \\Wm^{Q, \\ell}}}_F^2 = 0\n\\end{equation*}\nunder the rank-1 hypothesis for $\\Xm^\\ell$.\nLet $\\Xm_{1}^\\ell, \\dots \\Xm_{n}^\\ell \\in \\mathbb{R}^{d_v}$  be the representations for the $n$ tokens. Under the rank-1 assumption, each token can be written as a multiple of a single vector $\\bm{x} \\in \\mathbb{R}^{d_v}$, and hence there exists $a_1, \\dots, a_n \\in \\mathbb{R}$ such that $\\Xm_1 = a_1 \\bm{x}, \\dots, \\Xm_n = a_n \\bm{x}$. From Lemma \\ref{lemma:gradients_queries}, we know that:\n\\begin{equation*}\n    \\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\wQ} \\right\\|^2_F = \\frac{\\sigma^2_v\\sigma^2_k d^2}{n^2}\\cdot \\Exp \\left[ \\|\\Xm^{\\ell}\\|^2_F \\cdot  \\|(\\Xm^{\\ell})^\\top\\Xm^{\\ell} - n\\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top\\|^2_F\\right] .\n\\end{equation*}\nThe mean token simplifies to $\\bar{\\bm{x}}^l = \\frac{\\bm{x}}{n}\\sum_k a_k$ and hence $\\left(\\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top\\right)_{ij} = \\frac{1}{n^2} (\\sum_{k}a_k)^2 x_ix_j$. Similarly, $\\left((\\Xm^{\\ell})^\\top\\Xm^{\\ell}\\right)_{ij} = \\sum_k a_k^2 x_i x_j$. If furthermore all the coefficients $a_i$ are the same (which corresponds to the rank collapse assumption $\\Xm^{\\ell}=\\bm{1}_{n}\\bm{x}^T$ analyzed here), then it is easy to see that $\\left((\\Xm^{\\ell})^\\top\\Xm^{\\ell}\\right)_{ij} - n \\left(\\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top\\right)_{ij} = 0 \\; \\forall i,j$ and hence $\\|(\\Xm^{\\ell})^\\top\\Xm^{\\ell} - n\\bar{\\bm{x}}^{\\ell}(\\bar{\\bm{x}}^{\\ell})^\\top\\|^2_F = 0$.\n\\end{proof}",
                            "statement_html": "By using the chain rule and the fact that for two matrices $\\Am, \\Bm$ we have that $\\norm{\\Am\\Bm}_F^2 \\leq \\norm{\\Am}_F^2\\norm{\\Bm}_F^2$, we can upper bound the gradient as:\n\\begin{align*}\n    \\norm{\\frac{\\partial \\mathcal{L}}{\\partial{\\Wm^{Q,\\ell}}}}_F^2 &\\leq \\prod_{i=\\ell+1}^{L-1} \\norm{\\frac{\\partial \\Xm^{i+1}}{\\partial \\Xm^i}}_{F}^2 \\norm{\\frac{\\partial \\mathcal{L}}{\\partial \\Xm^L}}_F^2 \\norm{\\frac{\\partial \\Xm^{\\ell+1}}{\\partial \\Wm^{Q,\\ell}}}_F^2 \\\\\n    &\\leq \\prod_{i=\\ell+1}^{L-1} \\norm{\\frac{\\partial \\Xm^{i+1}}{\\partial \\Xm^i}}_{F}^2 \\norm{\\frac{\\partial \\mathcal{L}}{\\partial \\Xm^L}}_F^2 \\norm{\\frac{\\partial \\Xm^{\\ell+1}}{\\partial \\Zm^{\\ell}}}_F^2 \\norm{\\frac{\\partial \\Zm^\\ell}{\\partial \\Wm^{Q, \\ell}}}_F^2 \\\\\n    &\\leq \\prod_{i=\\ell+1}^{L-1} \\norm{\\frac{\\partial \\Xm^{i+1}}{\\partial \\Xm^i}}_{F}^2 \\norm{\\frac{\\partial \\mathcal{L}}{\\partial \\Xm^L}}_F^2 \\norm{\\frac{\\partial \\Xm^{\\ell+1}}{\\partial \\Zm^{\\ell}}}_F^2 \\left(\\norm{\\frac{\\partial \\alpha_1 \\Sm^{\\ell}}{\\partial \\Wm^{Q, \\ell}}}_F^2 + \\underbrace{\\norm{\\frac{\\partial \\Xm^{\\ell}}{\\partial \\Wm^{Q, \\ell}}}_F^2}_{=0} \\right) ,\n\\end{align*}\nwhere we recall that $\\Zm^{\\ell} = \\alpha_1 \\Sm^{\\ell}+ \\Xm^{\\ell}$ and in the last step we have used that $\\Xm^\\ell$ does not depend on $\\Wm^{Q,\\ell}$, hence the gradient vanishes. By taking expectation and using the tower property, we have that:\n\\begin{equation*}\n    \\Exp \\norm{\\frac{\\partial \\mathcal{L}}{\\partial{\\Wm^{Q,\\ell}}}}_F^2 \\leq \\Exp\\left[\\underbrace{\\Exp\\left[\\prod_{i=\\ell+1}^{L-1} \\norm{\\frac{\\partial \\Xm^{i+1}}{\\partial \\Xm^i}}_{F}^2 \\norm{\\frac{\\partial \\mathcal{L}}{\\partial \\Xm^L}}_F^2 \\norm{\\frac{\\partial \\Xm^{\\ell+1}}{\\partial \\Zm^{\\ell}}}_F^2\\right]}_{=:G(\\Xm^\\ell)} \\norm{\\frac{\\partial \\alpha_1 \\Sm^{\\ell}}{\\partial \\Wm^{Q, \\ell}}}_F^2   \\right],\n\\end{equation*}\nwhere the expectations are taken with respect to $\\Xm^\\ell$ for the outer one and conditioning on $\\Xm^\\ell$ for the inner one. Indeed, the first three terms only depend on the network values after $\\Xm^\\ell$. Now, a repeated application of the tower property in $G(\\Xm^\\ell)$, together with the results on the gradients of <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_13/index.html#lemma%3Agradients_queries\">Lemma 13</a>, easily shows that $G(\\Xm^\\ell)$ stays bounded under our hypothesis. To see this one can also simply note that, since the softmax and its derivatives are almost surely bounded, the boundedness of $G(\\Xm^\\ell)$ is implied by an analogous statement for a vanilla linear MLP~(i.e., removing the softmax). In this setting, the random variable inside the expectation in $G(\\Xm^\\ell)$ is a finite linear combination of Gaussian products --- which has bounded expectation.\n\nAll in all, we have that\n\\begin{equation*}\n    \\Exp \\norm{\\frac{\\partial \\mathcal{L}}{\\partial{\\Wm^{Q,\\ell}}}}_F^2\\le \\Exp\\left[ B_{\\Xm^\\ell}\\norm{\\frac{\\partial \\alpha_1 \\Sm^{\\ell}}{\\partial \\Wm^{Q, \\ell}}}_F^2\\right],\n\\end{equation*}\nwhere $B_{\\Xm^\\ell}$ is an almost-surely-bounded function of $\\Xm^{\\ell}$. Hence, to show that $\\Exp \\norm{\\frac{\\partial \\mathcal{L}}{\\partial{\\Wm^{Q,\\ell}}}}_F^2=0$, we now just need to show that:\n\\begin{equation*}\n    \\Exp \\norm{\\frac{\\partial \\alpha_1 \\Sm^{\\ell}}{\\partial \\Wm^{Q, \\ell}}}_F^2 = 0\n\\end{equation*}\nunder the rank-1 hypothesis for $\\Xm^\\ell$.\nLet $\\Xm_{1}^\\ell, \\dots \\Xm_{n}^\\ell \\in \\mathbb{R}^{d_v}$ be the representations for the $n$ tokens. Under the rank-1 assumption, each token can be written as a multiple of a single vector $\\mathbf{x} \\in \\mathbb{R}^{d_v}$, and hence there exists $a_1, \\dots, a_n \\in \\mathbb{R}$ such that $\\Xm_1 = a_1 \\mathbf{x}, \\dots, \\Xm_n = a_n \\mathbf{x}$. From <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_13/index.html#lemma%3Agradients_queries\">Lemma 13</a>, we know that:\n\\begin{equation*}\n    \\mathbb{E}\\left\\|\\frac{\\partial \\Sm^{\\ell}}{\\partial \\wQ} \\right\\|^2_F = \\frac{\\sigma^2_v\\sigma^2_k d^2}{n^2}\\cdot \\Exp \\left[ \\|\\Xm^{\\ell}\\|^2_F \\cdot  \\|(\\Xm^{\\ell})^\\top\\Xm^{\\ell} - n\\bar{\\mathbf{x}}^{\\ell}(\\bar{\\mathbf{x}}^{\\ell})^\\top\\|^2_F\\right] .\n\\end{equation*}\nThe mean token simplifies to $\\bar{\\mathbf{x}}^l = \\frac{\\mathbf{x}}{n}\\sum_k a_k$ and hence $\\left(\\bar{\\mathbf{x}}^{\\ell}(\\bar{\\mathbf{x}}^{\\ell})^\\top\\right)_{ij} = \\frac{1}{n^2} (\\sum_{k}a_k)^2 x_ix_j$. Similarly, $\\left((\\Xm^{\\ell})^\\top\\Xm^{\\ell}\\right)_{ij} = \\sum_k a_k^2 x_i x_j$. If furthermore all the coefficients $a_i$ are the same (which corresponds to the rank collapse assumption $\\Xm^{\\ell}=\\mathbf{1}_{n}\\mathbf{x}^T$ analyzed here), then it is easy to see that $\\left((\\Xm^{\\ell})^\\top\\Xm^{\\ell}\\right)_{ij} - n \\left(\\bar{\\mathbf{x}}^{\\ell}(\\bar{\\mathbf{x}}^{\\ell})^\\top\\right)_{ij} = 0 \\; \\forall i,j$ and hence $\\|(\\Xm^{\\ell})^\\top\\Xm^{\\ell} - n\\bar{\\mathbf{x}}^{\\ell}(\\bar{\\mathbf{x}}^{\\ell})^\\top\\|^2_F = 0$.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n<br>\n<br>1. <i>Initial Bound on Gradient</i>: The proof starts by using the chain rule and a matrix norm inequality to upper bound the gradient:\n<br>   \\[\n   \\norm{\\frac{\\partial \\mathcal{L}}{\\partial{\\Wm^{Q,\\ell}}}}_F^2 \\leq \\prod_{i=\\ell+1}^{L-1} \\norm{\\frac{\\partial \\Xm^{i+1}}{\\partial \\Xm^i}}_{F}^2 \\norm{\\frac{\\partial \\mathcal{L}}{\\partial \\Xm^L}}_F^2 \\norm{\\frac{\\partial \\Xm^{\\ell+1}}{\\partial \\Wm^{Q,\\ell}}}_F^2\n   \\]\n<br>\n<br>2. <i>Decomposition of Terms</i>: The terms are further decomposed, introducing \\(\\Zm^{\\ell}\\) and noting that \\(\\Xm^\\ell\\) does not depend on \\(\\Wm^{Q,\\ell}\\):\n<br>   \\[\n   \\norm{\\frac{\\partial \\Xm^{\\ell+1}}{\\partial \\Wm^{Q,\\ell}}}_F^2 = \\norm{\\frac{\\partial \\alpha_1 \\Sm^{\\ell}}{\\partial \\Wm^{Q, \\ell}}}_F^2 + \\underbrace{\\norm{\\frac{\\partial \\Xm^{\\ell}}{\\partial \\Wm^{Q, \\ell}}}_F^2}_{=0}\n   \\]\n<br>\n<br>3. <i>Expectation and Tower Property</i>: By taking the expectation and using the tower property, the proof introduces \\(G(\\Xm^\\ell)\\):\n<br>   \\[\n   \\Exp \\norm{\\frac{\\partial \\mathcal{L}}{\\partial{\\Wm^{Q,\\ell}}}}_F^2 \\leq \\Exp\\left[G(\\Xm^\\ell) \\norm{\\frac{\\partial \\alpha_1 \\Sm^{\\ell}}{\\partial \\Wm^{Q, \\ell}}}_F^2\\right]\n   \\]\n<br>\n<br>4. <i>Boundedness of \\(G(\\Xm^\\ell)\\)</i>: The boundedness of \\(G(\\Xm^\\ell)\\) is established using the properties of the softmax function and its derivatives, and by analogy to a vanilla linear MLP:\n<br>   \\[\n   G(\\Xm^\\ell) \\text{ is almost-surely bounded}\n   \\]\n<br>\n<br>5. <i>Final Bound and Rank-1 Hypothesis</i>: The proof concludes by showing that under the rank-1 hypothesis for \\(\\Xm^\\ell\\), the expectation of the gradient norm squared is zero:\n<br>   \\[\n   \\Exp \\norm{\\frac{\\partial \\alpha_1 \\Sm^{\\ell}}{\\partial \\Wm^{Q, \\ell}}}_F^2 = 0\n   \\]\n<br>\n<br>6. <i>Rank-1 Assumption Details</i>: The rank-1 assumption is detailed, showing that each token is a multiple of a single vector \\(\\mathbf{x}\\), leading to:\n<br>   \\[\n   \\|(\\Xm^{\\ell})^\\top\\Xm^{\\ell} - n\\bar{\\mathbf{x}}^{\\ell}(\\bar{\\mathbf{x}}^{\\ell})^\\top\\|^2_F = 0\n   \\]\n<br>\n<br>Thus, the proof shows that:\n<br>\\[\n\\Exp \\norm{\\frac{\\partial \\mathcal{L}}{\\partial{\\Wm^{Q,\\ell}}}}_F^2 = 0\n\\]"
                        }
                    },
                    {
                        "statement_id": "f0ae32e1-7429-428d-b63e-c40f626219ed",
                        "paper_id": "2a384494-8439-4b46-8c5a-7b8d6b3deea1",
                        "library_nr": 14,
                        "library_name": "Lemma 14",
                        "title": "Expectation of Product of Linear Transformations",
                        "statement_original_tex": "\\begin{lemma}[Expectation of Linear Layers]\n\\label{lemma:exp_linear}\nLet $\\Dm = \\Xm \\Wm$, where $\\Wm\\in\\mathbb{R}^{d\\times d}$ is a random matrix with i.i.d random entries with variance $\\sigma^2 = \\frac{1}{d}$ and $\\Xm\\in\\mathbb{R}^{n\\times d}$ is a fixed matrix:\n\\begin{equation*}\n    \\Exp[\\Dm_{kj}\\Dm_{k'j}] = \\frac{1}{d}\\langle \\Xm_k, \\Xm_{k'} \\rangle\n\\end{equation*}\n\\end{lemma}",
                        "statement_html": "Let $\\Dm = \\Xm \\Wm$, where $\\Wm \\in \\mathbb{R}^{d \\times d}$ is a random matrix with i.i.d random entries with variance $\\sigma^2 = \\frac{1}{d}$ and $\\Xm \\in \\mathbb{R}^{n \\times d}$ is a fixed matrix:\n\\begin{equation*}\n    \\Exp[\\Dm_{kj}\\Dm_{k'j}] = \\frac{1}{d}\\langle \\Xm_k, \\Xm_{k'} \\rangle\n\\end{equation*}",
                        "statement_type": "lemma",
                        "statement_motivation_html": "This result is useful in the context of random matrix theory and high-dimensional statistics. It shows how the entries of the product matrix $\\Dm$ are correlated based on the inner products of the rows of $\\Xm$. This can be particularly useful when analyzing the behavior of large datasets or in machine learning applications where random projections are used to reduce dimensionality while preserving the structure of the data.",
                        "html_url": "library/lemmas/lemma_14/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "ef3efe62-df3a-4750-923c-75763729cfe9",
                            "paper_id": "2a384494-8439-4b46-8c5a-7b8d6b3deea1",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\n\\begin{equation*}\n     \\Exp [\\Dm_{kj} \\Dm_{k'j}] = \\sum_{zz'} \\Xm_{kz} \\Xm_{k'z'} \\Exp[\\Wm_{zj}\\Wm_{z'j}] = \\sigma^2 \\sum_{z}\\Xm_{kz}\\Xm_{k'z} = \\frac{1}{d}\\langle \\Xm_k, \\Xm_{k'} \\rangle.\n\\end{equation*}\n   \n\\end{proof}",
                            "statement_html": "\\begin{equation*}\n     \\Exp [\\Dm_{kj} \\Dm_{k'j}] = \\sum_{zz'} \\Xm_{kz} \\Xm_{k'z'} \\Exp[\\Wm_{zj}\\Wm_{z'j}] = \\sigma^2 \\sum_{z}\\Xm_{kz}\\Xm_{k'z} = \\frac{1}{d}\\langle \\Xm_k, \\Xm_{k'} \\rangle.\n\\end{equation*}",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n<br>\n<br>1. <i></i>Expectation of Product<i></i>: The proof begins by considering the expectation of the product of two elements, \\(\\Dm_{kj}\\) and \\(\\Dm_{k'j}\\). This is expressed as:\n<br>   \\[\n   \\Exp [\\Dm_{kj} \\Dm_{k'j}]\n   \\]\n<br>\n<br>2. <i></i>Substitution with Sum<i></i>: Next, the elements \\(\\Dm_{kj}\\) and \\(\\Dm_{k'j}\\) are substituted with their respective sums involving \\(\\Xm\\) and \\(\\Wm\\):\n<br>   \\[\n   \\sum_{zz'} \\Xm_{kz} \\Xm_{k'z'} \\Exp[\\Wm_{zj}\\Wm_{z'j}]\n   \\]\n<br>\n<br>3. <i></i>Expectation of \\(\\Wm\\) Terms<i></i>: The expectation \\(\\Exp[\\Wm_{zj}\\Wm_{z'j}]\\) is simplified using the property that \\(\\Wm_{zj}\\) and \\(\\Wm_{z'j}\\) are independent and identically distributed with variance \\(\\sigma^2\\). This results in:\n<br>   \\[\n   \\sigma^2 \\sum_{z}\\Xm_{kz}\\Xm_{k'z}\n   \\]\n<br>\n<br>4. <i></i>Inner Product Representation<i></i>: Finally, the sum \\(\\sum_{z}\\Xm_{kz}\\Xm_{k'z}\\) is recognized as the inner product \\(\\langle \\Xm_k, \\Xm_{k'} \\rangle\\), and the factor \\(\\sigma^2\\) is adjusted by the dimension \\(d\\):\n<br>   \\[\n   \\frac{1}{d}\\langle \\Xm_k, \\Xm_{k'} \\rangle\n   \\]\n<br>\n<br>Thus, the proof shows that:\n<br>\\[\n\\Exp [\\Dm_{kj} \\Dm_{k'j}] = \\frac{1}{d}\\langle \\Xm_k, \\Xm_{k'} \\rangle\n\\]"
                        }
                    },
                    {
                        "statement_id": "58e31c93-5954-4c77-b560-71699e806636",
                        "paper_id": "2a384494-8439-4b46-8c5a-7b8d6b3deea1",
                        "library_nr": 15,
                        "library_name": "Lemma 15",
                        "title": "Linearity of Expectation with Skip Connection",
                        "statement_original_tex": "\\begin{lemma}[Expectation of skip connection]\n\\label{lemma:exp_skip}\n    Let $\\Am, \\Bm \\in \\mathbb{R}^{p \\times q}$. Let $\\Dm := \\alpha\\Am + \\Bm$ with $\\Exp[\\Am | \\Bm] = \\bm{0}$ and $\\alpha \\in \\mathbb{R}$. Then:\n    \\begin{equation}\n        \\Exp\\left[\\Dm_{ij}\\Dm_{i'j}\\right] = \\alpha^2 \\mathbb{E}[\\Am_{ij}\\Am_{i'j}] + \\mathbb{E}[\\Bm_{ij}\\Bm_{ij'}] \n    \\end{equation}\n    holds for all $i,i' \\in [p], j \\in [q]$.\n\\end{lemma}",
                        "statement_html": "Let $\\Am, \\Bm \\in \\mathbb{R}^{p \\times q}$. Let $\\Dm := \\alpha\\Am + \\Bm$ with $\\Exp[\\Am | \\Bm] = \\mathbf{0}$ and $\\alpha \\in \\mathbb{R}$. Then:\n\\begin{equation}\n    \\Exp\\left[\\Dm_{ij}\\Dm_{i'j}\\right] = \\alpha^2 \\mathbb{E}[\\Am_{ij}\\Am_{i'j}] + \\mathbb{E}[\\Bm_{ij}\\Bm_{ij'}] \n\\end{equation}\nholds for all $i,i' \\in [p], j \\in [q]$.",
                        "statement_type": "lemma",
                        "statement_motivation_html": "This result is useful in the context of random matrices and their expectations. It allows us to decompose the expectation of the product of elements in the matrix $\\Dm$ into contributions from the matrices $\\Am$ and $\\Bm$. This can be particularly helpful in statistical analysis and signal processing, where understanding the behavior of random matrices is crucial.",
                        "html_url": "library/lemmas/lemma_15/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "62e2b2dd-1467-491c-8434-a3566829c94d",
                            "paper_id": "2a384494-8439-4b46-8c5a-7b8d6b3deea1",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\n\\begin{align*}\n    \\mathbb{E}[\\Dm_{ij}\\Dm_{i'j}] &= \\mathbb{E}\\left[(\\alpha \\Am_{ij} + \\Bm_{ij})(\\alpha \\Am_{i'j} + \\Bm_{i'j})\\right]\\\\\n    &= \\mathbb{E}\\left[\\alpha^2 \\Am_{ij}\\Am_{i'j} + \\alpha \\Am_{ij}\\Bm_{i'j} + \\alpha \\Am_{i'j}\\Bm_{ij} + \\Bm_{ij}\\Bm_{i'j} \\right] \\\\\n    &= \\alpha^2\\mathbb{E}\\left[ \\Am_{kj}\\Am_{i'j}\\right] + \\mathbb{E}\\left[ \\Bm_{ij}\\Bm_{i',j} \\right],\n\\end{align*}\nwhere using iterated expectations $\\alpha \\Exp[\\Am_{i'j}\\Bm_{ij}] = \\alpha \\Exp[\\Exp [\\Am_{i'j} | \\Bm] \\Bm_{ij}]] = 0$ and identically $\\alpha \\Exp [\\Am_{ij}\\Bm_{i'j}] = 0$.\n\\end{proof}",
                            "statement_html": "\\begin{align*}\n    \\mathbb{E}[\\Dm_{ij}\\Dm_{i'j}] &= \\mathbb{E}\\left[(\\alpha \\Am_{ij} + \\Bm_{ij})(\\alpha \\Am_{i'j} + \\Bm_{i'j})\\right]\\\\\n    &= \\mathbb{E}\\left[\\alpha^2 \\Am_{ij}\\Am_{i'j} + \\alpha \\Am_{ij}\\Bm_{i'j} + \\alpha \\Am_{i'j}\\Bm_{ij} + \\Bm_{ij}\\Bm_{i'j} \\right] \\\\\n    &= \\alpha^2\\mathbb{E}\\left[ \\Am_{kj}\\Am_{i'j}\\right] + \\mathbb{E}\\left[ \\Bm_{ij}\\Bm_{i',j} \\right],\n\\end{align*}\nwhere using iterated expectations $\\alpha \\Exp[\\Am_{i'j}\\Bm_{ij}] = \\alpha \\Exp[\\Exp [\\Am_{i'j} | \\Bm] \\Bm_{ij}]] = 0$ and identically $\\alpha \\Exp [\\Am_{ij}\\Bm_{i'j}] = 0$.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break down each step:\n<br>\n<br>1. <i></i>Initial Expression<i></i>:\n<br>   \\[\n   \\mathbb{E}[\\Dm_{ij}\\Dm_{i'j}] = \\mathbb{E}\\left[(\\alpha \\Am_{ij} + \\Bm_{ij})(\\alpha \\Am_{i'j} + \\Bm_{i'j})\\right]\n   \\]\n<br>   This is the expectation of the product of two terms, each of which is a linear combination of \\(\\Am\\) and \\(\\Bm\\).\n<br>\n<br>2. <i></i>Expanding the Product<i></i>:\n<br>   \\[\n   \\mathbb{E}\\left[\\alpha^2 \\Am_{ij}\\Am_{i'j} + \\alpha \\Am_{ij}\\Bm_{i'j} + \\alpha \\Am_{i'j}\\Bm_{ij} + \\Bm_{ij}\\Bm_{i'j} \\right]\n   \\]\n<br>   Here, we expand the product inside the expectation. This results in four terms.\n<br>\n<br>3. <i></i>Separating the Expectation<i></i>:\n<br>   \\[\n   \\alpha^2\\mathbb{E}\\left[ \\Am_{ij}\\Am_{i'j}\\right] + \\mathbb{E}\\left[ \\Bm_{ij}\\Bm_{i'j} \\right]\n   \\]\n<br>   We use the linearity of expectation to separate the terms. The cross terms \\(\\alpha \\mathbb{E}[\\Am_{ij}\\Bm_{i'j}]\\) and \\(\\alpha \\mathbb{E}[\\Am_{i'j}\\Bm_{ij}]\\) are zero, which simplifies the expression.\n<br>\n<br>4. <i></i>Using Iterated Expectations<i></i>:\n<br>   \\[\n   \\alpha \\Exp[\\Am_{i'j}\\Bm_{ij}] = \\alpha \\Exp[\\Exp [\\Am_{i'j} | \\Bm] \\Bm_{ij}] = 0\n   \\]\n<br>   \\[\n   \\alpha \\Exp [\\Am_{ij}\\Bm_{i'j}] = 0\n   \\]\n<br>   These steps use the property of iterated expectations. Given that \\(\\Am\\) and \\(\\Bm\\) are independent, the inner expectation \\(\\Exp [\\Am_{i'j} | \\Bm]\\) is zero, leading to the cross terms being zero.\n<br>\n<br>By following these steps, we arrive at the final simplified expression."
                        }
                    },
                    {
                        "statement_id": "298ded94-86fc-4fbc-82dd-3514ada706e2",
                        "paper_id": "2a384494-8439-4b46-8c5a-7b8d6b3deea1",
                        "library_nr": 16,
                        "library_name": "Lemma 16",
                        "title": "Expectation of Softmax Products",
                        "statement_original_tex": "\\begin{lemma}[Expectation of Attention Layers]\n\\label{lemma:exp_softmax}\nUnder the uniform-attention assumption:\n\\begin{equation*}\n    \\Exp[\\Sm_{kj}\\Sm_{k'j}] = \\frac{1}{d_vn^2}\\Exp C(\\Xm).\n\\end{equation*}\n\\end{lemma}",
                        "statement_html": "Under the uniform-attention assumption:\n\\begin{equation*}\n    \\Exp[\\Sm_{kj}\\Sm_{k'j}] = \\frac{1}{d_vn^2}\\Exp C(\\Xm).\n\\end{equation*}",
                        "statement_type": "lemma",
                        "statement_motivation_html": "The given statement is useful in the context of analyzing the behavior of attention mechanisms in neural networks, particularly under the uniform-attention assumption. It provides a way to compute the expected value of the product of attention scores, which can be crucial for understanding how information is distributed and aggregated across different layers of the network. This can be particularly helpful when designing or optimizing attention-based models, such as transformers, to ensure they are effectively capturing and utilizing the input data.",
                        "html_url": "library/lemmas/lemma_16/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "5771ff74-1195-4ef6-be87-28516fb081e6",
                            "paper_id": "2a384494-8439-4b46-8c5a-7b8d6b3deea1",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\nNote that under the uniform-attention assumption:\n\\begin{equation*}\n    \\Sm_{kj} = \\frac{1}{n}\\left(\\bm{1}_{n\\times n}\\Xm \\Wm^V\\right)_{kj} = \\frac{1}{n}\\sum_{zi}\\Xm_{zi}\\Wm^V_{ij}.\n\\end{equation*}\nHence, using the fact that the weights are i.i.d with variance $\\sigma_v^2=\\frac{1}{d_v}$:\n\\begin{align*}\n    \\Exp[\\Sm_{kj}\\Sm_{k'j}] = \\frac{\\sigma_{v}^2}{n^2}\\sum_{z,z'}\\sum_i\\Exp[\\Xm_{zi} \\Xm_{z'i}] = \\frac{1}{d_vn^2}\\sum_{k,k'}\\langle\\Xm_z, \\Xm_{z'}\\rangle = \\frac{1}{d_vn^2}\\Exp C(\\Xm).\n\\end{align*}\n\\end{proof}",
                            "statement_html": "Note that under the uniform-attention assumption:\n\\begin{equation*}\n    \\Sm_{kj} = \\frac{1}{n}\\left(\\mathbf{1}_{n\\times n}\\Xm \\Wm^V\\right)_{kj} = \\frac{1}{n}\\sum_{zi}\\Xm_{zi}\\Wm^V_{ij}.\n\\end{equation*}\nHence, using the fact that the weights are i.i.d with variance $\\sigma_v^2=\\frac{1}{d_v}$:\n\\begin{align*}\n    \\Exp[\\Sm_{kj}\\Sm_{k'j}] = \\frac{\\sigma_{v}^2}{n^2}\\sum_{z,z'}\\sum_i\\Exp[\\Xm_{zi} \\Xm_{z'i}] = \\frac{1}{d_vn^2}\\sum_{k,k'}\\langle\\Xm_z, \\Xm_{z'}\\rangle = \\frac{1}{d_vn^2}\\Exp C(\\Xm).\n\\end{align*}",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "Here is a short explanation of the different steps in the proof:\n<br>\n<br>1. <i></i>Uniform-Attention Assumption<i></i>:\n<br>   Under the uniform-attention assumption, the matrix \\(\\Sm\\) is defined as:\n<br>   \\[\n   \\Sm_{kj} = \\frac{1}{n}\\left(\\mathbf{1}_{n\\times n}\\Xm \\Wm^V\\right)_{kj} = \\frac{1}{n}\\sum_{zi}\\Xm_{zi}\\Wm^V_{ij}.\n   \\]\n<br>   This equation expresses \\(\\Sm_{kj}\\) as the average of the product of elements from \\(\\Xm\\) and \\(\\Wm^V\\).\n<br>\n<br>2. <i></i>Expectation of Product of \\(\\Sm\\) Elements<i></i>:\n<br>   Using the fact that the weights \\(\\Wm^V_{ij}\\) are independent and identically distributed (i.i.d) with variance \\(\\sigma_v^2 = \\frac{1}{d_v}\\), we compute the expectation of the product \\(\\Sm_{kj}\\Sm_{k'j}\\):\n<br>   \\[\n   \\Exp[\\Sm_{kj}\\Sm_{k'j}] = \\frac{\\sigma_{v}^2}{n^2}\\sum_{z,z'}\\sum_i\\Exp[\\Xm_{zi} \\Xm_{z'i}].\n   \\]\n<br>   This step involves summing over all possible indices \\(z\\) and \\(z'\\) and taking the expectation of the product of corresponding elements in \\(\\Xm\\).\n<br>\n<br>3. <i></i>Inner Product and Expectation<i></i>:\n<br>   The expectation of the product \\(\\Xm_{zi} \\Xm_{z'i}\\) can be expressed in terms of the inner product of vectors \\(\\Xm_z\\) and \\(\\Xm_{z'}\\):\n<br>   \\[\n   \\frac{1}{d_vn^2}\\sum_{k,k'}\\langle\\Xm_z, \\Xm_{z'}\\rangle.\n   \\]\n<br>   Here, \\(\\langle\\Xm_z, \\Xm_{z'}\\rangle\\) denotes the inner product of the vectors \\(\\Xm_z\\) and \\(\\Xm_{z'}\\).\n<br>\n<br>4. <i></i>Final Expression<i></i>:\n<br>   Finally, the expectation is expressed in terms of the expected value of a function \\(C(\\Xm)\\):\n<br>   \\[\n   \\frac{1}{d_vn^2}\\Exp C(\\Xm).\n   \\]\n<br>   This step consolidates the previous results into a compact form involving the expected value of \\(C(\\Xm)\\).\n<br>\n<br>This proof demonstrates how the uniform-attention assumption and the properties of i.i.d weights lead to the final expression for the expectation of the product of elements in \\(\\Sm\\)."
                        }
                    },
                    {
                        "statement_id": "b13e0d85-82ee-4c0f-b48d-485f82298179",
                        "paper_id": "2a384494-8439-4b46-8c5a-7b8d6b3deea1",
                        "library_nr": 17,
                        "library_name": "Lemma 17",
                        "title": "Exponential Growth of Inner Products",
                        "statement_original_tex": "\\begin{lemma}[Propagation of inner products]\n\n\\label{lemma:propagation_of_inner_producets}\n Let $C(\\Xm^\\ell) = \\sum_{k,k'} \\langle \\Xm_{k}^\\ell, \\Xm_{k'}^\\ell \\rangle$ and $\\Xm$ the input sequence. Under the Assumption~\\ref{ass:uniform_softmax} and if $\\sigma$ is the linear activation function, we have that:\n \n \\begin{equation}\n     \\Exp \\left[C(\\Xm^{L})\\right] = (\\alpha_2^2 + 1)^{L}(\\alpha_1^2 + 1)^{L}C(\\Xm)  .\n \\end{equation}\n hence, under the depth scaling for the residual block parameters $\\alpha_1^2 = \\frac{\\tilde{\\alpha}_1}{L}, \\alpha_2^2 = \\frac{\\tilde{\\alpha}_2}{L}$ with $\\tilde{\\alpha}_1, \\tilde{\\alpha}_2 \\in \\mathbb{R}$ independent of $L$, we have that:\n \\begin{equation}\n      \\lim_{L\\to \\infty} \\Exp[C(\\Xm^L)] = \\text{e}^{\\tilde{\\alpha}_1 + \\tilde{\\alpha}_2}C(\\Xm).\n \\end{equation}\n\n\\end{lemma}",
                        "statement_html": "Let $C(\\Xm^\\ell) = \\sum_{k,k'} \\langle \\Xm_{k}^\\ell, \\Xm_{k'}^\\ell \\rangle$ and $\\Xm$ the input sequence. Under the Assumption 3.1 [in <a href=\"https://arxiv.org/pdf/2206.03126#ass.3.1\">original paper</a>] and if $\\sigma$ is the linear activation function, we have that:\n \n\\begin{equation}\n    \\Exp \\left[C(\\Xm^{L})\\right] = (\\alpha_2^2 + 1)^{L}(\\alpha_1^2 + 1)^{L}C(\\Xm)  .\n\\end{equation}\nhence, under the depth scaling for the residual block parameters $\\alpha_1^2 = \\frac{\\tilde{\\alpha}_1}{L}, \\alpha_2^2 = \\frac{\\tilde{\\alpha}_2}{L}$ with $\\tilde{\\alpha}_1, \\tilde{\\alpha}_2 \\in \\mathbb{R}$ independent of $L$, we have that:\n\\begin{equation}\n     \\lim_{L\\to \\infty} \\Exp[C(\\Xm^L)] = \\text{e}^{\\tilde{\\alpha}_1 + \\tilde{\\alpha}_2}C(\\Xm).\n\\end{equation}",
                        "statement_type": "lemma",
                        "statement_motivation_html": "This result is useful in the analysis of deep neural networks, particularly in understanding how the covariance of the input sequence evolves through the layers. It shows that under certain conditions, the covariance grows exponentially with the depth of the network. This insight can be used to design better initialization schemes and scaling strategies for the parameters of deep networks to ensure stable training and avoid issues like vanishing or exploding gradients.",
                        "html_url": "library/lemmas/lemma_17/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "fde18714-ade7-4f10-a827-22b70754d4a8",
                            "paper_id": "2a384494-8439-4b46-8c5a-7b8d6b3deea1",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\nFirst, note that for the residual blocks we have that $\\Exp [Y^{\\ell}_{kj} | Z^{\\ell}_{k'j}] = 0$ due to the independence assumption on the feedforward weights, and similarly $\\Exp [S^{\\ell}_{kj} | X^{\\ell}_{k'j}] = 0$. Hence, we can use Lemma \\ref{lemma:exp_skip} in both the skip connections of the Transformer architecture.\nTherefore, using Lemma \\ref{lemma:exp_skip} (skip), Lemma \\ref{lemma:exp_linear} (linear) and Lemma \\ref{lemma:exp_softmax} (attention):\n\\begin{align*}\n        &\\mathbb{E}[C(\\Xm^{\\ell+1})] \\\\\n        \\overset{\\text{skip}}&{=} \\alpha_2^2\\mathbb{E}C(\\Ym^\\ell) + \\mathbb{E}C(\\Zm^{\\ell}) \\\\ \n        \\overset{\\text{linear}}&{=} \\alpha_2^2\\mathbb{E}C(\\Zm^{\\ell}) + \\mathbb{E}C(\\Zm^{\\ell}) \\\\ \n        &= (\\alpha_2^2 + 1)\\mathbb{E}C(\\Zm^{\\ell}) \\\\\n        \\overset{\\text{skip}}&{=} (\\alpha_2^2 + 1)\\left(\\alpha_1^2\\mathbb{E}C(\\Sm^{\\ell}) + \\mathbb{E}C(\\Xm^{\\ell})\\right) \\\\ \n        \\overset{\\text{attention}}&{=}  (\\alpha_2^2 + 1)(\\alpha_1^2 + 1) \\mathbb{E}[C(\\Xm^{\\ell})] \\\\ \n        \\overset{\\text{unroll recurs.}}&{=} (\\alpha_2^2 + 1)^{\\ell+1}(\\alpha_1^2 + 1)^{\\ell+1}C(\\Xm) ,\n    \\end{align*}\n    where in the last step we have unrolled the recursion until the input layer.\n    \n    For the limit as $L\\to \\infty$, simply note that:\n    $$\\lim_{L\\to \\infty}\\left(\\frac{\\tilde{\\alpha}_i}{L}+1\\right)^{L} = \\text{e}^{\\tilde{\\alpha}_i} ,$$ \n    with $i \\in \\{1, 2\\}$.\n\\end{proof}",
                            "statement_html": "First, note that for the residual blocks we have that $\\Exp [Y^{\\ell}_{kj} | Z^{\\ell}_{k'j}] = 0$ due to the independence assumption on the feedforward weights, and similarly $\\Exp [S^{\\ell}_{kj} | X^{\\ell}_{k'j}] = 0$. Hence, we can use <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_15/index.html#lemma%3Aexp_skip\">Lemma 15</a> in both the skip connections of the Transformer architecture.\nTherefore, using <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_15/index.html#lemma%3Aexp_skip\">Lemma 15</a> (skip), <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_14/index.html#lemma%3Aexp_linear\">Lemma 14</a> (linear) and <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_16/index.html#lemma%3Aexp_softmax\">Lemma 16</a> (attention):\n\\begin{align*}\n        &\\mathbb{E}[C(\\Xm^{\\ell+1})] \\\\\n        &\\overset{\\text{skip}}{=} \\alpha_2^2\\mathbb{E}C(\\Ym^\\ell) + \\mathbb{E}C(\\Zm^{\\ell}) \\\\ \n        &\\overset{\\text{linear}}{=} \\alpha_2^2\\mathbb{E}C(\\Zm^{\\ell}) + \\mathbb{E}C(\\Zm^{\\ell}) \\\\ \n        &= (\\alpha_2^2 + 1)\\mathbb{E}C(\\Zm^{\\ell}) \\\\\n        &\\overset{\\text{skip}}{=} (\\alpha_2^2 + 1)\\left(\\alpha_1^2\\mathbb{E}C(\\Sm^{\\ell}) + \\mathbb{E}C(\\Xm^{\\ell})\\right) \\\\ \n        &\\overset{\\text{attention}}{=}  (\\alpha_2^2 + 1)(\\alpha_1^2 + 1) \\mathbb{E}[C(\\Xm^{\\ell})] \\\\ \n        &\\overset{\\text{unroll recurs.}}{=} (\\alpha_2^2 + 1)^{\\ell+1}(\\alpha_1^2 + 1)^{\\ell+1}C(\\Xm) ,\n    \\end{align*}\n    where in the last step we have unrolled the recursion until the input layer.\n    \n    For the limit as $L\\to \\infty$, simply note that:\n    $$\\lim_{L\\to \\infty}\\left(\\frac{\\tilde{\\alpha}_i}{L}+1\\right)^{L} = \\text{e}^{\\tilde{\\alpha}_i} ,$$ \n    with $i \\in \\{1, 2\\}$.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "First, note that for the residual blocks we have that $\\Exp [Y^{\\ell}_{kj} | Z^{\\ell}_{k'j}] = 0$ due to the independence assumption on the feedforward weights, and similarly $\\Exp [S^{\\ell}_{kj} | X^{\\ell}_{k'j}] = 0$. Hence, we can use <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_15/index.html#lemma%3Aexp_skip\">Lemma 15</a> in both the skip connections of the Transformer architecture.\n\nTherefore, using <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_15/index.html#lemma%3Aexp_skip\">Lemma 15</a> (skip), <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_14/index.html#lemma%3Aexp_linear\">Lemma 14</a> (linear) and <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_16/index.html#lemma%3Aexp_softmax\">Lemma 16</a> (attention):\n\\begin{align*}\n        &\\mathbb{E}[C(\\Xm^{\\ell+1})] \\\\\n        &\\overset{\\text{skip}}{=} \\alpha_2^2\\mathbb{E}C(\\Ym^\\ell) + \\mathbb{E}C(\\Zm^{\\ell}) \\\\ \n        &\\overset{\\text{linear}}{=} \\alpha_2^2\\mathbb{E}C(\\Zm^{\\ell}) + \\mathbb{E}C(\\Zm^{\\ell}) \\\\ \n        &= (\\alpha_2^2 + 1)\\mathbb{E}C(\\Zm^{\\ell}) \\\\\n        &\\overset{\\text{skip}}{=} (\\alpha_2^2 + 1)\\left(\\alpha_1^2\\mathbb{E}C(\\Sm^{\\ell}) + \\mathbb{E}C(\\Xm^{\\ell})\\right) \\\\ \n        &\\overset{\\text{attention}}{=}  (\\alpha_2^2 + 1)(\\alpha_1^2 + 1) \\mathbb{E}[C(\\Xm^{\\ell})] \\\\ \n        &\\overset{\\text{unroll recurs.}}{=} (\\alpha_2^2 + 1)^{\\ell+1}(\\alpha_1^2 + 1)^{\\ell+1}C(\\Xm) ,\n    \\end{align*}\n    where in the last step we have unrolled the recursion until the input layer.\n    \nFor the limit as $L\\to \\infty$, simply note that:\n$$\\lim_{L\\to \\infty}\\left(\\frac{\\tilde{\\alpha}_i}{L}+1\\right)^{L} = \\text{e}^{\\tilde{\\alpha}_i} ,$$ \nwith $i \\in \\{1, 2\\}$."
                        }
                    },
                    {
                        "statement_id": "824964c1-57b2-4543-b711-a1127d17581f",
                        "paper_id": "2a384494-8439-4b46-8c5a-7b8d6b3deea1",
                        "library_nr": 18,
                        "library_name": "Lemma 18",
                        "title": "Exponential Norm Growth in Deep Networks",
                        "statement_original_tex": "\\begin{lemma}[Propagation of the norm]\n\n\\label{thm:forward_pass}\n  Let $\\Xm^{L}$ be the representations of the input sequence at the final layer. Under the assumptions of Lemma \\ref{lemma:propagation_of_inner_producets}, we have that:\n \\begin{equation}\n     \\Exp \\norm{\\Xm^{L}}_{F}^2 = n (\\alpha_2^2+1)^{L}\\alpha_1^2 \\sum_{k=0}^{L-1}(\\alpha_1^2+1)^k \\norm{\\bar{\\bm{x}}}^2 + (\\alpha_2^2+1)^{L} ||\\Xm||_F^2  ,\n \\end{equation}\n hence, under the depth scaling for the residual block parameters $\\alpha_1^2 = \\frac{\\tilde{\\alpha}_1}{L}, \\alpha_2^2 = \\frac{\\tilde{\\alpha}_2}{L}$ with $\\tilde{\\alpha_1}, \\tilde{\\alpha_2} \\in \\mathbb{R}$ independent of $L$, we have that:\n \\begin{equation}\n     \\lim_{L\\to \\infty} \\Exp \\norm{\\Xm^{L}}_{F}^2 = n \\text{e}^{\\tilde{\\alpha}_2}(\\text{e}^{\\tilde{\\alpha}_1} - 1)\\norm{\\bar{\\bm{x}}}^2 + \\text{e}^{\\tilde{\\alpha}_2} ||\\Xm||_F^2.\n \\end{equation}\n\n\\end{lemma}",
                        "statement_html": "Let $\\Xm^{L}$ be the representations of the input sequence at the final layer. Under the assumptions of <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_17/index.html#lemma%3Apropagation_of_inner_producets\">Lemma 17</a>, we have that:\n\\begin{equation}\n    \\Exp \\norm{\\Xm^{L}}_{F}^2 = n (\\alpha_2^2+1)^{L}\\alpha_1^2 \\sum_{k=0}^{L-1}(\\alpha_1^2+1)^k \\norm{\\bar{\\mathbf{x}}}^2 + (\\alpha_2^2+1)^{L} ||\\Xm||_F^2  ,\n\\end{equation}\nhence, under the depth scaling for the residual block parameters $\\alpha_1^2 = \\frac{\\tilde{\\alpha}_1}{L}, \\alpha_2^2 = \\frac{\\tilde{\\alpha}_2}{L}$ with $\\tilde{\\alpha_1}, \\tilde{\\alpha_2} \\in \\mathbb{R}$ independent of $L$, we have that:\n\\begin{equation}\n    \\lim_{L\\to \\infty} \\Exp \\norm{\\Xm^{L}}_{F}^2 = n \\text{e}^{\\tilde{\\alpha}_2}(\\text{e}^{\\tilde{\\alpha}_1} - 1)\\norm{\\bar{\\mathbf{x}}}^2 + \\text{e}^{\\tilde{\\alpha}_2} ||\\Xm||_F^2.\n\\end{equation}",
                        "statement_type": "lemma",
                        "statement_motivation_html": "Understanding the behavior of the representations of input sequences in deep neural networks is crucial for designing and analyzing these models. The given result provides an explicit formula for the expected Frobenius norm of the representations at the final layer, which helps in understanding how the representations evolve as the network depth increases. This is particularly useful when choosing the scaling parameters for residual blocks, ensuring that the network remains stable and avoids issues like vanishing or exploding gradients. By applying the depth scaling for the residual block parameters, one can predict the asymptotic behavior of the network, which is essential for training very deep networks effectively.",
                        "html_url": "library/lemmas/lemma_18/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "81775d4e-0d17-45c7-a4f8-d2415a01499b",
                            "paper_id": "2a384494-8439-4b46-8c5a-7b8d6b3deea1",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\nDue to the rotational symmetries of the Gaussian random matrices, if the input  tokens have the same norm, then the expected norm at layer $\\ell \\in [L]$ is also the same across the token's representations. Hence, we can write $\\Exp\\norm{\\Xm^\\ell}_F^2 = n \\Exp\\norm{\\bm{x}^\\ell}^2$, where $\\norm{\\bm{x}^\\ell}^2$ is the norm of every token at layer $\\ell$. Furthermore, by definition of our correlation coefficient $\\rho^l_{kk'}$, we have that $\\Exp\\langle\\Xm^\\ell_k, \\Xm^\\ell_{k'}\\rangle = \\rho^\\ell_{kk'} \\Exp\\norm{\\bm{x}^\\ell}^2$. By summing over the indexes $k,k'$, we can expand the relation as:\n\\begin{equation*}\n    \\underbrace{\\sum_{k,k'}\\Exp\\langle\\Xm^\\ell_k, \\Xm^\\ell_{k'}\\rangle}_{\\Exp C(\\Xm)} = \\sum_{k,k'} \\rho^\\ell_{kk'} \\Exp\\norm{\\bm{x}^\\ell}^2 = (n + \\sum_{k\\neq k'}\\rho^\\ell_{k,k'})\\Exp\\norm{\\bm{x}^\\ell}^2 = \\underbrace{n\\Exp\\norm{\\bm{x}^\\ell}^2}_{\\Exp \\norm{\\Xm^\\ell}_F^2}(1 + (n-1) \\rho^\\ell).\n\\end{equation*}\nBy solving for $\\rho^\\ell$, we have that:\n\\begin{equation*}\n    \\rho^\\ell = \\frac{\\Exp C(\\Xm^\\ell)}{(n-1)\\Exp \\norm{\\Xm^\\ell}^2 } - \\frac{1}{n-1} .\n\\end{equation*}\nNow we plug in the expressions for $\\Exp C(\\Xm^\\ell)$ and $\\Exp \\norm{\\Xm^\\ell}^2 $ with the aid of Lemma \\ref{lemma:propagation_of_inner_producets} and Lemma \\ref{thm:forward_pass}, respectively. Finally, by taking the limits with respect to $L$, we get the desired result.\n\\end{proof}",
                            "statement_html": "Due to the rotational symmetries of the Gaussian random matrices, if the input tokens have the same norm, then the expected norm at layer $\\ell \\in [L]$ is also the same across the token's representations. Hence, we can write $\\Exp\\norm{\\Xm^\\ell}_F^2 = n \\Exp\\norm{\\mathbf{x}^\\ell}^2$, where $\\norm{\\mathbf{x}^\\ell}^2$ is the norm of every token at layer $\\ell$. Furthermore, by definition of our correlation coefficient $\\rho^\\ell_{kk'}$, we have that $\\Exp\\langle\\Xm^\\ell_k, \\Xm^\\ell_{k'}\\rangle = \\rho^\\ell_{kk'} \\Exp\\norm{\\mathbf{x}^\\ell}^2$. By summing over the indexes $k,k'$, we can expand the relation as:\n\\begin{equation*}\n    \\underbrace{\\sum_{k,k'}\\Exp\\langle\\Xm^\\ell_k, \\Xm^\\ell_{k'}\\rangle}_{\\Exp C(\\Xm)} = \\sum_{k,k'} \\rho^\\ell_{kk'} \\Exp\\norm{\\mathbf{x}^\\ell}^2 = (n + \\sum_{k\\neq k'}\\rho^\\ell_{k,k'})\\Exp\\norm{\\mathbf{x}^\\ell}^2 = \\underbrace{n\\Exp\\norm{\\mathbf{x}^\\ell}^2}_{\\Exp \\norm{\\Xm^\\ell}_F^2}(1 + (n-1) \\rho^\\ell).\n\\end{equation*}\nBy solving for $\\rho^\\ell$, we have that:\n\\begin{equation*}\n    \\rho^\\ell = \\frac{\\Exp C(\\Xm^\\ell)}{(n-1)\\Exp \\norm{\\Xm^\\ell}^2 } - \\frac{1}{n-1} .\n\\end{equation*}\nNow we plug in the expressions for $\\Exp C(\\Xm^\\ell)$ and $\\Exp \\norm{\\Xm^\\ell}^2 $ with the aid of <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_17/index.html#lemma%3Apropagation_of_inner_producets\">Lemma 17</a> and <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_18/index.html#thm%3Aforward_pass\">Lemma 18</a>, respectively. Finally, by taking the limits with respect to $L$, we get the desired result.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break down the different steps:\n<br>\n<br>1. <i></i>Rotational Symmetries and Norms<i></i>:\n<br>   - The proof begins by leveraging the rotational symmetries of Gaussian random matrices. This symmetry implies that if the input tokens have the same norm, then the expected norm at any layer $\\ell$ will also be the same across all token representations.\n<br>   - Mathematically, this is expressed as $\\Exp\\norm{\\Xm^\\ell}_F^2 = n \\Exp\\norm{\\mathbf{x}^\\ell}^2$, where $\\norm{\\mathbf{x}^\\ell}^2$ is the norm of each token at layer $\\ell$.\n<br>\n<br>2. <i></i>Correlation Coefficient Definition<i></i>:\n<br>   - The correlation coefficient $\\rho^\\ell_{kk'}$ is defined such that $\\Exp\\langle\\Xm^\\ell_k, \\Xm^\\ell_{k'}\\rangle = \\rho^\\ell_{kk'} \\Exp\\norm{\\mathbf{x}^\\ell}^2$.\n<br>   - This relationship helps in understanding how the inner products of token representations relate to their norms and the correlation coefficient.\n<br>\n<br>3. <i></i>Summing Over Indexes<i></i>:\n<br>   - By summing over the indexes $k$ and $k'$, the proof expands the relation to:\n<br>     \\[\n     \\sum_{k,k'}\\Exp\\langle\\Xm^\\ell_k, \\Xm^\\ell_{k'}\\rangle = \\sum_{k,k'} \\rho^\\ell_{kk'} \\Exp\\norm{\\mathbf{x}^\\ell}^2.\n     \\]\n<br>   - This can be further simplified to:\n<br>     \\[\n     (n + \\sum_{k\\neq k'}\\rho^\\ell_{k,k'})\\Exp\\norm{\\mathbf{x}^\\ell}^2 = n\\Exp\\norm{\\mathbf{x}^\\ell}^2(1 + (n-1) \\rho^\\ell).\n     \\]\n<br>\n<br>4. <i></i>Solving for $\\rho^\\ell$<i></i>:\n<br>   - By isolating $\\rho^\\ell$, the proof derives:\n<br>     \\[\n     \\rho^\\ell = \\frac{\\Exp C(\\Xm^\\ell)}{(n-1)\\Exp \\norm{\\Xm^\\ell}^2 } - \\frac{1}{n-1}.\n     \\]\n<br>\n<br>5. <i></i>Using Lemmas for Expressions<i></i>:\n<br>   - The proof then references <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_17/index.html#lemma%3Apropagation_of_inner_producets\">Lemma 17</a> and <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_18/index.html#thm%3Aforward_pass\">Lemma 18</a> to plug in the expressions for $\\Exp C(\\Xm^\\ell)$ and $\\Exp \\norm{\\Xm^\\ell}^2$.\n<br>   - These lemmas provide the necessary mathematical tools to express the expected values in terms of known quantities.\n<br>\n<br>6. <i></i>Taking Limits<i></i>:\n<br>   - Finally, by taking the limits with respect to $L$, the proof arrives at the desired result, completing the argument.\n<br>\n<br>This structured approach ensures that each step logically follows from the previous one, leading to a coherent and rigorous proof."
                        }
                    },
                    {
                        "statement_id": "4ee3afa8-44f0-4b88-b879-44aa8481c46d",
                        "paper_id": "2a384494-8439-4b46-8c5a-7b8d6b3deea1",
                        "library_nr": 19,
                        "library_name": "Lemma 19",
                        "title": "Vanishing Attention Lemma",
                        "statement_original_tex": "\\begin{lemma}\n\\label{app:convergence_A}\nConsider initializing each entry of $\\wQ\\in\\mathbb{R}^{d_{v}\\times d_k}$ and $\\wK\\in\\mathbb{R}^{d_{v}\\times d_k}$ independently with variance $\\sigma^2_k = 2/(d_{v}+d_k)$ --- i.e. Glorot initialization~\\citep{glorot2010understanding}. Let $\\Mm = \\frac{1}{\\sqrt{d_k}}\\Xm^\\ell\\Wm^{Q,\\ell}{\\Wm^{K,\\ell}}^\\top{\\Xm^\\ell}^\\top$; for any $(i,j)\\in[n]\\times[n]$ we have\n\\begin{equation}\n    \\Exp[\\Mm_{i,j} \\ | \\ \\Xm] = 0,\\qquad \\Exp[\\Mm_{i,j}^2 \\ | \\ \\Xm] = \\sigma_k^4 \\cdot \\|\\Xm_{i,:}\\|^2 \\cdot \\|\\Xm_{j,:}\\|^2.\n\\end{equation}\nWhile keeping $d_v<\\infty$ fixed, taking $d_k$ to infinity yields\n\\begin{equation}\n    \\Exp[\\Mm_{i,j}^2 \\ | \\ \\Xm] = \\mathcal{O}\\left(\\frac{1}{d_k^2}\\right).\n\\end{equation}\nIn other words, $\\Mm$ converges to $\\bm{0}_{n\\times n}$ in $L^2$ as $d_k\\to\\infty$. \n\\end{lemma}",
                        "statement_html": "Consider initializing each entry of $\\wQ\\in\\mathbb{R}^{d_{v}\\times d_k}$ and $\\wK\\in\\mathbb{R}^{d_{v}\\times d_k}$ independently with variance $\\sigma^2_k = 2/(d_{v}+d_k)$ --- i.e. Glorot initialization [<a href=\"https://arxiv.org/pdf/2206.03126#cite.glorot2010understanding\">Glorot and Bengio, 2010b</a>]. Let $\\Mm = \\frac{1}{\\sqrt{d_k}}\\Xm^\\ell\\Wm^{Q,\\ell}{\\Wm^{K,\\ell}}^\\top{\\Xm^\\ell}^\\top$; for any $(i,j)\\in[n]\\times[n]$ we have\n\\begin{equation}\n    \\Exp[\\Mm_{i,j} \\ | \\ \\Xm] = 0,\\qquad \\Exp[\\Mm_{i,j}^2 \\ | \\ \\Xm] = \\sigma_k^4 \\cdot \\|\\Xm_{i,:}\\|^2 \\cdot \\|\\Xm_{j,:}\\|^2.\n\\end{equation}\nWhile keeping $d_v<\\infty$ fixed, taking $d_k$ to infinity yields\n\\begin{equation}\n    \\Exp[\\Mm_{i,j}^2 \\ | \\ \\Xm] = \\mathcal{O}\\left(\\frac{1}{d_k^2}\\right).\n\\end{equation}\nIn other words, $\\Mm$ converges to $\\mathbf{0}_{n\\times n}$ in $L^2$ as $d_k\\to\\infty$.",
                        "statement_type": "lemma",
                        "statement_motivation_html": "Understanding the behavior of the matrix $\\Mm$ under Glorot initialization is crucial for analyzing the stability and convergence of neural networks, particularly in the context of attention mechanisms. The result shows that as the dimension $d_k$ increases, the entries of $\\Mm$ tend to zero in $L^2$, which implies that the influence of the initialized weights diminishes. This can be useful for ensuring that the initial conditions do not dominate the learning process, allowing the network to learn meaningful patterns from the data.",
                        "html_url": "library/lemmas/lemma_19/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "fe7fcd33-e1d5-4f91-841a-41873d17c456",
                            "paper_id": "2a384494-8439-4b46-8c5a-7b8d6b3deea1",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\nFirst, note that\n\\begin{align*}\n    \\Mm_{i,j} = \\frac{1}{\\sqrt{d_k}}\\sum_{a,c=1}^{d_{v}}\\sum_{b=1}^{d_k} \\Xm_{i,a}\\wQ_{a,b}\\wK_{c,b} \\Xm_{j,c}.\n\\end{align*}\nSince $\\wQ$ is independent from $\\wK$ at initialization, $\\Exp[\\Mm_{i,j} \\ | \\ \\Xm] = 0$. Next, we compute\n\\begin{align*}\n    \\Exp[\\Mm_{i,j}^2] &= \\frac{1}{d_k}\\sum_{a,c,a',c'=1}^{d_{v}}\\sum_{b,b'=1}^{d_{k}} \\Xm_{i,a} \\Xm_{i,a'} \\Xm_{j,c} \\Xm_{j,c'} \\Exp\\left[\\wQ_{a,b}\\wQ_{a',b'}\\wK_{c,b} \\wK_{c',b'}\\right] \\\\\n    &=\\frac{1}{d_k}\\sum_{a,c,a',c'=1}^{d_{v}}\\sum_{b,b'=1}^{d_{k}} \\Xm_{i,a} \\Xm_{i,a'} \\Xm_{j,c} \\Xm_{j,c'} \\Exp\\left[\\wQ_{a,b}\\wQ_{a',b'}\\right]\\Exp\\left[\\wK_{c,b} \\wK_{c',b'}\\right] \\\\\n    &= \\frac{\\sigma_k^4}{d_k}\\sum_{a,c=1}^{d_{v}}\\sum_{b=1}^{d_{k}}  \\Xm_{i,a}^2 \\Xm_{j,c}^2\\\\\n    &= \\sigma_k^4 \\|\\Xm_{i,:}\\|^2 \\|\\Xm_{j,:}\\|^2.\n\\end{align*}\nThis concludes the proof.\n\\end{proof}",
                            "statement_html": "First, note that\n\\begin{align*}\n    \\Mm_{i,j} = \\frac{1}{\\sqrt{d_k}}\\sum_{a,c=1}^{d_{v}}\\sum_{b=1}^{d_k} \\Xm_{i,a}\\wQ_{a,b}\\wK_{c,b} \\Xm_{j,c}.\n\\end{align*}\nSince $\\wQ$ is independent from $\\wK$ at initialization, $\\Exp[\\Mm_{i,j} \\ | \\ \\Xm] = 0$. Next, we compute\n\\begin{align*}\n    \\Exp[\\Mm_{i,j}^2] &= \\frac{1}{d_k}\\sum_{a,c,a',c'=1}^{d_{v}}\\sum_{b,b'=1}^{d_{k}} \\Xm_{i,a} \\Xm_{i,a'} \\Xm_{j,c} \\Xm_{j,c'} \\Exp\\left[\\wQ_{a,b}\\wQ_{a',b'}\\wK_{c,b} \\wK_{c',b'}\\right] \\\\\n    &=\\frac{1}{d_k}\\sum_{a,c,a',c'=1}^{d_{v}}\\sum_{b,b'=1}^{d_{k}} \\Xm_{i,a} \\Xm_{i,a'} \\Xm_{j,c} \\Xm_{j,c'} \\Exp\\left[\\wQ_{a,b}\\wQ_{a',b'}\\right]\\Exp\\left[\\wK_{c,b} \\wK_{c',b'}\\right] \\\\\n    &= \\frac{\\sigma_k^4}{d_k}\\sum_{a,c=1}^{d_{v}}\\sum_{b=1}^{d_{k}}  \\Xm_{i,a}^2 \\Xm_{j,c}^2\\\\\n    &= \\sigma_k^4 \\|\\Xm_{i,:}\\|^2 \\|\\Xm_{j,:}\\|^2.\n\\end{align*}\nThis concludes the proof.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "Here is a short explanation of the different steps in the proof:\n<br>\n<br>1. <i></i>Initial Expression<i></i>: The proof begins by expressing \\(\\Mm_{i,j}\\) as a sum involving the elements of matrices \\(\\Xm\\), \\(\\wQ\\), and \\(\\wK\\). Specifically, it is given by:\n<br>   \\[\n   \\Mm_{i,j} = \\frac{1}{\\sqrt{d_k}}\\sum_{a,c=1}^{d_{v}}\\sum_{b=1}^{d_k} \\Xm_{i,a}\\wQ_{a,b}\\wK_{c,b} \\Xm_{j,c}.\n   \\]\n<br>\n<br>2. <i></i>Expectation of \\(\\Mm_{i,j}\\)<i></i>: Since \\(\\wQ\\) and \\(\\wK\\) are independent at initialization, the expectation of \\(\\Mm_{i,j}\\) given \\(\\Xm\\) is zero:\n<br>   \\[\n   \\Exp[\\Mm_{i,j} \\ | \\ \\Xm] = 0.\n   \\]\n<br>\n<br>3. <i></i>Computing \\(\\Exp[\\Mm_{i,j}^2]\\)<i></i>: The next step is to compute the expectation of the square of \\(\\Mm_{i,j}\\):\n<br>   \\[\n   \\Exp[\\Mm_{i,j}^2] = \\frac{1}{d_k}\\sum_{a,c,a',c'=1}^{d_{v}}\\sum_{b,b'=1}^{d_{k}} \\Xm_{i,a} \\Xm_{i,a'} \\Xm_{j,c} \\Xm_{j,c'} \\Exp\\left[\\wQ_{a,b}\\wQ_{a',b'}\\wK_{c,b} \\wK_{c',b'}\\right].\n   \\]\n<br>\n<br>4. <i></i>Independence of \\(\\wQ\\) and \\(\\wK\\)<i></i>: Using the independence of \\(\\wQ\\) and \\(\\wK\\), the expectation can be separated:\n<br>   \\[\n   \\Exp\\left[\\wQ_{a,b}\\wQ_{a',b'}\\wK_{c,b} \\wK_{c',b'}\\right] = \\Exp\\left[\\wQ_{a,b}\\wQ_{a',b'}\\right]\\Exp\\left[\\wK_{c,b} \\wK_{c',b'}\\right].\n   \\]\n<br>\n<br>5. <i></i>Simplifying the Expectation<i></i>: Given that \\(\\wQ\\) and \\(\\wK\\) are initialized with variance \\(\\sigma_k^2\\), the expectation simplifies to:\n<br>   \\[\n   \\Exp\\left[\\wQ_{a,b}\\wQ_{a',b'}\\right] = \\sigma_k^2 \\delta_{a,a'} \\delta_{b,b'}, \\quad \\Exp\\left[\\wK_{c,b} \\wK_{c',b'}\\right] = \\sigma_k^2 \\delta_{c,c'} \\delta_{b,b'}.\n   \\]\n<br>   Substituting these into the sum, we get:\n<br>   \\[\n   \\Exp[\\Mm_{i,j}^2] = \\frac{\\sigma_k^4}{d_k}\\sum_{a,c=1}^{d_{v}}\\sum_{b=1}^{d_{k}}  \\Xm_{i,a}^2 \\Xm_{j,c}^2.\n   \\]\n<br>\n<br>6. <i></i>Final Simplification<i></i>: The final expression simplifies to:\n<br>   \\[\n   \\Exp[\\Mm_{i,j}^2] = \\sigma_k^4 \\|\\Xm_{i,:}\\|^2 \\|\\Xm_{j,:}\\|^2.\n   \\]\n<br>\n<br>This concludes the proof."
                        }
                    },
                    {
                        "statement_id": "46059442-6bb5-4564-92ab-979ce9fa9ef4",
                        "paper_id": "2a384494-8439-4b46-8c5a-7b8d6b3deea1",
                        "library_nr": 20,
                        "library_name": "Lemma 20",
                        "title": "Borel-Cantelli Lemma",
                        "statement_original_tex": "\\begin{lemma}[Borel-Cantelli]\nLet $(X_i)$ be a sequence of random variables. If for any $\\epsilon>0$\n\\begin{equation*}\n    \\sum_{i=0}^\\infty \\mathbb{P}[|X_i-X|>\\epsilon]<\\infty,\n\\end{equation*}\nthen $X_i$ converges to $X$ almost surely\\footnote{That is, $\\lim_{i\\to\\infty} X_i(\\omega) = X(\\omega)$ for almost every $\\omega\\in \\Omega$~(i.e. with probability one).}.\n\\end{lemma}",
                        "statement_html": "Let $(X_i)$ be a sequence of random variables. If for any $\\epsilon>0$\n$$\n\\sum_{i=0}^\\infty \\mathbb{P}[|X_i-X|>\\epsilon]<\\infty,\n$$\nthen $X_i$ converges to $X$ almost surely (that is, $\\lim_{i\\to\\infty} X_i(\\omega) = X(\\omega)$ for almost every $\\omega\\in \\Omega$ [i.e. with probability one]).",
                        "statement_type": "lemma",
                        "statement_motivation_html": "This result is known as the Borel-Cantelli Lemma. It is particularly useful in probability theory and stochastic processes. It provides a criterion for almost sure convergence of a sequence of random variables, which is a stronger form of convergence compared to convergence in probability or in distribution. This lemma is often used in the context of proving the strong law of large numbers and in various applications where almost sure events are of interest.",
                        "html_url": "library/lemmas/lemma_20/index.html",
                        "corollary_ids": [],
                        "proof": null
                    }
                ],
                "theorems": [
                    {
                        "statement_id": "a73ae6f1-c133-41ef-bb17-f4f8e889d430",
                        "paper_id": "2a384494-8439-4b46-8c5a-7b8d6b3deea1",
                        "library_nr": 19,
                        "library_name": "Theorem 19",
                        "title": "Isserlis' theorem",
                        "statement_original_tex": "\\begin{theorem}[Isserlis]\n    Let $X_1, \\dots X_m$ be $m$ zero-mean Gaussian random variables. Then:\n    \\begin{equation}\n        \\Exp[X_1 \\cdots X_m] = \\begin{cases}\n    \\sum_{p\\in P_m^2} \\prod_{(i,j) \\in p} \\Exp[X_iX_j] & m \\text{ even} \\\\\n    0 & m \\text{ odd}\n    \\end{cases} \n    \\end{equation}\n    \n    where $P_m^2$ is the set of all the possible pairings of the indexes $1,\\dots, m$. \n    \\end{theorem}",
                        "statement_html": "Let $X_1, \\dots X_m$ be $m$ zero-mean Gaussian random variables. Then:\n\\begin{equation}\n    \\Exp[X_1 \\cdots X_m] = \\begin{cases}\n\\sum_{p\\in P_m^2} \\prod_{(i,j) \\in p} \\Exp[X_iX_j] & m \\text{ even} \\\\\n0 & m \\text{ odd}\n\\end{cases} \n\\end{equation}\n\nwhere $P_m^2$ is the set of all the possible pairings of the indexes $1,\\dots, m$.",
                        "statement_type": "theorem",
                        "statement_motivation_html": "This result is useful in the context of Gaussian random variables, particularly when dealing with moments and expectations. It provides a way to compute the expectation of the product of multiple zero-mean Gaussian random variables. For an even number of variables, it sums over all possible pairings, while for an odd number, the expectation is zero. This can be particularly helpful in fields like statistical signal processing and machine learning, where Gaussian assumptions are common.",
                        "html_url": "library/theorems/theorem_19/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "5d5878a4-ce69-4594-a270-060fa4b67641",
                            "paper_id": null,
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "",
                            "statement_html": "See [<a href=\"https://arxiv.org/pdf/2206.03126#cite.isserlis1918formula\">Isserlis, 1918</a>]",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof of Isserlis' theorem, we can break it down into several key steps:\n<br>\n<br>1. <i></i>Introduction to Isserlis' Theorem<i></i>: Isserlis' theorem, also known as Wick's theorem, provides a way to compute the expected value of the product of an even number of jointly Gaussian random variables. Specifically, it expresses this expectation as a sum of products of covariances.\n<br>\n<br>2. <i></i>Statement of the Theorem<i></i>: For jointly Gaussian random variables \\(X_1, X_2, \\ldots, X_{2n}\\), the expectation \\(E[X_1 X_2 \\cdots X_{2n}]\\) can be written as a sum over all possible pairings of the variables. Each term in the sum is the product of the expectations of the pairs.\n<br>\n<br>3. <i></i>Base Case (n=1)<i></i>: For \\(n=1\\), the theorem states that \\(E[X_1 X_2] = \\text{Cov}(X_1, X_2)\\). This is straightforward since it follows directly from the definition of covariance for jointly Gaussian variables.\n<br>\n<br>4. <i></i>Inductive Step<i></i>: Assume the theorem holds for \\(n=k\\). For \\(n=k+1\\), consider the expectation \\(E[X_1 X_2 \\cdots X_{2(k+1)}]\\). By the properties of Gaussian distributions, this can be decomposed into sums of products of expectations of pairs of variables, leveraging the linearity of expectation and the independence of the pairs.\n<br>\n<br>5. <i></i>Pairing and Summation<i></i>: The key idea is to systematically pair the variables and sum over all possible pairings. Each pairing contributes a product of covariances to the total expectation. This step involves combinatorial arguments to account for all possible ways to pair the variables.\n<br>\n<br>6. <i></i>Conclusion<i></i>: By induction, the theorem holds for all \\(n\\). Thus, the expectation of the product of an even number of jointly Gaussian random variables can be expressed as a sum of products of covariances, as stated in Isserlis' theorem.\n<br>\n<br>This structured approach helps in understanding the proof of Isserlis' theorem, which is fundamental in the study of Gaussian processes and has applications in various fields such as statistics, physics, and finance."
                        }
                    },
                    {
                        "statement_id": "0c6fd0d4-e89d-49ca-98e1-1d73c928b931",
                        "paper_id": "2a384494-8439-4b46-8c5a-7b8d6b3deea1",
                        "library_nr": 20,
                        "library_name": "Theorem 20",
                        "title": "Asymptotic Uniformity of Softmax Attention",
                        "statement_original_tex": "\\begin{theorem}[Almost-sure convergence]\n\\label{thm:soft_assumption_proof}\nConsider initializing each entry of $\\wQ\\in\\mathbb{R}^{d_v\\times d_k}$ and $\\wK\\in\\mathbb{R}^{d_v\\times d_k}$ independently with variance $\\sigma^2_k = 2/(d_v+d_k)$ --- i.e. Glorot initialization~\\citep{glorot2010understanding}. Let $d_v<\\infty$ be fixed, as $d_k\\to\\infty$ we have that, for any $\\Xm$,\n\\begin{equation*}\n    \\Am := \\soft\\left(\\frac{1}{\\sqrt{d_k}}\\Xm\\Wm^{Q}{\\Wm^{K}}^\\top{\\Xm}^\\top\\right)\\stackrel{a.s.}{\\to} \\frac{1}{n}\\bm{1}_{n\\times n}\n\\end{equation*}\nand\n\\begin{equation*}\n    \\frac{\\partial\\Am}{\\partial\\Mm} \\stackrel{a.s.}{\\to} \\frac{1}{n}\\Im_n \\otimes \\left(\\Im_n - \\frac{1}{n}\\bm{1}_{n\\times n} \\right).\n\\end{equation*}\n\\end{theorem}",
                        "statement_html": "Consider initializing each entry of $\\wQ\\in\\mathbb{R}^{d_v\\times d_k}$ and $\\wK\\in\\mathbb{R}^{d_v\\times d_k}$ independently with variance $\\sigma^2_k = 2/(d_v+d_k)$ --- i.e. Glorot initialization [<a href=\"https://arxiv.org/pdf/2206.03126#cite.glorot2010understanding\">Glorot and Bengio, 2010b</a>]. Let $d_v<\\infty$ be fixed, as $d_k\\to\\infty$ we have that, for any $\\Xm$,\n\\begin{equation*}\n    \\Am := \\soft\\left(\\frac{1}{\\sqrt{d_k}}\\Xm\\Wm^{Q}{\\Wm^{K}}^\\top{\\Xm}^\\top\\right)\\stackrel{a.s.}{\\to} \\frac{1}{n}\\mathbf{1}_{n\\times n}\n\\end{equation*}\nand\n\\begin{equation*}\n    \\frac{\\partial\\Am}{\\partial\\Mm} \\stackrel{a.s.}{\\to} \\frac{1}{n}\\Im_n \\otimes \\left(\\Im_n - \\frac{1}{n}\\mathbf{1}_{n\\times n} \\right).\n\\end{equation*}",
                        "statement_type": "theorem",
                        "statement_motivation_html": "Understanding the behavior of the attention matrix $\\Am$ as the dimension $d_k$ grows large is crucial in the context of neural network training, particularly in transformer models. This result shows that under Glorot initialization, the attention weights become uniform, which can simplify the analysis and understanding of the model's behavior. It is particularly useful when designing and initializing large-scale neural networks, as it provides insights into how the attention mechanism will behave in the limit, ensuring that the gradients remain well-behaved and preventing issues such as vanishing or exploding gradients.",
                        "html_url": "library/theorems/theorem_20/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "b8f272c2-ded0-4403-aeaa-9add160703d6",
                            "paper_id": "2a384494-8439-4b46-8c5a-7b8d6b3deea1",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\nThanks to Lemma~\\ref{app:convergence_A} and Markov Inequality, we have fast convergence in probability: for any fixed $\\Xm$,\n\\begin{equation*}\n    \\mathbb{P}[|\\Mm_{i,j}|>\\epsilon]\\le\\frac{\\Exp[\\Mm^2_{i,j}]}{\\epsilon^2} \\le \\frac{C_\\epsilon}{d_k^2}.\n\\end{equation*}\nBorel Cantelli then directly yields almost sure convergence of $\\Mm$ to $\\bm{0}_{n\\times n}$ as $d_k\\to\\infty$. Next, note that both $\\Am$ and $\\frac{\\partial\\Am}{\\partial\\Mm}$ are continuous functions of $\\Am$, hence we can apply standard continuity event-per-event. For almost every $\\omega\\in\\Omega$,\n\\begin{equation*}\n    \\lim_{d_k\\to\\infty} \\Am(\\Am(\\omega)) = \\Am\\left(\\lim_{d_k\\to\\infty}\\Am(\\omega)\\right) = \\Am( \\bm{0}_{n\\times n}) = \\frac{1}{n}\\bm{1}_{n\\times n}.\n\\end{equation*}\nHence $\\Am\\to \\frac{1}{n}\\bm{1}_{n\\times n}$ almost surely. This can also be seen as a simple application of the continuous mapping theorem. The same reasoning yields almost sure convergence of\n\\begin{equation*}\n    \\frac{\\partial \\Am}{\\partial \\Mm} = \\blockdiag\\Bigg(\\diag(\\Am_{i:}) - \\Am_{i:}\\Am_{i:}^\\top\\Bigg),\n\\end{equation*}\nto the corresponding limiting quantity.\n\\end{proof}",
                            "statement_html": "Thanks to <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_19/index.html#app%3Aconvergence_A\">Lemma 19</a> and Markov Inequality, we have fast convergence in probability: for any fixed $\\Xm$,\n\\begin{equation*}\n    \\mathbb{P}[|\\Mm_{i,j}|>\\epsilon]\\le\\frac{\\Exp[\\Mm^2_{i,j}]}{\\epsilon^2} \\le \\frac{C_\\epsilon}{d_k^2}.\n\\end{equation*}\nBorel Cantelli then directly yields almost sure convergence of $\\Mm$ to $\\mathbf{0}_{n\\times n}$ as $d_k\\to\\infty$. Next, note that both $\\Am$ and $\\frac{\\partial\\Am}{\\partial\\Mm}$ are continuous functions of $\\Am$, hence we can apply standard continuity event-per-event. For almost every $\\omega\\in\\Omega$,\n\\begin{equation*}\n    \\lim_{d_k\\to\\infty} \\Am(\\Am(\\omega)) = \\Am\\left(\\lim_{d_k\\to\\infty}\\Am(\\omega)\\right) = \\Am( \\mathbf{0}_{n\\times n}) = \\frac{1}{n}\\mathbf{1}_{n\\times n}.\n\\end{equation*}\nHence $\\Am\\to \\frac{1}{n}\\mathbf{1}_{n\\times n}$ almost surely. This can also be seen as a simple application of the continuous mapping theorem. The same reasoning yields almost sure convergence of\n\\begin{equation*}\n    \\frac{\\partial \\Am}{\\partial \\Mm} = \\blockdiag\\Bigg(\\diag(\\Am_{i:}) - \\Am_{i:}\\Am_{i:}^\\top\\Bigg),\n\\end{equation*}\nto the corresponding limiting quantity.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "<i></i>Proof Explanation:<i></i>\n<br>\n<br>1. <i></i>Application of Lemma 19 and Markov Inequality:<i></i>\n<br>   - Using Lemma 19 and the Markov Inequality, we establish fast convergence in probability. For any fixed matrix \\(\\Xm\\), we have:\n<br>   \\[\n   \\mathbb{P}[|\\Mm_{i,j}|>\\epsilon]\\le\\frac{\\Exp[\\Mm^2_{i,j}]}{\\epsilon^2} \\le \\frac{C_\\epsilon}{d_k^2}.\n   \\]\n<br>   This inequality shows that the probability of the elements of \\(\\Mm\\) being larger than \\(\\epsilon\\) decreases as \\(d_k\\) increases.\n<br>\n<br>2. <i></i>Borel-Cantelli Lemma:<i></i>\n<br>   - By applying the Borel-Cantelli Lemma, we conclude that \\(\\Mm\\) converges almost surely to the zero matrix \\(\\mathbf{0}_{n\\times n}\\) as \\(d_k \\to \\infty\\).\n<br>\n<br>3. <i></i>Continuity of \\(\\Am\\) and its Derivative:<i></i>\n<br>   - We note that both \\(\\Am\\) and its derivative \\(\\frac{\\partial\\Am}{\\partial\\Mm}\\) are continuous functions of \\(\\Am\\). This allows us to use standard continuity arguments.\n<br>\n<br>4. <i></i>Limit of \\(\\Am\\):<i></i>\n<br>   - For almost every \\(\\omega \\in \\Omega\\), we have:\n<br>   \\[\n   \\lim_{d_k\\to\\infty} \\Am(\\Am(\\omega)) = \\Am\\left(\\lim_{d_k\\to\\infty}\\Am(\\omega)\\right) = \\Am( \\mathbf{0}_{n\\times n}) = \\frac{1}{n}\\mathbf{1}_{n\\times n}.\n   \\]\n<br>   This shows that \\(\\Am\\) converges almost surely to \\(\\frac{1}{n}\\mathbf{1}_{n\\times n}\\).\n<br>\n<br>5. <i></i>Continuous Mapping Theorem:<i></i>\n<br>   - The convergence of \\(\\Am\\) can also be viewed as an application of the continuous mapping theorem.\n<br>\n<br>6. <i></i>Convergence of the Derivative:<i></i>\n<br>   - Using similar reasoning, we establish the almost sure convergence of the derivative:\n<br>   \\[\n   \\frac{\\partial \\Am}{\\partial \\Mm} = \\blockdiag\\Bigg(\\diag(\\Am_{i:}) - \\Am_{i:}\\Am_{i:}^\\top\\Bigg),\n   \\]\n<br>   to its corresponding limiting quantity."
                        }
                    }
                ],
                "corollaries": []
            },
            "mathjax_macros": [
                "emph: [\"\\\\textit{#1}\", 1]",
                "bm: [\"\\\\boldsymbol{\\\\mathbf{#1}}\", 1]",
                "mathds: [\"\\\\mathbf{#1}\", 1]",
                "textsl: [\"\\\\textit{#1}\", 1]",
                "vspace: [\"\", 1]",
                "xspace: \"\"",
                "soft: \"\\\\operatorname{softmax}\"",
                "blockdiag: \"\\\\operatorname{blockdiag}\"",
                "diag: \"\\\\operatorname{diag}\"",
                "vect: \"\\\\operatorname{vec}\"",
                "tr: \"\\\\operatorname{tr}\"",
                "rank: \"\\\\operatorname{rank}\"",
                "SA: \"\\\\operatorname{SA}\"",
                "mb: \"\\\\mathbf\"",
                "kro: \"  \\\\mathbin{\\\\mathop{\\\\otimes}}\"",
                "wQ: \"\\\\mb W^{Q}\"",
                "wQT: \"\\\\mb W^{Q \\\\top}\"",
                "wK: \"\\\\mb W^{K}\"",
                "wKT: \"\\\\mb W^{K \\\\top}\"",
                "wV: \"\\\\mb W^{V}\"",
                "wVT: \"\\\\mb W^{V \\\\top}\"",
                "p: \"\\\\bm p\"",
                "Exp: \"\\\\mathbb{E}\"",
                "R: \"\\\\mathbb{R}\"",
                "wM: [\"\\\\mb W^{#1}\", 1]",
                "wMt: [\"\\\\mb W^{#1^\\\\top}\", 1]",
                "norm: [\"\\\\left\\\\lVert#1\\\\right\\\\rVert\", 1]",
                "extra: [\"{ #1}\", 1]",
                "luca: [\"{\\\\color{red} Luca: ``#1''}\", 1]",
                "antonio: [\"{\\\\color{magenta} Antonio: ``#1''}\", 1]",
                "lorenzo: [\"{\\\\color{blue} Lorenzo: ``#1''}\", 1]",
                "sotiris: [\"{\\\\color{green} Sotiris: ``#1''}\", 1]",
                "sidak: [\"{\\\\color{orange} Sidak: ``#1''}\", 1]",
                "aurelien: [\"{\\\\color{ForestGreen} Aurelien: ``#1''}\", 1]",
                "Im: \"{\\\\bf I}\"",
                "Km: \"{\\\\bf K}\"",
                "Am: \"{\\\\bf A}\"",
                "Em: \"{\\\\boldsymbol \\\\epsilon}\"",
                "Lm: \"{\\\\bf L}\"",
                "Bm: \"{\\\\bf B}\"",
                "Cm: \"{\\\\bf C}\"",
                "Dm: \"{\\\\bf D}\"",
                "Sm: \"{\\\\bf S}\"",
                "Xm: \"{\\\\bf X}\"",
                "Ym: \"{\\\\bf Y}\"",
                "Nm: \"{\\\\bf N}\"",
                "Mm: \"{\\\\bf M}\"",
                "Tm: \"{\\\\bf T}\"",
                "Wm: \"{\\\\bf W}\"",
                "Zm: \"{\\\\bf Z}\"",
                "Sm: \"{\\\\bf S}\""
            ],
            "mathjax_environments": [
                "subequations: [\"{\", \"}\"]"
            ],
            "label2statementid": {
                "lemma:prop_kro": "ad659961-5f5f-4afa-b748-82ab4f9c289c",
                "eq:trace_kro": "ad659961-5f5f-4afa-b748-82ab4f9c289c",
                "eq:prod_kro": "ad659961-5f5f-4afa-b748-82ab4f9c289c",
                "lemma:grads_SA": "6024845f-e810-41c6-bdf9-60e2c45d03d2",
                "eq:grad_soft_complete": "6024845f-e810-41c6-bdf9-60e2c45d03d2",
                "eq:grad_soft": "6024845f-e810-41c6-bdf9-60e2c45d03d2",
                "lemma:grads_SA_X": "0c204a91-22dc-4547-93a3-10c7b9c98988",
                "eq:grad_inp": "0c204a91-22dc-4547-93a3-10c7b9c98988",
                "eq:grad-SA-X": "0c204a91-22dc-4547-93a3-10c7b9c98988",
                "lemma:gradients_queries": "b3f81e84-9174-4452-ba0a-008c842507dc",
                "eq:jacobian_values": "b3f81e84-9174-4452-ba0a-008c842507dc",
                "eq:jacobian_queries": "b3f81e84-9174-4452-ba0a-008c842507dc",
                "lemma:exp_linear": "f0ae32e1-7429-428d-b63e-c40f626219ed",
                "lemma:exp_skip": "58e31c93-5954-4c77-b560-71699e806636",
                "lemma:exp_softmax": "298ded94-86fc-4fbc-82dd-3514ada706e2",
                "lemma:propagation_of_inner_producets": "b13e0d85-82ee-4c0f-b48d-485f82298179",
                "thm:forward_pass": "824964c1-57b2-4543-b711-a1127d17581f",
                "app:convergence_A": "4ee3afa8-44f0-4b88-b879-44aa8481c46d",
                "thm:soft_assumption_proof": "0c6fd0d4-e89d-49ca-98e1-1d73c928b931"
            }
        },
        {
            "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
            "title": "Transformers as Support Vector Machines",
            "authors": [
                "Davoud Ataee Tarzanagh",
                "Yingcong Li",
                "Christos Thrampoulidis",
                "Samet Oymak"
            ],
            "year": 2023,
            "source_url": "https://arxiv.org/abs/2308.16898",
            "html_url": "library/papers/transformers_as_support_vector_machines/index.html",
            "bibtex": "@misc{tarzanagh2024transformerssupportvectormachines,\n\ttitle={Transformers as Support Vector Machines},\n\tauthor={Davoud Ataee Tarzanagh and Yingcong Li and Christos Thrampoulidis and Samet Oymak},\n\tyear={2024},\n\teprint={2308.16898},\n\tarchivePrefix={arXiv},\n\tprimaryClass={cs.LG},\n\turl={https://arxiv.org/abs/2308.16898}\n}",
            "original_tex": "==== BEGINNING OF /2308.16898/notation.tex ====\n\n\\usepackage[utf8]{inputenc} % allow utf-8 input\n\\usepackage[T1]{fontenc}    % use 8-bit T1 fonts\n\\usepackage{hyperref}       % hyperlinks\n\\usepackage{url}            % simple URL typesetting\n\\usepackage{booktabs}       % professional-quality tables\n\\usepackage{amsfonts,amsmath,amssymb}       % blackboard math symbols\n\\usepackage{nicefrac}       % compact symbols for 1/2, etc.\n\\usepackage{microtype}      % microtypography\n\\usepackage{xcolor}         % colors\n\\usepackage{txfonts}\n\\usepackage[inline]{enumitem}\n\n\n\\newcommand{\\RSIS}{RSI$+$Smooth}\n\\usepackage{graphicx}\n\\usepackage{subfigure}\n%\\usepackage{enumitem}\n\\usepackage{wrapfig,bm,comment,color}\n\\usepackage{breakurl,epsfig,epsf,fmtcount,semtrans,multirow,boldline}\n\\usepackage{tcolorbox}\n\\usepackage{xcolor}\n\\tcbuselibrary{skins}\n\\usepackage{tikz}\n\\definecolor{darkred}{RGB}{150,0,0}\n\\definecolor{darkgreen}{RGB}{0,150,0}\n\\definecolor{darkblue}{RGB}{0,0,200}\n\\hypersetup{colorlinks=true, linkcolor=darkred, citecolor=darkgreen, urlcolor=darkblue}\n\n%\\setcounter{secnumdepth}{4}\n\\newtheorem{theorem}{Theorem}%[section]\n\\newtheorem{result}{Result}\n\\newtheorem{fact}{Fact}\n\\newtheorem{problem}{Problem}\n\\newtheorem{claim}{Claim}\n\\newtheorem{assumption}{Assumption}\n\\renewcommand*{\\theassumption}{\\Alph{assumption}}\n\\newtheorem{question}{Question}\n\\newtheorem{lemma}{Lemma}\n\\newtheorem{corollary}{Corollary}\n\\newtheorem{proposition}{Proposition}\n\\newtheorem{definition}{Definition}\n\\newtheorem{condition}{Condition}\n\\newtheorem{conjecture}{Conjecture}\n\\newtheorem{remark}{Remark}\n\\newtheorem{remarks}{Remarks}\n\\newtheorem{example}{Example}\n\n%\\numberwithin{equation}{section} \n\n\\newenvironment{assbis}[1]\n  {\\renewcommand{\\theassumption}{\\ref{#1}$'$}%\n   \\addtocounter{assumption}{-1}%\n   \\begin{assumption}}\n  {\\end{assumption}}\n  \n\\def \\endprf{\\hfill {\\vrule height6pt width6pt depth0pt}\\medskip}\n\n\\newenvironment{proof}{\\noindent {\\bf Proof.} }{\\endprf\\par}\n\\newenvironment{proofsk}{\\noindent {\\bf Proof sketch.} }{\\endprf\\par}\n\n\\newcommand{\\qed}{{\\unskip\\nobreak\\hfil\\penalty50\\hskip2em\\vadjust{}\n           \\nobreak\\hfil$\\Box$\\parfillskip=0pt\\finalhyphendemerits=0\\par}}\n\n\n\\newcommand{\\goodbox}[1]{\\begin{center} \n\\begin{tcolorbox}[boxsep=1pt,left=7pt,right=7pt,top=0pt,bottom=2pt,enhanced, width=14cm,colframe=white!3!black,colback=black!2!white,colbacktitle=orange!5!yellow!10!white,\nfonttitle=\\bfseries,coltitle=black,attach boxed title to top center=\n{yshift=-0.25mm-\\tcboxedtitleheight/2,yshifttext=2mm-\\tcboxedtitleheight/2},\nboxed title style={boxrule=0.2mm,\nframe code={ \\path[tcb fill frame] ([xshift=-4mm]frame.west)\n-- (frame.north west) -- (frame.north east) -- ([xshift=4mm]frame.east)\n-- (frame.south east) -- (frame.south west) -- cycle; },\ninterior code={ \\path[tcb fill interior] ([xshift=-2mm]interior.west)\n-- (interior.north west) -- (interior.north east)\n-- ([xshift=2mm]interior.east) -- (interior.south east) -- (interior.south west)\n-- cycle;} }] #1 \\end{tcolorbox}\\end{center}\n}\n\\newcommand{\\nicebox}[1]{\\begin{center} \\begin{tcolorbox}[boxsep=1pt,left=7pt,right=7pt,top=0pt,bottom=2pt,enhanced, width=14cm,colframe=green!3!black,colback=green!3!white,colbacktitle=orange!5!yellow!10!white,\nfonttitle=\\bfseries,coltitle=black,attach boxed title to top center=\n{yshift=-0.25mm-\\tcboxedtitleheight/2,yshifttext=2mm-\\tcboxedtitleheight/2},\nboxed title style={boxrule=0.2mm,\nframe code={ \\path[tcb fill frame] ([xshift=-4mm]frame.west)\n-- (frame.north west) -- (frame.north east) -- ([xshift=4mm]frame.east)\n-- (frame.south east) -- (frame.south west) -- cycle; },\ninterior code={ \\path[tcb fill interior] ([xshift=-2mm]interior.west)\n-- (interior.north west) -- (interior.north east)\n-- ([xshift=2mm]interior.east) -- (interior.south east) -- (interior.south west)\n-- cycle;} }] #1 \\end{tcolorbox}\\end{center} }\n\n\n\\newcommand{\\LM}{\\texttt{LMM}\\xspace}\n\\newcommand{\\GM}{\\texttt{GMM}\\xspace}\n\\newcommand{\\oo}{{11}}\n\\newcommand{\\red}{\\textcolor{red}}\n\\newcommand{\\blue}{\\textcolor{blue}}\n\\newcommand{\\redp}[1]{\\textcolor{red}{[#1]}}\n\\newcommand{\\grn}{\\textcolor{darkgreen}}\n\\newcommand{\\so}[1]{\\textcolor{darkblue}{#1}}\n\\newcommand{\\SO}[1]{\\textcolor{red}{[SO: #1]\\\\}}\n\\newcommand{\\dat}[1]{\\textcolor{darkgreen}{#1}}\n\\newcommand{\\DAT}[1]{\\textcolor{green}{[DAT: #1]}}\n\\newcommand{\\yl}[1]{\\textcolor{orange}{#1}}\n\\newcommand{\\YL}[1]{\\textcolor{orange}{[YL: #1]}}\n\\newcommand{\\ct}[1]{\\textcolor{magenta}{[CT: #1]}}\n\\newcommand{\\todo}[1]{\\textcolor{darkred}{TODO: #1}}\n\\newcommand{\\clr}[1]{\\red{#1}}\n\\newcommand{\\cln}[1]{\\red{}}\n\\newcommand{\\som}[1]{\\marginpar{\\color{darkblue}\\tiny\\ttfamily SO: #1}}\n\n\n%% TT's definitions\n\\newcommand{\\xat}{\\ob}\n\\newcommand{\\rfn}{\\texttt{SVMeq}}\n\\newcommand{\\ont}{\\texttt{1token}}\n\\newcommand{\\xast}{\\xat^\\st}\n\\newcommand{\\Wf}{{\\mtx{W}^\\tsc{fin}}}\n\\newcommand\\tr{{{\\operatorname{trace}}}}\n\\newcommand{\\noi}{\\noindent}\n\\newcommand{\\new}{\\text{new}}\n\\newcommand{\\prm}{\\text{prompt}}\n\\newcommand{\\TF}{{\\texttt{TF}}}\n\\newcommand{\\outr}[1]{\\text{outer\\_update}(#1)}\n% \\newcommand{\\eps}{\\varepsilon}\n\\newcommand{\\eps}{\\epsilon}\n\\newcommand{\\epsd}{\\varepsilon_\\dm}\n\\newcommand{\\dv}{\\rho}\n\\newcommand{\\beps}{\\boldsymbol{\\eps}}\n\\newcommand{\\bota}{\\boldsymbol{\\iota}}\n\\newcommand{\\ept}{\\eps_{\\TF}}\n\\newcommand{\\cz}{c_0}\n\\newcommand{\\cc}[1]{\\Cc(#1)}%textbf{d}\n\\newcommand{\\kz}{\\nu}\n\\newcommand{\\bxi}{\\boldsymbol{\\xi}}\n\\newcommand{\\vsp}{\\vspace}\n\\newcommand{\\fnn}{f_{\\text{nn}}}\n\\newcommand{\\ff}{f_{\\text{nn}}}\n\\newcommand{\\wi}{k_\\st}\n\\newcommand{\\hbm}{\\vct{\\bar{h}}}\n\\newcommand{\\fF}[1]{f_{\\text{nn},#1}}\n\\newcommand{\\vc}{\\text{vec}}\n\\newcommand{\\flin}{f_{\\text{lin}}}\n\\newcommand{\\bhbg}{{\\bar h}_{\\tsc{gap}}}\n\\newcommand{\\fln}[1]{f_{\\text{lin},#1}}\n\\newcommand{\\rest}{\\text{rest}}\n\\newcommand{\\hp}{\\tilde{\\mtx{h}}}\n\\newcommand{\\exc}[1]{\\mathcal{R}_{\\tsc{gap}}(#1)}\n\\newcommand{\\Mb}{\\bar{\\M}}\n\\newcommand{\\Nb}{\\bar{N}}\n\\newcommand{\\epsr}{\\varepsilon_R}\n\\newcommand{\\mur}{\\mu_R}\n\\newcommand{\\bp}{\\bar{p}}\n\\newcommand{\\zig}{\\nu}\n\\newcommand{\\bh}{\\bar{h}}\n\\newcommand{\\hb}{\\vct{h}}\n\\newcommand{\\al}{\\alpha}\n\\newcommand{\\als}{\\alpha}\n\\newcommand{\\fs}{f^{\\Dc}}\n\\newcommand{\\Ncov}{\\mathcal{N}_\\eps(\\Bal)}\n\\newcommand{\\ft}{h}\n\\newcommand{\\ftv}{f^{\\Tc\\cup\\Vc}}\n\\newcommand{\\deff}{h_{\\text{eff}}}\n\\newcommand{\\defz}{\\bar{h}_{\\text{eff}}}\n\\newcommand{\\deft}{\\tilde{h}_{\\text{eff}}}\n\\newcommand{\\defg}{\\bar{h}^{\\nabla}_{\\text{eff}}}\n\\newcommand{\\defF}{\\bar{h}^{\\FB}_{\\text{eff}}}\n\\newcommand{\\bC}{\\bar{C}}\n\\newcommand{\\st}{\\star}\n\\newcommand{\\dm}{{\\diamond}}\n\\newcommand{\\nt}{n_\\mathcal{T}}\n\\newcommand{\\h}{h}\n\\newcommand{\\nv}{n_\\mathcal{V}}\n\\newcommand{\\distas}{\\overset{\\text{i.i.d.}}{\\sim}}\n\n\\newcommand{\\pleq}{\\overset{{P}}{\\leq}}\n\\newcommand{\\np}{{n\\wedge p}}\n\\newcommand{\\KB}{\\bar{K}}\n\\newcommand{\\bG}{\\boldsymbol{\\Gamma}}\n\\newcommand{\\rP}{\\stackrel{{P}}{\\longrightarrow}}\n%\\newcommand{\\xw}{\\x_{\\w}}\n\\newcommand{\\xv}{\\x_{\\vb}}\n\\newcommand{\\xa}{\\x^\\bal}\n\\newcommand{\\xw}{\\x'_{\\vb}}\n\\newcommand{\\xu}{\\x_{\\ub}}\n\\newcommand{\\xvt}[1]{\\x_{\\vb_{#1}}}\n\\newcommand{\\xvtt}[1]{\\x_{\\vb,#1}}\n\\newcommand{\\xwt}[1]{\\x_{#1,\\w}}\n\\newcommand{\\xws}[1]{\\x_{#1,\\ws}}\n\\newcommand{\\xs}{\\x_{\\ws}}\n\\newcommand{\\yst}{\\hat{\\y}}\n\\newcommand{\\yb}{\\bar{\\y}}\n\\newcommand{\\link}[1]{\\lnk(#1)}\n\\newcommand{\\lnk}{\\psi}\n\\newcommand{\\eig}{\\text{eigvec}}\n\\newcommand{\\hu}{h}\n\\newcommand{\\ru}{r}\n\\newcommand{\\pbar}{{{\\bar{p}}}}\n\\newcommand{\\pbd}{p}\n\\newcommand{\\lay}[1]{{\\vct{k}}^{(#1)}}\n\\newcommand{\\lah}[1]{{\\vct{\\hat{k}}}^{(#1)}}\n\\newcommand{\\lan}{\\vct{k}}\n\\newcommand{\\cmp}{P_{\\text{nn}}}\n%\\newcommand{\\rest}{{\\bf{r}}}\n\\newcommand{\\dcp}{\\vct{d}^{\\text{cnn}}}\n\\newcommand{\\fcp}{\\vct{g}^{\\text{CNN}}}\n\\newcommand{\\beq}{\\begin{equation}}\n\\newcommand{\\ba}{\\begin{align}}\n\\newcommand{\\ea}{\\end{align}}\n\\newcommand{\\bea}[1]{\\begin{align}#1\\end{align}}\n\\newcommand{\\nrest}{\\bar{n}}\n\\newcommand{\\eeq}{\\end{equation}}\n\\newcommand{\\prox}{{{\\text{\\bf{prox}}}}}\n\\newcommand{\\cov}{{{\\text{\\bf{cov}}}}}\n\\newcommand{\\ex}{{{\\text{\\bf{ex}}}}}\n\\newcommand{\\modu}{{{\\text{{mod}}}}}\n\\newcommand{\\map}{{{\\text{\\bf{map}}}}}\n\\newcommand{\\var}[1]{{{\\text{\\bf{var}}}}[#1]}\n\\newcommand{\\vrn}{\\sigma}\n\\newcommand{\\Rcm}{\\Rcc_m}\n\\newcommand{\\lowo}{\\texttt{low}_\\oo^{\\alpha}}\n\\newcommand{\\sgt}[1]{\\tilde{\\sigma}^{(#1)}}\n\\newcommand{\\tsig}{\\tilde{\\sigma}}\n\\newcommand{\\func}[1]{{f_{\\text{CNN}}}(#1)}\n\\newcommand{\\funh}[1]{{{\\hat{f}}_{\\text{CNN}}}(#1)}\n\\newcommand{\\funcp}[1]{{f'_{\\text{cnn}}}(#1)}\n\\newcommand{\\funcw}[1]{{{{f}}'_{\\text{cnn}}}(#1)}\n\\newcommand{\\dev}{{{\\text{\\bf{dev}}}}}\n\\newcommand{\\meas}{{{\\text{\\bf{mean}}(\\sigma)}}}\n\\newcommand{\\nn}{\\nonumber}\n\\newcommand{\\la}{\\lambda}\n\\newcommand{\\laz}{\\la_0}\n\\newcommand{\\blaz}{\\bar{\\la}_0}\n\\newcommand{\\caz}{\\la_C}\n\\newcommand{\\saz}{L_\\sigma}\n\\newcommand{\\smax}{\\bar{\\sigma}_{\\max}}\n\\newcommand{\\Lat}{\\tilde{\\Lambda}}\n\\newcommand{\\K}{\\mtx{K}}\n\\newcommand{\\A}{{\\mtx{A}}}\n\\newcommand{\\Aa}{{\\mtx{A}}}\n\\newcommand{\\Bb}{{\\mtx{B}}}\n\\newcommand{\\N}{{\\mtx{N}}}\n\\newcommand{\\amp}{\\alpha_{\\text{cnn}}}\n\\newcommand{\\btm}{\\bigotimes}\n\\newcommand{\\robt}[1]{\\bigotimes_{\\ell=1}^D{#1}_{\\ell}}\n\\newcommand{\\robtu}[1]{\\bigotimes_{\\ell=1}^D{#1}^{(\\ell)}}\n\\newcommand{\\bd}{\\bigodot}\n\\newcommand{\\kapp}{s_{\\max}^{\\nu}}\n\\newcommand{\\kal}{\\prod_{i=1}^{D-1}\\tn{\\lay{i}}}\n%\\newcommand{\\kall}[1]{\\prod_{i=1}^{#1}\\tn{\\lay{i}}}\n\\newcommand{\\kall}[1]{1_{\\lan #1}}\n\\newcommand{\\lall}[1]{{\\order{\\lip_{\\lan #1}}}}%{L^{#1}\\prod_{i=1}^{#1}\\tn{\\lay{i}}}\n\\newcommand{\\lell}[1]{{\\lip}_{\\lan #1}}\n\\newcommand{\\bell}[1]{{{{\\beta}}}_{\\lan #1}}\n\\newcommand{\\Ub}{{\\mtx{U}}}\n\\newcommand{\\Ubb}{{\\mtx{U}}}\n\\newcommand{\\kron}{\\otimes}\n\\newcommand{\\M}{{\\mtx{M}}}\n\\newcommand{\\dimondf}{\\W^\\svm}\n\\newcommand{\\name}{Att-SVM}\n\\newcommand{\\ten}{(\\{\\vb_\\ell\\}_{\\ell=1}^D)}\n\\newcommand{\\uten}{(\\{\\ub_\\ell\\}_{\\ell=1}^D)}\n\\newcommand{\\ken}{(\\{\\lay{i}\\}_{i=1}^D)}\n\\newcommand{\\B}{{{\\mtx{B}}}}\n\\newcommand{\\alx}{{{\\alpha_\\X}}}\n\\newcommand{\\aly}[1]{{{\\alpha_{\\X_{#1}}}}}\n\\newcommand{\\Ib}{{{\\mtx{I}}}}\n\\newcommand{\\fp}{F}\n\\newcommand{\\Xbin}{{{\\mtx{B}}}}\n\\newcommand{\\Gbin}{{{\\mtx{S}}}}\n\\newcommand{\\xbin}{{{\\mtx{b}}}}\n\\newcommand{\\Sb}{{{\\mtx{S}}}}\n\\newcommand{\\Sbt}{{\\mathbb{S}}^t}\n\\newcommand{\\Sbtt}{{\\mathbb{S}}'^{t}}\n\\newcommand{\\Gb}{{\\mtx{G}}}\n\\newcommand{\\ymean}{{\\bar{\\y}(\\w;\\text{avg})}}\n\\newcommand{\\yavg}{{{y}_{\\text{avg}}}}\n\\newcommand{\\ravg}[1]{{{r}_{{\\text{avg}}}(#1)}}\n\\newcommand{\\fmean}{{\\funcw{\\w;\\text{avg}}}}\n\\newcommand{\\diag}[1]{\\text{diag}(#1)}\n\\newcommand{\\out}{\\text{out}}\n\\newcommand{\\linest}{\\text{lin}}\n\\newcommand{\\inp}{\\text{in}}\n\\newcommand{\\inh}{\\hat{\\text{in}}}\n\\newcommand{\\sexp}{subexponential }\n\\newcommand{\\Lc}{{\\cal{L}}}\n\\newcommand{\\Hc}{{\\cal{H}}}\n\\newcommand{\\Lcz}{{\\cal{L}}^{0-1}}\n\\newcommand{\\Lczh}{{\\hat{\\cal{L}}}^{0-1}}\n\\newcommand{\\Lch}{{\\widehat{\\cal{L}}}}\n\\newcommand{\\Lcv}[1]{{\\cal{L}}^{#1}_{\\text{up}}}\n\\newcommand{\\Lct}{{\\tilde{\\cal{L}}}}%[1]{{\\cal{L}}^{#1}_{\\text{low}}}\n\\newcommand{\\Lcb}{{\\bar{\\cal{L}}}}\n\\newcommand{\\Nc}{{\\cal{N}}}\n\\newcommand{\\xm}{\\x^{\\tsc{mix}}}\n\\newcommand{\\Jc}{{\\cal{J}}}\n\\newcommand{\\Dc}{{\\cal{D}}}\n\\newcommand{\\Dci}{{\\cal{D}}_{\\text{init}}}\n%\\newcommand{\\smn}{\\s_{\\min}}\n\\newcommand{\\Ro}{{\\cal{RO}}}\n\\newcommand{\\PC}{{\\cal{PC}}}\n\\newcommand{\\Pb}{{\\mtx{P}}}\n\\newcommand{\\Tb}{{\\mtx{T}}}\n\\newcommand{\\Tn}{{T_0}}\n\\newcommand{\\Kba}{{\\vct{k}}_{\\text{all}}}\n\\newcommand{\\Qb}{{\\mtx{Q}}}\n\\newcommand{\\QK}{{\\mtx{Q} \\mtx{K}^\\top}}\n\\newcommand{\\rng}{\\gamma}\n\\newcommand{\\ta}{\\tau}\n\\newcommand{\\gmax}{\\gamma^\\star}\n\\newcommand{\\ggap}{\\bar{\\gamma}_{\\tsc{gap}}}\n\\newcommand{\\Cb}{{\\mtx{C}}}\n\\newcommand{\\BC}{{\\bar{C}}}\n\\newcommand{\\Eb}{{\\mtx{E}}}\n\\newcommand{\\Hb}{{\\mtx{H}}}\n\\newcommand{\\Gc}{{\\cal{G}}}\n\\newcommand{\\Zc}{{\\cal{Z}}}\n\\newcommand{\\F}{{\\mtx{F}}}\n\\newcommand{\\Fa}{{\\mtx{F}}^\\bal}\n\\newcommand{\\diff}{{\\text{diff}}}\n\\newcommand{\\La}{{\\boldsymbol{{\\Lambda}}}}\n\\newcommand{\\noresamp}[1]{{\\textcolor{red}{#1}}}\n\\newcommand{\\sigmap}{\\phi'}\n\\newcommand{\\relu}[1]{\\phi(#1)}\n\\newcommand{\\one}[1]{{\\bm{1}}(#1)}\n\\newcommand{\\sigmal}[1]{\\sigma^{(#1)}}\n\\newcommand{\\sigmalp}[1]{\\sigma'^{(#1)}}\n\\newcommand{\\sigmaa}{\\sigma'_{\\text{all}}}\n\\newcommand{\\sigmai}[1]{\\sigma'_{\\text{all},#1}}\n\\newcommand{\\bSi}{{\\boldsymbol{{\\Sigma}}}}\n\\newcommand{\\bSii}{{\\boldsymbol{{\\Sigma}}}_{i,i}}\n\\newcommand{\\bSib}{\\bar{{\\boldsymbol{{\\Sigma}}}}}\n\\newcommand{\\bSit}{{\\boldsymbol{{\\tilde{\\Sigma}}}}}\n\\newcommand{\\bSih}{{\\boldsymbol{{\\hat{\\Sigma}}}}}\n\\newcommand{\\bmu}{{\\boldsymbol{{\\mu}}}}\n\\newcommand{\\Db}{{\\mtx{D}}}\n\\newcommand{\\bB}{{\\bar{B}}}\n\\newcommand{\\tB}{{\\tilde{B}}}\n\\newcommand{\\db}{{\\vct{d}}}\n\\newcommand{\\oneb}{{\\mathbb{1}}}\n\\newcommand{\\onebb}{{\\mathbf{1}}}\n\\newcommand{\\Iden}{{\\mtx{I}}}\n%\\newcommand{\\M}{{\\mtx{M}}}\n\\newcommand{\\gm}{\\gamma_m}\n\\newcommand{\\order}[1]{{\\cal{O}}(#1)}\n\\newcommand{\\OR}{\\text{OR}}\n\\newcommand{\\ordet}[1]{{\\widetilde{\\cal{O}}}(#1)}\n\\newcommand{\\rmax}[1]{{\\bf{r}_{\\max}(#1)}}\n\\newcommand{\\rbmax}[1]{{\\bf{\\bar{r}}_{\\max}(#1)}}\n\\newcommand{\\rmin}[1]{{\\bf{r}_{\\min}(#1)}}\n\\newcommand{\\gmmin}[1]{{\\gamma_{\\min}(#1)}}\n\\newcommand{\\gmmax}[1]{{\\gamma_{\\max}(#1)}}\n%\\newcommand{\\gmean}[1]{{\\gamma_{\\textnormal{mean}}(#1)}}\n\\newcommand{\\smn}[1]{{s_{\\min}(#1)}}\n\\newcommand{\\smx}[1]{{s_{\\max}(#1)}}\n\\newcommand{\\z}{{\\vct{z}}}\n\\newcommand{\\zt}{{\\tilde{\\vct{z}}}}\n\\newcommand{\\fab}{f^\\bal_\\bt}\n\\newcommand{\\zb}{{\\bar{\\z}}}\n\\newcommand{\\el}{{\\ell}}\n\\newcommand{\\isnr}[1]{\\texttt{ISNR}(#1)}\n\\newcommand{\\sft}[1]{\\mathbb{S}(#1)}%{\\texttt{sfmx}(#1)}\n\\newcommand{\\sftk}[1]{\\mathbb{S}_k(#1)}\n\\newcommand{\\sftx}{\\mathbb{S}}\n\\newcommand{\\sfp}[1]{\\mathbb{S}'(#1)}\n\\newcommand{\\distd}[1]{\\texttt{dist}_\\dm\\left(#1\\right)}\n\\newcommand{\\tn}[1]{\\|{#1}\\|}\n\\newcommand{\\td}[1]{\\|{#1}\\|_\\dm}\n\\newcommand{\\tl}[1]{\\|{#1}\\|_{L_2}}\n\\newcommand{\\ts}[1]{\\|{#1}\\|_{\\Sc}}\n\\newcommand{\\ti}[1]{\\|{#1}\\|_{\\infty}}\n\\newcommand{\\nrm}[1]{\\|{#1}\\|}%_{#2}}\n\\newcommand{\\inr}[1]{\\left<#1\\right>}\n\\newcommand{\\tone}[1]{\\|{#1}\\|_{\\ell_1}}\n\\newcommand{\\lix}[1]{\\|{#1}\\|_{\\Xc}}\n\\newcommand{\\lif}[1]{\\text{dist}_{\\FB}({#1})}\n\\newcommand{\\lit}[1]{\\text{max}(#1)}%_{\\Ttc}\n\\newcommand{\\lia}[1]{\\text{avg}(#1)}%\n\\newcommand{\\lin}[1]{\\|{#1}\\|_{L_\\infty}}\n\\newcommand{\\tff}[1]{\\|{#1}\\|_{\\ell_4}}\n\\newcommand{\\tin}[1]{\\|{#1}\\|_{\\ell_\\infty}}\n\\newcommand{\\trow}[1]{\\|{#1}\\|_{2,\\infty}}\n\\newcommand{\\bad}{{\\bar{d}}}\n\\newcommand{\\Lcg}{\\tilde{\\Lc}}\n\\newcommand{\\Dp}{{D^+}}\n\\newcommand{\\tf}[1]{\\|{#1}\\|_{F}}\n\\newcommand{\\tnuc}[1]{\\|{#1}\\|_{\\star}}\n\\newcommand{\\te}[1]{\\|{#1}\\|_{\\psi_1}}\n\\newcommand{\\tsub}[1]{\\|{#1}\\|_{\\psi_2}}\n\\newcommand{\\tsut}[1]{\\|{#1}\\|_{\\psi_{2/3}}}\n\\newcommand{\\tsup}[1]{\\|{#1}\\|_{\\psi_a}}\n\\newcommand{\\dist}[1]{\\texttt{dist}\\left(#1\\right)}\n\\newcommand{\\upp}{{\\cal{B}}_{\\alpha,\\Gamma}}\n\\newcommand{\\upz}{{\\cal{B}}_{\\alpha_0,\\Gamma}}\n\\newcommand{\\dpz}{{\\cal{D}}_{\\alpha_0,\\Gamma}}\n\\newcommand{\\dpp}{{\\cal{D}}_{\\alpha,\\Gamma}}\n\\newcommand{\\mpp}{M_{\\alpha,\\Gamma}}\n%\\newcommand{\\supp}[1]{\\text{supp}(#1)}\n\\newcommand{\\paf}{\\partial f(\\x)}\n\\newcommand{\\Cc}{\\mathcal{C}}\n\\newcommand{\\Rcc}{\\mathcal{R}}\n\\newcommand{\\Qcc}{\\mathcal{Q}}\n\\newcommand{\\Kcc}{\\mathcal{K}}\n\\newcommand{\\Ac}{\\mathcal{A}}\n\\newcommand{\\Al}[1]{\\Ac(#1)}\n\\newcommand{\\Alg}[1]{\\Ac_g(#1)}\n\\newcommand{\\Acg}{\\Ac_g}\n\\newcommand{\\At}{\\mathcal{A}}\n%\\newcommand{\\Pc}{\\mathcal{P}}\n\\newcommand{\\Acr}{\\mathcal{A_\\text{ridge}}}\n\\newcommand{\\Bal}{{\\boldsymbol{\\Delta}}}\n\\newcommand{\\pbb}{\\vct{\\bar{p}}}\n\\newcommand{\\pbs}{\\tilde{\\pb}^\\svm}\n\\newcommand{\\del}{\\delta}\n\\newcommand{\\GG}{\\texttt{GG}}\n\\newcommand{\\BB}{\\texttt{BB}}\n\\newcommand{\\BG}{\\texttt{BG}}\n\\newcommand{\\bGam}{\\bar{\\Gamma}_y}\n\\newcommand{\\GB}{\\texttt{GB}}\n\\newcommand{\\delw}{\\delta_w}\n\\newcommand{\\delq}{\\delta_q}\n\\newcommand{\\bdel}{\\boldsymbol{\\delta}}\n\\newcommand{\\Ccb}{\\bar{\\mathcal{C}}}\n\\newcommand{\\Rc}{\\mathcal{O}}\n\\newcommand{\\Rcp}{\\mathcal{\\bar{O}}'}\n%\\newcommand{\\Rcb}{\\mathcal{N}}\n\\newcommand{\\btrue}{\\bbeta_{true}}\n\\newcommand{\\bt}{{\\boldsymbol{\\theta}}}\n\\newcommand{\\bet}{{\\boldsymbol{\\beta}}}\n\\newcommand{\\bT}{\\Theta}%{\\boldsymbol{\\Theta}}}\n\\newcommand{\\bts}{{\\boldsymbol{\\beta}_\\st}}\n\\newcommand{\\btb}{\\bar{\\boldsymbol{\\theta}}}\n\\newcommand{\\btid}{\\boldsymbol{\\theta}^\\text{ideal}}\n\\newcommand{\\btt}{\\tilde{\\boldsymbol{\\theta}}}\n\\newcommand{\\bal}{{\\boldsymbol{\\alpha}}}\n\\newcommand{\\bgam}{\\boldsymbol{\\gamma}}\n\\newcommand{\\bga}{\\boldsymbol{\\gamma}^\\bal}\n\\newcommand{\\gamb}{\\bar{\\gamma}}\n\\newcommand{\\bab}{{\\boldsymbol{\\bar{\\alpha}}}}\n\\newcommand{\\sbl}[1]{\\sigma_{\\boldsymbol{\\alpha^{(#1)}}}}\n\\newcommand{\\bl}[1]{{\\boldsymbol{\\alpha}}^{(#1)}}\n\\newcommand{\\blb}[1]{\\bar{\\boldsymbol{\\alpha}}^{(#1)}}\n\\newcommand{\\bah}{{\\widehat{\\bal}}}\n\\newcommand{\\berm}{\\bal^{\\texttt{ERM}}}\n\\newcommand{\\bas}{{\\boldsymbol{\\alpha}_\\st}}\n\\newcommand{\\bth}{{\\boldsymbol{\\hat{\\beta}}}}\n\\newcommand{\\bPhi}{{\\boldsymbol{\\Phi}}}\n\\newcommand{\\bbteta}{\\widetilde{\\boldsymbol{\\theta}}}\n\\newcommand{\\bbeta}{{\\boldsymbol{\\beta}}}\n\\newcommand{\\ddelta}{{\\boldsymbol{\\delta}}}\n\\newcommand{\\DD}{{D}}\n\\newcommand{\\babeta}{{\\bar{\\beta}}}\n\\newcommand{\\balpha}{{\\bar{\\alpha}}}\n\\newcommand{\\bgamma}{\\gamma^\\star}\n\\newcommand{\\agam}{{\\bar{\\gamma}}_t}\n\\newcommand{\\No}{N}\n\\newcommand{\\Bc}{\\mathcal{B}}\n\\newcommand{\\Sc}{\\mathcal{S}}\n\\newcommand{\\Scc}{\\bar{\\mathcal{S}}}\n\\newcommand{\\Sca}{\\mathcal{S}_{\\text{all}}}\n\\newcommand{\\Dca}{\\mathcal{D}_{\\text{all}}}\n\\newcommand{\\Scn}{\\mathcal{S}_{\\new}}\n\\newcommand{\\Dcn}{\\mathcal{D}_{new}}\n\\newcommand{\\Scb}{\\bar{\\mathcal{S}}}\n\\newcommand{\\Sci}{\\mathcal{S}_{\\text{in}}}\n\\newcommand{\\Sco}{\\mathcal{S}_{\\text{out}}}\n\\newcommand{\\Ect}{\\mathcal{S}_{\\text{top}}}\n\\newcommand{\\Mc}{\\mathcal{M}}\n\\newcommand{\\pa}{{\\partial}}\n\\newcommand{\\Nn}{\\mathcal{N}}\n\\newcommand{\\pol}{^\\circ}\n\\newcommand{\\vb}{\\vct{v}}\n\\newcommand{\\Jb}{\\mtx{J}}\n\\newcommand{\\Jt}{\\mtx{\\tilde{J}}}\n\\newcommand{\\vbl}{\\vct{v}^{\\text{lin}}}\n\\newcommand{\\fb}{\\vct{f}}\n\\newcommand{\\Fb}{\\vct{F}}\n\\newcommand{\\FB}{\\mathbb{F}}\n\\newcommand{\\Ft}{\\tilde{\\vct{F}}}\n\\newcommand{\\fa}{\\tilde{\\vct{f}}}\n\\newcommand{\\ib}{{\\bf{i}}}\n\\newcommand{\\hib}{{\\bf{\\hat{i}}}}\n\\newcommand{\\Ic}{{\\mathcal{I}}}\n\\newcommand{\\all}{{\\text{all}}}\n\\newcommand{\\vh}{\\vct{\\hat{v}}}\n\\newcommand{\\vbb}{\\vct{\\bar{v}}}\n\\newcommand{\\Xb}{\\mtx{\\bar{X}}}\n\\newcommand{\\xb}{\\vct{\\bar{x}}}\n\\newcommand{\\abb}{\\mtx{\\bar{a}}}\n\\newcommand{\\ap}{\\mtx{a}'}\n\\newcommand{\\cb}{\\mtx{c}}\n\\newcommand{\\cbb}{\\mtx{\\bar{c}}}\n\\newcommand{\\kbb}{\\mtx{\\bar{k}}}\n\\newcommand{\\bbb}{\\mtx{\\bar{b}}}\n\\newcommand{\\nei}{\\text{support index}\\xspace}\n\\newcommand{\\neis}{\\text{support indices}\\xspace}\n\\newcommand{\\Nei}{\\text{Support index}\\xspace}\n\\newcommand{\\Neis}{\\text{Support indices}\\xspace}\n\\newcommand{\\NEIS}{\\text{Support Indices}\\xspace}\n\\newcommand{\\w}{\\vct{w}}\n\\newcommand{\\ww}{\\vct{V}}\n\\newcommand{\\lgt}{\\texttt{lgt}'}\n\\newcommand{\\ist}{i_\\st}\n\\newcommand{\\cdm}{c_\\dm}\n\\newcommand{\\cop}{c_\\texttt{up}}\n\\newcommand{\\cdn}{c_\\texttt{dn}}\n\\newcommand{\\Wp}{\\mtx{W}^\\dagger}\n\\newcommand{\\tilW}{\\widetilde{\\mtx{W}}}\n\\newcommand{\\tilw}{\\widetilde{\\vct{w}}}\n\\newcommand{\\ob}{\\mtx{o}}\n\\newcommand{\\obo}{\\mtx{o}_1(t)}\n\\newcommand{\\obt}{\\mtx{o}_2(t)}\n\\newcommand{\\obh}{\\mtx{\\hat{o}}}\n\\newcommand{\\wh}{{\\hat{\\mtx{w}}}}\n\\newcommand{\\li}{\\left<}\n\\newcommand{\\xdm}{\\Xi_\\dm}\n\\newcommand{\\ri}{\\right>}\n\\newcommand{\\s}{\\vct{s}}\n\\newcommand{\\sik}{\\s^{(\\ik)}}\n\\newcommand{\\sir}{\\s^R}\n\\newcommand{\\abik}{\\ab^{(\\ik)}}\n\\newcommand{\\abr}{\\ab^R}\n\\newcommand{\\ab}{{\\vct{a}}}\n\\newcommand{\\abm}{\\vct{\\bar{a}}}%^\\svm\n\\newcommand{\\abg}{\\vct{a}_{\\tsc{gap}}}\n\\newcommand{\\bgag}{{\\gamma}_{\\tsc{gap}}}\n\\newcommand{\\bgg}{\\gamma^{\\tsc{gap}}}\n\\newcommand{\\bggm}{\\gamma^{\\tsc{gap}}_{\\min}}\n\\newcommand{\\bgm}{\\bar{\\gamma}^{\\tsc{gap}}}\n\\newcommand{\\abp}{\\vct{a}^\\pb}\n\\newcommand{\\bb}{\\vct{b}}\n\\newcommand{\\ub}{{\\vct{u}}}\n\\newcommand{\\ubb}{\\bar{\\vct{u}}}\n%\\newcommand{\\h}{\\vct{h}}\n\\newcommand{\\hh}{{\\vct{h}}}\n%\\newcommand{\\g}{{\\vct{g}}}\n\\newcommand{\\ii}{{\\vct{i}}}\n\\newcommand{\\zm}[1]{{\\texttt{{{zm}}}[#1]}}\n\\newcommand{\\dd}{{\\vct{d}}}\n\\newcommand{\\ddt}{{\\vct{d}}_\\tau}\n\\newcommand{\\Zb}{\\mathbb{Z}}\n%\\newcommand{\\nst}{\\text{negative support token}\\xspace}\n\\newcommand{\\hf}{\\hat{f}}\n\\newcommand{\\corrCA}{\\rho_\\Cc(\\M)}\n\\newcommand{\\corr}[1]{{\\texttt{corr\\_coef}}(#1)}\n\\newcommand{\\Tc}{\\mathcal{T}}\n\\newcommand{\\Tcb}{\\bar{\\mathcal{T}}}\n\\newcommand{\\TVc}{\\Tc\\cup\\Vc}\n\\newcommand{\\Ttc}{\\mathcal{T}_{\\text{test}}}\n\\newcommand{\\Fc}{\\mathcal{F}}\n\\newcommand{\\Fcl}{\\mathcal{F}^{\\text{lin}}}\n\\newcommand{\\Xc}{\\mathcal{X}}\n\\newcommand{\\Yc}{\\mathcal{Y}}\n\\newcommand{\\rel}{optimal\\xspace}\n\\newcommand{\\iopt}{non-optimal\\xspace}\n\\newcommand{\\irel}{non-optimal\\xspace}\n\n\n%%YP's macros\n\\newcommand{\\qqq}[1]{{\\textcolor{red}{?{#1}?}}}\n\\newcommand{\\ihere}{{\\textcolor{red}{I AM HERE! }}}\n\\newcommand{\\mat}[1]{{\\text{mat}{#1}}}\n\\newcommand{\\gb}{\\bar{\\g}}\n\\newcommand{\\bs}{\\bar{s}}\n\\newcommand{\\cgain}{\\alpha_{\\text{CNN}}}\n\\newcommand{\\cgainp}{\\tn{\\E_{\\x\\sim\\Nn(0,\\Iden)}[\\gcnn{}]}}\n\\newcommand{\\gcnn}[1]{{\\vct{g}}_{\\text{cnn}#1}}\n\\newcommand{\\ccorr}{\\rho_{\\text{cnn}}}\n\\newcommand{\\kb}{\\vct{k}}\n\\newcommand{\\kbo}{\\vct{k}^\\op}\n\\newcommand{\\xbo}{\\vct{x}^\\op}\n\\newcommand{\\xh}{\\hat{\\x}}\n% \\newcommand{\\cone}{\\texttt{cone}}\n\\newcommand{\\cone}{\\Sc}\n\\newcommand{\\conb}{\\bar{\\Cc}}\n\\newcommand{\\con}[1]{\\texttt{cone}_{\\eps}(#1)}\n\n%\\newcommand{\\yh}{\\hat{\\y}}\n\\newcommand{\\xbr}{\\bar{\\h}}\n%\\newcommand{\\low}[1]{\\texttt{low}_{\\alpha{#1}}}\n%\\newcommand{\\high}[1]{\\texttt{high}_{\\alpha{#1}}}\n\\newcommand{\\low}{\\texttt{low}^{\\alpha}(\\X)}\n\\newcommand{\\high}{\\texttt{high}^{\\alpha}(\\X)}\n\\newcommand{\\lowi}{\\texttt{low}_\\ik^{\\alpha}}\n\\newcommand{\\higi}{\\texttt{high}_\\ik^{\\alpha}}\n\\newcommand{\\xdr}{\\tilde{\\x}}\n\\newcommand{\\xp}[1]{\\x^{(#1)}_\\prm}\n\\newcommand{\\Sn}[1]{\\Sc^{(#1)}}\n\\newcommand{\\Dn}[1]{\\Dc^{(#1)}}\n%\\newcommand{\\m}{\\vct{m}}\n\\newcommand{\\xt}[1]{\\x^{(#1)}}\n\\newcommand{\\yt}[1]{y^{(#1)}}\n\\newcommand{\\ybt}{\\tilde{\\y}}\n\\newcommand{\\Xt}{\\tilde{\\X}}\n\\newcommand{\\gh}{\\hat{\\g}}\n\\newcommand{\\gt}{\\tilde{\\g}}\n\\newcommand{\\ir}{q}\n\\newcommand{\\sbs}{\\vct{s}^{\\texttt{ref}}}\n\\newcommand{\\bbg}{\\bgam^{\\tsc{gap}}}\n\\newcommand{\\bbs}{\\bar{\\s}}\n\\newcommand{\\gmb}{\\bar{\\gamma}}\n\\newcommand{\\irm}{q^\\pb_{\\max}}\n\\newcommand{\\ira}{q^{\\pb'}_{\\max}}\n\\newcommand{\\vbs}{\\tilde{\\vb}^\\svm}\n\\newcommand{\\vs}{\\vb^\\svm}\n\\newcommand{\\ps}{\\W^\\svm}\n\\newcommand{\\Ws}{\\W^\\svm}\n\\newcommand{\\Wma}{\\W^\\svm_\\bal}\n\\newcommand{\\Wsf}{\\W^\\svm}\n\\newcommand{\\Wsb}{\\bar{\\W}^\\svm}\n\\newcommand{\\Wcs}{\\Wc^\\svm}\n\\newcommand{\\ik}{{ik}}\n\\newcommand{\\itt}{{it}}\n\\newcommand{\\ittt}{{i\\tau}}\n\\newcommand{\\ikt}{{ikt}}\n\\newcommand{\\iktt}{{ik\\tau}}\n\\newcommand{\\ikix}{_{ik=(1,1)}^{(n,k)}}\n\\newcommand{\\inn}[1]{\\left<#1\\right>}\n\\newcommand{\\Ccd}{\\Cc_{\\eps,R_0}^\\dm}\n\\newcommand{\\aik}{\\alpha_{ik}}\n\\newcommand{\\prr}{\\pb^\\tsc{relax}}\n\\newcommand{\\pre}{\\pb^\\tsc{$\\eps$-rlx}}\n\\newcommand{\\pbr}{\\tilde{\\pb}^\\tsc{$\\eps$-rlx}}\n\\newcommand{\\qbr}{\\tilde{\\qb}^\\tsc{relax}}\n%\\newcommand{\\vbr}{\\tilde{\\vb}^\\tsc{relax}}\n\\newcommand{\\pseb}{\\pbb^\\svm_\\eps}\n\\newcommand{\\pset}{\\tilde{\\pb}^\\svm_\\eps}\n\\newcommand{\\psdb}{\\pbb^\\svm_\\delta}\n\\newcommand{\\pse}{\\pb^\\svm_\\eps}\n\\newcommand{\\psd}{\\pb^\\svm_\\delta}\n\\newcommand{\\pst}{\\pb^\\star}\n\\newcommand{\\wst}{\\W^\\star}\n\\newcommand{\\gamp}{\\Gamma_\\eps}\n\\newcommand{\\gamt}{\\Gamma^{\\geq 2}_\\eps}\n\\newcommand{\\damp}{\\Gamma_\\delta}\n%\\newcommand{\\damt}{\\Gamma_\\delta}\n\\newcommand{\\pt}{\\tilde{\\pb}^\\svm}\n\\newcommand{\\RR}{\\bar{R}}\n\\newcommand{\\MM}{\\bar{M}}\n\\newcommand{\\psb}{\\bar{\\pb}^\\svm}\n\\newcommand{\\psp}{\\pb^\\beta}\n\\newcommand{\\pso}{\\pb^{\\svm\\star}}\n\\newcommand{\\Wso}{\\W^{\\svm\\star}}\n%\\newcommand{\\lip}{L}\n\\newcommand{\\mipp}{L_{\\max}}\n\\newcommand{\\mupp}{\\bar{\\mu}_{\\max}}\n\\newcommand{\\mapp}{\\mu}\n\\newcommand{\\tcnn}{{\\mtx{T}}_{\\text{cnn}}}\n\\newcommand{\\lcnn}{{\\mtx{L}}_{\\text{CNN}}}\n\\newcommand{\\smo}{S}\n\\newcommand{\\SM}{{{\\bf{S}}}_{L,\\kb}}\n\\newcommand{\\SMB}{{{\\bf{\\bar{S}}}}_{L,\\kb}}\n\\newcommand{\\ws}{{\\W^\\star}}\n\\newcommand{\\wss}{{\\w^\\star}}\n\\newcommand{\\yp}[1]{\\textcolor{red}{ #1}}\n\\newcommand{\\zeronorm}[1]{\\left\\|#1 \\right\\|_0}\n\\newcommand{\\unorm}[1]{\\left\\|#1 \\right\\|_u}\n\\newcommand{\\ynorm}[1]{\\left\\|#1 \\right\\|_{\\bar{y}}}\n\\newcommand{\\onetwonorm}[1]{\\left\\|#1\\right\\|_{1,2}}\n\\newcommand{\\opnorm}[1]{\\left\\|#1\\right\\|}\n\\newcommand{\\fronorm}[1]{\\left\\|#1\\right\\|_{F}}\n\\newcommand{\\onenorm}[1]{\\left\\|#1\\right\\|_{\\ell_1}}\n\\newcommand{\\twonorm}[1]{\\left\\|#1\\right\\|_{\\ell_2}}\n\\newcommand{\\wnorm}[2]{\\left\\|#1\\right\\|_{#2}}\n\\newcommand{\\Dnorm}[1]{\\left\\|#1\\right\\|_{D}}\n\\newcommand{\\oneinfnorm}[1]{\\left\\|#1\\right\\|_{1,\\infty}}\n\\newcommand{\\infnorm}[1]{\\left\\|#1\\right\\|_{\\ell_\\infty}}\n\\newcommand{\\nucnorm}[1]{\\left\\|#1\\right\\|_*}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\lab}{\\bar{\\la}}\n\\newcommand{\\avg}[1]{\\left< #1 \\right>}\n\n\n\\newcommand{\\mult}{B^D\\bar{M}N}\n\\newcommand{\\liptwo}{20R^3B\\nt^2\\laz^{-2}(B\\nt+1)}\n\\newcommand{\\lipp}{5R^2\\sqrt{B^3\\nt^3\\h}\\laz^{-2}\\tn{\\yT}}\n\\newcommand{\\lips}{6R^3B^2\\sqrt{\\nt^3\\h}\\laz^{-2}\\tn{\\yT}}\n\\newcommand{\\lipl}{5(RB\\nt\\sqrt{\\h}+1)RB\\nt\\laz^{-2}}\n\\newcommand{\\lip}{\\frac{5B^2\\tn{\\yT}}{\\laz^2}}\n\\newcommand{\\lipt}{6R^3B^2\\Gamma\\sqrt{\\nt^3\\h}\\laz^{-2} \\tn{\\yT}}\n\\newcommand{\\lipf}{{20R^4B^2\\laz^{-2}\\Gamma\\nt^2\\tn{\\yT}}}\n%\\newcommand{\\lipv}{{30R^4B^4\\laz^{-2}\\Gamma\\sqrt{\\nt^3\\nv^2d} \\tn{\\y}}}\n\\newcommand{\\lipsum}{30R^4B^2\\laz^{-2}\\Gamma(\\nt^2+\\nv^2) \\tn{\\yT}}\n%\\newcommand{\\lipnn}{120c_0^4B^4\\laz^{-2}\\Gamma(\\nt^2+\\nv^2) \\tn{\\yT}}\n\\newcommand{\\lipnn}{120B^4\\blaz^{-2}\\Gamma(\\nt^2+\\nv^2) \\tn{\\yT}}\n\\newcommand{\\bL}{\\bar{L}}\n\\newcommand{\\scl}{M}\n\\renewcommand{\\d}{\\mathrm{d}}\n\n\\newcommand{\\cA}{\\mathcal{A}}\n\\newcommand{\\x}{\\vct{x}}\n\\newcommand{\\xx}[1]{\\vct{x}^{(#1)}}\n\\newcommand{\\rb}{\\vct{r}}\n\\newcommand{\\rbb}{\\vct{\\widetilde{r}}}\n\\newcommand{\\y}{\\vct{y}}\n\\newcommand{\\yT}{\\vct{y}}\n\\newcommand{\\yh}{\\hat{y}}\n\\newcommand{\\ybh}{\\vct{\\hat{y}}}\n\\newcommand{\\W}{\\mtx{W}}\n\\newcommand{\\Ww}[1]{\\mtx{W}^{(#1)}}\n\\newcommand{\\Wt}{\\tilde{\\mtx{W}}}\n\\newcommand{\\Wc}{{\\cal{W}}}\n\\newcommand{\\Wcb}{{\\cal{W}}}\n\\newcommand{\\Vc}{{\\cal{V}}}\n\\newcommand{\\bgl}{{~\\big |~}}\n\n%--------------\n\n% EJC's macros\n\n\\definecolor{emmanuel}{RGB}{255,127,0}\n\\newcommand{\\ejc}[1]{\\textcolor{emmanuel}{EJC: #1}}\n\n\\newcommand{\\p}{{\\vct{p}}}\n\\newcommand{\\Kb}{{\\mtx{K}}}\n\\newcommand{\\Kbb}{{\\mtx{\\bar{K}}}}\n\\newcommand{\\Qbb}{{\\mtx{\\bar{Q}}}}\n\\newcommand{\\Wb}{{\\mtx{\\bar{W}}}}\n\\newcommand{\\Wpro}{\\mtx{\\W}_{\\textnormal{prod}}}\n\\newcommand{\\Kbh}{{\\widehat{\\mtx{K}}}}\n\\newcommand{\\somelog}{16\\Kb\\log (\\Kb)}\n\\newcommand{\\somelg}{\\Kb\\log (\\Kb)}\n\\newcommand{\\pb}{{\\vct{p}}}\n\\newcommand{\\pp}[1]{{\\vct{p}}(#1)}\n\\newcommand{\\pr}[1]{{\\vct{\\bar{p}}}(#1)}\n\\newcommand{\\prl}[1]{{\\vct{\\tilde{p}}}(#1)}%^{\\tsc{loc}}\n\\newcommand{\\prb}[1]{{\\vct{\\bar{p}}}(#1)}\n\\newcommand{\\wrb}[1]{{\\vct{\\bar{W}}}(#1)}\n\\newcommand{\\wrt}[1]{{\\vct{\\bar{W}}}_0(#1)}\n\\newcommand{\\dpb}{{\\vct{\\dot p}}}\n\\newcommand{\\drb}{{\\vct{\\dot r}}}\n\\newcommand{\\qb}{{\\vct{q}}}\n\\newcommand{\\qstar}{\\vct{q}_\\star}%{{\\vct{q}}_\\star}\n\\newcommand{\\qtt}{{\\vct{\\tilde{q}}}}\n\\newcommand{\\wstar}{\\vct{v}_\\star}%{{\\vct{w}}_\\star}\n\\newcommand{\\vstar}{\\vct{v}_\\star}\n\\newcommand{\\wstab}{\\bar{\\vb}_\\star}\n\\newcommand{\\vstab}{\\bar{\\vb}_\\star}\n\\newcommand{\\qstab}{\\bar{\\qb}_\\star}\n\\newcommand{\\qbb}{\\bar{\\qb}}\n\\newcommand{\\wbb}{\\bar{\\w}}\n\\newcommand{\\wtt}{{\\vct{\\tilde{w}}}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\Pro}{\\mathbb{P}}\n\\newcommand{\\C}{\\mathbb{C}}\n\\newcommand{\\Z}{\\mtx{Z}}\n\\newcommand{\\V}{\\mtx{V}}\n\\newcommand{\\Za}{\\mtx{Z}^\\bal}\n\\newcommand{\\<}{\\langle}\n\\renewcommand{\\>}{\\rangle}\n\\newcommand{\\Var}{\\textrm{Var}}\n\\newcommand{\\sgn}[1]{\\textrm{sgn}(#1)}\n\\renewcommand{\\P}{\\operatorname{\\mathbb{P}}}\n\\newcommand{\\E}{\\operatorname{\\mathbb{E}}}\n\\newcommand{\\Eh}{\\operatorname{\\mathbb{\\hat{E}}}}\n%\\newcommand{\\var}{\\operatorname{\\mathbf{var}}}\n\n%\\newcommand{\\todo}[1]{{\\bf TODO: #1}}\n\\newcommand{\\grad}[1]{{\\nabla\\Lc(#1)}}\n\\newcommand{\\gradf}[1]{{\\nabla f(#1)}}\n\\newcommand{\\hessf}[1]{{\\nabla^2f(#1)}}\n\\newcommand{\\gradw}[1]{{\\nabla{\\Lc}(#1)}}\n\\newcommand{\\grd}[2]{{\\nabla\\Lc_{#1}(#2)}}\n\\newcommand{\\e}{\\mathrm{e}}\n\\newcommand{\\eb}{\\vct{e}}\n\\renewcommand{\\i}{\\imath}\n\n\\newcommand{\\vct}[1]{\\bm{#1}}%{{\\bf{\\emph{#1}}}}\n\\newcommand{\\mtx}[1]{\\bm{#1}}\n\\newcommand{\\vba}{{\\bf{\\emph{v}}}}\n\\newcommand{\\pba}{{\\bf{\\emph{p}}}}\n\n% MS's macros\n\n\\newcommand{\\rank}[1]{\\texttt{rank}(#1)}\n\\newcommand{\\supp}{\\Sc}%[1]{\\operatorname{supp}(#1)}\n\\newcommand{\\restrict}[1]{\\big\\vert_{#1}}\n\\newcommand{\\Id}{\\text{\\em I}}\n\\newcommand{\\OpId}{\\mathcal{I}}\n\n\\newcommand{\\Real}{\\operatorname{Re}}\n\\newcommand{\\Imag}{\\operatorname{Im}}\n\n\\newcommand{\\piyp}{\\pi'_1(\\vct{y})}\n\\newcommand{\\piy}{\\pi_1(\\vct{y})}\n\\newcommand{\\piar}{\\pi_1(\\vct{a}_r)}\n\\newcommand{\\piarp}{\\pi'_1(\\vct{a}_r)}\n\\newcommand{\\set}{{\\cal{F}}}\n\\newcommand{\\des}{{\\x_0}}\n\n\\newcommand{\\Pc}{{\\cal{P}}}\n\\newcommand{\\X}{{\\mtx{X}}}\n\\newcommand{\\Y}{{\\mtx{Y}}}\n\\newcommand{\\Vb}{{\\mtx{V}}}\n\\newcommand{\\Vh}{\\hat{{\\mtx{V}}}}\n\\newcommand{\\Vbd}{{\\mtx{V}^\\dagger}}\n\\newcommand{\\Rb}{{\\mtx{R}}}\n\\newcommand{\\bR}{{\\bar{R}}}\n\n\\newcommand{\\calF}{\\mathcal{I}}\n\\newcommand{\\calS}{\\mathcal{N}}\n\n\n\\newcommand{\\note}[1]{{\\bf [{\\em Note:} #1]}}\n\n\\newcommand{\\iprod}[2]{\\left\\langle #1 , #2 \\right\\rangle}\n\\newcommand{\\gi}{\\ab_{\\gamma_i}}\n\\newcommand{\\ang}{\\text{ang}}\n\\newcommand{\\ham}[2]{{\\|#1,#2\\|_H}}\n\n%%%DA\n\\newcommand{\\mc}{\\mathcal}\n\\newcommand{\\m}[1]{{\\bf{#1}}}\n\\renewcommand{\\mc}[1]{\\ensuremath{\\mathcal{#1}}} \n\\newcommand{\\g}{\\vct{g}}\n\\newcommand{\\mb}[1]{{\\mathbb{#1}}}\n\n\\renewcommand{\\qed}{\\hfill\\blacksquare}\n\n%\\renewcommand{\\qedsymbol}{$\\blacksquare$}\n%\\renewcommand{\\qedsymbol}{\\ensuremath{\\blacksquare}}\n\n==== END OF /2308.16898/notation.tex ====\n==== BEGINNING OF /2308.16898/self_attn.tex ====\n\\documentclass{article} % DO NOT CHANGE THIS\n\\usepackage[margin=1in]{geometry}\n\n\n% if you need to pass options to natbib, use, e.g.:\n%     \\PassOptionsToPackage{numbers, compress}{natbib}\n% before loading neurips_2023\n\n\n% ready for submission\n%\\usepackage[preprint]{neurips_2023}\n\n\n% to compile a preprint version, e.g., for submission to arXiv, add add the\n% [preprint] option:\n%     \\usepackage[preprint]{neurips_2023}\n\n\n% to compile a camera-ready version, add the [final] option, e.g.:\n%     \\usepackage[final]{neurips_2023}\n\n\n% to avoid loading the natbib package, add option nonatbib:\n%    \\usepackage[nonatbib]{neurips_2023}\n\n\n%\\usepackage{fullpage}\n%\\usepackage{neurips_2023}\n\\input{notation}\n\\usepackage{xspace}\n\\usepackage{pifont}\n\\newcommand{\\tsc}{\\textsl}\n\\newcommand{\\Prml}{\\text{Primal}\\xspace}\n\\newcommand{\\cls}{\\texttt{[CLS]}\\xspace}\n\\newcommand{\\prml}{\\text{primal}\\xspace}\n\\newcommand{\\fat}{f_{\\tsc{sa}}}\n\\newcommand{\\fapt}[1]{f_{\\tsc{cls}}(#1\\bT)}\n\\newcommand{\\fatt}{f_{\\tsc{cls}}}\n\\newcommand{\\faptt}{f^\\top_{\\tsc{cls}}}\n\\newcommand{\\fapn}[1]{f_{\\lnk}(#1\\bT)}\n\\newcommand{\\thetab}{\\boldsymbol{\\theta}}\n\\newcommand{\\acc}{\\text{acc}}\n\\newcommand{\\svm}{\\tsc{mm}}\n\\newcommand{\\Wm}{\\W^\\svm}\n\\newcommand{\\Wmt}{\\tilde\\W^\\svm}\n\\newcommand{\\Wu}{\\W^{\\texttt{uni}}}\n\\newcommand{\\Wbi}{\\W^{\\texttt{bi}}}\n\\newcommand{\\Wub}{\\bar{\\W}^{\\texttt{uni}}}\n\\newcommand{\\Wbb}{\\bar{\\W}^{\\texttt{bi}}}\n\\newcommand{\\op}{\\texttt{opt}}\n\\newcommand{\\opt}{\\texttt{opt}}\n\\newcommand{\\xop}{\\x^\\texttt{opt}}\n\\newcommand{\\reg}{\\tsc{reg}}\n\\newcommand{\\vstap}{\\vstar'}\n\\newcommand{\\Cbp}{\\Cb'}\n\\newcommand{\\Rcb}{\\bar{\\Rc}}\n\\newcommand{\\err}{\\texttt{err}}\n\\newcommand{\\ones}{\\onebb}\n%\\newcommand{\\Lcp}{\\Lc^{\\tsc{pt}}}\n\\newcommand{\\onet}{\\bar{\\onebb}}\n\\newcommand{\\taub}{{\\bar{\\tau}}}\n\\newcommand{\\taut}{\\tau}\n\\newcommand{\\Qc}{{\\cal{Q}}}\n\\newcommand{\\zX}[1]{\\z\\{#1\\}}\n\\newcommand{\\aX}[1]{\\ab\\{#1\\}}\n\\usepackage[utf8]{inputenc} % allow utf-8 input\n\\usepackage[T1]{fontenc}    % use 8-bit T1 fonts\n\\usepackage{hyperref}       % hyperlinks\n\\usepackage{url}            % simple URL typesetting\n\\usepackage{booktabs}       % professional-quality tables\n\\usepackage{amsfonts}       % blackboard math symbols\n\\usepackage{nicefrac}       % compact symbols for 1/2, etc.\n\\usepackage{microtype}      % microtypography\n\\usepackage{xcolor}         % colors\n\\usepackage{enumitem}\n\\usepackage{tikz}\n% Make the \"Part I\" text invisible\n\\renewcommand \\thepart{}\n\\renewcommand \\partname{}\n\\usepackage[toc,page,header]{appendix}\n\\usepackage{minitoc}\n\n\n%\\title{Transformers are SVMs: Inductive Bias and\\\\Global Convergence of Self-Attention}\n\\title{%Transformers are Support Vector Machines that Separate Tokens\\\\\nTransformers as Support Vector Machines}\n%\\title{\\Large{Inductive Bias and Global Convergence of Self-Attention on Separable Tokens}}\n%via Margin Maximization\n% The \\author macro works with any number of authors. There are two commands\n% used to separate the names and addresses of multiple authors: \\And and \\AND.\n%\n% Using \\And between authors leaves it to LaTeX to determine where to break the\n% lines. Using \\AND forces a line break at that point. So, if LaTeX puts 3 of 4\n% authors names on the first line, and the last on the second line, try using\n% \\AND instead of \\And before the third author name.\n%\\author{Davoud A. Tarzanagh $^\\pi$\\quad\\quad Yingcong Li $^\\rho$\\quad\\quad Xuechen Zhang $^\\rho$\\quad\\quad Samet Oymak $^{\\mu\\rho}$\\\\\\\\\n%$\\pi$ University of Pennsylvania~~~~~~~~~~~~~~\\\\\n%$\\rho$ University of California, Riverside~~\\\\\n%$\\mu$ University of Michigan, Ann Arbor}\n\n%\\author{Samet Oymak\\\\University of Michigan\\\\UC Riverside\\\\\\texttt{oymak@umich.edu}}\n\n%\\author{{Davoud Ataee Tarzanagh}\\thanks{University of Michigan and University of Pennsylvania, Email: \\texttt{tarzanaq@upenn.edu}}\n%\\and{ Yingcong Li, Xuechen Zhang }\\thanks{University of California, Riverside, Email: \\texttt{yli692,xzhan394\\}@ucr.edu} \n%}\n%\\and{{Samet Oymak}\\thanks{University of Michigan and University of California, Riverside, Email: \\texttt{oymak@umich.edu}}\n%}\n%}\n\n\\date{}\n\\begin{document}\n\n\\author{\\\\Davoud Ataee Tarzanagh$^{1\\star}$\\quad Yingcong Li$^{2\\star}$\\qquad Christos Thrampoulidis$^{3}$\\qquad Samet Oymak$^{4\\dagger}$}\n\n%\\\\\\\\ \\normalsize\\qquad{University of Pennsylvania\\quad\\quad University of Michigan\\quad University of British Columbia\\quad University of Michigan}\n%################################################\n\\addtocontents{toc}{\\protect\\setcounter{tocdepth}{0}}\n%################################################\n\\maketitle\n\n%\\author{\\qquad\\qquad Davoud Ataee Tarzanagh$^{\\star}$\\qquad Yingcong Li$^{\\star}$\\qquad Christos Thrampoulidis\\qquad Samet Oymak}\n{\\let\\thefootnote\\relax\\footnotetext{$^1$ University of Pennsylvania, \\texttt{tarzanaq@upenn.edu}. $^2$ University of California, Riverside, \\texttt{yli692@ucr.edu}. $^3$ University of British Columbia, \\texttt{cthrampo@ece.ubc.ca}. $^4$ University of Michigan, \\texttt{oymak@umich.edu}. $^\\star$ Equal contribution. $^\\dagger$ Corresponding author.}}\n\n%%%% ARXIV ABSTRACT %%%%\n%Since its inception in \"Attention Is All You Need\", transformer architecture has led to revolutionary advancements in NLP. The attention layer within the transformer admits a sequence of input tokens X and makes them interact through pairwise similarities computed as softmax$(XQK^\\top X^\\top)$, where $(K,Q)$ are the trainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism allows us to characterize the implicit bias of 1-layer transformers optimized with gradient descent: (1) Optimizing the attention layer with vanishing regularization, parameterized by $(K,Q)$, converges in direction to an SVM solution minimizing the nuclear norm of the combined parameter $W=KQ^\\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius norm objective. We characterize this convergence, highlighting that it can occur toward locally-optimal directions rather than global ones. (2) Complementing this, we prove the local/global directional convergence of gradient descent under suitable geometric conditions. Importantly, we show that over-parameterization catalyzes global convergence by ensuring the feasibility of the SVM problem and by guaranteeing a benign optimization landscape devoid of stationary points. (3) While our theory applies primarily to binary classification with linear prediction heads, we propose a more general SVM equivalence that predicts the implicit bias of nonlinear or multiclass heads. We validate our theory via comprehensive numerical experiments. We also introduce several open problems and research directions. We believe these findings inspire the interpretation of transformers as a hierarchy of SVMs that separates and selects optimal tokens.\n\\begin{abstract} \nSince its inception in ``Attention Is All You Need'', the transformer architecture has led to revolutionary advancements in natural language processing. The attention layer within the transformer admits a sequence of input tokens $\\X$ and makes them interact through pairwise similarities computed as $\\texttt{softmax}(\\X\\Qb\\Kb^\\top\\X^\\top)$, where $(\\Kb,\\Qb)$ are the trainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism allows us to characterize the implicit bias of 1-layer transformers optimized with gradient descent, as follows. \\textbf{(1)} Optimizing the attention layer, parameterized by $(\\Kb,\\Qb)$, with vanishing regularization, converges in direction to an SVM solution minimizing the nuclear norm of the combined parameter $\\W:=\\Kb\\Qb^\\top$. Instead, directly parameterizing by $\\W$ minimizes a Frobenius norm SVM objective. \nWe  characterize this convergence, highlighting that it can occur in locally-optimal directions rather than global ones.\n\\textbf{(2)} Complementing this, for $\\W$-parameterization, we prove the local/global directional convergence of gradient descent under suitable geometric conditions. Importantly, we show that over-parameterization catalyzes global convergence by ensuring the feasibility of the SVM problem and by guaranteeing a benign optimization landscape devoid of stationary points. \\textbf{(3)} While our theory  applies primarily to linear prediction heads, we propose a more general SVM equivalence that  predicts the implicit bias of 1-layer transformers with nonlinear heads/MLPs. \nOur findings apply to general datasets, trivially extend to cross-attention layer, and their practical validity is verified via thorough numerical experiments. We also introduce open problems and future research directions. We believe these findings inspire a new perspective, interpreting multilayer transformers as a hierarchy of SVMs that separates and selects optimal tokens.\n%Since its inception in ``Attention Is All You Need'', the transformer architecture has led to revolutionary advancements in natural language processing. The attention layer within the transformer admits a sequence of input tokens $\\X$ and makes them interact through pairwise similarities computed as $\\texttt{softmax}(\\X\\Qb\\Kb^\\top\\X^\\top)$, where $(\\Kb,\\Qb)$ are the trainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism allows us to characterize the implicit bias of 1-layer transformers optimized with gradient descent: \\textbf{(1)} Optimizing the attention layer, parameterized by $(\\Kb,\\Qb)$, with vanishing regularization, converges in direction to an SVM solution minimizing the nuclear norm of the combined parameter $\\W:=\\Kb\\Qb^\\top$. Instead, directly parameterizing by $\\W$ minimizes a Frobenius norm SVM objective. \n%We  characterize this convergence, highlighting that it can occur in locally-optimal directions rather than global ones. \\textbf{(2)} Complementing this, for $\\W$-parameterization, we prove the local/global directional convergence of gradient descent under suitable geometric conditions. Importantly, we show that over-parameterization catalyzes global convergence by ensuring the feasibility of the SVM problem and by guaranteeing a benign optimization landscape devoid of stationary points. \\textbf{(3)} While our theory  applies primarily to linear prediction heads, we propose a more general SVM equivalence that  predicts the implicit bias of 1-layer transformers with nonlinear heads/MLPs. \n\\end{abstract}\n%\n\\input{sections/fig_main}\n\\input{sections/introduction}\n\\input{sections/prelim}\n\\input{sections/fig_rank}\n\\input{sections/inductive_bias}\n\\input{sections/sa-gd-converge}\n\\input{sections/sa-local-gd}\n\\input{sections/multitoken}\n\\input{sections/extensions}\n% \\input{sections/exp_report}\n%\\input{sections/experiments}\n\\input{sections/related}\n\\input{sections/conclusion}\n\\input{sections/acknowledgements}\n\\bibliographystyle{alpha}\n\\bibliography{refs}\n%\n\\newpage\n\\appendix\n\\input{supp/supp-roadmap.tex}\n\\addtocontents{toc}{\\protect\\setcounter{tocdepth}{3}}\n\\tableofcontents\n%\\input{supp/supp-roadmap.tex}\n%\\part{}\\parttoc \n\\input{supp/separation}\n\\input{supp/app-sa-basics.tex}\n\\input{supp/proof_convergence}\n%\\input{supp/great_leap_forward}\n\\input{supp/reg_path}\n\\input{supp/reg_path_analysis}\n%\\input{supp/toy_dataset}\n\n\\input{supp/app_exp.tex}\n\n%\\input{reg_path}\n\n\\end{document}\n==== END OF /2308.16898/self_attn.tex ====\n==== BEGINNING OF /2308.16898/sections/fig_rank.tex ====\n\\begin{figure}\n    \\centering\n    \\hspace{-10pt}\n    \\subfigure[Rank of attention SVM solutions with fixed $T=5$]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.25\\columnwidth, trim={1.3cm 1.4cm 0 0}, clip]{figs/rank_svm_diff_n.pdf}};\n        \\node at (-0.95,1.5) {\\small{$\\Ws$}};\n        \\node at (-0.95,1.) {\\small{$\\Ws_\\star$}};\n        \\node at (0,-2.2) {\\small{Varying $d$}};\n        \\node[rotate=90] at (-3,0) {\\small{Rank of SVM solution}};\n        \\end{tikzpicture}\n        \\label{fig rank svm n}\n    }\n    \\hspace{30pt}\n    \\subfigure[Rank of attention SVM solutions with fixed $n=5$]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.25\\columnwidth, trim={1.3cm 1.4cm 0 0}, clip]{figs/rank_svm_diff_T.pdf}};\n        \\node at (1.85,-0.8) {\\small{$\\Ws$}};\n        \\node at (1.85,-1.28) {\\small{$\\Ws_\\star$}};\n        \\node at (0,-2.2) {\\small{Varying $d$}};\n        \\node[rotate=90] at (-3.1,0) {\\small{Rank of SVM solution}};\n        \\end{tikzpicture}\n        \\label{fig rank svm T}\n    }\n    \\caption{Rank range of solutions for  \\eqref{eqn:sattnsvm} and \\eqref{eqn:sattnsvmst}, denoted as $\\Ws$ and $\\Ws_{\\star}$, solved using optimal tokens $(\\opt_i)_{i=1}^n$ and setting  $m=d$ (the rank constraint is eliminated). Both figures confirm ranks of $\\Ws$ and $\\Ws_\\star$ are bounded by $\\max(n,d)$, validating Lemma~\\ref{lem:rank}.}\n        \\label{fig rank}\n\\end{figure}\n==== END OF /2308.16898/sections/fig_rank.tex ====\n==== BEGINNING OF /2308.16898/sections/fig_main.tex ====\n\\begin{figure}\n    \\centering\n    \\begin{minipage}{.58\\textwidth}\n    \\centering\n    % \\subfigure[Convergence behaviour of  GD for $\\W$-parameterization]{\n    \\hspace{-15pt}\n    \\subfigure[$\\W$-parameterization]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.37\\columnwidth]{figs/GD_converge_path_W.pdf}};\n        \\node[right] at (1.13,0.23) {\\scriptsize{$\\Wm\\z_1$}};\n        \\node[right] at (1.13,-0.07) {\\scriptsize{$\\Wm\\z_2$}};\n        \\end{tikzpicture}\n        \\label{fig path W}\n    }\n    \\hspace{-12pt}\n    % \\subfigure[Convergence  behaviour of GD for $(\\Kb,\\Qb)$-parameterization]{\n    \\subfigure[$(\\Kb,\\Qb)$-parameterization]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.37\\columnwidth]{figs/GD_converge_path_KQ.pdf}};\n        \\node[right] at (1.13,0.23) {\\scriptsize{$\\Wm_\\st\\z_1$}};\n        \\node[right] at (1.13,-0.07) {\\scriptsize{$\\Wm_\\st\\z_2$}};\n        % \\node at (-.85,1.55) {{$(\\Kb,\\Qb)$}};\n        \\end{tikzpicture}\n        \\label{fig path KQ}\n    }\n    \\vspace{0pt}\n    \\caption{GD convergence during training of cross-attention weight $\\W$ or $(\\Kb,\\Qb)$ with data. Teal and yellow markers represent tokens from $\\X_1$ and $\\X_2$, while stars mark optimal tokens. Solid lines in Figures {\\color{blue}(a)} and {\\color{blue}(b)} depict \\ref{eqn:sattnsvm} and  \\ref{eqn:sattnsvmst}  directions mapped to $\\z_1$ (red) and $\\z_2$ (blue), respectively.  Arrows illustrating GD trajectories converging towards these SVM directions. Red and blue dotted lines represent the corresponding separating hyperplanes.} \n    \\label{fig path}\n    \\end{minipage}\n    \\hspace{4pt}\n    \\begin{minipage}{0.4\\textwidth}\n    \\centering\n    \\vspace{7pt}\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.47\\columnwidth, trim={1.3cm 1.3cm 3.2cm 0}, clip]{figs/overparam_W_bar_intro.pdf}};\n        \\node[rotate=90] at (-3.3,0) {\\footnotesize{Percentage}};\n        \\node at (-0.7,-1.8){\\footnotesize{Varying $d$}};\n        \\node[right] at (1.8,1.2){\\scriptsize{Not Local}};\n        \\node[right] at (1.8,0.92){\\scriptsize{Global}};\n        \\node[right] at (1.8,0.64){\\scriptsize{Local}};\n        \\end{tikzpicture}\n    \\vspace{-17pt}\n    \\caption{\nPercentage of different convergence types when training cross-attention weights ($\\W$) using GD and varying dimension ($d$).  Red and blue bars represent the percentages of convergence to globally-optimal and locally-optimal (including global) SVM solutions, respectively. Teal bars are complements of the blue bars. \n% Green bars correspond to Assumption \\ref{assum:token:supp} that all tokens act as support vectors. \nLarger overparameterization ($d$) increases the likelihood of global convergence.\n    } \n    \\label{fig overparam W bar}\n% \\end{figure}\n    \\end{minipage}\n\\end{figure}\n\n\n%%%%%%%  Original\n% \\begin{figure}[t]\n%     \\centering\n%         \\vspace{-7pt}\n%         \\begin{tikzpicture}\n%             \\node at (0,0) {\\includegraphics[height=.56\\columnwidth, trim={0 -0.4cm 0 0.5cm}]{figs/GD_converge_path_v2.pdf}};\n%             \\node at (1.55,-0.7){\\includegraphics[height=.22\\columnwidth]{figs/GD_converge_path_data_v2.pdf}};\n%             % \\node at (-1.05,2.) {\\small{$\\W\\z_i$}};\n%             % \\node at (-.9,1.5) {\\small{$\\Kb\\Qb^\\top\\z_i$}};\n%             \\node at (-1.05,2.03) {{$\\W$}};\n%             \\node at (-.85,1.55) {{$(\\Kb,\\Qb)$}};\n%             \\draw[ darkgreen, line width=1pt] (-0.7,-1.7) circle (0.3cm);\n%             \\draw[ darkgreen, line width=1pt] (-0.,-1.7) circle (0.3cm);\n%             \\draw[ darkgreen, line width=1pt] (0.35,2.5) circle (0.3cm);\n%             \\draw[ darkgreen, line width=1pt] (1.05,2.5) circle (0.3cm);\n%         \\end{tikzpicture}\n%        \\vspace{-9pt}\n%         \\caption{GD convergence during training of cross-attention weight $\\W$ or $(\\Kb,\\Qb)$ with data. Teal and yellow markers represent tokens from $\\X_1$ and $\\X_2$, while stars mark optimal tokens. Solid and dashed lines depict attention-SVM (\\ref{eqn:sattnsvm} and \\ref{eqn:sattnsvmst} respectively) directions mapped to $\\z_1$ (red) and $\\z_2$ (blue), with arrows illustrating GD trajectories converging towards these SVM directions. Green circles denote GD $\\leftrightarrow$ SVM pairings.\n%         }\n%         \\label{fig path}\n%     \\end{figure}\n%%%%%%%%%%%%%\n   \n   \n% \\begin{figure*}\n%     \\centering\n%     \\hspace{-10pt}\n%     \\begin{minipage}{.45\\textwidth}\n%     \\centering\n%         \\begin{tikzpicture}\n%             \\node at (0,0) {\\includegraphics[height=.58\\columnwidth, trim={0 -0.4cm 0 0.5cm}]{figs/GD_converge_path_v2.pdf}};\n%             \\node at (1.5,-0.7){\\includegraphics[height=.22\\columnwidth]{figs/GD_converge_path_data_v2.pdf}};\n%             % \\node at (-1.05,2.) {\\small{$\\W\\z_i$}};\n%             % \\node at (-.9,1.5) {\\small{$\\Kb\\Qb^\\top\\z_i$}};\n%             \\node at (-1.05,1.83) {\\small{$\\W$}};\n%             \\node at (-.85,1.35) {\\small{$(\\Kb,\\Qb)$}};\n%             \\draw[ darkgreen, line width=1pt] (-0.7,-1.6) circle (0.3cm);\n%             \\draw[ darkgreen, line width=1pt] (-0.,-1.6) circle (0.3cm);\n%             \\draw[ darkgreen, line width=1pt] (0.3,2.25) circle (0.3cm);\n%             \\draw[ darkgreen, line width=1pt] (1.,2.25) circle (0.3cm);\n%         \\end{tikzpicture}\n%        % \\vspace{-15pt}\n%         \\caption{GD convergence during training of cross-attention weight $\\W$ or $(\\Kb,\\Qb)$ with data. Teal and yellow markers represent tokens from $\\X_1$ and $\\X_2$, while stars mark optimal tokens. Solid and dashed lines depict attention-SVM (\\ref{eqn:sattnsvm} and \\ref{eqn:sattnsvmst} respectively) directions mapped to $\\z_1$ (red) and $\\z_2$ (blue), with arrows illustrating GD trajectories converging towards these SVM directions. Green circles denote GD $\\leftrightarrow$ SVM pairings.\n%         }\n%         \\label{fig path}\n%     \\end{minipage}\n%     \\hspace{10pt}\n%     %\\end{figure}\n%     %\\begin{figure}\n%    \\begin{minipage}{.48\\textwidth}\n%     \\centering\n%     % \\vspace{-20pt}\n%     \\begin{tikzpicture}\n%         \\node at (0,0) {\\includegraphics[height=.5\\columnwidth, trim={1.3cm 1.3cm 3.2cm 0}, clip]{figs/overparam_W_bar_intro.pdf}};\n%         \\node[rotate=90] at (-4.2,0) {\\small{Percentage}};\n%         \\node at (-1,-2.4){\\small{Varying $d$}};\n%         % \\node at (3.15,1.55){\\small{Not Local}};\n%         % \\node at (3.15,1.15){\\small{Global}};\n%       %  \\node at (3,.8){\\small{$\\W(k) \\rightarrow \\Ws$}};\n%         % \\node at (3.15,0.8){\\small{Local}};\n%         % \\node at (3.15,0.5){\\small{Assum B.1}};\n%         \\node at (3.15,1.55){\\small{Not Local}};\n%         \\node at (2.95,1.15){\\small{Global}};\n%         \\node at (2.9,0.8){\\small{Local}};\n%         \\end{tikzpicture}\n%     %\\vspace{-14pt}\n%     \\caption{\n% Percentage of different convergence types when training cross-attention weights ($\\W$) using GD and varying dimension ($d$).  Red and blue bars represent the percentages of convergence to globally-optimal and locally-optimal (including global) SVM solutions, respectively. Teal bars are complements of the blue bars. \n% % Green bars correspond to Assumption \\ref{assum:token:supp} that all tokens act as support vectors. \n% Larger overparameterization ($d$) increases the likelihood of global convergence.\n%     } \n%     \\label{fig overparam W bar}\n%    \\end{minipage}\n%     \\hspace{-10pt}\n% \\end{figure*}\n\n\n% \\begin{figure}\n%     \\centering\n%     \\hspace{-10pt}\n%     \\begin{minipage}{.48\\textwidth}\n%     \\centering\n%         \\begin{tikzpicture}\n%             \\node at (0,0) {\\includegraphics[height=.58\\columnwidth, trim={0 -0.4cm 0 0.5cm}]{figs/GD_converge_path.pdf}};\n%             \\node at (1.5,-0.7){\\includegraphics[height=.22\\columnwidth]{figs/GD_converge_path_data.pdf}};\n%             % \\node at (-1.05,2.) {\\small{$\\W\\z_i$}};\n%             % \\node at (-.9,1.5) {\\small{$\\Kb\\Qb^\\top\\z_i$}};\n%             \\node at (-1.05,1.83) {\\small{$\\W$}};\n%             \\node at (-.85,1.35) {\\small{$(\\Kb,\\Qb)$}};\n%             \\draw[ darkgreen, line width=1pt] (-2.1,-1.6) circle (0.3cm);\n%             \\draw[ darkgreen, line width=1pt] (-2.4,-1.) circle (0.3cm);\n%             \\draw[ darkgreen, line width=1pt] (2.6,2.25) circle (0.3cm);\n%             \\draw[ darkgreen, line width=1pt] (1.85,2.35) circle (0.3cm);\n%         \\end{tikzpicture}\n%       %  \\vspace{-7pt}\n%         \\caption{GD convergence during training of cross-attention weight $\\W$ or $(\\Kb,\\Qb)$ with data. Teal and yellow markers represent tokens from $\\X_1$ and $\\X_2$, while stars mark optimal tokens. Solid and dashed lines depict attention-SVM (\\ref{eqn:sattnsvm} and \\ref{eqn:sattnsvmst} respectively) directions mapped to $\\z_1$ (red) and $\\z_2$ (blue), with arrows illustrating GD trajectories converging towards these SVM directions. Green circles denote GD $\\leftrightarrow$ SVM pairings.\n%         }\n%         \\label{fig path}\n%     \\end{minipage}\n%     \\hspace{10pt}\n%     \\begin{minipage}{.48\\textwidth}\n%     \\centering\n%     \\vspace{15pt}\n%     \\begin{tikzpicture}\n%         \\node at (0,0) {\\includegraphics[height=.5\\columnwidth, trim={1.3cm 1.3cm 3.2cm 0}, clip]{figs/overparam_W_bar_intro.pdf}};\n%         \\node[rotate=90] at (-4.2,0) {\\small{Percentage}};\n%         \\node at (-1,-2.2){\\small{Varying $d$}};\n%         % \\node at (3.15,1.55){\\small{Not Local}};\n%         % \\node at (3.15,1.15){\\small{Global}};\n%       %  \\node at (3,.8){\\small{$\\W(k) \\rightarrow \\Ws$}};\n%         % \\node at (3.15,0.8){\\small{Local}};\n%         % \\node at (3.15,0.5){\\small{Assum B.1}};\n%         \\node at (3.15,1.55){\\small{Not Local}};\n%         \\node at (2.95,1.15){\\small{Global}};\n%         \\node at (2.9,0.8){\\small{Local}};\n\n%         \\end{tikzpicture}\n%     %\\vspace{-14pt}\n%     \\caption{\n% Percentage of different convergence types when training cross-attention weights ($\\W$) using GD and varying dimension ($d$).  Red and blue bars represent the percentages of convergence to globally-optimal and locally-optimal (including global) SVM solutions, respectively. Teal bars are complements of the blue bars. \n% % Green bars correspond to Assumption \\ref{assum:token:supp} that all tokens act as support vectors. \n% Larger overparameterization ($d$) increases the likelihood of global convergence.\n%     } \n%     \\label{fig overparam W bar}\n%     \\end{minipage}\n%     \\hspace{-10pt}\n% \\end{figure}\n==== END OF /2308.16898/sections/fig_main.tex ====\n==== BEGINNING OF /2308.16898/sections/svm_obj.tex ====\n\\input{sections/fig_svm_margin}\n\\subsection{Investigation on SVM objectives and GD convergence}\nUntil now, we have discussed the global and local convergence performances of gradient descent (GD). Theorem~\\ref{thm:local:gd} suggests that, without specific restrictions on tokens, when training with GD, the attention weight $\\W$ converges towards $\\Wma$. Here, the selected token indices $\\bal = (\\alpha_{i})_{i=1}^n$ may not necessarily be identical to $\\opt = (\\op_i)_{i=1}^n$. Experiments presented in Figures~\\ref{fig overparam W bar}, \\ref{fig overparam bar}, and \\ref{fig overparam} also support this observation. In this section, we focus on scenarios where $\\bal \\neq \\opt$ (e.g., when $\\Wm$ is not feasible) and investigate the question: Towards which local direction is GD most likely to converge?\n\nTo this goal, in Figure~\\ref{fig svm margin}, we consider SVM margin, which is defined by $1/\\tf{\\Wma}$, and investigate its connection to the convergence performance of GD.  On the left, we set $T=d=5$ and vary $n$ among $1, 5, 10, 15$; on the right, we fix $T=n=5$ and change $d$ to $2, 5, 10, 15$. All tokens are randomly generated from the unit sphere. The SVM margins corresponding to the selected tokens $\\bal$ are depicted as blue curves in the upper subfigures, while the SVM margins corresponding to the globally--optimal token indices ($\\bal=\\opt$) are shown as red curves. The red and blue bars in the lower subfigures represent the percentages of global and local convergence, respectively. Combining all these findings empirically demonstrates that when the global SVM objective yields a solution with a small margin (i.e., $1/\\tf{\\Wm}$ is small, and 0 when global SVM is not feasible), GD tends to converge towards a local direction with a comparatively larger margin.%, rather than towards the global direction. \n%To \n==== END OF /2308.16898/sections/svm_obj.tex ====\n==== BEGINNING OF /2308.16898/sections/fig_general.tex ====\n\\begin{figure}[t]\n    \\centering\n    \\hspace{-10pt}\n    \\subfigure[Evolution of softmax probabilities]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.22\\columnwidth, trim={1.3cm 1.3cm 0 0}, clip]{figs/sfx_prob_general.pdf}};\n        \\node at (1.1,-0.55) {\\small{$\\W$}};\n        \\node at (1.3,-1.05) {\\small{$(\\Kb,\\Qb)$}};\n        \\node at (0.2,-2.) {\\small{Iterations}};\n        \\node[rotate=90] at (-2.65,0) {\\small{Softmax probability}};\n        \\end{tikzpicture}\n        \\label{fig general sfx prob}\n    }\n    \\hspace{-10pt}\n    \\subfigure[ Corr. coeff. of GD and $\\Ws_\\bal$]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.22\\columnwidth, trim={1.3cm 1.3cm 0 0}, clip]{figs/corr_fro_general.pdf}};\n        \\node at (1.1,-0.55) {\\small{$\\W$}};\n        \\node at (1.3,-1.05) {\\small{$(\\Kb,\\Qb)$}};\n        \\node at (0.3,-2.) {\\small{Iterations}};\n        \\node[rotate=90] at (-2.65,0) {\\small{Correlation coefficient}};\n        \\end{tikzpicture}\n        \\label{fig general fro corr}\n    }\n    \\hspace{-10pt}\n    \\subfigure[ Corr. coeff. of GD and $\\Ws_{\\star,\\bal}$]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.22\\columnwidth, trim={1.3cm 1.3cm 0 0}, clip]{figs/corr_nuc_general.pdf}};\n        \\node at (1.1,-0.55) {\\small{$\\W$}};\n        \\node at (1.3,-1.05) {\\small{$(\\Kb,\\Qb)$}};\n        \\node at (0.3,-2.) {\\small{Iterations}};\n        \\node[rotate=90] at (-2.65,0) {\\small{Correlation coefficient}};\n        \\end{tikzpicture}\n        \\label{fig general nuc corr}\n    }\n   \\caption{Local convergence behaviour of GD when training cross-attention weights $\\W$ (blue) or $(\\Kb,\\Qb)$ (red) with random data: \\textbf{  (a)} displays the largest entry of the softmax outputs averaged over the dataset;  \\textbf{(b\\&c)} display the Pearson correlation coefficients of GD trajectories and the SVM solutions \\textbf{(b)} with the Frobenius norm objective $\\Ws_\\bal$ (solution of \\eqref{eqn:sattnsvm}) and \\textbf{(c)} with the nuclear norm objective $\\Ws_{\\st,\\bal}$ \n (solution of \\eqref{eqn:sattnsvmst}). These demonstrate the Frobenius norm bias of $\\W(k)$ and the nuclear norm bias of $\\Kb(k)\\Qb(k)^\\top$.}\n     \\label{fig general}\n\\end{figure}\n==== END OF /2308.16898/sections/fig_general.tex ====\n==== BEGINNING OF /2308.16898/sections/related.tex ====\n\\section{Related work}\\label{sec:related}\n%\n%\n\\subsection{Implicit regularization, matrix factorization, and sparsity}\n%\nExtensive research has delved into gradient descent's implicit bias in separable classification tasks, often using logistic or exponentially-tailed losses for margin maximization \\cite{soudry2018implicit,gunasekar2018characterizing,nacson2019convergence,ji2021characterizing,kini2021label,moroshko2020implicit,ji2020directional}. The findings have also been extended to non-separable data using gradient-based techniques \\cite{ji2018risk,ji2019implicit,ji2020gradient}. Implicit bias in regression problems and losses has been investigated, utilizing methods like mirror descent \\cite{woodworth2020kernel, gunasekar2018characterizing,\nyun2020unifying, vaskevicius2019implicit, amid2020winnowing, amid2020reparameterizing,azizan2021stochastic,sun2022mirror}. Stochastic gradient descent has also been a subject of interest regarding its implicit bias \\cite{li2019towards,blanc2020implicit,liang2020just, haochen2020shape, li2022what, damian2021label, zou2021benefits}. This extends to the implicit bias of adaptive and momentum-based methods \\cite{qian2019implicit, wang2021momentum, wang2021implicit, ji2021fast}.\n\nIn linear classification, GD iterations on logistic loss and separable datasets converge to the hard margin SVM solution \\cite{soudry2018implicit,rosset2003margin,zhang2005boosting}. The attention layer's softmax nonlinearity behaves similarly, potentially favoring margin-maximizing solutions. Yet, the layer operates on tokens in input sequences, not for direct classification. Its bias leans toward an \\eqref{eqn:sattnsvm}, selecting relevant tokens while suppressing others. However, formalizing this intuition presents significant challenges: Firstly, our problem is nonconvex (even in terms of the $\\W$-parameterization), introducing new challenges and complexities. Secondly, it requires the introduction of novel concepts such as locally-optimal tokens, demanding a tailored analysis focused on the cones surrounding them. Our findings on the implicit bias of $(\\Kb,\\Qb)$-parameterization share conceptual similarities with \\cite{srebro2004maximum}, which proposes and analyzes a max-margin matrix factorization problem. Similar problems have also been studied more recently   in the context of neural-collapse phenomena \\cite{papyan2020prevalence} through an analysis of the implicit bias and regularization path of the unconstrained features model with cross-entropy  loss \\cite{thrampoulidis2022imbalance}. However, a fundamental distinction from these works lies in the fact that attention solves a different max-margin problem that separate tokens. Moreover, our results on $(\\Kb,\\Qb)$-parameterization are inherently connected to the rich literature on low-rank factorization \\cite{gunasekar2017implicit,arora2019implicit,timor2023implicit,tu2016low,stoger2021small}, stimulating further research. \\cite{tarzanagh2023margin} is the first work to establish the connection between attention and SVM, which is closest to our work. Here, we augment their framework, initially developed for a simpler attention model, to transformers by providing the first guarantees for self/cross-attention layers, nonlinear prediction heads, and realistic global convergence guarantees. While our Assumption \\ref{assum:opt:token} and local-convergence analysis align with \\cite{tarzanagh2023margin}, our contributions in global convergence analysis, benefits of overparameterization, and the generalized SVM-equivalence in Section \\ref{sec:multi} are unique to this work.\n\n%\\paragraph{Sparsity and feature selection.} \nIt is well-known that attention map (i.e.~softmax outputs) act as a feature selection mechanism and reveal the tokens that are relevant to classification. On the other hand, sparsity and lasso regression (i.e.~$\\ell_1$ penalization) \\cite{donoho2006compressed,tibshirani1996regression,tropp2007signal,chen2001atomic,candes2006robust} have been pivotal tools in the statistics literature for feature selection. Softmax and lasso regression exhibit interesting parallels: The Softmax output $\\s=\\sft{\\X\\W\\z}$ obeys $\\|\\s\\|_{\\ell_1}=1$ by design. Softmax is also highly receptive to being sparse because decreasing the temperature (i.e.~scaling up the weights $\\W$) eventually leads to a one-hot vector unless all logits are equal. We (also, \\cite{tarzanagh2023margin}) have used these intuitions to formalize attention as a \\emph{token selection mechanism}. This aspect is clearly visible in our primary SVM formulation \\eqref{eqn:sattnsvm} which selects precisely one token from each input sequence (i.e.~hard attention). Section \\ref{sec:multi} has also demonstrated how  \\eqref{eqn:mattnsvm} can explain more general sparsity patterns by precisely selecting desired tokens and suppressing others. We hope that this SVM-based token-selection viewpoint will motivate future work and deeper connections to the broader feature-selection and compressed sensing literature.\n%\n%\n\\subsection{Attention mechanism and transformers}\n\nTransformers, as highlighted by \\cite{vaswani2017attention}, revolutionized the domains of NLP and machine translation. Prior work on self-attention \\cite{cheng2016long,parikh2016decomposable,paulus2017deep,lin2017structured} laid the foundation for this transformative paradigm. In contrast to conventional models like MLPs and CNNs, self-attention models employ global interactions to capture feature representations, resulting in exceptional empirical performance.\n\nDespite their achievements, the mechanisms and learning processes of attention layers remain enigmatic. Recent investigations \\cite{edelman2022inductive,sahiner2022unraveling,ergen2022convexifying,baldi2022quarks,dong2021attention} have concentrated on specific aspects such as sparse function representation, convex relaxations, and expressive power. Expressivity discussions concerning hard-attention \\cite{hahn2020theoretical} or attention-only architectures \\cite{dong2021attention} are connected to our findings when $h(\\cdot)$ is linear. In fact, our work reveals how linear $h$ results in attention's optimization dynamics to collapse on a single token whereas nonlinear $h$ provably requires attention to select and compose multiple tokens. This supports the benefits of the MLP layer for expressivity of transformers. There is also a growing body of research aimed at a theoretical comprehension of in-context learning and the role played by the attention mechanism \\cite{akyurek2022learning,li2023transformers,ahn2023transformers,zhang2023trained,bai2023transformers,giannou2023looped}. \\cite{sahiner2022unraveling} investigate self-attention with linear activation instead of softmax, while \\cite{ergen2022convexifying} approximate softmax using a linear operation with unit simplex constraints. Their primary goal is to derive convex reformulations for training problems grounded in empirical risk minimization (ERM). In contrast, our methodologies, detailed in equations \\eqref{eqn:erm:w} and \\eqref{eqn:erm:kq}, delve into the nonconvex domain.\n\n\\cite{merrill2020effects,boix2023transformers} offer insights into the implicit bias of optimizing transformers. Specifically, \\cite{merrill2020effects} provide empirical evidence that an increase in attention weights results in a sparser softmax, which aligns with our theoretical framework. \\cite{boix2023transformers} study incremental learning and furnish both theory and numerical evidence that increments of the softmax attention weights ($\\Kb\\Qb^\\top$) are low-rank. Our theory aligns with this concept, as the SVM formulation \\eqref{eqn:qk:svm} of $(\\Kb,\\Qb)$ parameterization inherently exhibits low-rank properties through the nuclear norm objective, rank-$m$ constraint, and implicit constraint induced by Lemma \\ref{lem:rank}.\n\nSeveral recent works \\cite{jelassi2022vision,li2023theoretical,tian2023scan,noci2023shaped,oymak2023role,nguyen2023primal,fu2023can} aim to delineate the optimization and generalization dynamics of transformers. However, their findings usually apply under strict statistical assumptions about the data, while our study offers a comprehensive optimization-theoretic analysis of the attention model, establishing a formal linkage to max-margin problems and SVM geometry. This allows our findings to encompass the problem geometry and apply to diverse datasets. Overall, the max-margin equivalence  provides a fundamental comprehension of the optimization geometry of transformers, offering a framework for prospective research endeavors, as outlined in the subsequent section.\n==== END OF /2308.16898/sections/related.tex ====\n==== BEGINNING OF /2308.16898/sections/fig_nn_diff_d.tex ====\n\\begin{figure}\n    \\centering\n    \\hspace{-10pt}\n    \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[width=0.985\\textwidth, trim={1.3cm 1.5cm 0 0}, clip]{figs/nn_corr_diff_d.pdf}};\n        \\node at (-5.75,-1.7) {\\small{Iterations}};\n        \\node at (-5.75,1.7) {\\small{$d=4$}};\n        \\node at (-1.8,-1.7) {\\small{Iterations}};\n        \\node at (-1.8,1.7) {\\small{$d=6$}};\n        \\node at (2.15,-1.7) {\\small{Iterations}};\n        \\node at (2.15,1.7) {\\small{$d=8$}};\n        \\node at (6.15,-1.7) {\\small{Iterations}};\n        \\node at (6.15,1.7) {\\small{$d=10$}};\n        \\node[rotate=90] at (-8.35, 0) {\\small{$1-$correlation coefficient}};\n\n        \\node at (-6.63,-0.4) {\\scriptsize{$\\W^\\rfn$}};\n        \\node at (-6.74,-0.7) {\\scriptsize{$\\Ws$}};\n        \\node at (-6.6,-.95) {\\scriptsize{$\\W^\\ont$}};\n\n        \\node at (-2.67,-0.4) {\\scriptsize{$\\W^\\rfn$}};\n        \\node at (-2.78,-0.7) {\\scriptsize{$\\Ws$}};\n        \\node at (-2.64,-.95) {\\scriptsize{$\\W^\\ont$}};\n\n        % \\node at (1.27,-0.4) {\\scriptsize{$\\W^\\rfn$}};\n        % \\node at (1.16,-0.7) {\\scriptsize{$\\Ws$}};\n        % \\node at (1.3,-.95) {\\scriptsize{$\\W^\\ont$}};\n\n        \\node at (3.39,1.15) {\\scriptsize{$\\W^\\rfn$}};\n        \\node at (3.28,0.85) {\\scriptsize{$\\Ws$}};\n        \\node at (3.42,0.6) {\\scriptsize{$\\W^\\ont$}};\n\n        \\node at (7.33,1.15) {\\scriptsize{$\\W^\\rfn$}};\n        \\node at (7.22,0.85) {\\scriptsize{$\\Ws$}};\n        \\node at (7.36,0.6) {\\scriptsize{$\\W^\\ont$}};\n    \\end{tikzpicture}\n    \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[width=0.985\\textwidth, trim={1.3cm 1.5cm 0 0}, clip]{figs/nn_thred_diff_d.pdf}};\n        \\node at (-5.75,-1.7) {\\small{$\\max_{i,\\tau}s_{i\\tau},~\\tau\\in\\Rcb_i$}};\n        \\node at (-1.8,-1.7) {\\small{$\\max_{i,\\tau}s_{i\\tau},~\\tau\\in\\Rcb_i$}};\n        \\node at (2.15,-1.7) {\\small{$\\max_{i,\\tau}s_{i\\tau},~\\tau\\in\\Rcb_i$}};\n        \\node at (6.15,-1.7) {\\small{$\\max_{i,\\tau}s_{i\\tau},~\\tau\\in\\Rcb_i$}};\n        \\node[rotate=90] at (-8.35, 0) {\\small{$1-$correlation coefficient}};\n    \\end{tikzpicture}\n    \\caption{ Behavior of GD  with nonlinear nonconvex prediction head and multi-token compositions. \\textbf{Upper:} The correlation between GD solution and three distinct baselines: ({\\color{black}{\\textbf{$\\cdots$}}}) $\\Ws$ obtained from \\eqref{eqn:mattnsvm};  ({\\color{black}{\\textbf{---}}}) $\\W^\\rfn$ obtained by calculating $\\Wf$ and determining the best linear combination $\\Wf+\\gamma \\Wsb$ that maximizes correlation with the GD solution; and  ({\\color{black}{\\textbf{-~-}}}) $\\W^\\ont$ obtained by solving \\eqref{eqn:sattnsvm} and selecting the highest probability token from the GD solution.  \\textbf{Lower:} Scatterplot of the largest softmax probability over masked tokens (per our $s_{i\\tau}\\leq 10^{-6}$ criteria) vs correlation coefficient.}\n    \\label{fig nn diff d}\n\\end{figure}\n==== END OF /2308.16898/sections/fig_nn_diff_d.tex ====\n==== BEGINNING OF /2308.16898/sections/fig_nn_main.tex ====\n\\begin{figure}[t]\n    \\centering\n    \\begin{minipage}{.37\\textwidth}\n    % \\subfigure[Train attention weights $\\Kb,\\Qb\\in\\R^{d\\times m}$]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.6\\columnwidth, trim={1.2cm 1.4cm 0 -1cm}, clip]{figs/rank_diff_m.pdf}};\n        \\node at (0,-2.) {\\small{{Varying $m$}}};\n        \\node[rotate=90] at (-2.7,0) {\\small{$1-$correlation coefficient}};\n        \\end{tikzpicture}\n        \\vspace{10pt}\n        \\caption{Convergence behavior of GD when training attention weights $(\\Kb,\\Qb)\\in\\R^{d\\times m}$ with random data and varying $m$. The misalignment between attention SVM and GD, $1-\\corr{\\Ws_{\\star,\\bal},\\Kb\\Qb^\\top}$, is studied. $\\Ws_{\\star,\\bal}$ is from \\eqref{eqn:sattnsvmst} with GD tokens $\\bal$ and $m=d$. Subfigures with fixed $n=5$ and $T=5$ show that as $m$ approaches or exceeds $n$, $\\Kb\\Qb^\\top$ aligns more with $\\Ws_{\\star,\\bal}$.  }\n        \\label{fig rank m}\n    % }\n    \\end{minipage}\n    \\hspace{5pt}\n    \\begin{minipage}{.6\\textwidth}\n    % \\hspace{-10pt}\n    \\subfigure[Evolution of correlation under varying $d$]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.33\\columnwidth, trim={1.3cm 1.3cm 0 0}, clip]{figs/nn_corr_itr_svm.pdf}};\n        \\node at (0.2,-1.9) {\\small{Iterations}};\n        \\node[rotate=90] at (-2.55,0) {\\small{$1-$correlation coefficient}};\n        \\end{tikzpicture}\n        \\label{fig nn itr}\n    }\n    \\hspace{-10pt}\n    % \\subfigure[]{\n    %     \\begin{tikzpicture}\n    %     \\node at (0,0) {\\includegraphics[height=.22\\columnwidth, trim={1.3cm 1.3cm 0 0}, clip]{figs/nn_corr_sfx_zero_scatter.pdf}};\n    %     \\node at (0,-2.) {\\small{$\\max_{i,\\tau}\\s_{i\\tau},~\\tau\\in\\Rcb_i$}};\n    %     \\node[rotate=90] at (-2.65,0) {\\small{$1-$correlation coefficient}};\n    %     \\end{tikzpicture}\n    %     \\label{fig nn sfx zero scatter}\n    % }\n    % \\hspace{30pt}\n    \\subfigure[$\\Gamma$ vs correlation coefficient]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.33\\columnwidth, trim={1.3cm 1.3cm 0 0}, clip]{figs/nn_corr_sfx_zero.pdf}};\n        % \\node[rotate=90] at (-2.85,0) {\\small{$1-$correlation coefficient}};\n        % \\node at (0.2,-2.3){\\small{$\\max_{i,\\tau}s_{i\\tau},~\\tau\\in\\Rcb_i$}};\n        \\node at (0.2,-1.9){\\small{Masked token threshold ($\\Gamma$)}};\n        \\end{tikzpicture}\n        \\label{fig nn sfx zero}\n    }\n    \\vspace{-10pt}\n    \\caption{Behavior of GD  with nonlinear nonconvex prediction head and multi-token compositions. \\textbf{(a)}: Blue, green, red and teal curves represent the evolution of $1-\\corr{\\W,\\W^{\\rfn}}$ for $d=4,6,8$ and $10$ respectively, which have been displayed in Figure~\\ref{fig nn diff d}(upper). \\textbf{(b)}: Over the $500$ random instances as discussed in Figure~\\ref{fig nn diff d}, we filter different instances by constructing masked set with tokens whose softmax output $<\\Gamma$ and vary $\\Gamma$ from $10^{-16}$ to $10^{-6}$. The corresponding results of $1-\\corr{\\W,\\W^\\rfn}$ are displayed in blue, green, red and teal curves.} \n    \\label{fig nn main}\n    \\end{minipage}\n\\end{figure}\n==== END OF /2308.16898/sections/fig_nn_main.tex ====\n==== BEGINNING OF /2308.16898/sections/inductive_bias.tex ====\n\\section{Understanding the Implicit Bias of Self-Attention}\\label{sec:bias}\n\nWe start by motivating the optimal token definition and establishing the global convergence of RPs which shed light on the implicit bias of attention parameterizations. Throughout, we maintain the following assumption regarding the loss function.\n\\begin{assumption}\\label{assum:loss:prope}\nOver any bounded interval $[a,b]$: (i) $\\ell:\\R\\rightarrow\\R$ is strictly decreasing; (ii) The derivative $\\ell^{\\prime}$ is bounded as $|\\ell^{\\prime}(u)|\\leq M_1$; (iii) $\\ell'$ is $M_0$-Lipschitz continuous.\n% \\begin{enumerate}[label=(\\roman*),itemjoin={\\quad}]\n% \\item  \\item \\item \n% \\end{enumerate}\n\\end{assumption}\nAssumption~\\ref{assum:loss:prope} encompasses many common loss functions, e.g., logistic  $\\ell\\left(u\\right)=\\log\\left(1+e^{-u}\\right)$, exponential  $\\ell\\left(u\\right)=e^{-u}$, and correlation $\\ell(u)=-u$ losses.  \n%\n\\begin{lemma}[Optimal Tokens Minimize Training Loss]\\label{lem min risk} Suppose Assumption \\ref{assum:loss:prope} (i)-(ii) hold, and not all tokens are optimal per Definition~\\ref{score def}. Then, training risk obeys $\\Lc(\\W)>\\Lc_\\st:=\\frac{1}{n}\\sum_{i=1}^n \\ell(\\bgam_{i\\op_i})$. Additionally, suppose there are optimal indices $(\\op_i)_{i=1}^n$ for which \\eqref{eqn:sattnsvm} is feasible, i.e.~there exists a $\\W$ separating optimal tokens. This $\\W$ choice obeys $\\lim_{R\\rightarrow\\infty}\\Lc(R\\cdot\\W)=\\Lc_\\st$.\n\\end{lemma}\n%\nThe result presented in Lemma~\\ref{lem min risk} originates from the observation that the output tokens of the attention layer constitute a convex combination of the input tokens. Consequently, when subjected to a strictly decreasing loss function, attention optimization inherently leans towards the selection of a singular token, specifically, the optimal token $(\\op_i)_{i=1}^n$. Our following theorem unveils the implicit bias ingrained within both attention parameterizations through RP analysis.\n\n\\begin{theorem}\\label{thm global reg path}\nSuppose Assumptions \\ref{assum:loss:prope} holds, optimal indices $(\\op_i)_{i=1}^n$ are unique, and \\eqref{eqn:sattnsvm} is feasible. Let $\\Wm$ be the unique solution of \\eqref{eqn:sattnsvm}, and let $\\Wc^\\svm_\\star$ be the solution set of \\eqref{eqn:sattnsvmst} with nuclear norm achieving objective $C_\\st$. Then, Algorithms~\\ref{RP-W} and \\ref{RP-QK}, respectively, satisfy:\n\\begin{itemize}\n\\item $\\W$-parameterization has Frobenius norm bias: $\\underset{R\\rightarrow\\infty}{\\lim} \\frac{\\Wb_R}{R}=\\frac{\\Wm}{\\tf{\\Wm}}$.\n\\item $(\\Kb,\\Qb)$-parameterization has nuclear norm bias: $\\underset{R\\rightarrow\\infty}{\\lim} \\dist{\\frac{\\Kbb_R\\Qbb_R^\\top}{R},\\frac{\\Wc^\\svm_\\star}{C_\\st}}=0$.\n\\begin{itemize}\n\\item Setting $m=d$: \\eqref{eqn:sattnsvmst} is a convex problem without rank constraints.\n\\end{itemize} \n\\end{itemize}\n\\end{theorem}\n%\nTheorem~\\ref{thm global reg path} demonstrates that the RP of the $(\\Kb,\\Qb)$-parameterization converges to a max-margin solution of \\eqref{eqn:sattnsvmst} with nuclear norm objective on $\\W = \\Kb \\Qb^\\top$. When self-attention is directly parameterized by $\\W$, the RP converges to the solution of  \\eqref{eqn:sattnsvm} with a Frobenius norm objective. This result is the first to distinguish the optimization dynamics of $\\W$ and $(\\Kb,\\Qb)$ parameterizations, revealing the low-rank bias of the latter. These findings also provide a clear characterization of token optimality (Definition \\ref{score def}) and extend naturally to the setting with  multiple optimal tokens per sequence (Theorem \\ref{local RP thm} in appendix). By definition, the RP captures the global geometry and cannot be used for the implicit bias of GD towards locally-optimal directions. Sections \\ref{provable global} and \\ref{sec local} accomplish this goal through gradient-descent and localized RP analysis to obtain locally-applicable SVM equivalences. Note that, this theorem requires each input sequence has a unique optimal token per Definition~\\ref{score def}. Fortunately, this is a very mild condition as it holds for almost all datasets, namely, as soon as input features are slightly perturbed. \n\n\nTheorem \\ref{thm global reg path} establishes the implicit bias of attention from the perspective of RP analysis. This leads to the question: To what extent is this RP theory predictive of the implicit bias exhibited by GD?  To delve into this, we examine the gradient paths of $\\W(k)$ or $(\\Kb(k),\\Qb(k))$ and present the findings in Figure~\\ref{fig path}. We consider a scenario where $n=d=m=2$ and $T=5$, and employ cross-attention, where tokens $(\\z_1,\\z_2)$ are generated independently of the inputs $(\\X_1,\\X_2)$. The teal and yellow markers correspond to tokens from $\\X_1$ and $\\X_2$, respectively. The stars indicate the optimal token for each input. To provide a clearer view of the gradient convergence path, we illustrate the outcomes of training the attention weight $\\W$ or $(\\Kb,\\Qb)$ in the form of $\\W\\z_i$ or $\\Kb\\Qb^\\top\\z_i$, where $i={1,2}$. With reference to Equations (\\ref{eqn:sattnsvm}) and (\\ref{eqn:sattnsvmst}), the red and blue solid lines in Fig.~\\ref{fig path W} delineate the directions of $\\Wsf\\z_1$ and $\\Wsf\\z_2$, correspondingly. Conversely, the red and blue solid lines in Fig.~\\ref{fig path KQ} show the directions of $\\Ws_\\star\\z_1$ and $\\Ws_\\star\\z_2$. The red/blue arrows denote the corresponding directions of gradient evolution with the dotted lines representing the corresponding separating hyperplanes. Figure~\\ref{fig path} provides a clear depiction of the incremental alignment of $\\W(k)$ and $\\Kb(k)\\Qb(k)^\\top$ with their respective attention SVM solutions as $k$ increases. This strongly supports the assertions of Theorem~\\ref{thm global reg path}.\n\n% To delve into this, we examine the gradient paths of $\\W(k)$ or $(\\Kb(k),\\Qb(k))$ and present the findings in Figure~\\ref{fig path}. We consider a scenario where $n=d=m=2$ and $T=3$, and employ cross-attention, where tokens $(\\z_1,\\z_2)$ are generated independently of the inputs $(\\X_1,\\X_2)$. In Figure~\\ref{fig path}'s lower right corner, the teal and yellow markers correspond to tokens from $\\X_1$ and $\\X_2$, respectively. The stars indicate the optimal token for each input. To provide a clearer view of the gradient convergence path, we illustrate the outcomes of training the attention weight $\\W$ or $(\\Kb,\\Qb)$ in the form of $\\W\\z_i$ or $\\Kb\\Qb^\\top\\z_i$, where $i={1,2}$. With reference to Equations (\\ref{eqn:sattnsvm}) and (\\ref{eqn:sattnsvmst}), the red and blue solid lines delineate the directions of $\\Wsf\\z_1$ and $\\Wsf\\z_2$, correspondingly. Conversely, the red and blue dashed lines show the directions of $\\Ws_\\star\\z_1$ and $\\Ws_\\star\\z_2$. The red/blue solid/dashed arrows denote the corresponding directions of gradient evolution. Figure~\\ref{fig path} provides a clear depiction of the incremental alignment of $\\W(k)$ and $\\Kb(k)\\Qb(k)^\\top$ with their respective attention SVM solutions as $k$ increases. This strongly supports the assertions of Theorem~\\ref{thm global reg path}.\n\n\nIt is worth noting that \\eqref{eqn:sattnsvmst} imposes a nonconvex rank constraint, i.e., $\\texttt{rank}(\\W)\\leq m$. Nevertheless, this constraint becomes inconsequential if the unconstrained problem, with $m$ set to be greater than or equal to $d$, admits a low-rank solution, as demonstrated in Lemma \\ref{lem:rank}. Consequently, in our experimental endeavors, we have the flexibility to employ the unconstrained attention SVM for predicting the implicit bias. This concept is succinctly summarized by the following lemma.\n\n\n\\begin{lemma} \nLet $\\Wc^\\svm_\\star$ be the solution set of \\eqref{eqn:sattnsvmst} with nuclear norm achieving objective $C_\\st$. Further let $\\Wcs_{\\texttt{cvx}}$ be the solution set of \\eqref{eqn:sattnsvmst} with $m=d$ achieving objective $C_{\\texttt{cvx}}$. If $\\Wcs_\\st\\cap \\Wcs_{\\texttt{cvx}}\\neq\\emptyset$, then $C_\\st=C_{\\texttt{cvx}}$ and $\\Wcs_\\st\\subseteq \\Wcs_{\\texttt{cvx}}$. Also, if the elements of $\\Wcs_{\\texttt{cvx}}$ have rank at most $m$, then,  $\\Wcs_\\st=\\Wcs_{\\texttt{cvx}}$.  \n\\end{lemma}\n\n\n==== END OF /2308.16898/sections/inductive_bias.tex ====\n==== BEGINNING OF /2308.16898/sections/fig_multi_corrs.tex ====\n\\begin{figure}[t]\n    \\centering\n    \\hspace{-10pt}\n    \\subfigure[$\\lambda$ vs \\# selected tokens]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.22\\columnwidth, trim={1.3cm 1.3cm 0 0}, clip]{figs/multi_n_selected_lambda.pdf}};\n        \\node at (0,-2.) {\\small{$\\lambda$}};\n        \\node[rotate=90] at (-2.65,0) {\\small{\\# of selected tokens}};\n        \\end{tikzpicture}\n        \\label{fig multi ns diff lambda}\n    }\n    \\hspace{-10pt}\n    \\subfigure[$\\lambda$ vs correlation coefficient]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.22\\columnwidth, trim={1.3cm 1.3cm 0 0}, clip]{figs/multi_corr_itr.pdf}};\n        \\node[rotate=90] at (-2.7,0) {\\small{$1-$correlation coefficient}};\n        \\node at (0,-2.){\\small{Iterations}};\n        \\end{tikzpicture}\n        \\label{fig multi corr diff lambda}\n    }\n    \\hspace{-10pt}\n    \\subfigure[\\# selected tokens vs correlation coefficient]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.22\\columnwidth, trim={1.3cm 1.3cm 0 0}, clip]{figs/multi_corr_itr_n_token.pdf}};\n        \\node at (0,-2.) {\\small{Iterations}};\n        \\node[rotate=90] at (-2.65,0) {\\small{$1-$correlation coefficient}};\n        \\node at (1.25,1.4) {\\footnotesize{$\\text{\\# selected}=1$}};\n        \\node at (1.25,1.02) {\\footnotesize{$\\text{\\# selected}=2$}};\n        \\node at (1.25,0.64) {\\footnotesize{$\\text{\\# selected}=5$}};\n        \\node at (1.25,0.26) {\\footnotesize{$\\text{\\# selected}=9$}};\n        \\end{tikzpicture}\n        \\label{fig multi corr diff ns}\n    }\n\\caption{ Behavior of GD when selecting multiple tokens. \\textbf{(a)} The number of selected tokens increases with $\\lambda$. \\textbf{(b)} Predictivity of attention SVM solutions for varying $\\lambda$; Dotted curves depict the correlation corresponding to $\\Ws$ calculated via \\eqref{eqn:mattnsvm} and solid curves represent the correlation to $\\W^\\rfn$, which incorporates the $\\Wf$ correction. \\textbf{(c)} Similar to (b), but evaluating correlations over different numbers of selected tokens.}\n    \\label{fig multi corrs}\n\\end{figure}\n==== END OF /2308.16898/sections/fig_multi_corrs.tex ====\n==== BEGINNING OF /2308.16898/sections/fig_overparam_bar.tex ====\n\\begin{figure}[t]\n    \\centering\n    % \\subfigure[$\\W$-parameterization]{\n    %     \\begin{tikzpicture}\n    %     \\node at (0,0) {\\includegraphics[height=.22\\columnwidth, trim={1.3cm 1.3cm 3.2cm 0}, clip]{figs/overparam_W_bar.pdf}};\n    %     \\node[rotate=90] at (-4,0) {\\small{Probabilities}};\n    %     \\node at (-1,-2.){\\small{$d$}};\n    %     \\node at (2.9,1.4){\\small{Not-local}};\n    %     \\node at (2.9,1.1){\\small{Global}};\n    %     \\node at (2.9,0.75){\\small{Local}};\n    %     \\node at (2.9,0.4){\\small{Asmp B.1}};\n    %     \\end{tikzpicture}\n    %     \\label{fig overparam W bar2}\n    % }\n    \\subfigure[Convergence behaviour of  GD for $\\W$-parameterization]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.23\\columnwidth, trim={1.3cm 1.2cm 3.2cm 0}, clip]{figs/overparam_W_bar_v2.pdf}};\n        \\node[rotate=90] at (-4.,0) {\\small{Percentage}};\n        \\node at (-1,-2.2){\\small{Varying $d$}};\n        \\node at (2.88,1.5){\\footnotesize{Not Local}};\n        \\node at (2.71,1.15){\\footnotesize{Global}};\n        \\node at (2.66,0.8){\\footnotesize{Local}};\n        \\node at (2.95,0.45){\\footnotesize{Assum B.1}};\n        \\end{tikzpicture}\n        \\label{fig overparam W bar all}\n    }\n    \\hspace{-16pt}\n    \\subfigure[Convergence  behaviour of GD for $(\\Kb,\\Qb)$-parameterization]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.23\\columnwidth, trim={1.3cm 1.2cm 3.2cm 0}, clip]{figs/overparam_KQ_bar_v2.pdf}};\n        \\node[rotate=90] at (-4.,0) {\\small{Percentage}};\n        \\node at (-1,-2.2){\\small{Varying $d$}};\n        \\node at (2.88,1.5){\\footnotesize{Not Local}};\n        \\node at (2.71,1.15){\\footnotesize{Global}};\n        \\node at (2.66,0.8){\\footnotesize{Local}};\n        \\node at (2.95,0.45){\\footnotesize{Assum B.1}};\n        \\end{tikzpicture}\n        \\label{fig overparam KQ bar all}\n    }\n    \\caption{Percentage of different convergence types of GD when training cross-attention weights \\textbf{(a)}: $\\W$ or \\textbf{(b)}: $(\\Kb,\\Qb)$ with varying $d$. In both figures, red, blue, and teal bars represent the percentages of Global, Local (including Global), and Not Local convergence, respectively. The green bar corresponds to Assumption \\ref{assum:token:supp} where all tokens act as support vectors. Larger overparameterization ($d$) relates to a higher percentage of globally-optimal SVM convergence.}\n    \\label{fig overparam bar}\n\\end{figure}\n==== END OF /2308.16898/sections/fig_overparam_bar.tex ====\n==== BEGINNING OF /2308.16898/sections/fig_svm_margin.tex ====\n\\begin{figure}[t]\n    \\centering\n    \\hspace{-10pt}\n    \\begin{tikzpicture}\n        \\node at (-2.8,0) {\\includegraphics[height=0.45\\textwidth, trim={0 0 745 0}, clip]{figs/svm_margin.pdf}};\n        \\node at (3.3,0) {\\includegraphics[height=0.45\\textwidth, trim={548 0 0 0}, clip]{figs/svm_margin.pdf}};\n        \\node at (-2.5,0.) {\\small{Varying $n$}};\n        \\node at (2.2,0.) {\\small{Varying $d$}};\n        \\node[rotate=90] at (-5.25, 2) {\\small{SVM margin}};\n        \\node[rotate=90] at (-5.25, -1.8) {\\small{Percentage}};\n\n\n        \\node[right] at (5.2,3.08) {\\small{Global}};\n        \\node[right] at (5.2,2.6) {\\small{Local}};\n\n        \\node at (-2.5,-3.8) {\\small{Varying $n$}};\n        \\node at (2.2,-3.8) {\\small{Varying $d$}};\n        \\node[right] at (5.2,-.77) {\\small{Global}};\n        \\node[right] at (5.2,-1.25) {\\small{Local}};\n        % \\node at (3.7,-1.35) {\\small{$\\Xi_\\bal\\geq\\Xi$}};\n    \\end{tikzpicture}\n    \\caption{Performance of GD convergence and corresponding SVM margin. \\textbf{Upper:} The SVM margins correspond to globally-optimal (red) and locally-optimal (blue) token indices, denoted as $1/\\tf{\\Wm}$ and $1/\\tf{\\Wma}$, respectively. \\textbf{Lower:} Percentages of global convergence (when $\\bal=\\opt$, red) and local convergence (when $\\bal\\neq\\opt$, blue).\n    }\n    \\label{fig svm margin}\n\\end{figure}\n==== END OF /2308.16898/sections/fig_svm_margin.tex ====\n==== BEGINNING OF /2308.16898/sections/conclusion.tex ====\n\\section{Discussion, Future Directions, and Open Problems}\\label{sec:conc}\n\nOur optimization-theoretic characterization of the self-attention model provides a comprehensive understanding of its underlying principles. The developed framework, along with the research presented in \\cite{tarzanagh2023margin}, introduces new avenues for studying transformers and language models. The key findings include:\n\\begin{enumerate}[label=$\\checkmark$, wide, labelwidth=!,itemindent=!, labelindent=1pt]\n\\item The optimization geometry of self-attention exhibits a fascinating connection to  hard-margin SVM problems. By leveraging linear constraints formed through outer products of token pairs, optimal input tokens can be effectively separated from non-optimal ones.\n\\item When gradient descent is employed without early-stopping, implicit regularization and convergence of self-attention naturally occur. This convergence leads to the maximum margin solution when minimizing specific requirements using logistic loss, exp-loss, or other smooth decreasing loss functions. Moreover, this implicit bias is unaffected by the step size, as long as it is sufficiently small for convergence, and  remains independent of the initialization process.\n\\end{enumerate}\nThe fact that gradient descent leads to a maximum margin solution may not be surprising to those who are familiar with the relationship between regularization path and gradient descent in linear and nonlinear neural networks  \\cite{soudry2018implicit,\ngunasekar2018characterizing, nacson2019convergence, ji2021characterizing,moroshko2020implicit,ji2020directional}. However, there is a lack of prior research or discussion regarding this connection to the  attention mechanism. Moreover, there has been no rigorous analysis or investigation into the exactness and independence of this bias with respect to the initialization and step size. Thus, we believe our findings and insights deepen our understanding of transformers and language models, paving the way for further research in this domain. Below, we discuss some notable directions and highlight open problems that are not resolved by the existing theory.\n\\begin{itemize}\n\\item \\textbf{Convergence Rates}: The current paper establishes asymptotic convergence of gradient descent; nonetheless, there is room for further exploration to characterize non-asymptotic convergence rates. Indeed, such an exploration can also provide valuable insights into the choice of learning rate, initialization, and the optimization method. %Furthermore, it could ideally reveal the impact of the ranks of key and query matrices on the process and offer suggestions for improvement.\n\n\\item \\textbf{Gradient descent on $(\\Kb,\\Qb)$ parameterization:} We find it remarkable that regularization path analysis was able to predict the implicit bias of gradient descent. Complete analysis of gradient descent is inherently connected to the fundamental question of low-rank factorization \\cite{gunasekar2017implicit,li2018algorithmic}. We believe formalizing the implicit bias of gradient descent under margin constraints presents an exciting open research direction for further research.\n\n\\item \\textbf{Generalization analysis:} An important direction is the generalization guarantees for gradient-based algorithms. The established connection to hard-margin SVM can facilitate this because the SVM problem is amenable to statistical analysis. This would be akin to how kernel/NTK analysis for deep nets enabled a rich literature on generalization analysis for traditional deep learning.\n\\item \\textbf{Global convergence of gradient descent:} We lack a complete characterization of the directional convergence of gradient descent. We ask: \\emph{Where does gradient descent directionally-converge from arbitrary initialization for 1-layer self-attention?} The role of over-parameterization as conjectured in Section \\ref{sec overparam} and the notion of locally-optimal directions discussed in Section \\ref{sec local} constitute important pieces of this puzzle (also see the discussion in \\cite{tarzanagh2023margin}).\n%Based on empirical evidence in \\cite{tarzanagh2023margin} and discussion in Section \\ref{sec overparam}, we believe that gradient descent will converge to a locally-optimal direction and, with sufficient overparameterization, to the globally-optimal direction.\n\\item \\textbf{Realistic architectures:} Naturally, we wish to explore whether max-margin equivalence can be extended to more realistic settings: Can the theory be expanded to handle multi-head attention, multi-layer architectures, and MLP nonlinearities? We believe the results in Section \\ref{sec:multi} take an important step towards this direction by including analytical formulae for the implicit bias of the attention layer under nonlinear prediction heads. \n%We note that, as a precursor to theory, more extensive empirical evaluations are warranted.\n%\\item \\textbf{Beyond hard-margin SVM:}\n\n\\item \\textbf{Jointly optimizing attention and prediction head:} It would be interesting to study the joint optimization dynamics of attention weights and prediction head $h(\\cdot)$. This problem can be viewed as a novel low-rank factorization type problem where $h(\\cdot)$ and $\\W$ are factors of the optimization problem, only, here, $\\W$ passes through the softmax nonlinearity. To this aim, \\cite{tarzanagh2023margin} provides a preliminary geometric characterization of the implicit bias for a simpler attention model using regularization path analysis. Such findings can potentially be generalized to the analysis of gradient methods and full transformer block.\n\\end{itemize}\n==== END OF /2308.16898/sections/conclusion.tex ====\n==== BEGINNING OF /2308.16898/sections/acknowledgements.tex ====\n\\section*{Acknowledgements}\n\nThis work was supported by the NSF grants CCF-2046816 and CCF-2212426, Google Research Scholar award, and Army Research Office grant W911NF2110312. The authors thank Xuechen Zhang, Ankit Singh Rawat, Mahdi Soltanolkotabi, Jason Lee, Arkadas Ozakin, Ramya Korlakai Vinayak, and Babak Hassibi for helpful suggestions and discussion.\n==== END OF /2308.16898/sections/acknowledgements.tex ====\n==== BEGINNING OF /2308.16898/sections/sa-gd-converge.tex ====\n\\section{Global Convergence of Gradient Descent}\\label{provable global}\n\n\nIn this section, we will establish conditions that guarantee the global convergence of GD. Concretely, we will investigate when GD solution selects the \\emph{optimal token within each input sequence} through the softmax nonlinearity and coincides with the solution of the RP. Section~\\ref{sec local} will complement this with showing that self-attention can more generally converge to locally-optimal max-margin directions. We identify the following conditions as provable catalysts for global convergence: (i) Optimal tokens have relatively large scores; (ii) Initial gradient direction is favorable; (iii) Overparameterization, i.e.~$d$ is appropriately large. \n\n\\subsection{Properties of optimization landscape}\nWe start by establishing some fundamental properties of Objectives \\eqref{eqn:erm:w} and \\eqref{eqn:erm:kq}.% and gradient iterations.\n%\n\\begin{lemma}\\label{lem:lip}\nUnder Assumption~\\ref{assum:loss:prope}, $ \\nabla\\Lc(\\W)$,   $ \\nabla_{\\Kb} \\Lc(\\Kb,\\Qb)$, and  $\\nabla_{\\Qb} \\Lc(\\Kb,\\Qb)$ are $L_{\\W}$,  $L_{\\Kb}$, $L_{\\Qb}$--Lipschitz continuous, respectively, where $a_i=\\|\\vb\\|~\\|\\z_i\\|^2 \\|\\X_i \\|^3$,  $b_i= M_0\\|\\vb\\|~\\|\\X_i\\|+ 3  M_1 $ for all $i\\in[n]$,\n\\begin{align}\\label{eqn:lip:cons:erm}\nL_{\\W}:=\\frac{1}{n}\\sum_{i=1}^{n} a_i b_i, \\quad L_{\\Kb}:= \\|\\Qb\\|L_{\\W}, \\quad \\textnormal{and} \\quad L_{\\Qb}:= \\|\\Kb\\|L_{\\W}.\n\\end{align}\n\\end{lemma}\n\nThe next assumption will play an important role ensuring the attention layer has a benign optimization landscape.\n%\\yl{[Should we remove Assumption B.2 (although it guarantees the divergence of the norm)?]}\n\\begin{assumption}\\label{assum:token}\nOptimal tokens' indices $(\\op_i)_{i=1}^n$ are unique and one of the following conditions on the tokens holds:\n\\begin{enumerate}[label={\\textnormal{{\\textbf{B.\\arabic*}}}}, wide, labelwidth=!,itemindent=!, labelindent=1pt]\n\\item \\label{assum:token:supp} All tokens are support vectors, i.e., $ (\\x_{i\\op_i}-\\x_{it})^\\top\\Ws\\z_i= 1$ for all $t\\neq \\op_i$ and $i\\in[n]$.\n%$Y_i\\cdot\\vb^\\top \\x_{it_1}=Y_i\\cdot\\vb^\\top \\x_{it_2}<Y_i\\cdot\\vb^\\top \\zo_i$\n\\item \\label{assum:opt:token} The tokens' scores, as defined in Definition~\\ref{score def}, satisfy $\\bgam_{it}=\\bgam_{i\\tau}<\\bgam_{i\\op_i}$,\nfor all $t,\\tau\\neq \\op_i$ and $i\\in[n]$.\n\\end{enumerate}\n\\end{assumption}\n% \\redp{Add a version of Figure 2 here with the green bars. Consider adding K,Q figure as well next to it.}\n\n\nAssumption \\ref{assum:token:supp} is directly linked to overparameterization and holds practical significance. In scenarios such as classical SVM classification, where the goal is to separate labels, overparameterization leads to the situation where \\emph{all training points become support vectors}. Consequently, the SVM solution aligns with the least-squares minimum-norm interpolation, a concept established in \\cite{muthukumar2021classification, hsu2021proliferation} under broad statistical contexts. Assumption \\ref{assum:token:supp} represents an analogous manifestation of this condition. Therefore, in cases involving realistic data distributions with sufficiently large $d$, we expect the same phenomena to persist, causing the SVM solution $\\Wm$ to coincide with \\eqref{eqn:sattnsvm}. \n\nDrawing on insights from \\cite[Theorem 1]{hsu2021proliferation} and our  Theorem \\ref{thm:separation}, we expect that the necessary degree of overparameterization remains moderate. Specifically, in instances where input sequences follow an independent and identically distributed (IID) pattern and tokens exhibit IID isotropic distributions, we posit that $d\\gtrsim (T+n)\\log(T+n)$ will suffice. More generally, the extent of required overparameterization will be contingent on the covariance of tokens \\cite{bartlett2020benign, muthukumar2021classification} and the distribution characteristics of input sequences \\cite{wang2022binary}. \n\nAssumption \\ref{assum:opt:token} stipulates that non-optimal tokens possess identical scores which constitutes a relatively stringent assumption that we will subsequently relax. Under Assumption~\\ref{assum:token}, we establish that when optimization problem \\eqref{eqn:erm:w} is trained using GD, the norm of parameters will diverge. \n% \\redp{Add a remark that under Assumption B.1, solution is just least-squares.}\n% Assumption \\ref{assum:token:supp} is directly linked to overparameterization and is practically meaningful. In settings like the classical SVM classification, where the goal is to separate labels, overparameterization results in \\emph{all training points being support vectors}, and the SVM solution coincides with the least-squares minimum-norm interpolation, as known from \\cite{muthukumar2021classification,hsu2021proliferation} under general statistical settings. Our Assumption \\ref{assum:token:supp} is an equivalent version of this condition, thus, under realistic data distributions with suitably large $d$, we expect same phenomena to hold and the SVM solution $\\Wm$ to coincide with \\eqref{eqn:sattnsvm}.  \n% Extrapolating from \\cite[Theorem 1]{hsu2021proliferation} and our Theorem \\ref{thm:separation}, we expect the required overparameterization to be mild: Concretely, when input sequences are IID and tokens have IID isotropic distributions, we suspect that $d\\gtrsim (T+n)\\log(T+n)$ will suffice. More generally, the level of overparameterization required will depend on the covariance of the tokens \\cite{bartlett2020benign,muthukumar2021classification} and the distribution of the input sequences \\cite{wang2022binary}. Assumption \\ref{assum:opt:token} requires non-optimal tokens to have identical scores. This is a relatively strong assumption which will later be relaxed.  Assuming \\ref{assum:token}, we establish that optimization problems \\eqref{eqn:erm:w} and \\eqref{eqn:erm:kq} lack stationary points, and when trained using GD, the norm of parameters will diverge.\n\n\\begin{theorem}%[\\red{Global Convergence of GD}]\n\\label{diverg:norm:w}%\\redp{TO FIX}\nSuppose Assumption~\\ref{assum:loss:prope} on the loss function $\\ell$ and Assumption \\ref{assum:token} on the tokens hold.  \n\n\\begin{itemize}\n\\item %\\textbf{No stationary points:}\nThere is no $\\W\\in\\R^{d\\times d}$ satisfying $\\nabla \\Lc(\\W)=0$.\n    \\item  Algorithm~\\ref{GD-W} with the step size $\\eta \\leq 1 /L_{\\W}$ and any starting point $\\W(0)$ satisfies %$\\lim_{k \\rightarrow \\infty}\n%\\left\\|\\nabla\\mathcal{L}\\left(\\W\\left(k\\right)\\right)\\right\\|_F=0$, and  \n$\\lim_{k\\rightarrow\\infty} \\tf{\\W(k)}=\\infty$.\n%\\item %\\red{(Informal, see Thm \\ref{diverg:norm:qk})} \n%Algorithm~\\ref{GD-QK}, with a proper stepsize schedule and a non-degenerate starting point $(\\Kb(0), \\Qb(0))$, obeys $\\lim_{k \\rightarrow \\infty} \\tf{\\nabla \\Lc_\\Kb(\\Kb(k))}\\vee\\tf{\\nabla\\Lc_\\Qb(\\Qb(k))}=0$, and  $\\lim_{k\\rightarrow\\infty} \\tf{\\Kb(k)}\\wedge\\tf{\\Qb(k)}=\\infty$.\n% \\item Algorithm~\\ref{GD-QK} with the step size $\\eta \\leq \\mc{O}(1/ \\max(L_\\Qb,L_\\Kb))$ \\redp{Problem: $L_\\Qb,L_\\Kb$ depends on $\\|\\Qb\\|,\\|\\Kb\\|$ which goes to $\\infty$ so $\\eta\\rightarrow 0$} and any starting point $(\\Kb(0), \\Qb(0))$ satisfies  $\\lim_{k \\rightarrow \\infty}\n% \\left\\|\\nabla \\Lc(\\Kb(k), \\Qb(k))\\right\\|_F=0$, and  $\\lim_{k\\rightarrow\\infty} \\|\\left( \\Kb(k), \\Qb(k)\\right)\\|_F=\\infty$.\n\\end{itemize}\n\\end{theorem}\nThe feasibility of SVM (per Theorem \\ref{thm:separation}) is a necessary condition for the convergence of GD to the $\\Wm$ direction. However, it does not inform us about the optimization landscape. Two additional criteria are essential for convergence: the absence of stationary points $\\nabla\\Lc(\\W)=0$ and divergence of parameter norm to infinity. Theorem \\ref{diverg:norm:w} above precisely guarantees both of these criteria under Assumption \\ref{assum:token}. %adaptive to the norms of $\\Qb(k),\\Kb(k)$ \n%\n\\input{sections/fig_overparam_bar}\n%\n\\subsection{Provable global convergence of 1-layer transformer}\nIn the quest for understanding the global convergence of a 1-layer transformer,  \\cite[Theorem 2]{tarzanagh2023margin} provided the first global convergence analysis of the attention in a restrictive scenario where $n=1$ and under the assumption \\ref{assum:opt:token}. Here, we present two new conditions for achieving global convergence towards the max-margin direction $\\Wm$ based on: \\textbf{(I)} the initial gradient direction, and \\textbf{(II)} over-parameterization. For the first case, we provide precise theoretical guarantees. For the second, we offer strong empirical evidence, supported by Theorem \\ref{diverg:norm:w}, and a formal conjecture described in Section \\ref{sec overparam}. We remind the reader that we optimize the attention weights $\\W$ while fixing the linear prediction head $h(\\x)=\\vb^\\top\\x$. This approach avoids trivial convergence guarantees where an over-parameterized $h(\\cdot)$whether it is a linear model or an MLPcan be used to achieve zero training loss without providing any meaningful insights into the functionality of the attention mechanism.\n%\n%\n% \\paragraph{(I) Global convergence under score constraints.}  Our next result establishes the global convergence of GD to the max-margin direction $\\Wm$ under Assumption \\ref{assum:opt:token} that non-optimal tokens have identical scores but lower than the score of the optimal token.\n%\n% \\begin{theorem}\\label{conv:gd:kq:global}\n% Suppose Assumption~\\ref{assum:loss:prope} on the loss $\\ell$ and Assumption \\ref{assum:opt:token} on the tokens' score hold. Then,  Algorithm~\\ref{GD-W} with $\\eta \\leq 1 /L_{\\W}$ and any starting point $\\W(0)$ satisfies $\\lim_{k\\rightarrow\\infty}\\frac{\\W(k)}{\\tf{\\W(k)}}=\\frac{\\Wm}{\\tf{\\Wm}}$.\n% \\end{theorem}\n%\n% To proceed, we also provide a relaxation of Assumption \\ref{assum:opt:token} and, show that, global convergence still happens under approximately equal scores. \n% \\begin{theorem}\\label{thm:non-equal-score main}\n% Suppose Assumption~\\ref{assum:loss:prope} holds. For any initialization $\\W(0)$, there exists a dataset-dependent sufficiently small $\\delta>0$ such that the following holds: Suppose non-optimal scores obey $|\\bgam_{it} - \\bgam_{i\\tau}| \\leq \\delta$ for all $t,\\tau \\neq \\op_i,~i \\in [n]$. Then, Algorithm~\\ref{GD-W}, with  $\\eta \\leq 1 /(2L_{\\W})$ obeys $\\lim_{k\\rightarrow\\infty} \\tf{\\W(k)}=\\infty$ and $\\lim_{k\\rightarrow\\infty}\\frac{\\W(k)}{\\tf{\\W(k)}}=\\frac{\\Wm}{\\tf{\\Wm}}$.\n% \\end{theorem}\n\n\\paragraph{(I) Global convergence under good initial gradient.} To ensure global convergence, we identify an assumption that prevents GD from getting trapped at suboptimal tokens that offer no scoring advantage compared to other choices. To establish a foundation for providing the convergence of GD to the globally optimal solution $\\Ws$, we need the following definitions.  For parameters $\\mu >0$ and $R>0$, define\n%\\begin{subequations}\n\\begin{align}\\label{eqn:con:nabla0:main}\n\\conb_{\\mu,R}:=\\Bigg\\{ \\tf{\\W}\\geq R~\\Big|~  \\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\frac{\\W}{\\tf{\\W}}\\ri\\geq \\mu \\quad \\textnormal{for all}\\quad t\\neq \\op_i,~  i\\in[n]\\Bigg\\}.\n\\end{align}\nThis is the set of all $\\W$s that separate the optimal tokens from the rest with margin $\\mu$. We will show that, for any $\\mu>0$, the optimization landscape of this set is favorable and, if the updates remain in the set, the gradient descent will maximize the margin and find $\\Wm$.\n\n\\begin{assumption}[First GD Step is Separating]\\label{assum:nabla0} For some $\\iota>0$ and all $t\\neq \\op_i,~i\\in[n]$: $ (\\x_{it}-\\x_{i\\op_i})^\\top\\nabla\\Lc(0)\\z_i\\geq \\iota$.\n\\end{assumption}\n%}\n\n\\begin{theorem}\\label{conv:gd:w:global:nabla0}\nSuppose Assumption~\\ref{assum:loss:prope} on the loss function $\\ell$ and Assumption \\ref{assum:nabla0} on the initial gradient hold. \n%$\\mu \\in  (0,\\min (0.5,\\iota \\tf{\\Ws}/\\tf{\\nabla \\Lc(0)})$\n\\begin{itemize}\n%\\item %\\textbf{No stationary points:}\n%There is no $\\W\\in\\R^{d\\times d}$ satisfying $\\nabla \\Lc(\\W)=0$.\n\\item  For any $\\mu>0$, there exists  $R>0$ such  that   $\\conb_{\\mu,R}$ does not contain any  stationary points. \n\\item Fix any $\\mu \\in  (0, \\iota/\\tf{\\nabla \\Lc(0)})$. Consider GD iterations with $\\W(0)=0$, $\\W(1)=-R\\nabla\\,\\Lc(0)/\\tf{\\nabla\\Lc(0)}$, and $\\W(k+1)=\\W(k)-\\eta\\nabla\\Lc(\\W(k))$ for $k\\ge 1$, where $\\eta\\le 1/L_{\\W}$ and $R$ sufficiently large. If all iterates remain within $\\conb_{\\mu,R}$, then $\\lim_{k\\rightarrow\\infty} \\tf{\\W(k)}=\\infty$ and $\\lim_{k\\rightarrow\\infty}\\frac{\\W(k)}{\\tf{\\W(k)}}=\\frac{\\Wm}{\\tf{\\Wm}}$.\n\\end{itemize}\n\\end{theorem}\nNote that the second result of Theorem~\\ref{conv:gd:w:global:nabla0}, i.e.,\nthe divergence of parameter norm to infinity and directional convergence requires that all GD iterations remain within $\\conb_{\\mu,R}$ defined in \\eqref{eqn:con:nabla0:main}. In Appendix~\\ref{app B4}, we show that if for all $\\W \\in \\conb_{\\mu,R}(\\Ws)$, $\\min_{i \\in [n]}\\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\W-\\eta\\nabla\\Lc(\\W) \\ri-\\min_{i \\in [n]}\\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\W \\ri$ is lower bounded by $(2\\eta\\mu/\\tf{\\Wm})\\iprod{-\\nabla\\mc{L}(\\W)}{\\Wm}$, then all GD iterations $\\W(k)$ remain within $\\conb_{\\mu,R}$. While this condition may appear complicated, it is essentially a tight requirement for updates to remain within $\\conb_{\\mu,R}$. Finally, it is worth mentioning that, if a stronger correlation condition between initial gradient $\\nabla\\Lc(0)$ and $\\Wm$ holds, one can also prove that updates remain within a tighter cone around $\\Wm$ through ideas developed in Theorem \\ref{thm:local:gd} by landing $\\W(1)$ around $\\Wm$ direction. However, we opt to state here the result for the milder condition  $\\conb_{\\mu,R}$. \n\n%\\texttt{improve}_\\eta:=\n%Specifically, since our choice of $\\mu>0$ is arbitrary, and $\\eta > 0$ is a positive constant, this condition holds for sufficiently small values of both $\\mu$ and $\\tf{\\nabla \\Lc(\\W(k))}$.\n%may appear to be a strong assumption, but we note that $\\tf{\\nabla \\Lc(\\W(k))}\\rightarrow 0$, as shown in Lemma~\\ref{lem:grad:descent}. \n% {\\color{blue} Theorem\n%  \\red{0.5 is not needed for the first item. Just tried to simplify theorem}\n% Let $D_\\eta(\\W):=(\\W-\\eta\\nabla \\Lc(\\W))/\\tf{\\W}$.  If for all $\\W \\in \\conb_{\\mu,R}(\\Ws)$,   $  \\min_{i \\in [n]}\\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, D_\\eta(\\W)\\ri > \\mu /\\tf{\\Ws}$, then we have  $\\lim_{k\\rightarrow\\infty}\\frac{\\W(k)}{\\tf{\\W(k)}}=\\frac{\\Wm}{\\tf{\\Wm}}$.\n%  \\begin{align}\\label{assum:extra}\n%     \\min_{i \\in [n]}\\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, D_\\eta(\\W) \\ri \\geq     \\min_{i \\in [n]}\\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\frac{\\W}{\\tf{\\W}}\\ri + \\bar{\\mu} \\eta \\iprod{-\\nabla\\mc{L}(\\W)}{\\frac{\\Wm}{\\tf{\\Wm}}}.\n%  \\end{align}\n% }\n\n \\input{sections/fig_overparam}\n%\n\n\n\\paragraph{(II) Global convergence via overparameterization.} \nIn the context of standard neural networks, overparameterization has been recognized as a pivotal factor for the global convergence of GD \\cite{du2018gradient,allen2019convergence,li2018learning,oymak2019overparameterized}. However, conventional approaches like the neural tangent kernel \\cite{jacot2018neural} do not seamlessly apply to our scenario, given our assumption on fixed $h$ and the avoidance of achieving zero loss by trivially fitting $h$. Furthermore, even when we train $h$ to achieve zero loss, it doesn't provide substantial insights into the implicit bias of the attention weights $\\W$. Conversely, Theorem \\ref{diverg:norm:w} illustrates the benefits of over-parameterization in terms of convergence. Considering that Assumption \\ref{assum:token:supp} is anticipated to hold as the dimension $d$ increases, the norm of the GD solution is bound to diverge to infinity. This satisfies a prerequisite for converging towards the globally-optimal SVM direction $\\Ws$. \n\n\n\nThe trend depicted in Figure \\ref{fig overparam bar}, where the percentage of global convergence (red bars) approaches $100\\%$ and Assumption \\ref{assum:token:supp} holds with higher probability (green bars) as $d$ grows, reinforces this insight. Specifically, Fig.~\\ref{fig overparam W bar all} is similar to Figure~\\ref{fig overparam W bar} but with additional green bars representing the percentage of the scenarios where almost all tokens act as support vectors (Assumption~\\ref{assum:token:supp}), and Fig.~\\ref{fig overparam KQ bar all} displays the same evaluation over $(\\Kb,\\Qb)$-parameterization setting. In both experiments, and for each chosen $d$ value, a total of $500$ random instances are conducted under the conditions of $n=T=5$. The outcomes are reported in terms of the percentages of Not Local, Local, and Global convergence, represented by the teal, blue, and red bars, respectively. We validate Assumption~\\ref{assum:token:supp} as follows: Given a problem instance, we compute the average margin over all non-optimal tokens of all inputs and declare that problem satisfies Assumption~\\ref{assum:token:supp}, if the average margin is below 1.1 (where 1 is the minimum). %\\red{Here, recall that margin of a non-optimal token is defined as $(\\x_{i\\op_i}-\\x_{it})^\\top\\Ws\\z_i$ or $(\\x_{i\\op_i}-\\x_{it})^\\top\\Ws_\\st\\z_i$ for $t\\neq \\op_i$.}\n%This corresponds to a slight relaxation of the exact assumption and aims to account for numerical/convergence issues.\n\n%is computed, and the percentage of instances for which the equality constraint violation, denoted as $ (\\x_{i\\op_i}-\\x_{it})^\\top\\Ws\\z_i-1$, falls below $0.1$, is displayed as green bars.\n%\n% $\\frac{1}{n(T-1)}\\sum_{i\\in[n],t\\neq\\op_i}(\\x_{i\\op_i}-\\x_{it})\\Ws\\z_i<1.1$ \n%\n\nFurthermore, the observations in Figure \\ref{fig overparam} regarding the percentages of achieving global convergence reaching 100 with larger $d$ reaffirm that overparameterization leads the attention weights to converge directionally towards the optimal max-margin direction outlined by \\eqref{eqn:sattnsvm} and \\eqref{eqn:sattnsvmst}. \n\nIn the upcoming section, we will introduce locally-optimal directions, to which GD can be proven to converge when appropriately initialized. We will then establish a condition that ensures the \\emph{globally-optimal direction is the sole viable locally-optimal direction}. This culmination will result in a formal conjecture detailed in Section \\ref{sec overparam}.\n\n% For standard neural nets, overparameterization is known to be a key ingredient for global convergence of GD \\cite{du2018gradient,allen2019convergence,li2018learning,oymak2019overparameterized}. However, standard approaches, such as neural tangent kernel \\cite{jacot2018neural}, don't extend to our setting as we fix $h$ and avoid achieving zero loss by trivially fitting $h$ only. Additionally, even if we train $h$ and achieve zero loss, it still wouldn't really inform us regarding the implicit bias of the attention weights $\\W$. On the other hand, Theorem \\ref{diverg:norm:w} demonstrates the convergence benefits of over-parameterization: Recall that Assumption \\ref{assum:token:supp} is expected to hold as $d$ grows. Then, the norm of the GD solution is guaranteed to diverge to infinity. This fulfills a prerequisite for the convergence towards the globally-optimal SVM direction $\\Ws$. Figure \\ref{fig overparam W bar} demonstrates that, as $d$ grows, the percentage of global convergence (red bars) indeed approaches to $100\\%$ and Assumption~\\ref{assum:token:supp} holds with higher probability (green bars), which support this intuition. Additionally, in Figure~\\ref{fig overparam}, we study the convergence behavior of GD under different $(n,T,d)$ settings. The phenomenon that probabilities of global convergence achieve $1$ with larger $d$ again validates that under over-parameterization, the attention weights directionally converge toward the optimal max-margin direction given by \\eqref{eqn:sattnsvm} and \\eqref{eqn:sattnsvmst}. In the next section, we will introduce locally-optimal directions to which GD can provably converge if properly initialized. \n% We will then identify a condition which ensures that the \\emph{globally-optimal direction is the only viable locally-optimal direction}. This will culminate in a formal conjecture in Section \\ref{sec overparam}.\n\n\n==== END OF /2308.16898/sections/sa-gd-converge.tex ====\n==== BEGINNING OF /2308.16898/sections/exp_report.tex ====\n\\section{Experiments}\n\n\\begin{figure}[t]\n\\centering\n\\hspace{-10pt}\n\\subfigure[Gradient path]{\n    \\includegraphics[width=.33\\columnwidth]{figs/GD_path.pdf}\n    \\label{fig:path}\n    \n}%\\caption{Global convergence}\n\\hspace{-10pt}\n\\subfigure[Correlations when training $\\W$]{\n    \\includegraphics[width=.33\\columnwidth]{figs/corr_W.pdf}\n    \\label{fig:corr W}\n    \n}%\\caption{Convergence to global and local optima}\n\\hspace{-10pt}\n\\subfigure[Correlations when training $\\Kb,\\Qb$]{\n    \\includegraphics[width=.33\\columnwidth]{figs/corr_KQ.pdf}\n    \\label{fig:corr KQ}\n}\n\\vspace{-2mm}\n\\caption{The convergence behavior of the gradient descent on the attention weights $\\W$ or $(\\Kb,\\Qb)$ using the logistic loss. \n% (a) Let $\\W$ The arrows ({\\color{gray} \\footnotesize{--->---}}) represent trajectories from different initializations. Here, ({\\color{red}-~-~-}) and ({\\color{blue}-~-~-}) denote the \\textbf{g}lobally- and \\textbf{l}ocally-optimal \\textbf{m}ax-\\textbf{m}argin directions (\\GM, \\LM). $\\gamma$ denotes the \\emph{score} of a token per Definition \\ref{score def}. \n}\n\\label{fig:main_fig}\\vspace{-5pt}\n\\end{figure}\n\\begin{figure}[t]\n\\centering\n\\hspace{-10pt}\n\\subfigure[Gradient path]{\n    \\includegraphics[width=.33\\columnwidth]{figs/GD_path_1.pdf}\n    \\label{fig:path 1}\n    \n}%\\caption{Global convergence}\n\\hspace{-10pt}\n\\subfigure[Correlations when training $\\W$]{\n    \\includegraphics[width=.33\\columnwidth]{figs/corr_W_1.pdf}\n    \\label{fig:corr W 1}\n    \n}%\\caption{Convergence to global and local optima}\n\\hspace{-10pt}\n\\subfigure[Correlations when training $\\Kb,\\Qb$]{\n    \\includegraphics[width=.33\\columnwidth]{figs/corr_KQ_1.pdf}\n    \\label{fig:corr KQ 1}\n}\n\\vspace{-2mm}\n\n\\hspace{-10pt}\n\\subfigure[Gradient path]{\n    \\includegraphics[width=.33\\columnwidth]{figs/GD_path_0.pdf}\n    \\label{fig:path 0}\n    \n}%\\caption{Global convergence}\n\\hspace{-10pt}\n\\subfigure[Correlations when training $\\W$]{\n    \\includegraphics[width=.33\\columnwidth]{figs/corr_W_0.pdf}\n    \\label{fig:corr W 0}\n    \n}%\\caption{Convergence to global and local optima}\n\\hspace{-10pt}\n\\subfigure[Correlations when training $\\Kb,\\Qb$]{\n    \\includegraphics[width=.33\\columnwidth]{figs/corr_KQ_0.pdf}\n    \\label{fig:corr KQ 0}\n}\n\\vspace{-2mm}\n\\caption{Other results when using different $\\z_i$s.\n}\n\\label{fig:main_fig 01}\\vspace{-5pt}\n\\end{figure}\n\\begin{itemize}\n    \\item Set $n=d=2$, $T=3$. $\\x_{it}$ are randomly sampled from unit sphere, and $\\vb=[0~1]^\\top$ and labels $Y_i$ are all ones. \n    \\item Figure~\\ref{fig:main_fig} shows the convergence results when training with attention weights $\\W$ or $(\\Kb,\\Qb)$. For simplicity, let $\\tilde{\\W}=\\Qb\\Kb^\\top$. To visualize the gradient evolution, we focus on $\\W\\z_i$ and $\\tilde\\W\\z_i$ for $i\\in[n]$, so that it will clearly show how $\\W\\z_i$ ($\\tilde\\W\\z_i$) converges in direction to the max-margin direction within each input. Fig.~\\ref{fig:path}: Cyan and green markers represent different inputs and the stars are the optimal token within each input. Solid lines (red and blue) represent $\\Ws_F\\z_1$ and $\\Ws_F\\z_2$ and dashed lines represent $\\Ws_\\star\\z_1$ and $\\Ws_\\star\\z_2$ (we are overlaping). We also plot the trajectories $\\W(t)\\z_i$ and $\\tilde\\W(t)\\z_i$, $i\\in\\{1,2\\}$ in solid and dashed arrows. \n    \\item Fig.~\\ref{fig:corr W} and \\ref{fig:corr KQ}: Define the correlation of two matrices $\\W_1,\\W_2\\in\\R^{d\\times d}$ as follows: \n    \\[\n    \\text{corr($\\W_1,\\W_2$)}=1-\\left\\|\\frac{\\W_1}{\\|\\W_1\\|_F}-\\frac{\\W_2}{\\|\\W_2\\|_F}\\right\\|_F^2.\n    \\]\n    Then in Fig.~\\ref{fig:corr W}, blue and orange curves show evolutions of corr($\\W(t),\\Ws_F$) and corr($\\W(t),\\Ws_\\star$), respectively, and in Fig.~\\ref{fig:corr KQ}, blue and orange curves represent corr($\\tilde\\W(t),\\Ws_F$) and corr($\\tilde\\W(t),\\Ws_\\star$).\n    \\item \\textbf{Random Data Experiments}: Set $n=T=3$ and $d=4$. $\\x_{it}$ and $\\vb$ are randomly sampled from unit sphere, and $Y_i$ are uniformly $\\pm1$. Conduct $100$ random and successful trials ensuring that 1) attention succeeds in selecting one token per input, $\\max_{t\\in[T]}\\sft{\\X_i\\W\\z_i}_t=1$; 2) problems are separable and \\eqref{eqn:sattnsvm} are feasible. Then over the $100$ trials, the averaged correlations are\n    \\[\n    [\\text{corr($\\W,\\Ws_F$)}~~\\text{corr($\\W,\\Ws_\\star$)}]=[0.997~~ 0.726],\n    \\]\n    \\[\n    [\\text{corr($\\tilde\\W,\\Ws_F$)}~~\\text{corr($\\tilde\\W,\\Ws_\\star$)}]=[0.749~~0.898].\n    \\]\n    If setting $\\tilde\\W(0)=\\Ws_\\star$, then\n    \\[\n    [\\text{corr($\\tilde\\W,\\Ws_F$)}~~\\text{corr($\\tilde\\W,\\Ws_\\star$)}]=[0.769~~0.982].\n    \\]\n    \\item \\textbf{Random Data Experiments v2}: Set $n=4,T=4,d=20$ and train with cross attention where $\\z_{i}$'s are also randomly sampled from unit sphere. Results averaged form $100$ trials are\n    \\[\n    \\text{Frob:}~[\\text{corr($\\tilde\\W,\\Ws_F$)}~~\\text{corr($\\tilde\\W,\\Ws_\\star$)}]=[0.996~~0.607].\n    \\]\n    \\[\n    \\text{Nuc:}~~[\\text{corr($\\tilde\\W,\\Ws_F$)}~~\\text{corr($\\tilde\\W,\\Ws_\\star$)}]=[0.692~~0.840].\n    \\]\n    \\begin{enumerate}\n        \\item After setting $\\Kb,\\Qb\\in\\R^{d\\times m}$ to be low rank with  $m=rank(\\Wso)$, we obtain\n        \\[\n    \\text{Nuc:}~~[\\text{corr($\\tilde\\W,\\Ws_F$)}~~\\text{corr($\\tilde\\W,\\Ws_\\star$)}]=[0.695~~0.837].\n    \\]\n    \\item After setting $\\Kb(0)=\\Ub\\bSi^{1/2}+0.1\\cdot\\Nc(0,\\Iden)$ and $\\Qb(0)=\\Vb\\bSi^{1/2}+0.1\\cdot\\Nc(0,\\Iden)$ where $\\Ub\\bSi\\Vb=\\Wso/\\|\\Wso\\|$, we obtain\n    \\[\n    \\text{Nuc:}~~[\\text{corr($\\tilde\\W,\\Ws_F$)}~~\\text{corr($\\tilde\\W,\\Ws_\\star$)}]=[0.634~~0.968].\n    \\]\n    \\item Combining low rank and initialization together, we obtain\n    \\[\n    \\text{Nuc:}~~[\\text{corr($\\tilde\\W,\\Ws_F$)}~~\\text{corr($\\tilde\\W,\\Ws_\\star$)}]=[0.598~~0.979].\n    \\]\n    \\end{enumerate}\n\n\\end{itemize}\n\n\\begin{figure}[t]\n\\centering\n\\hspace{-10pt}\n    \\includegraphics[width=.5\\columnwidth]{figs/diff_nT.pdf}\n    \\label{fig:diff nT}\n    \n\n\\caption{The probabilities of global convergence. We train attention weight $\\W$ with different $(n,T,d)$ choices, and examine the probabilities of global convergence under each setting. Here, $\\W$ globally converges in direction to $\\Wso$ only if $\\arg\\max_{t\\in[T]}\\sft{\\X_i\\W\\z_{i}}_t=\\op_i$ for all $i\\in[n]$.\n}\n\\label{fig:main_fig}\\vspace{-5pt}\n\\end{figure}\n==== END OF /2308.16898/sections/exp_report.tex ====\n==== BEGINNING OF /2308.16898/sections/sa-local-gd.tex ====\n\\section{Understanding Local Convergence of 1-Layer Transformer}\\label{sec local}\n\nSo far, we have primarily focused on the convergence to the global direction dictated by \\eqref{eqn:sattnsvm}. In this section, we investigate and establish the local directional convergence of GD as well as RP.\n%\n\\input{sections/fig_general}\n%\n\\subsection{Local convergence of gradient descent}\\label{local GD sec}\n\nTo proceed, we introduce locally-optimal directions by adapting Definition 2 of \\cite{tarzanagh2023margin}. \n\n\\begin{definition}[\\NEIS and Locally-Optimal Direction]\\label{def loc opt} \nFix token indices $\\bal=(\\alpha_i)_{i=1}^n$. Solve \\eqref{eqn:sattnsvm} with $ (\\opt_i)_{i=1}^n$ replaced with $\\boldsymbol{\\alpha} = (\\alpha_i)_{i=1}^n$ to obtain $\\Wma$. Consider the set $\\Tc_i\\subset[T]$ such that $(\\x_{i\\alpha_i}-\\x_{it})^\\top \\Wma \\z_i=1$ for all $t\\in\\Tc_i$. We refer to $(\\Tc_i)_{i=1}^n$ as the \\neis of $\\bal$. Additionally, if for all $i\\in[n]$ and $t\\in\\Tc_i$ scores per Definition~\\ref{score def} obey $\\bgam_{i\\alpha_i}>\\bgam_{it}$, indices $\\bal=(\\alpha_i)_{i=1}^n$ are called \\emph{locally-optimal} and $\\Wma$ is called a \\emph{locally-optimal direction}.\n\\end{definition}\n\nIn words,  the concept of local optimality requires that the selected tokens denoted as $\\bal$ should have scores that are higher than the scores of their neighboring tokens referred to as \\neis. It is important to observe that the tokens defined as $\\op=(\\op_i)_{i=1}^n$, which we term as the optimal tokens, inherently satisfy the condition of local optimality. Moving forward, we will provide Theorem $\\ref{thm:local:gd}$ which establishes that when the process of GD is initiated along a direction that is locally optimal, it gradually converges in that particular direction, eventually aligning itself with $\\Wma$. This theorem immediately underscores the fact that if there exists a direction of local optimality (apart from the globally optimal direction $\\Wm$), then when GD commences from any arbitrary starting point, it does not achieve global convergence towards $\\Wm$. \n\n% \\red{In the context of Theorem ?% $\\ref{conv:gd:kq:global}$ \n% , this insight further demonstrates the general necessity of Assumption B.2 [in <a href=\"https://arxiv.org/pdf/2308.16898#Item.5\">original paper</a>]. This assumption is pivotal because without it, scenarios in which token scores are not equal might potentially lead to the emergence of directions that are locally optimal (as specified in Definition $\\ref{def loc opt}$), thereby impeding the possibility of achieving global convergence.}\n\n\nTo provide a basis for discussing local convergence of GD, we establish a cone centered around $\\Wma$ \nusing the following construction. For parameters $\\mu \\in (0,1)$ and $R>0$, we define $\\Cc_{\\mu,R}(\\Wma)$ as the set of matrices $\\W \\in\\R^{d\\times d}$ such that $\\tf{\\W}\\geq R$ and  the correlation coefficient between $\\W$ and $\\Wma$ is at least $1-\\mu$:\n%\n%\n%\\begin{subequations}\n\\begin{align}\\label{eqn:coneofw:r:main}\n% \\Sc_{\\mu}(\\Wma)&:= \\left\\{\\W\\in\\R^{d\\times d}~:~\\left\\langle\\frac{\\W}{\\tf{\\W}},\\frac{{\\Wma}}{\\tf{{\\Wma}}} \\right\\rangle \\geq 1-\\mu\\right\\}, \\\\\n% \\Cc_{\\mu,R}({\\Wma})&:= \\Sc_{\\mu}(\\Wma) \\cap  \\left\\{\\W\\in\\R^{d\\times d}~:~\\tf{\\W}\\geq R\\right\\}.\n\\Cc_{\\mu,R}({\\Wma})=\\left\\{ \\tf{\\W}\\geq R~\\Big|~  \\left\\langle \\frac{\\W}{\\tf{\\W}},\\frac{\\Wma}{\\tf{{\\Wma}}} \\right\\rangle \\geq 1-\\mu \\right\\}.\n\\end{align}\n%\\end{subequations}\n% \\begin{align}\n% \\Cc_{\\mu,R}(\\bar{\\V}):= \\left\\{\\W\\in\\R^{d\\times d}~:~\\left\\langle\\frac{\\W}{\\tf{\\W}},\\frac{\\bar{\\V}}{\\tf{\\bar{\\V}}} \\right\\rangle \\geq 1-\\mu,~\\tf{\\W}\\geq R\\right\\}.\n% \\end{align}\n%\n%\n\n\\begin{theorem}%[Local Convergence of Gradient Descent]\n\\label{thm:local:gd} \nSuppose Assumption~\\ref{assum:loss:prope} on the loss $\\ell$ holds, and let $\\bal=(\\alpha_i)_{i=1}^n$ be locally optimal tokens according to Definition \\ref{def loc opt}. Let $ \\Wma$ denote the SVM solution obtained via \\eqref{eqn:sattnsvm} by  replacing $(\\opt_i)_{i=1}^n$ with $\\boldsymbol{\\alpha} = (\\alpha_i)_{i=1}^n$. \n% \\begin{enumerate}[label={\\textnormal{\\textbf{T\\arabic*.}}}, wide, labelwidth=!,itemindent=!, labelindent=5pt]\n\\begin{itemize}\n    \\item \\label{lem:cond:t1}  There exist parameters $\\mu=\\mu(\\bal) \\in (0,1)$ and  $R>0$ such  that   $ \\Cc_{\\mu,R} (\\Wma)$ does not contain any  stationary points.\n    \\item  Algorithm~\\ref{GD-W} with $\\eta \\leq 1 /L_{\\W}$ and any $\\W(0) \\in \\Cc_{\\mu,R}(\\Wma)$ satisfies $\\lim_{k\\rightarrow\\infty} \\tf{\\W(k)} = \\infty$  and $\\lim_{k\\rightarrow\\infty} \\frac{\\W(k)}{\\tf{\\W(k)}} = \\frac{\\Wma}{\\tf{\\Wma}}$.\n\\end{itemize}\n\\end{theorem}    \nThis theorem establishes the existence of positive parameters $\\mu=\\mu(\\bal)>0$ and $R>0$ such that there are no stationary points within  $\\Cc_{\\mu,R}(\\Wma)$. Furthermore, if GD is initiated within $\\Cc_{\\mu,R}(\\Wma)$, it will converge in the direction of $\\Wma/\\tf{\\Wma}$. It is worth mentioning that stronger Theorem \\ref{diverg:norm:w} (e.g. global absence of stationary points) is applicable whenever all tokens are support i.e.~$\\Tcb_i=\\emptyset$ for all $i\\in[n]$. %Importantly, Theorem \\ref{thm:local:gd} does not make any assumptions on the tokens as opposed to Theorem \\ref{conv:gd:kq:global}. However, it does require a proper initialization that exhibits a correlation between the initial direction and $\\Wma$. Additionally, \n%We observe that the stepsize is adjusted based on the Lipschitz constant of the gradient of the objective function.\n%,  and the correlation of the initial direction with $\\Wma$, which is parameterized by $\\mu$. %\\red{the correlation of the negative gradient at $\\W(k)$ and $\\Wma$, parameterized by  $\\rho$.}\n\n\n\nIn Figure~\\ref{fig general}, we consider setting where $n=6$, $T=8$, and $d=10$. The displayed results are averaged from $100$ random trials. We train cross-attention models with $\\x_{it},\\z_{i},\\vb\\in\\R^d$ randomly sampled from unit sphere, and apply the normalized GD approach with fixed step size $\\eta=0.1$.  In Figure~\\ref{fig general sfx prob} we calculate the softmax probability via $\\frac{1}{n}\\sum_{i=1}^n\\max_{t\\in[T]}\\sft{\\X_i\\tilde\\W(k)\\z_i}_t$ for either $\\tilde\\W=\\W$ or $\\Kb\\Qb^\\top$ at each iteration. Both scenarios result in probability $1$, which indicates that attention weights succeed in selecting one token per input. Then following Definition~\\ref{def loc opt} let $\\bal=(\\alpha_i)_{i=1}^n$ be the token indices selected by GD and denote $\\Ws_{\\star,\\bal}$ as the corresponding SVM solution of \\eqref{eqn:sattnsvmst}. Define the correlation coefficient of two matrices as \n$\\texttt{corr\\_coef}(\\W_1,\\W_2):=\\li\\W_1,\\W_2\\ri/\\|\\W_1\\|_F\\|\\W_2\\|_F$. \nFigures~\\ref{fig general fro corr}\n and \\ref{fig general nuc corr} illustrate the correlation coefficients of attention weights ($\\W(k)$ and $\\Kb(k)\\Qb(k)^\\top$) with respect to $\\Ws_\\bal$ and $\\Ws_{\\star,\\bal}$. The results demonstrate that $\\W$ ($\\Kb\\Qb^\\top$) ultimately reaches a $1$ correlation with $\\Wsf_\\bal$ ($\\Ws_{\\star,\\bal}$), which suggests that $\\W$ ($\\Kb\\Qb^\\top$) converges in the direction of $\\Wsf_\\bal$ ($\\Ws_{\\star,\\bal}$). This further validates Theorem~\\ref{thm:local:gd}. \n%\n%\n\\input{sections/overparam}\n\\input{sections/svm_obj}\n\\input{sections/local-reg-path}\n==== END OF /2308.16898/sections/sa-local-gd.tex ====\n==== BEGINNING OF /2308.16898/sections/fig_overparam_app.tex ====\n\\begin{figure}[t]\n    \\centering\n    % \\subfigure[$\\W$-parameterization]{\n    %     \\begin{tikzpicture}\n    %     \\node at (0,0) {\\includegraphics[height=.22\\columnwidth, trim={1.3cm 1.3cm 3.2cm 0}, clip]{figs/overparam_W_bar.pdf}};\n    %     \\node[rotate=90] at (-4,0) {\\small{Probabilities}};\n    %     \\node at (-1,-2.){\\small{$d$}};\n    %     \\node at (2.9,1.4){\\small{Not-local}};\n    %     \\node at (2.9,1.1){\\small{Global}};\n    %     \\node at (2.9,0.75){\\small{Local}};\n    %     \\node at (2.9,0.4){\\small{Asmp B.1}};\n    %     \\end{tikzpicture}\n    %     \\label{fig overparam W bar2}\n    % }\n    \\subfigure[Train attention weights $\\Kb,\\Qb\\in\\R^{d\\times m}$]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.25\\columnwidth, trim={1.2cm 1.4cm 0 0}, clip]{figs/rank_diff_m.pdf}};\n        \\node at (0,-2.2) {\\small{{Varying $m$}}};\n        \\node[rotate=90] at (-3.1,0) {\\small{$1-$correlation coefficient}};\n        \\end{tikzpicture}\n        \\label{fig rank m}\n    }\n    \\hspace{10pt}\n    \\subfigure[Convergence types of $(\\Kb,\\Qb)$-parameterization]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.25\\columnwidth, trim={1.3cm 1.2cm 3.2cm 0}, clip]{figs/overparam_KQ_bar.pdf}};\n        \\node[rotate=90] at (-4.3,0) {\\small{Percentage}};\n        \\node at (-1,-2.2){\\small{Varying $d$}};\n        \\node at (3.2,1.6){\\small{Not Local}};\n        \\node at (3.2,1.24){\\small{Global}};\n        \\node at (3.2,0.87){\\small{Local}};\n        \\node at (3.2,0.5){\\small{Assum B.1}};\n        \\end{tikzpicture}\n        \\label{fig overparam KQ bar}\n    }\n    \\caption{ Convergence behavior of GD when training cross-attention weights $\\W$ (solid) or $(\\Kb,\\Qb)$ (dashed) with random data and varying $m$ and $d$. \\textbf{(a)}: Fixing dimensionality at $d=20$, attention weights $\\Kb,\\Qb\\in\\R^{d\\times m}$ are trained for $m\\leq10$. The misalignment between attention SVM and GD, $1-\\corr{\\Ws_{\\star,\\bal},\\Kb\\Qb^\\top}$, is studied. $\\Ws_{\\star,\\bal}$ is from \\eqref{eqn:sattnsvmst} with GD tokens $\\bal$ and $m=d$. Subfigures with fixed $n=5$ and $T=5$ show that as $m$ approaches or exceeds $n$, $\\Kb\\Qb^\\top$ aligns more with $\\Ws_{\\star,\\bal}$.  \\textbf{(b)}: Red, blue, and teal bars represent the percentages of Global, Local (including Global), and Not Local convergence, respectively. The green bar corresponds to Assumption \\ref{assum:token:supp} where all tokens act as support vectors. Larger overparameterization ($d$) relates to a higher percentage of optimal SVM convergence.} \n    \\label{fig overparam bar}\n\\end{figure}\n==== END OF /2308.16898/sections/fig_overparam_app.tex ====\n==== BEGINNING OF /2308.16898/sections/overparam.tex ====\n\n\n\\subsection{Overparameterization conjecture: When local-optimal directions disappear}\\label{sec overparam}\n\nIn Section \\ref{provable global} we demonstrated that larger $d$ serves as a catalyst for global convergence to select the optimal indices $\\op=(\\op_i)_{i=1}^n$. However, Section \\ref{local GD sec} shows that the convergence can be towards locally-optimal directions rather than global ones. How do we reconcile these? Under what precise conditions, can we expect global convergence?\n\nThe aim of this section is gathering these intuitions and stating a concrete conjecture on the global convergence of the attention layer under geometric assumptions related to overparameterization. To recap, Theorem \\ref{thm:separation} characterizes when \\eqref{eqn:sattnsvm} is feasible and Theorem \\ref{diverg:norm:w} characterizes when the parameter norm provably diverges to infinity, i.e. whenever all tokens are support vectors of \\eqref{eqn:sattnsvm} (Assumption \\ref{assum:token:supp} holds). On the other hand, this is not sufficient for global convergence, as GD can converge in direction to locally-optimal directions per Section \\ref{local GD sec}. Thus, to guarantee global convergence, we need to ensure that \\textbf{globally-optimal direction is the only viable one}. Our next assumption is a fully-geometric condition that precisely accomplishes this.\n\n\n\n%\\noindent\\textbf{$\\bullet$ Step 1: Optimal tokens are separable.} Our first result shows that, the SVM problem \\eqref{eqn:sattnsvm} is feasible under mild over-parameterization. \n\n%\\noindent\\textbf{$\\bullet$ Step 2: Parameter norm diverges.} The feasibility of SVM on optimal tokens $\\op=(\\op_i)_{i=1}^n$ is a necessary condition for global convergence to $\\Wm$, however it is far from sufficient. The second criteria is the absence of stationary points $\\nabla\\Lc(\\W)=0$ and divergence of parameter norm. Both of these criteria are guaranteed by our earlier Theorem \\ref{diverg:norm:w} under Assumption \\ref{assum:token}. While Assumption \\ref{assum:opt:token} is a relatively strong requirement, Assumption \\ref{assum:token:supp} is directly linked to overparameterization and is practically meaningful: Consider the classical SVM classification setting where the goal is separating the labels rather than tokens. It is known that \\cite{muthukumar2021classification,hsu2021proliferation}, under fairly general statistical settings (e.g.~IID~features with general covariance), overparameterized SVM problem results in \\emph{all training points are support vectors} and SVM solution coincide with least-squares minimum-norm interpolation. Our Assumption \\ref{assum:token:supp} is an equivalent version of this condition, thus, under realistic data distributions with suitably large $d$, we expect same phenomena to hold and the SVM solution $\\Wm$ to coincide with \\ct{equality below:}\n%\\begin{equation}\n%    \\Wm=\\arg\\min_{\\W}\\tf{\\W}\n%\\quad \\text{subj. to} \\quad (\\x_{i\\op_i}-\\x_\\itt)^\\top\\W\\z_i\\geq 1\\quad \\text{for all} \\quad \\op_i \\neq t \\in [T], \\quad  i\\in[n]     \\label{eqn:sattnreg}.\n%\\end{equation}\n%Extrapolating from \\cite[Theorem 1]{hsu2021proliferation} and our Theorem \\ref{separation thm}, we expect the required overparameterization to be mild: Concretely, when input sequences are IID and tokens have IID isotropic distributions, we suspect that $d\\gtrsim (T+n)\\log(T+n)$ can suffice. More generally, it will depend on the covariance of the tokens (and more generally distribution of the input sequences. Given the optimization theoretic nature of this work, we leave these statistical investigations to future work.\\smallskip\n\n%\\noindent\\textbf{$\\bullet$ Step 3: Global direction is the only viable one.} While Assumption \\ref{assum:token:supp} guarantees the divergence of parameter norm, it does not have to converge to $\\Wm$ in direction, there can be alternative directions. This is primarily the topic of the next section, which will state our results on local convergence. However, we will start discussing this issue here to explain what is needed for global convergence. \n\n\n%\\begin{assumption}[Optimal tokens are support vectors]\\label{assume:all_opt_supp} For any choice of $\\bal=(\\alpha_i)_{i=1}^n$ when solving \\eqref{eqn:sattnsvm} with $\\op\\gets\\bal$, all optimal indices $(\\op_i)_{i=1}^n$ are support vectors of the SVM. That is, either $\\op_i=\\alpha_i$ or $\\op_i\\in\\Tc_i$, $\\forall~i\\in[n]$.\\end{assumption}\n\n\\begin{assumption}[There is always an optimal \\nei]\\label{assume:all_opt_supp} For any choice of $\\bal=(\\alpha_i)_{i=1}^n$ with $\\bal\\neq \\op$ when solving \\eqref{eqn:sattnsvm} with $\\op\\gets\\bal$, there exists $i\\in[n]$ such that $\\alpha_i\\neq\\op_i$ and $\\op_i\\in \\Tc_i$.\n%all optimal indices $(\\op_i)_{i=1}^n$ are support vectors of the SVM. That is, either $\\op_i=\\alpha_i$ or $\\op_i\\in\\Tc_i$, $\\forall~i\\in[n]$.\n\\end{assumption}\n\n%\\redp{ discuss all optimal}\n\nThis guarantees that no $\\bal\\neq \\opt$ can be locally-optimal because it has a \\nei with higher score at the input $i$ with $\\alpha_i\\neq \\op_i$. Thus, this ensures that global direction $\\Wm$ is the unique locally-optimal direction obeying Def.~\\ref{def loc opt}. Finally, note  that local-optimality in Def.~\\ref{def loc opt} is one-sided: GD can provably converge to locally-optimal directions, while we do not provably exclude the existence of other directions. Yet, Theorem 4 of \\cite{tarzanagh2023margin} shows that local RPs (see Section \\ref{sec:local reg path} for details) can only converge to locally-optimal directions for almost all datasets\\footnote{To be precise, they prove this for their version of Def.~\\ref{def loc opt}, which is stated for an attention model $f(\\pb)=\\vb^\\top\\X^\\top\\sft{\\X\\W\\pb}$ admitting an analogous SVM formulation. }. This and Figure \\ref{fig overparam bar} provide strong evidence that Def.~\\ref{def loc opt} captures all possible convergence directions of GD, and as a consequence, that Assumption \\ref{assume:all_opt_supp} guarantees that $\\Wm$ is the only viable direction to converge.\\smallskip\n\n%\\footnote{Specifically, all directions that select exactly one token from each input sequence.}\n\\noindent\\ding{225} \\textbf{Integrating the results and global convergence conjecture.} Combining Assumptions \\ref{assum:token:supp} and \\ref{assume:all_opt_supp}, we have concluded that gradient norm diverges and $\\Wm$ is the only viable direction to converge. Thus, we conclude this section with the following \\textbf{conjecture}: Suppose $\\op=(\\op_i)_{i=1}^n$ are the unique optimal indices with strictly highest score per sequence and Assumptions \\ref{assum:token:supp} and \\ref{assume:all_opt_supp} hold. Then, for almost all datasets (e.g.~add small IID gaussian noise to input features), GD with a proper constant learning rate converges to $\\Wm$ of \\eqref{eqn:sattnsvm} in direction.\n\n% To gain some intuition, consider Figures~\\ref{fig overparam diff n}\\&\\ref{fig overparam}.\n\nTo gain some intuition, consider Figure \\ref{fig overparam bar}: Here, red bars denote the frequency of global convergence whereas green bars denote the frequency of Assumption \\ref{assum:token:supp} holding over random problem instances. In short, this suggests that Assumption \\ref{assum:token:supp} occurs less frequently than global convergence, which is consistent with our conjecture. \n% \\redp{Lizzie: Have you ever seen an instance Asmp \\ref{assum:token:supp} holds but global fails?}\nOn the other hand, verifying Assumption \\ref{assume:all_opt_supp} is more challenging due to its combinatorial nature. A stronger condition that implies Assumption \\ref{assume:all_opt_supp} is when \\emph{all optimal indices $(\\op_i)_{i=1}^n$ are support vectors of the SVM. That is, either $\\op_i=\\alpha_i$ or $\\op_i\\in\\Tc_i$, $\\forall~i\\in[n]$.} When the data follows a statistical model, this stronger condition could be verified through probabilistic tools building on our earlier ``all training points are support vectors'' discussion \\cite{muthukumar2021classification}. More generally, we believe a thorough statistical investigation of \\eqref{eqn:sattnsvm} is a promising direction for future work.\n\n%\\redp{Add experiments and their discussion, Figure \\ref{fig overparam W}}\n==== END OF /2308.16898/sections/overparam.tex ====\n==== BEGINNING OF /2308.16898/sections/prelim.tex ====\n\\section{Preliminaries}\\label{sec:prelim}\n\n\\begin{wrapfigure}{r}{0.31\\linewidth}\\vspace{-0.5cm}\n\t\\centering\n\t\\includegraphics[scale=0.57]{figs/attn_bias_this.pdf}\n \\label{fig:main_bias}\\vspace{-0.8cm}\\caption{Implicit biases of the attention layer and logistic regression.}\\vspace{-0.5cm}\n\\end{wrapfigure}\n\n\\textbf{Unveiling the relationship between attention and linear SVMs.}  For linear classification, it is well-established that GD iterations on logistic loss and separable datasets converge towards the hard margin SVM solution, which effectively separates the two classes within the data \\cite{soudry2018implicit,rosset2003margin,zhang2005boosting}. \nThe softmax nonlinearity employed by the attention layer exhibits an exponentially-tailed behavior similar to the logistic loss; thus, attention may also be biased towards margin-maximizing solutions. However, the attention layer operates on tokens within an input sequence, rather than performing classification directly. Therefore, its bias is towards an SVM, specifically \\eqref{eqn:sattnsvm}, which aims to separate the tokens of input sequences by selecting the relevant ones and suppressing the rest.  Nonetheless, formalizing this intuition is considerably more challenging: The presence of the highly nonlinear and nonconvex softmax operation renders the analysis of standard GD algorithms intricate. Additionally, the 1-layer transformer in \\eqref{eq:erm:init} does not inherently exhibit a singular bias towards a single \\eqref{eqn:sattnsvm} problem, even when using a linear head $h$. Instead, it can result in multiple locally optimal directions induced by their associated SVMs. We emphasize that \\cite{tarzanagh2023margin} is the first work to make this attention$\\leftrightarrow$SVM connection. Here, we augment their framework to transformers by developing the first guarantees for self/cross-attention layer, nonlinear prediction heads, and global convergence.\n\n\n\\smallskip\n\\noindent \\textbf{Notation.} \nFor any integer $N \\geq 1$, let $[N]:=\\{1, \\dots, N\\}$. We use lowercase and uppercase bold letters (e.g., $\\ab$ and $\\A$) to represent vectors and matrices, respectively. The entries of a vector $\\ab$ are denoted as $\\ab_i$. For a matrix $\\A$, $\\tn{\\A}$ denotes the spectral norm, i.e.~maximum singular value, $\\tnuc{\\A}$ denotes the nuclear norm, i.e.~summation of all singular values, and $\\tf{\\A} := \\sqrt{ \\tr(\\A^\\top \\A)}$ denotes the Frobenius norm. $\\dist{\\cdot,\\cdot}$ denotes the Euclidean distance between two sets.\n%we use $\\{\\sigma_1, \\ldots, \\sigma_n\\}$ to represent the set of singular values, with $\\smax(\\A)$ indicating the maximum singular value of $\\A$. We define $\\tn{\\A}:= \\smax(\\A)$, $\\tnuc{\\A}:= \\sum_{i=1}^n\\sigma_i(\\A)$, and $\\tf{\\A} := \\sqrt{ \\tr(\\A^\\top \\A)}$. \nThe minimum / maximum of two numbers $a$, $b$ is denoted as $a\\wedge b$ / $a\\vee b$. The big-O notation $\\mc{O}(\\cdot)$ hides the universal constants. %The loss functions are represented as $\\Lc(\\W)$ and $\\Lc (\\Kb,\\Qb)$, corresponding to Objective \\eqref{eq:erm:init} with fixed prediction head $h(\\cdot)$.\n% Let $\\tf{\\cdot}$ represent the Frobenius norm and $ \\tnuc{\\cdot}$ represent the nuclear norm of a matrix. $\\li \\A,\\B\\ri$ denotes the trace of the product of $\\A^\\top$ and $\\B$, where $\\A$ and $\\B$ are matrices with the same dimensions.\n%Our training dataset is denoted as $(Y_i,\\X_i,\\z_i){i=1}^n$, where $Y_i\\in{-1,1}$, $\\X_i\\in\\R^{T\\times d}$, and $\\z_i\\in\\R^{d}$. \n%\n%\\subsection{Preliminaries}\\label{sec:prelims:sa}\n%\n\n\\smallskip\n\\noindent\\textbf{Optimization problem definition.} \nWe use a linear head $h(\\x)=\\vb^\\top\\x$ for most of our theoretical exposition. Given dataset $(Y_i,\\X_i,\\z_i)_{i=1}^n$, we minimize the empirical risk of an 1-layer transformer using combined weights $\\W\\in\\R^{d\\times d}$ or individual weights $\\Kb,\\Qb\\in\\R^{d\\times m}$ for a fixed head and decreasing loss function:\\vspace{-3pt}\n\\begin{align}\\label{eqn:erm:w}\n\\Lc(\\W)&=\\frac{1}{n}\\sum_{i=1}^n \\ell\\left(Y_i\\cdot \\vb^\\top\\X_i^\\top \\sft{\\X_i\\W\\z_{i}}\\right), \\tag{W-ERM}\\\\\n\\Lc(\\Kb,\\Qb)&=\\frac{1}{n}\\sum_{i=1}^n \\ell\\left(Y_i\\cdot \\vb^\\top\\X_i^\\top \\sft{\\X_i\\Kb\\Qb^\\top\\z_{i}}\\right). \\label{eqn:erm:kq} \\tag{KQ-ERM}\n\\end{align}\n%\n%\\redp{Replace K and Q positions due to the implicit transpose throughout. Expose self-attention better.}\n%For our gradient analysis, we will restrict our attention to the linear head $h(\\x)=\\vb^\\top\\x$ where $\\vb\\in\\R^d$ corresponds to the value weights. \nWe can recover the self-attention model by setting $\\z_i$ to be the first token of $\\X_i$, i.e.,~$\\z_i\\gets \\x_{i1}$. While the above formulation regresses a single label $Y_i$ per $(\\X_i,\\z_i)$, in Sections \\ref{sec:multi} and  \\ref{sec multioutput}, we show that our findings gracefully extend to the sequence-to-sequence classification setting where we output and classify $T$ tokens per inputs $\\X_i,\\Z_i\\in\\R^{T\\times d}$. See Sections \\ref{sec:multi} and \\ref{sec multioutput} for results on nonlinear prediction heads.\n\n\n\\smallskip\n\\noindent\\textbf{Optimization algorithms.} \nGiven a parameter $R>0$, we consider an $\\ell_2$-norm bound $R$, and define the regularized path solution associated with  Objectives \\eqref{eqn:erm:w} and \\eqref{eqn:erm:kq}, respectively as \\eqref{RP-W} and \\eqref{RP-QK}. These update rules allow us to find the solution within a constrained region defined by the norm bound. The RP illustrates the evolution of $\\Wb_R$ as $R$ increases, capturing the essence of GD where the ridge constraint serves as an approximation for the number of iterations. Previous studies, including \\cite{rosset2003margin,suggala2018connecting,ji2020gradient,tarzanagh2023margin}, have examined the implicit bias of logistic regression and established a connection between the directional convergence of the RP (i.e., $\\lim_{R\\rightarrow \\infty} \\Wb_R/R$) and GD. In \\cite{tarzanagh2023margin}, the concept of a local RP was also employed to investigate implicit bias along local directions. For GD, with appropriate initialization and step size $\\eta>0$, we describe the optimization process associated with \\eqref{eqn:erm:w} and \\eqref{eqn:erm:kq} as \\eqref{GD-W} and \\eqref{GD-QK}, respectively.\n\n\\smallskip\n\\tcbset{colback=white!5!white,colframe=black!5!black,colback=green!1!white}\n\\begin{tcolorbox}[height=2cm, sidebyside,righthand width=7cm]\n\\begin{small}\n\\vspace{-.25cm}\n\\hspace{-.2cm} Given $\\W(0) \\in \\R^{d\\times d}$, $\\eta>0$, for $k\\geq 0$ do:\n\\begin{equation}\\tag{\\small{W-GD}}\n%\\hspace{-.4cm}\n\\W(k+1) = \\W(k) -\\eta \\nabla \\Lc(\\W(k)).\n\\label{GD-W}    \n\\end{equation}\n\\end{small}\n\\tcblower\n\\begin{small}\n\\hspace{-.2cm} Given $\\Qb(0), \\Kb(0)  \\in \\R^{d\\times m}$, $\\eta>0$, for $k\\geq 0$ do:\n\\begin{equation}\\label{GD-QK}\n\\hspace{-.1cm}\n\\begin{bmatrix}\n\\Kb(k+1)   \\\\\n \\Qb(k+1) \\\\\n\\end{bmatrix}\n= \\begin{bmatrix}\n\\Kb(k)   \\\\\n \\Qb(k) \\\\\n\\end{bmatrix} \n-\\eta \n\\begin{bmatrix}\n    \\nabla_{\\Kb} \\Lc\\left(\\Kb(k), \\Qb(k)\\right)\\\\\n    \\nabla_{\\Qb} \\Lc\\left(\\Kb(k), \\Qb(k)\\right)\n\\end{bmatrix}.\n% \\begin{align}\n%  \\Kb(k+1) &= \\Kb(k)- \\eta \\nabla \\Lc_{\\Kb}\\left(\\Kb(k), \\Qb(k)\\right),   \\\\\n%  \\Qb(k+1) &= \\Qb(k) - \\eta ,\n% \\end{align}\n\\tag{\\small{KQ-GD}}\n\\end{equation}\n\\end{small}\n\\end{tcolorbox}\n%\\vspace{-.25cm}\n%\\tcbset{colback=white!5!white,colframe=black!75!black,fonttitle=\\bfseries}\n\\begin{tcolorbox}[height=1.8cm, sidebyside,righthand width=7cm]\n\\begin{small}\n\\hspace{-.2cm} Given $R>0$, find $d\\times d$ matrix:\n\\begin{align}\\tag{W-RP}\n\\Wb_R=\\underset{\\tf{\\W}\\leq R}{\\arg\\min}~\\Lc(\\W). \n\\label{RP-W}\n\\end{align}\n\\end{small}\n\\tcblower\n\\begin{small}\n\\hspace{-.2cm} Given $R>0$, find $d\\times m$ matrices:\n\\begin{align}\\tag{KQ-RP}\n(\\Kbb_R,\\Qbb_R)=\\underset{\\tf{\\Kb}^2+\\tf{\\Qb}^2\\leq 2R}{\\arg\\min}\\Lc(\\Kb,\\Qb).\n\\label{RP-QK}\n\\end{align}\n\\end{small}\n\\end{tcolorbox}\n\n%\n%\n% \\noindent\\textbf{}.  \n\\subsection{Optimal tokens and hard-margin SVM problem for cross attention} \nGiven $\\X_i\\in\\R^{T\\times d},\\z_i\\in\\R^d$,  we present a convex hard-margin SVM problem, denoted as \\eqref{eqn:sattnsvm}, that aims to separate a specific token from the remaining tokens in the input sequence $\\X_i$. This problem is jointly solved for all inputs, allowing us to examine the optimization properties of cross-attention. To delve deeper, we introduce the concept of optimal tokens, which are tokens that minimize the training objective under the decreasing loss function $\\ell(\\cdot)$ as shown in Lemma~\\ref{lem min risk}. This exploration will introduce the notions of token scores and optimality, providing insights into the underlying principles of self-attention mechanisms \\cite{tarzanagh2023margin}.\n\n\\begin{definition}[Token Score and Optimality]\\label{score def}\nGiven a prediction head $\\vb\\in\\R^d$, the score of a token $\\x_{it}$ of input $\\X_i$ is defined as $\\bgam_{it} = Y_i \\cdot \\vb^\\top\\x_{it}$. The optimal token for each input $\\X_i$ is given by the index $\\op_i \\in \\arg\\max_{t \\in [T]} \\bgam_{it}$ for all $i \\in [n]$.\n\\end{definition}\nBy introducing token scores and identifying optimal tokens, we can better understand the importance of individual tokens and their impact on the overall objective. The token score quantifies the contribution of a token to the prediction or classification task, while the optimal token represents the token that exhibits the highest relevance within the corresponding input sequence. \n\n\\smallskip\n\\noindent$\\bullet$ \\textbf{Hard-margin SVM for $\\W$-parameterization.} Equipped with the set of optimal indices  $\\opt:=(\\opt_i)_{i=1}^n$ as per Definition~\\ref{score def}, we introduce the following SVM formulation associated to \\eqref{eqn:erm:w}:\n\n\\begin{tcolorbox}[colback=white!5!white,colframe=black!5!black,colback=green!1!white]\n\\vspace{5pt}\n\\begin{equation}\\tag{\\name}\n\\Wm=\\arg\\min_{\\W}\\tf{\\W}\n\\quad \\text{subj. to} \\quad (\\x_{i\\op_i}-\\x_\\itt)^\\top\\W\\z_i\\geq 1\\quad  \\text{for all} \\quad t \\neq \\op_i, \\quad  i\\in[n]\n\\label{eqn:sattnsvm}.\n\\end{equation}\n\\end{tcolorbox}\n\nThe existence of matrix $\\Wm$ implies the separability of tokens $(\\opt_i)_{i=1}^n$ from the others. The terms $a_{\\itt}:=\\x_\\itt^\\top\\W\\z_i$ represent the dot product between the key-query features before applying the softmax nonlinearity. This dot product is a crucial characteristic of self-attention and we can express the SVM constraints in \\eqref{eqn:sattnsvm} as $a_{i\\op_i}\\geq a_\\itt+1$. Thus,  \\eqref{eqn:sattnsvm} finds the most efficient direction that ensures the optimal token $\\x_{i\\op_i}$ achieves the highest similarity with query $\\z_i$ among all key embeddings. Our first result shows that \\eqref{eqn:sattnsvm} is feasible under mild over-parameterization. \n\n\\begin{theorem}\\label{thm:separation} Suppose $d\\geq \\max(T-1,n)$. Then, almost all datasets $(Y_i,\\X_i,\\z_i)_{i=1}^n$ -- including the self-attention setting with $\\z_i\\gets\\x_{i1}$ -- obey the following: \n\\eqref{eqn:sattnsvm} is feasible i.e.,~$\\Wm$ separates the desired tokens $\\opt=(\\opt_i)_{i=1}^n$.\n\\end{theorem}\n% \\red{The poof of this theorem is provided under more general norm setting in Appendix~\\ref{app sep}}.\n\nWe note that the convex formulation \\eqref{eqn:sattnsvm} does not  fully capture the GD geometry on \\eqref{eqn:erm:w}. In a more general sense, GD can provably converge to an SVM solution over locally-optimal tokens, as detailed in Section~\\ref{local GD sec}. For deeper insight into \\eqref{eqn:sattnsvm}, consider that the attention layer's output is a convex mixture of input tokens. Thus, if minimizing the training loss involves choosing the optimal token $\\x_{i\\op_i}$, the softmax similarities should eventually converge to a one-hot vector, precisely including $\\x_{i\\op_i}$ (assigned 1), while ignoring all other tokens (assigned 0s). This convergence requires the attention weights $\\W$ to diverge in norm to saturate the softmax probabilities.  Due to the exponential-tailed nature of the softmax function, the weights converge directionally to the max-margin solution. This phenomenon resembles the implicit bias of logistic regression on separable data \\cite{soudry2018implicit,ji2018risk}. Lemma \\ref{lem min risk} formalizes this intuition and rigorously motivates optimal tokens.\n\n\\smallskip\n\\noindent$\\bullet$ \\textbf{Non-convex SVM for $(\\Kb,\\Qb)$-parameterization.} The objective function \\eqref{eqn:erm:kq} has an extra layer of nonconvexity compared to \\eqref{eqn:erm:w} as $(\\Kb,\\Qb)$ corresponds to a matrix factorization of $\\W$. To study this, we introduce the following nonconvex SVM problem over $(\\Kb,\\Qb)$ akin to \\eqref{eqn:sattnsvm}:\n\n\\smallskip\n\\begin{tcolorbox}%[height=1.5cm]\n\\vspace{-7pt}\n\\begin{equation}\\tag{KQ-SVM}\n\\min_{\\Kb,\\Qb}~\\frac{1}{2} \\left(\\|\\Kb\\|_F^2+\\|\\Qb\\|_F^2\\right)\n\\quad \\text{subj. to} \\quad (\\x_{i\\op_i}-\\x_\\itt)^\\top\\Kb\\Qb^\\top\\z_i\\geq 1\\quad \\text{for all} \\quad t \\neq \\op_i, \\quad  i\\in[n]\n\\label{eqn:qk:svm}.\n\\end{equation}\n\\end{tcolorbox}\nEven if the direction of GD is biased towards the SVM solution, it does not have to converge to the global minima of \\eqref{eqn:qk:svm}. Instead, it can  converge towards a KarushKuhnTucker (KKT) point of the max-margin SVM. Such KKT convergence has been studied by \\cite{lyu2019gradient,ji2020directional} in the context of other nonconvex margin maximization problems. Fortunately, \\eqref{eqn:erm:kq} may not be as daunting as it may initially seem: Our experiments in Figures~\\ref{fig path} and \\ref{fig general} reveal that GD is indeed biased towards the global minima of \\eqref{eqn:qk:svm}. This global minima is achieved by setting $\\W:=\\Kb\\Qb^\\top$ and finding the factorization of $\\W$ that minimizes the quadratic objective, yielding the following $\\W$-parameterized SVM with nuclear norm objective:\n\n\\smallskip\n\\begin{tcolorbox}[colback=white!5!white,colframe=black!5!black,colback=green!1!white]\n\\vspace{5pt}\n\\begin{equation}\\tag{Att-SVM$_\\star$}\n\\Wm_\\st\\in\\underset{\\texttt{rank}(\\W)\\leq m}{\\arg\\min}\\tnuc{\\W}\n\\quad \\text{subj. to} \\quad (\\x_{i\\op_i}-\\x_\\itt)^\\top\\W\\z_i\\geq 1\\quad   \\text{for all} \\quad t \\neq \\op_i, \\quad  i\\in[n]\n\\label{eqn:sattnsvmst}.\n\\end{equation}\n\\vspace{-10pt}\n\\end{tcolorbox}\n% Above, the nonconvex rank constraint arises from the fact that the rank of $\\W\\gets\\Kb\\Qb^\\top$ is at most $m$. On the other hand, under the full parameterization $m\\geq d$, the rank constraint goes away and we obtain a convex nuclear norm minimization problem. Additionally, the nuclear norm objective already promotes a low-rank solution \\cite{recht2010guaranteed}. Thus, if the solution set of the unconstrained convex problem satisfies $\\texttt{rank}(\\W)\\leq m$, the convex problem will recover the solutions of \\eqref{eqn:sattnsvm} even if $m<d$. Lemma \\ref{lem:rank} below shows that this is guaranteed to happen whenever $n\\leq m$, which is also corroborated through our experiments (Fig.~\\ref{fig rank}). Thus, it provides a simple intuition on why setting $m<d$ is reasonable in practice, specifically in the low-data regime. \n\nAbove, the nonconvex rank constraint arises from the fact that the rank of $\\W = \\Kb\\Qb^\\top$ is at most $m$. However, under the condition of the full parameterization where $m \\geq d$, the rank constraint disappears, leading to a convex nuclear norm minimization problem. Besides, the nuclear norm objective inherently encourages a low-rank solution \\cite{recht2010guaranteed,fazel2002matrix,srebro2004maximum}. %\\red{Therefore, if the solution set of the unconstrained convex problem satisfies $\\texttt{rank}(\\W) \\leq m$, the convex problem will recover the solution of \\eqref{eqn:sattnsvm}, even when $m < d$.}\nLemma \\ref{lem:rank}, presented below, demonstrates that this guarantee holds whenever $n \\leq m$. This observation is further supported by our experiments (see Fig.~\\ref{fig rank}). Thus, it offers a straightforward rationale for why setting $m < d$ is a reasonable practice, particularly in scenarios involving limited data.\n%\\input{sections/fig_rank}\n\n\\begin{lemma}\\label{lem:rank} Any optimal solution of \\eqref{eqn:sattnsvm} or \\eqref{eqn:sattnsvmst} is at most rank $n$. More precisely, the  row space of $\\Ws$ or $\\Ws_\\st$ lies within $\\texttt{span}(\\{\\z_i\\}_{i=1}^n)$.\n\\end{lemma}\n\n\nFigure~\\ref{fig rank} illustrates rank range of solutions for  \\eqref{eqn:sattnsvm} and \\eqref{eqn:sattnsvmst}, denoted as $\\Ws$ and $\\Ws_{\\star}$, solved using optimal tokens $(\\opt_i)_{i=1}^n$ and setting  $m=d$ (the rank constraint is eliminated). Each result is averaged over 100 trials, and for each trial, ${\\x}_{it}$, ${\\z}_i$, and linear head ${\\vb}$ are randomly sampled from the unit sphere. In Fig.~\\ref{fig rank svm n}, we fix $T=5$ and vary $n$ across $\\{5,10,15\\}$. Conversely, in Fig.~\\ref{fig rank svm T}, we keep $n=5$ constant and alter $T$ across $\\{5,10,15\\}$.  Both figures confirm rank of $\\Ws$ and $\\Ws_\\star$ are bounded by $\\max(n,d)$, validating Lemma~\\ref{lem:rank}.\n\n\n==== END OF /2308.16898/sections/prelim.tex ====\n==== BEGINNING OF /2308.16898/sections/local-reg-path.tex ====\n\\subsection{Guarantees on local regularization path}\\label{sec:local reg path}\n\nIn this section, we provide a \\emph{localized} regularization path analysis for general objective functions. As we shall see, this will also allow us to predict the local solutions of gradient descent described in Section \\ref{local GD sec}. Let $\\dm$ denote a general norm objective. Given indices $\\bal=(\\alpha_i)_{i=1}^{n}$, consider the formulation\n%\\nicebox{\n\\begin{equation}\\tag{$\\dm$-SVM}\n \\Wm_{\\dm, \\alpha}=\\underset{\\texttt{rank}(W)\\leq m}{\\arg\\min}\\|\\W\\|_{\\diamond}\n\\quad \\text{subj. to} \\quad (\\x_{i\\alpha_{i}}-\\x_\\itt)^\\top\\W\\z_i\\geq 1\\quad   \\text{for all} \\quad t \\neq \\alpha_{i}, \\quad \n i\\in[n]\\label{dmattnsvm}.\n\\end{equation}\n%\n\nIn this section, since $\\dm$ is clear from the context, we will use the shorthand $\\Wm_{\\alpha} := \\Wm_{\\dm, \\alpha}$ and denote the optimal solution set of \\eqref{dmattnsvm} as $\\Wcs := \\Wcs_{\\dm, \\alpha}$. It is important to note that if the $\\dm$-norm is not strongly convex, $\\Wcs$ may not be a singleton. Additionally, when $m=d$, the rank constraint becomes vacuous, and the problem becomes convex. The following result is a slight generalization of Theorem \\ref{thm:separation} and demonstrates that choosing a large $d$ ensures the feasibility of \\eqref{dmattnsvm} uniformly over all choices of $\\bal$. The proof is similar to that of Theorem~\\ref{thm:separation}, as provided in Appendix~\\ref{app sep}.\n%\n\\begin{theorem}\\label{separation thm} Suppose $d\\geq \\max(T-1,n)$ and $m=d$. Then, almost all datasets\\footnote{Here, \\emph{``almost all datasets''} means that adding i.i.d.~gaussian noise, with arbitrary nonzero variance, to the input features will almost surely result in SVM's feasibility.} $(Y_i,\\X_i,\\z_i)_{i=1}^n$ -- including the self-attention setting with $\\z_i\\gets\\x_{i1}$ -- obey the following: For any choice of indices $\\bal=(\\alpha_i)_{i=1}^n\\subset[T]$, \\eqref{dmattnsvm} is feasible,  i.e.~the attention layer can separate and select indices $\\bal$.\n\\end{theorem}\n%\n\n\n\nTo proceed, we define the \\emph{local regularization path}, which is obtained by solving the $\\dm$-norm-constrained problem over a $\\bal$-dependent cone denoted as $\\con{\\bal}$. This cone has a simple interpretation: it prioritizes tokens with a lower score than $\\bal$ over tokens with a higher score than $\\bal$. This interpretation sheds light on the convergence towards locally optimal directions: lower-score tokens create a barrier for $\\bal$ and prevent optimization from moving towards higher-score tokens.\n%\n\\begin{definition}[Low\\&High Score Tokens and Separating Cone]\\label{HL cone def main} Given $\\al\\in[T]$, input sequence $\\X$ with label $Y$, $h(\\cdot):\\R^d\\rightarrow\\R$, and score $\\bgam_t=Y\\cdot h(\\x_t)$ for all $t\\in[T]$, define the low and high score tokens as\n\\[\n\\low:=\\left\\{t\\in[T]\\bgl \\bgam_t<\\bgam_\\al\\right\\},\\quad \\high:=\\left\\{t\\in[T]-\\{\\alpha\\}\\bgl \\bgam_t\\geq \\bgam_\\al\\right\\}.\n\\]\nFor input $\\X_i$ and index $\\alpha_i$, we use the shorthand notations $\\texttt{low}^\\alpha_i$ and $\\texttt{high}^\\alpha_i$. Finally define $\\con{\\bal}$ as% is defined\n\\begin{align}\n\\con{\\bal}:=\\left\\{\\texttt{rank}(W)\\leq m\\bgl \\min_{i\\in[n]}\\max_{t\\in\\texttt{low}^\\alpha_i}\\min_{\\tau\\in\\texttt{high}^\\alpha_i} (\\x_\\itt-\\x_\\ittt)^\\top\\W\\z_i\\geq \\eps\\tf{\\W}\\right\\}.\\label{cone alpha eq1}\n\\end{align}\n\\end{definition}\n%\n\nOur next lemma relates this cone definition to locally-optimal directions of Definition \\ref{def loc opt}.\n\\begin{lemma} \\label{lemma cone main}Suppose \\eqref{dmattnsvm} is feasible. If indices $\\bal$ are locally-optimal, $\\Wma\\in \\con{\\bal}$ for all sufficiently small $\\eps>0$. Otherwise, $\\Wma\\not\\in \\con{\\bal}$ for all $\\eps>0$. Additionally, suppose optimal indices $\\op_i\\in\\arg\\max_{t\\in[T]}\\bgam_\\itt$ are unique and set $\\bal\\gets\\op$. Then, $\\con{\\opt}$ is the set of all rank-$\\leq$$m$ matrices (i.e.~global set).\n\\end{lemma}\n\nLemma~\\ref{lemma cone main} can be understood as follows: Among the SVM solutions $\\Wma$, only those that are locally optimal demonstrate a barrier of low-score tokens, effectively acting as a protective shield against higher-score tokens. Moreover, in the case of globally optimal tokens (with the highest scores), the global set $\\con{\\opt}$ can be chosen, as they inherently do not require protective measures. The subsequent result introduces our principal theorem, which pertains to the regularization path converging towards the locally-optimal direction over $\\con{\\bal}$ whenever $\\bal$ is locally optimal.\n\n\\begin{theorem} [Convergence of Local Regularization Path]\\label{local RP thm1} Suppose Assumption \\ref{assum:loss:prope} holds. Fix locally-optimal token indices $\\bal=(\\al_i)_{i=1}^n$ and $R_0,\\eps>0$. Consider the norm-constrained variation of \\eqref{cone alpha eq1} defined as \n\\[\n\\Ccd:=\\con{\\bal}\\bigcap \\left\\{\\W\\bgl \\td{\\W}\\geq R_0\\right\\}.\n\\]\nDefine local RP as $\\Wb_R=\\min_{\\Ccd,\\td{\\W}\\leq R}\\Lc(\\W)$ where $\\Lc(\\W)$ is given by \\eqref{eqn:erm:w}. Let $\\Wcs$ be the set of minima for \\eqref{dmattnsvm} and $\\xdm>0$ be the associated margin i.e.~$\\xdm=1/\\td{\\Wma}$. For any sufficiently small $\\eps>0$ and sufficiently large $R_0= \\order{1/\\eps}>0$, $\\lim_{R\\rightarrow\\infty} \\dist{\\frac{\\Wb_R}{R\\xdm},\\Wcs}=0$. Additionally, suppose optimal indices $\\op=(\\op_i)_{i=1}^n$ are unique and set $\\bal\\gets\\op$. Then, the same convergence guarantee on regularization path holds by setting $\\Ccd$ as the set of rank-$\\leq$$m$ matrices.\n\\end{theorem}\n\nNote that when setting $m=d$, the rank constraint is eliminated. Consequently, specializing this theorem to the Frobenius norm aligns it with Theorem \\ref{thm:local:gd}. On the other hand, by assigning $\\dm$ as the nuclear norm and $\\bal\\gets\\op$, the global inductive bias of the nuclear norm is recovered, as stated in Theorem \\ref{thm global reg path}.\n\nWe would like to emphasize that both this theorem and Theorem \\ref{thm global reg path} are specific instances of Theorem \\ref{local RP thm} found in Appendix \\ref{sec multioutput}. It is worth noting that, within this appendix, we establish all regularization path results for sequence-to-sequence classification, along with a general class of \\emph{monotonicity-preserving} prediction heads outlined in Assumption \\ref{ass cvx seq}. The latter significantly generalizes linear heads, highlighting the versatility of our theory. The following section presents our discoveries concerning general nonlinear heads.\n==== END OF /2308.16898/sections/local-reg-path.tex ====\n==== BEGINNING OF /2308.16898/sections/multitoken.tex ====\n%\\section{Understanding Multi-token Compositions: Toward A More General Max-Margin and Directional Convergence Theory}\\label{sec:multi}\n\n\\section{Toward A More General SVM Equivalence for Nonlinear Prediction Heads}\\label{sec:multi}\n%\n%\nSo far, our theory has focused on the setting where the attention layer selects a single optimal token within each sequence. As we have discussed, this is theoretically well-justified under linear head assumption and certain nonlinear generalizations. On the other hand, for arbitrary nonconvex $h(\\cdot)$ or multilayer transformer architectures, it is expected that attention will select multiple tokens per sequence. This motivates us to ask:\n\\begin{quote}\n    \\textbf{Q:}~What is the implicit bias and the form of $\\W(k)$ when the GD solution is composed by multiple tokens?\n\\end{quote}\n\nIn this section, our goal is to derive and verify the generalized behavior of GD. Let $\\xat_i=\\X_i^\\top \\s^{\\W}_i$ denote the composed token generated by the attention layer where $\\s^{\\W}_i=\\sft{\\X_i\\W\\z_i}$ are the softmax probabilities corresponding to $\\W$. Suppose GD trajectory converges to achieve the risk $\\Lc_\\star=\\min_{\\W}\\Lc(\\W)$, and the eventual token composition achieving $\\Lc_\\star$ is given by \n\\[\n\\xast_i=\\X_i^\\top \\s^\\st_i,\n\\]\nwhere $\\s^\\st_i$ are the eventual softmax probability vectors that dictate the token composition. Since attention maps are sparse in practice, we are interested in the scenario where $\\s^\\st_i$ is sparse i.e.~it contains some zero entries. This can only be accomplished by letting $\\tf{\\W}\\rightarrow\\infty$. However, unlike the earlier sections, we wish to allow for arbitrary $\\s^\\st_i$ rather than a one-hot vector which selects a single token. %For instance, \n\nTo proceed, we aim to understand the form of GD solution $\\W(k)$ responsible for composing $\\xast_i$ via the softmax map $\\s^\\st_i$ as $\\tf{\\W}\\rightarrow\\infty$. Intuitively, $\\W(k)$ should be decomposed into two components via\n\\begin{align}\n\\W(k)\\approx \\Wf+\\tf{\\W(k)}\\cdot \\Wsb,\\label{multi-token soln}\n\\end{align}\nwhere $\\Wf$ is the {finite component} and $\\Wsb$ is the {directional component} with $\\tf{\\Wsb}=1$. Define the {selected set} $\\Rc_i\\subseteq[T]$ to be the indices $\\s^\\st_\\itt\\neq 0$ and the {masked (i.e.~suppressed) set} as $\\Rcb_i=[T]-\\Rc_i$ where softmax entries are zero. In the context of earlier sections, we could also call these the \\emph{optimal set} and the \\emph{non-optimal set}, respectively.\n\\begin{itemize}[label=$\\bullet$, wide, labelwidth=!,itemindent=!, labelindent=5pt]\n\\item \\textbf{Finite component:} The job of $\\Wf$ is to assign nonzero softmax probabilities within each $\\s^\\st_i$. This is accomplished by ensuring that, $\\Wf$ induces the probabilities of $\\s^\\st_i$ over $\\Rc_i$ by satisfying the softmax equations\n\\[\n\\frac{e^{\\x_\\itt^\\top \\Wf\\z_i}}{e^{\\x_\\ittt^\\top \\Wf\\z_i}}=e^{(\\x_\\itt-\\x_\\ittt)^\\top \\Wf\\z_i}=\\s^\\st_\\itt/\\s^\\st_\\ittt,\n\\]\nfor $t,\\tau\\in\\Rc_i$. Consequently, this $\\Wf$ should satisfy the following linear constraints\n\\begin{equation}\n(\\x_\\itt-\\x_\\ittt)^\\top \\Wf\\z_i=\\log(\\s^\\st_\\itt/\\s^\\st_\\ittt)\\quad\\text{for all}\\quad t,\\tau\\in\\Rc_i,~i\\in[n].\\label{smax eqn}\n\\end{equation}\n% \\ct{Or(?):\n% \\[\n% \\x_{it}^\\top\\Wf\\z_i = a_{it}, t\\in\\Rc_i\n% \\]\n% where $a_{it}$ are convex-combination coefficients such that for $\\ab_i=[a_{it}]_{t\\in\\Rc_i}$ it holds $\\mathbb{S}_t(\\ab)=s_{it}^\\star, t\\in\\Rc_i$ (set of logits $\\ab$ is not unique, but any such set suffices?)\n% }\n%If the above linear system is overparameterized, $\\Wf$ will intuitively be given by the minimum norm solution of the system i.e.~via pseudo-inverse. However, there might also be some interaction with the directional term which is discussed next. \n%For GD, some adjustment is needed depending on the initialization $\\W(0)$.\n\\item \\textbf{Directional component:} While $\\Wf$ creates the composition by allocating the nonzero softmax probabilities, it does not explain sparsity of attention map. This is the role of $\\Wsb$, which is responsible for selecting the selected tokens $\\Rc_i$ and suppressing the masked ones $\\Rcb_i$ by assigning zero softmax probability to them. To predict direction component, we build on the theory developed in earlier sections. Concretely, there are two constraints $\\Wsb$ should satisfy\n\\begin{enumerate}\n\\item \\textbf{Equal similarity over selected tokens:} For all $t,\\tau\\in\\Rc_i$, we have that $(\\x_\\itt-\\x_\\ittt)^\\top \\W\\z_i=0$. This way, softmax scores assigned by $\\Wf$ are not disturbed by the directional component and $\\Wf+R\\cdot\\Wsb$ will still satisfy the softmax equations \\eqref{smax eqn}.\n\\item \\textbf{Max-margin against masked tokens:} For all $t\\in\\Rc_i,\\tau\\in\\Rcb_i$, enforce the margin constraint $(\\x_\\itt-\\x_\\ittt)^\\top \\W\\z_i\\geq 1$ subject to minimum norm $\\tf{\\W}$. \n\\end{enumerate}\nCombining these yields the following convex generalized SVM formulation\n \\begin{tcolorbox}[colback=white!5!white,colframe=black!5!black,colback=green!1!white]\n \\vspace{-7pt}\n\\begin{align}\\tag{Gen-SVM}\n\\Wm=\\arg\\min_{\\W}\\tf{\\W}\n\\quad \\text{subj. to} \\quad\\begin{cases} \\forall~t\\in\\Rc_i,\\tau\\in\\Rcb_i:~(\\x_\\itt-\\x_\\ittt)^\\top\\W\\z_i\\geq 1,\\\\\n\\forall~t,\\tau\\in\\Rc_i:~\\quad\\quad(\\x_\\itt-\\x_\\ittt)^\\top\\W\\z_i=0,\\end{cases}\\quad  \\forall  1\\leq i\\leq n.\n\\label{eqn:mattnsvm}\n\\end{align}\n\\end{tcolorbox}\n\\noindent and set the normalized direction in \\eqref{multi-token soln} to $\\Wsb=\\Ws/\\tf{\\Ws}$.\n\\end{itemize}\nIt is important to note that  \\eqref{eqn:mattnsvm} offers a substantial generalization beyond the scope of the previous sections, where the focus was on selecting a single token from each sequence, as described in the main formulation \\eqref{eqn:sattnsvm}. This broader solution class introduces a more flexible approach to the problem.\n%\n%\n\\input{sections/fig_nn_diff_d}\n%\n\nWe present experiments showcasing the predictive power of the \\eqref{eqn:mattnsvm} equivalence in nonlinear scenarios. We conducted these experiments on random instances using an MLP denoted as $h(\\cdot)$, which takes the form of $\\onebb^\\top\\texttt{ReLU}(\\x)$. We begin by detailing the preprocessing step and our setup. For the attention SVM equivalence analytical prediction, clear definitions of the selected and masked sets are crucial. These sets include token indices with nonzero and zero softmax outputs, respectively. However, practically, reaching a precisely zero output is not feasible. Hence, we define the selected set as tokens with softmax outputs exceeding $10^{-3}$, and the masked set as tokens with softmax outputs below $10^{-6}$. We also excluded instances with softmax outputs falling between $10^{-6}$ and $10^{-3}$ to distinctly separate the concepts of \\emph{selected} and \\emph{masked} sets, thereby enhancing the predictive accuracy of the attention SVM equivalence. In addition to the filtering process, we focus on scenarios where the label $Y=-1$ exists to enforce \\emph{non-convexity} of prediction head $Y_i\\cdot h(\\cdot)$. It is worth mentioning that when all labels are $1$, due to the convexity of $Y_i\\cdot h(\\cdot)$, GD tends to select one token per input, and Equations \\eqref{eqn:mattnsvm} and \\eqref{eqn:sattnsvm} yield the same solutions.  The results are displayed in Figure~\\ref{fig nn diff d}, where $n=3$, $T=4$, and $d$ varies within ${4, 6, 8, 10}$. We conduct 500 random trials for different choices of $d$, each involving ${\\x}_{it}$, ${\\z}_i$, and ${\\vb}$ randomly sampled from the unit sphere. We apply normalized GD with a step size $\\eta=0.1$ and run $2000$ iterations for each trial.\n\n\\begin{enumerate}[label=$\\bullet$, wide, labelwidth=!,itemindent=!, labelindent=5pt]\n\\item Figure \\ref{fig nn diff d} (upper) illustrates the correlation evolution between the GD solution and three distinctive baselines: ({\\color{black}{\\textbf{$\\cdots$}}}) $\\Ws$ obtained from \\eqref{eqn:mattnsvm};  ({\\color{black}{\\textbf{---}}}) $\\W^\\rfn$ obtained by calculating $\\Wf$ and determining the best linear combination $\\Wf+\\gamma \\Wsb$ that maximizes correlation with the GD solution; and  ({\\color{black}{\\textbf{-~-}}}) $\\W^\\ont$ obtained by solving \\eqref{eqn:sattnsvm} and selecting the highest probability token from the GD solution.  For clearer visualization, the logarithmic scale of correlation misalignment is presented in Figure~\\ref{fig nn diff d}. In essence, our findings show that $\\W^\\ont$ yields unsatisfactory outcomes, whereas $\\Ws$ attains a significant correlation coefficient in alignment with our expectations. Ultimately, our comprehensive SVM-equivalence $\\W^\\rfn$ further enhances correlation, lending support to our analytical formulas. It's noteworthy that SVM-equivalence displays higher predictability in a larger $d$ regime (with an average correlation exceeding $0.99$). This phenomenon might be attributed to more frequent directional convergence in higher dimensions, with overparameterization contributing to a smoother loss landscape, thereby expediting optimization. \n\\item Figure \\ref{fig nn diff d} (lower) offers a scatterplot overview of the $500$ random problem instances that were solved. The $x$-axis represents the largest softmax probability over the masked set, denoted as $\\max_{i,\\tau}s_{i\\tau}$ where $\\tau\\in\\Rcb_i$. Meanwhile, the $y$-axis indicates the predictivity of the SVM-equivalence, quantified as $1-\\texttt{corr\\_coef}(\\W,\\W^\\rfn)$. From this analysis, two significant observations arise. Primarily, there exists an inverse correlation between softmax probability and SVM-predictivity. This correlation is intuitive, as higher softmax probabilities signify a stronger divergence from our desired \\emph{masked set} state (ideally set to $0$). Secondly, as dimensionality ($d$) increases, softmax probabilities over the masked set tend to converge towards the range of $10^{-15}$ (effectively zero). Simultaneously, attention SVM-predictivity improves, creating a noteworthy correlation.\n\\end{enumerate}\n\n% we study the evolution of correlation between the GD solution and three distinct baselines: \\textbf{(1)} $\\Ws$ which is the max-margin direction obtained from \\eqref{eqn:mattnsvm} but using GD-determined selected/masked token sets where $\\Rc_i:=\\{t:s_{it}>10^{-3}\\}$ and $\\Rcb_i=[T]-\\Rc_i$. \\textbf{(2)} $\\W^\\rfn$ which is obtained by calculating $\\Wf$ over GD-determined $\\Rc_i$ and $s_{it}$, and finding the best linear combination $\\Wf+\\gamma \\Wsb$ that maximize correlation to the GD solution, e.g. $\\gamma=\\arg\\max_\\gamma\\corr{\\W,\\Wf+\\gamma\\Wsb}$. \\textbf{(3)} $\\W^\\ont$ obtained by solving \\eqref{eqn:sattnsvm} focusing on the highest probability token from the GD solution. For better visualization, we present the correlation misalignment in logarithm scale in Figure~\\ref{fig nn diff d}.In short, we find that $\\W^\\ont$ performs poorly whereas $\\Ws$ achieves high correlation coefficient in line with our intuition. Finally, our full SVM-equivalence $\\W^\\rfn$ provides a further correlation boost which is supportive of our analytical formulas. Note that, SVM-equivalence is more predictive in larger $d$ regime (i.e.~$>0.99$ average correlation). This might be because the directional convergence occurs more frequently for larger $d$, and overparameterization tends to make loss landscape more benign and accelerate optimization.\n% Figure \\ref{fig nn diff d}\\textbf{(lower)} is a scatterplot of the $500$ random problem instances we solve. The $x$-axis depicts the largest softmax probability over the masked set defined as $\\max_{i,\\tau}s_{i\\tau},\\tau\\in\\Rcb_i$ whereas $y$-axis is the predictivity of the SVM-equivalence defined as $1-\\texttt{corr\\_coef}(\\W,\\W^\\rfn)$. The first conclusion is that, softmax probability is inversely correlated with the SVM-predictivity. This certainly makes sense because larger softmax probability implies a stronger violation of our \\emph{masked set} definition (ideally we want it to be $0$). The second conclusion is that, as $d$ grows, the softmax probabilities over the masked set collapse to $10^{-15}$ range (essentially zero) and, simultaneously, SVM-predictivity improves.\n\n\\subsection{When does attention select multiple tokens?}\\label{sec when}\nIn this section, we provide a concrete example where the optimal solution indeed requires combining multiple tokens in a nontrivial fashion. Here, by nontrivial we mean that, we select more than 1 tokens from an input sequence but we don't select all of its tokens. Recall that, for linear prediction head, attention will ideally select the single token with largest score for almost all datasets. Perhaps not surprisingly, this behavior will not persist for nonlinear prediction heads. For instance in Figure~\\ref{fig nn diff d}, the GD output $\\W$ aligned better in direction with $\\Ws$ than $\\W^\\ont$. Specifically, here we prove that if we make the function $h_Y(\\x):=Y\\cdot h(\\x)$ concave, then optimal softmax map can select multiple tokens in a controllable fashion. $h_Y(\\x)$ can be viewed as generalization of the linear score function $Y\\cdot \\vb^\\top\\x$. In the example below, we induce concavity by incorporating a small $-\\la\\tn{\\x}^2$ term within a linear prediction head and setting $h(\\x)=\\vb^\\top\\x-\\la\\tn{\\x}^2$ with $Y=1$.\n\n\\begin{lemma}\\label{example dataset} Given $\\vb\\in\\R^d$, recall the score vector $\\bgam=\\X\\vb$. Without losing generality, assume $\\bgam$ is non-increasing. Define the vector of score gaps $\\bbg\\in\\R^{T-1}$ with entries $\\bbg_t=\\bgam_{t}-\\bgam_{t+1}$. Suppose all tokens within the input sequence are orthonormal and for some $\\tau\\geq 2$, we have that \n\\begin{align}\n\\tau\\bbg_\\tau/2>\\bbg_1.\\label{tau description}\n\\end{align}\nSet $h(\\x)=\\vb^\\top\\x-\\la\\tn{\\x}^2$ where $\\tau\\bbg_\\tau/2>\\la>\\bbg_1$, $\\ell(x)=-x$, and $Y=1$. Let $\\Bal_T$ denote the $T$-dimensional simplex. Define the unconstrained softmax optimization associated to the objective $h$ where we make $\\s:=\\sft{\\X\\W\\z}$ a free variable, namely,\n\\begin{align} \n\\min_{\\s\\in\\Bal_T}\\ell(h(\\X\\s))=\\min_{\\s\\in\\Bal_T}\\la \\tn{\\X^\\top \\s}^2-\\vb^\\top\\X^\\top \\s.\\label{direct opt}\n\\end{align}\nThen, the optimal solution $\\s^\\st$ contains at least $2$ and at most $\\tau$ nonzero entries.\n\\end{lemma}\n%\n\\input{sections/fig_multi_corrs}\n%\nFigure \\ref{fig multi corrs} presents experimental findings concerning Lemma \\ref{example dataset} across random problem instances. For this experiment, we set $n=1$, $T=10$, and $d=10$. The results are averaged over $100$ random trials, with each trial involving the generation of randomly orthonormal vectors $\\x_{1t}$ and the random sampling of vector $\\vb$ from the unit sphere. Similar to the processing step in Figure~\\ref{fig nn diff d}, and following Figure~\\ref{fig nn diff d} (lower) which illustrates that smaller softmax outputs over masked sets correspond to higher correlation coefficients, we define the selected and masked token sets. Specifically, tokens with softmax outputs $>10^{-3}$ are considered selected, while tokens with softmax outputs $<10^{-8}$ are masked. Instances with softmax outputs between $10^{-8}$ and $10^{-3}$ are filtered out.\n\nFigure \\ref{fig multi ns diff lambda} shows that the number of selected tokens grows alongside $\\lambda$, a prediction consistent with Lemma \\ref{example dataset}. When $\\lambda=0$, the head $h(\\x)=\\vb^\\top\\x$ is linear, resulting in the selection of only one token per input. Conversely, as $\\lambda$ exceeds a certain threshold (e.g., $\\lambda>2.0$ based on our criteria), the optimization consistently selects all tokens. Figure \\ref{fig multi corr diff lambda} and \\ref{fig multi corr diff ns} delve into the predictivity of attention SVM solutions for varying $\\lambda$ and different numbers of selected tokens. The dotted curves in both figures represent $1-\\corr{\\W,\\Ws}$, while solid curves indicate $1-\\corr{\\W,\\W^{\\rfn}}$, where $\\W$ denotes the GD solution. Overall, the SVM-equivalence demonstrates a strong correlation with the GD solution (consistently above $0.95$). However, selecting more tokens (aligned with larger $\\lambda$ values) leads to reduced predictivity.\n\nTo sum up, we have showcased the predictive capacity of the generalized SVM equivalence regarding the inductive bias of 1-layer transformers with nonlinear heads. Nevertheless, it's important to acknowledge that this section represents an initial approach to a complex problem, with certain caveats requiring further investigation (e.g., the use of filtering in Figures \\ref{fig nn diff d} and \\ref{fig multi corrs}, and the presence of imperfect correlations). We aspire to conduct a more comprehensive investigation, both theoretically and empirically, in forthcoming work.\n\n\n==== END OF /2308.16898/sections/multitoken.tex ====\n==== BEGINNING OF /2308.16898/sections/fig_nn_corrs.tex ====\n\\begin{figure}\n    \\centering\n    \\hspace{-10pt}\n    \\subfigure[$\\W^{\\text{adapt}}$]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.22\\columnwidth, trim={1.3cm 1.3cm 0 0}, clip]{figs/nn_corr_itr_adapt.pdf}};\n        \\node at (0,-2.) {\\small{Iterations}};\n        \\node[rotate=90] at (-2.65,0) {\\small{$1-$correlation coefficient}};\n        \\end{tikzpicture}\n        \\label{fig nn corr adapt}\n    }\n    \\hspace{-10pt}\n    \\subfigure[$\\Ws$]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.22\\columnwidth, trim={1.3cm 1.3cm 0 0}, clip]{figs/nn_corr_itr_svm.pdf}};\n        % \\node[rotate=90] at (-2.7,0) {\\small{$1-$correlation coefficient}};\n        \\node at (0,-2.){\\small{Iterations}};\n        \\end{tikzpicture}\n        \\label{fig nn corr svm}\n    }\n    \\hspace{-10pt}\n    \\subfigure[$\\W^\\dagger$]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.22\\columnwidth, trim={1.3cm 1.3cm 0 0}, clip]{figs/nn_corr_itr_single.pdf}};\n        \\node at (0,-2.) {\\small{Iterations}};\n        % \\node[rotate=90] at (-2.65,0) {\\small{$1-$correlation coefficient}};\n        \\end{tikzpicture}\n        \\label{fig nn corr single}\n    }\n    \\caption{} \n    \\label{fig nn corrs}\n\\end{figure}\n==== END OF /2308.16898/sections/fig_nn_corrs.tex ====\n==== BEGINNING OF /2308.16898/sections/fig_multi_diff_tau.tex ====\n\\begin{figure}[t]\n    \\centering\n    \\hspace{-10pt}\n    \\subfigure[$\\tau$ and $\\lambda$ parameters relationship]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.22\\columnwidth, trim={1cm 1.3cm 0 0}, clip]{figs/multi_lambda_tau.pdf}};\n        \\node at (0,-2.) {\\small{$\\tau$}};\n        \\node[rotate=90] at (-2.65,0) {\\small{$\\lambda$}};\n        \\end{tikzpicture}\n        \\label{fig tau lambda}\n    }\n    \\hspace{-10pt}\n    \\subfigure[ $\\tau$ and  \\# of selected tokens relationship]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.22\\columnwidth, trim={1cm 1.3cm 0 0}, clip]{figs/multi_n_selected_tau.pdf}};\n        \\node at (0,-2.) {\\small{$\\tau$}};\n        \\node[rotate=90] at (-2.7,0) {\\small{\\# of selected tokens}};\n        \\end{tikzpicture}\n        \\label{fig tau ns}\n    }\n    \\hspace{-10pt}\n    \\subfigure[Distribution of \\# selected tokens over varying $\\tau$]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.22\\columnwidth, trim={1.3cm 1.3cm 0 0}, clip]{figs/multi_prob_tau.pdf}};\n        \\node[rotate=90] at (-2.7,0) {\\small{Probabilities}};\n        \\node at (0,-2.){\\small{\\# of selected tokens}};\n        \\end{tikzpicture}\n        \\label{fig tau prob}\n    }\n    \\caption{ Behavior of GD when selecting multiple tokens.} \n    \\label{fig multi tau}\n\\end{figure}\n==== END OF /2308.16898/sections/fig_multi_diff_tau.tex ====\n==== BEGINNING OF /2308.16898/sections/introduction.tex ====\n\\section{Introduction}\nSelf-attention, the central component of the transformer architecture, has revolutionized natural language processing (NLP) by empowering the model to identify complex dependencies within input sequences \\cite{vaswani2017attention}. By assessing the relevance of each token to every other token, self-attention assigns varying degrees of importance to different parts of the input sequence. This mechanism has proven highly effective in capturing long-range dependencies, which is essential for applications arising in NLP ~\\cite{kenton2019bert,brown2020language,raffel2020exploring}, computer vision~\\cite{fan2021multiscale,liu2021swin,touvron2021training,chen2023jigsaw}, and reinforcement learning~\\cite{janner2021offline,chen2021decision,wu2022flowformer}.  \nRemarkable success of the self-attention mechanism and transformers has paved the way for the development of sophisticated language models such as GPT4 \\cite{gpt4},\nBard \\cite{bard}, LLaMA \\cite{touvron2023llama}, and  ChatGPT \\cite{openai_chatgpt}. \n\\begin{quote}\n\\textbf{Q:}~Can we characterize the optimization landscape and implicit bias of transformers? \nHow does the attention layer select and compose tokens when trained with gradient descent?\n\\end{quote}\n\nWe address these questions by rigorously connecting the optimization geometry of the attention layer and a hard max-margin SVM problem, namely \\eqref{eqn:sattnsvm}, that separates and selects the optimal tokens from each input sequence. This formalism, which builds on the recent work \\cite{tarzanagh2023margin}, is practically meaningful as demonstrated through experiments, and sheds light on the intricacies of self-attention. \n%and helps unravel the mysteries of self-attention.\nThroughout, given  input sequences $\\X,\\Z\\in\\R^{T\\times d}$ with length $T$ and embedding dimension $d$, we study the core cross-attention and self-attention models:\n\\begin{subequations}\\label{eqn:sa:obj}\n\\begin{align}\nf_{\\texttt{cross}}(\\X,\\Z)&:= \\sftx(\\Z \\Qb\\K^\\top\\X^\\top)\\X\\V, %\\tag{XAtt}\n\\label{xatt eq}\\\\\nf_{\\texttt{self}}(\\X)&:= \\sftx(\\X \\Qb\\K^\\top\\X^\\top)\\X\\V.%\\tag{SAtt}.\n\\label{satt eq}\n\\end{align}\n\\end{subequations}\nHere, $\\Kb, \\Qb  \\in \\R^{d\\times m}$, $\\V \\in \\R^{d\\times v} $ are the trainable key, query, value matrices respectively; $\\sft{\\cdot}$ denotes the softmax nonlinearity, which is applied row-wise on  $\\X \\Qb\\K^\\top\\X^\\top$.  Note that self-attention  \\eqref{satt eq} is a special instance of the cross-attention \\eqref{xatt eq} by setting $\\Z\\gets \\X$. To expose our main results, suppose the first token of $\\Z$ -- denoted by $\\z$ -- is used for prediction. Concretely, given a training dataset $(Y_i,\\X_i, \\z_i)_{i=1}^n$ with labels $Y_i\\in \\{-1,1\\}$ and inputs $\\X_i\\in\\R^{T\\times d},\\z_i\\in\\R^d$, we consider the empirical risk minimization with a decreasing loss function $\\ell(\\cdot):\\R\\rightarrow\\R$, represented as follows:\n\\begin{align}\\label{eq:erm:init} \n\\Lc(\\K,\\Qb)=\\frac{1}{n}\\sum_{i=1}^n \\ell \\left(Y_i\\cdot f(\\X_i,\\z_i)\\right),\\quad\\text{where}~~f(\\X_i,\\z_i)=h\\left(\\X^\\top_i \\sftx\\left(\\X_i \\K\\Qb^\\top\\z_i\\right)\\right).\n\\end{align}\nHere, $h(\\cdot):\\R^{d}\\rightarrow\\R$ is the prediction head that subsumes the value weights $\\Vb$. In this formulation, the model $f(\\cdot)$ precisely represents a one-layer transformer where an MLP follows the attention layer. Note that, we recover the  self-attention in \\eqref{eq:erm:init} by setting $\\z_i\\gets \\x_{i1}$, where $\\x_{i1}$ denotes the first token of the sequence $\\X_i$\\footnote{Note that for simplicity, we set $\\z_i = \\x_{i1}$, but it can be any other row of $\\X_i$.}. The softmax operation, due to its nonlinear nature, poses a significant challenge when optimizing \\eqref{eq:erm:init}. The problem is nonconvex and nonlinear even when the prediction head is fixed and linear. In this study, we focus on optimizing the attention weights ($\\Kb,\\Qb$ or $\\W$) and overcome such challenges to establish a fundamental SVM equivalence.\\footnote{We fix $h(\\cdot)$ and only optimize the attention weights. This is partly to avoid the degenerate case where $h(\\cdot)$ can be used to achieve zero training loss (e.g.~via standard arguments like NTK \\cite{jacot2018neural}) without providing any meaningful insight into the functionality of the attention mechanism.} \n\n%%%%Short version%%%%%%%%%%%%%%%\n% We establish a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism allows us to characterize the induction bias of 1-layer transformers optimizaiton: \n% \\begin{itemize}\n%     \\item Optimizing the attention parameters $(\\Kb,\\Qb)$ with vanishing regularization converges in direction towards a max-margin solution of \\eqref{eqn:sattnsvmst} with the nuclear norm objective of the combined parameter $\\W:=\\K\\Qb^\\top$ (Thm \\ref{thm global reg path}). Instead, directly parameterizing by $\\W$ minimizes a Frobenius norm SVM objective \\eqref{eqn:sattnsvm}. We characterize this convergence, highlighting that it can occur in locally-optimal directions rather than global ones.\n\n%     \\item Complementing this,  for $\\W$-parameterization, we prove the local/global directional convergence of gradient descent under suitable geometric conditions. Importantly, we show that over-parameterization catalyzes global convergence by ensuring the feasibility of the SVM problem and by guaranteeing a benign optimization landscape devoid of stationary points.\n%     \\item While our theory applies primarily to linear prediction heads, we propose a more general SVM  equivalence that predicts the implicit bias of 1-layer transformers with nonlinear heads/MLPs. \n% \\end{itemize}\n% \\textbf{(1)} Optimizing the attention parameters $(\\Kb,\\Qb)$ with vanishing regularization converges in direction towards a max-margin solution of \\eqref{eqn:sattnsvmst} with the nuclear norm objective of the combined parameter $\\W:=\\K\\Qb^\\top$ (Thm \\ref{thm global reg path}). Instead, directly parameterizing by $\\W$ minimizes a Frobenius norm SVM objective. We characterize this convergence, highlighting that it can occur in locally-optimal directions rather than global ones. \\textbf{(2)} Complementing this,  for $\\W$-parameterization, we prove the local/global directional convergence of gradient descent under suitable geometric conditions. Importantly, we show that over-parameterization catalyzes global convergence by ensuring the feasibility of the SVM problem and by guaranteeing a benign optimization landscape devoid of stationary points. \\textbf{(3)} While our theory applies primarily to linear prediction heads, we propose a more general SVM equivalence that predicts the implicit bias of 1-layer transformers with nonlinear heads/MLPs. \n%LONG version%\n\nThe paper's main contributions are as follows:\n\n\n\n\\begin{enumerate}[label=$\\bullet$, wide, labelwidth=!,itemindent=!, labelindent=5pt]\n\\item \\textbf{Implicit bias of the attention layer (Secs. \\ref{sec:prelim}-\\ref{sec:bias}).}  Optimizing the attention parameters $(\\Kb,\\Qb)$ with vanishing regularization converges in direction towards a max-margin solution of \\eqref{eqn:sattnsvmst} with the nuclear norm objective of the combined parameter $\\W:=\\K\\Qb^\\top$ (Thm \\ref{thm global reg path}). %\\ct{should we refer to Att-SVM$_*$ instead?} \nIn the case of directly parameterizing cross-attention by the combined parameter $\\W$, the regularization path (RP) directionally converges to \\eqref{eqn:sattnsvm} solution with the Frobenius norm objective. To our knowledge, this is the first result to formally distinguish the optimization dynamics of $\\W$ vs $(\\Kb,\\Qb)$ parameterizations, revealing the low-rank bias of the latter.  Our theory clearly characterizes the \\emph{optimality} of selected tokens (Definition~\\ref{score def}) and naturally extends to sequence-to-sequence or causal classification settings (see \\ref{seqattnsvm} and Theorem \\ref{local RP thm} in appendix). \n\n\n\\item \\textbf{Convergence of gradient descent (Secs. \\ref{provable global}-\\ref{sec local}).} Gradient descent (GD) iterates for the combined key-query variable $\\W$ converge in direction to a \\emph{locally-optimal} solution of \\eqref{eqn:sattnsvm} with appropriate initialization and a linear head $h(\\cdot)$ (Sec. \\ref{sec local}). For local optimality, selected tokens must have higher scores than their neighboring tokens. Locally-optimal directions are not necessarily unique and are characterized in terms of the problem geometry.  As a key contribution, we identify geometric conditions that guarantee convergence to the globally-optimal direction (Sec. \\ref{provable global}). Besides these, we show that over-parameterization (i.e.~dimension $d$ being large, and equivalent conditions) catalyzes global convergence by ensuring \\textbf{(1)} feasibility of \\eqref{eqn:sattnsvm}, and, \\textbf{(2)} benign optimization landscape, in the sense that there are no stationary points and no spurious locally-optimal directions (see Sec.~\\ref{sec overparam}). These are illustrated in Figures \\ref{fig path} and \\ref{fig overparam W bar}. \n\n% \\item \\textbf{Convergence of Gradient Descent (Secs. \\ref{provable global}--\\ref{sec local}):} Gradient descent (GD) iterations for the combined key-query variable $\\mathbf{W}$ converge in direction to a \\emph{locally-optimal} solution of \\eqref{eqn:sattnsvm}, given appropriate initialization and a linear head $h(\\cdot)$ (Sec. \\ref{sec local}). For local optimality, it is essential that selected tokens outperform neighboring tokens in scores. These locally-optimal directions, influenced by problem geometry, may vary. A significant finding is identifying geometric conditions that ensure convergence towards the globally-optimal direction (Sec. \\ref{provable global}). Furthermore, we demonstrate that over-parameterization (i.e., large dimension $d$) promotes global convergence by \\textbf{(1)} ensuring the feasibility of \\eqref{eqn:sattnsvm}, and \\textbf{(2)} creating a benign optimization landscape devoid of stationary points and spurious locally-optimal directions (Sec.~\\ref{sec overparam}), as depicted in Figures \\ref{fig path} and \\ref{fig overparam W bar}.\n\n\n\\item \\textbf{Generality of SVM equivalence (Sec. \\ref{sec:multi}).} When optimizing with linear $h(\\cdot)$, the attention layer is inherently biased towards selecting a single token from each sequence (a.k.a.~hard attention). This is reflected in \\eqref{eqn:sattnsvm} and arises from output tokens being convex combinations of the input tokens. In contrast, we show that nonlinear heads necessitate composing multiple tokens, highlighting their importance in the transformer's dynamics (Sec. \\ref{sec when}). Using insights gathered from our theory, we propose a more general SVM equivalence. Remarkably, we demonstrate that our proposal accurately predicts the implicit bias of attention trained by gradient descent under general scenarios not covered by theory (e.g.~$h(\\cdot)$ being an MLP). Specifically, our general formulae decouple attention weights into two components: A \\textbf{directional component} governed by SVM which selects the tokens by applying a 0-1 mask, and a \\textbf{finite component} which dictates the precise composition of the selected tokens by adjusting the softmax probabilities.\n\\end{enumerate}\n\nAn important feature of these findings is that they apply to arbitrary datasets (whenever SVM is feasible) and are numerically verifiable. We extensively validate the max-margin equivalence and implicit bias of transformers through enlightening experiments. We hold the view that these findings aid in understanding transformers as hierarchical max-margin token-selection mechanisms, and we hope that our outcomes will serve as a foundation for upcoming studies concerning their optimization and generalization dynamics.\n\n\\smallskip\n\\noindent\\textbf{Overview.}~The paper is structured as follows: Section \\ref{sec:prelim} introduces preliminaries on self-attention and optimization. Section~\\ref{sec:bias} analyzes self-attention's optimization geometry, showing the RP of attention parameters converges to a max-margin solution. Sections \\ref{provable global} and \\ref{sec local} present global and local gradient descent analyses, respectively, demonstrating convergence of $\\W$, the key-query variable, towards the solution of \\eqref{eqn:sattnsvm}. Section \\ref{sec:multi} provides our results on nonlinear prediction heads and generalized SVM equivalence. \n%Section~\\ref{sec seq extend} extends the theory to sequential and causal predictions.\nSection~\\ref{sec:related} discusses relevant literature. Finally, Section~\\ref{sec:conc} concludes the paper with open problems and future research directions inspired by our findings. All proofs are deferred to the appendix.\n\n% The paper's main contributions are as follows:\\redp{Make these one paragraph!}\n% \\noindent $\\bullet$ \\textbf{Implicit bias of the attention layer (Secs. \\ref{sec:prelim}-\\ref{sec:bias}).}  Optimizing the attention parameters $(\\Kb,\\Qb)$ with vanishing regularization converges in direction towards a max-margin solution of \\eqref{eqn:sattnsvmst} with the nuclear norm objective of the combined parameter $\\W:=\\K\\Qb^\\top$ (Thm \\ref{thm global reg path}). %\\ct{should we refer to Att-SVM$_*$ instead?} \n% In the case of directly parameterizing cross-attention by the combined parameter $\\W$, the regularization path (RP) directionally converges to \\eqref{eqn:sattnsvm} solution with the Frobenius norm objective. To our knowledge, this is the first result to formally distinguish the optimization dynamics of $\\W$ vs $(\\Kb,\\Qb)$ parameterizations, revealing the low-rank bias of the latter.  Our theory clearly characterizes the \\emph{optimality} of selected tokens (Definition~\\ref{score def}) and naturally extends to sequence-to-sequence or causal classification settings (see \\ref{seqattnsvm} and Theorem \\ref{local RP thm} in appendix). \n%\\noindent $\\bullet$ \\textbf{Convergence of gradient descent (Secs. \\ref{provable global}-\\ref{sec local}).} The gradient descent (GD) iterates for the combined key-query variable $\\W$ converge in direction to a \\emph{locally-optimal} solution of \\eqref{eqn:sattnsvm} with appropriate initialization and a linear head $h(\\cdot)$ (Sec. \\ref{sec local}). For local optimality, selected tokens must have higher scores than their neighboring tokens. Locally-optimal directions are not necessarily unique and are characterized in terms of the problem geometry following \\cite{tarzanagh2023margin}.  As a key contribution, we identify geometric conditions that guarantee convergence to the globally-optimal direction (Sec. \\ref{provable global}). \n% These include: (i) optimal tokens being clearly distinguishable in terms of their \\emph{scores}, or, (ii) the initial gradient direction being aligned with optimal tokens. Beyond these, we show that over-parameterization (i.e.~dimension $d$ being large, and equivalent conditions) catalyzes global convergence by ensuring \\textbf{(1)} feasibility of \\eqref{eqn:sattnsvm}, and, \\textbf{(2)} benign optimization landscape, in the sense that there are no stationary points and no spurious locally-optimal directions (see Sec.~\\ref{sec overparam}). These are illustrated in Figures \\ref{fig path} and \\ref{fig overparam W bar}. \n\n% \\noindent $\\bullet$ \\textbf{Generality of SVM equivalence (Sec. \\ref{sec:multi}).} When optimizing with linear $h(\\cdot)$, the attention layer is inherently biased towards selecting a single token from each sequence (a.k.a.~hard attention). This is reflected in \\eqref{eqn:sattnsvm} and arises from output tokens being convex combinations of the input tokens. In contrast, we show that nonlinear heads necessitate composing multiple tokens, highlighting their importance in the transformer's dynamics (Sec \\ref{sec when}). Using insights gathered from our theory, we propose a more general SVM equivalence. Remarkably, we demonstrate that our proposal accurately predicts the implicit bias of attention trained by gradient descent under general scenarios not covered by theory (e.g.~$h(\\cdot)$ being an MLP). Specifically, our general formulae decouple attention weights into two components: A \\textbf{directional component} governed by SVM which selects the tokens by applying a 0-1 mask, and a \\textbf{finite component} which dictates the precise composition of the selected tokens by adjusting the softmax probabilities.\n\n% An important feature of these findings is that they apply to arbitrary datasets (whenever SVM is feasible) and are numerically verifiable. We extensively validate the max-margin equivalence and implicit bias of transformers through enlightening experiments. We hold the view that these findings aid in understanding transformers as hierarchical max-margin token-selection mechanisms, and we hope that our outcomes will serve as a foundation for upcoming studies concerning their optimization and generalization dynamics.\n% \\vspace{5pt}\n% \\noindent\\textbf{Overview.}~The paper is structured as follows: Section \\ref{sec:prelim} introduces preliminaries on self-attention and optimization. Section~\\ref{sec:bias} analyzes self-attention's optimization geometry, showing the RP of attention parameters converges to a max-margin solution. Sections \\ref{provable global} and \\ref{sec local} present global and local gradient descent analyses, respectively, demonstrating convergence of $\\W$, the key-query variable, towards the solution of \\eqref{eqn:sattnsvm}. Section \\ref{sec:multi} provides our results on nonlinear prediction heads and generalized SVM equivalence.\n% Section~\\ref{sec:related} discusses relevant literature. Finally, Section~\\ref{sec:conc} concludes the paper with open problems and future research directions inspired by our findings. All proofs are deferred to the appendix.\n\n\n==== END OF /2308.16898/sections/introduction.tex ====\n==== BEGINNING OF /2308.16898/sections/zzz_prelim.tex ====\n\\begin{comment}\nLet us now shift our focus towards solving the SVM problem using optimal indices denoted by $(\\opt_i)_{i=1}^n$. It is important to note that the problem can be formulated for arbitrary sets of indices $\\boldsymbol{\\alpha} = (\\alpha_i)_{i=1}^n$, which will reveal locally optimal directions, as defined in Section~\\ref{sec local}. We introduce the following formulation:\n\\begin{tcolorbox}[colback=white!5!white,colframe=black!5!black,colback=green!1!white]\n\\begin{equation}\\tag{\\name}\n\\Wm_\\dm=\\arg\\min_{\\W}\\td{\\W}\n\\quad \\text{subject to} \\quad \\min_{t\\neq \\op_i}(\\x_{i\\op_i}-\\x_\\itt)^\\top\\W\\z_i\\geq 1\\quad \\text{for all} \\quad 1\\leq i\\leq n\\label{eqn:sattnsvm}.\n\\end{equation}\n\\end{tcolorbox}\nIn \\eqref{eqn:sattnsvm}, we use $\\diamond$ to denote a generic objective function that captures the inductive bias of the optimization problem. Throughout our discussion, we will use $\\Wm$ to represent the unique solution of \\eqref{eqn:sattnsvm} with the Frobenius norm objective, and $\\Wm_\\star$ to denote the solution of \\eqref{eqn:sattnsvm} with the nuclear norm. In the following, we provide insights and discussions on the implicit bias of gradient descent applied to \\eqref{eqn:erm:w} towards $\\Wm$. In the next section, we delve into discussions on $\\Wm_\\star$ and its connection to \\eqref{eqn:erm:kq} and the SVM problem parameterized by $(\\Kb,\\Qb)$.\n%, which represents the unique solution of \\eqref{eqn:sattnsvm} with the Frobenius norm.\n%As we shall see, $\\Wm$ corresponds to the implicit bias of gradient descent applied to the \\eqref{eqn:erm:w} formulation parameterized by $\\W$. \n\n\nIt is worth noting that the existence of matrix $\\Wm$ implies the separability of tokens $\\op_i$ from the others. Additionally, the term $a_{\\itt}=\\x_\\itt^\\top\\W\\z_i$ represents the dot product between the key-query features before applying the softmax nonlinearity. This dot product is a crucial characteristic of self-attention. Therefore, we can express the SVM constraints in \\eqref{eqn:sattnsvm} as $a_{i\\op_i}\\geq a_{\\itt}+1$. This formulation aims to find the most efficient direction that ensures the optimal token $\\x_{i\\op_i}$ achieves the highest similarity with query $\\z_i$ among all key embeddings.\n\nTo gain further insight into equation \\eqref{eqn:sattnsvm}, let's consider that the output token of the attention layer is a convex mixture of the input tokens. If the training loss is minimized by selecting the optimal token $\\x_{i\\op_i}$, then the softmax similarities should converge to a one-hot vector that precisely includes $\\x_{i\\op_i}$ (assigned a value of 1) and disregards all other tokens (assigned 0s). For this convergence to occur, the attention weights $\\W$ should diverge in norm to saturate the softmax probabilities. However, the weights converge in direction to the max-margin SVM due to the exponential-tailed nature of the softmax function. This phenomenon resembles the inductive bias of logistic regression on separable data \\cite{soudry2018implicit,ji2018risk}. Lemma \\ref{lem min risk} formalizes this intuition and provides a rigorous motivation for our definition of optimal tokens.\n\n\n% We now turn our attention to solving the SVM problem using optimal indices denoted by $ (\\opt_i)_{i=1}^n$. It is important to note, however, that the problem can be formulated for arbitrary sets of indices $\\boldsymbol{\\alpha} = (\\alpha_i)_{i=1}^n$, which will reveal locally-optimal directions; see Definition~\\ref{sec local}.\n% %\n% \\tcbset{colback=white!5!white,colframe=black!5!black,colback=green!1!white}\n% \\begin{tcolorbox}%[height=1cm, width=15cm]\n% \\begin{equation}\\tag{\\name}\n% \\Wm_\\dm=\\arg\\min_{\\W}\\td{\\W}\n%  \\quad \\text{subj. to}   \\quad \\min_{t\\neq \\op_i}(\\x_{i\\op_i}-\\x_\\itt)^\\top\\W\\z_i\\geq 1\\quad \\text{for all} \\quad 1\\leq i\\leq n\\label{eqn:sattnsvm}.\n% \\end{equation}\n% \\end{tcolorbox}\n% Here, we use $\\diamond$ to denote a generic objective function capturing the inductive bias of the optimization problem. Thourghout, we will use to denote $\\Wm$ be the unique solution of \\eqref{eqn:sattnsvm} with Frobenius norm, and $\\Wc^\\svm_\\star$ be the solution set of \\eqref{eqn:sattnsvm} with nuclear norm and cost function $\\tnuc{\\Wc^\\svm_\\star}$. \n\n\n\n% We first provide insights and discussions on the implicit bias of gradient descent applied to \\eqref{eqn:erm:w} towrads  $\\Wm$ as be the unique solution of \\eqref{eqn:sattnsvm} with Frobenius norm. As we shall see,  $\\Wm$ will correspond to implicit bias of gradient descent applied to the \\eqref{eqn:erm:w} parameterized by $\\W$. In the next section, we provide discussions on $\\Wm_c$ and how it is connected to   \\eqref{eqn:erm:kq} and $(\\Kb,\\Qb)$ parameterized SVM problem.\n\n\n\n% Note that the existence of matrix $\\Wm$ implies the separability of tokens $\\op_i$ from the others. Additionally, the term $a_{\\itt}=\\x_\\itt^\\top\\W\\z_i$ represents the dot product between the key-query features before applying the softmax nonlinearity. This dot product is a crucial characteristic of self-attention. Therefore, we can express the SVM constraints in \\eqref{eqn:sattnsvm} as $a_{i\\op_i}\\geq a_{\\itt}+1$. This formulation aims to find the most efficient direction that ensures the optimal token $\\x_{i\\op_i}$ achieves the highest similarity with query $\\z_i$ among all key embeddings.  To gain further insight into equation \\eqref{eqn:sattnsvm}, let's consider that the output token of the attention layer is a convex mixture of the input tokens. If the training loss is minimized by selecting the optimal token $\\x_{i\\op_i}$, then the softmax similarities should converge to a one-hot vector that precisely includes $\\x_{i\\op_i}$ (assigning a value of 1) and disregards all other tokens (assigning 0s). In order for this convergence to happen, the attention weights $\\W$ should diverge in norm to saturate the softmax probabilities. However, the weights converge in direction to the max-margin SVM due to the exponential-tailed nature of the softmax function. This phenomenon is similar to the inductive bias of logistic regression on separable data \\cite{soudry2018implicit,ji2018risk}. Lemma \\ref{lem min risk} formalizes this intuition and provides rigorous motivation for our optimal token definition.\n\n% Note that the existence of $\\Wm$ implies the separability of tokens $\\op_i$ from the others. Furthermore, $a_{\\itt}=\\x_\\itt^\\top\\W\\z_i$ represents the dot product between the key-query features before the softmax nonlinearity, which is an essential feature of self-attention. Therefore, the SVM constraints in \\eqref{eqn:sattnsvm} can be expressed as $a_{i\\op_i}\\geq a_{\\itt}+1$. This formulation can be interpreted as seeking the most efficient direction that ensures the optimal token $\\x_{i\\op_i}$ achieves the highest similarity with query $\\z_i$ among all key embeddings.  To provide further insight into \\eqref{eqn:sattnsvm}, consider that the output token of the attention layer is a convex mixture of the input tokens. If the training loss is minimized by selecting the optimal token $\\x_{i\\op_i}$, then the softmax similarities should converge to a one-hot vector that precisely includes $\\x_{i\\op_i}$  (assigning a value of 1) and disregards all other tokens (assigning 0s). For this to occur, the attention weights $\\W$ should diverge in norm to saturate the softmax probabilities. However, the weights converge in direction to the max-margin SVM due to the exponential-tailed nature of the softmax function. This phenomenon is similar to the inductive bias of logistic regression on separable data \\cite{soudry2018implicit,ji2018risk}. This intuition is formalized in Lemma \\ref{lem min risk} which also rigorously motivates our optimal token definition.\n\n\n\n\n%\\smallskip\n\n% Above $\\diamond$ denotes a generic objective function capturing the inductive bias of the optimization problem: As we shall see, it will correspond to the Frobenius norm when the problem is parameterized by $\\W$. On the other hand, $(\\Kb,\\Qb)$ parameterization with $\\W\\gets\\Kb\\Qb^\\top$ will be biased towards minimizing nuclear norm. \n\n\\subsection{Non-convex hard-margin SVM problem for cross attention with parameters $(\\mathbf{Q},\\mathbf{K})$ } \n\nThe objective function in \\eqref{eqn:erm:kq} generally lacks convexity in terms of $(\\Qb, \\Kb)$. However, when training without regularization \\footnote{ maybe give $\\lambda=1e-16$...} the token separation with a maximum margin occurs. In this case, gradient descent update \\ref{GD-QK}  produces iterations that diverge in norm but converge in direction (refer to Figure~\\ref{fig path}). This behavior resembles the margin maximization observed in non-convex problems such as homogeneous neural networks \\cite{lyu2019gradient,ji2020directional}, which is an implicit regularization effect of gradient descent. Notably, \\cite{lyu2019gradient,ji2020directional} has demonstrated that solutions obtained through gradient descent and gradient flow converge in direction towards a KarushKuhnTucker (KKT) point of the max-margin SVM. Motivated by the aforementioned works and supported by preliminary experiments illustrated in Figure~\\ref{fig path}, along with a comprehensive numerical study detailed in Section~\\ref{sec:exprim}, we introduce a nonconvex SVM problem. This problem is parameterized by $(\\Qb, \\Kb)$ similar to its twin problem \\eqref{eqn:erm:kq},  and its objective is to achieve a separation between the optimal tokens indexed by $(\\opt_i)_{i=1}^n$ and\nfrom the remaining tokens in the input sequence $\\{\\X_i\\}_{i=1}^n$.\n% Motivated by these works, and supported by initial experiments depicted in Figure~\\ref{fig path} as well as a more extensive numerical study discussed in Section~\\ref{sec:exprim}, we propose a nonconvex SVM problem \n% paramterized by $(\\Qb, \\Kb)$ that aims to separate  \n% optimal tokens indexed by $(\\opt_i)_{i=1}^n$ and\n% from the remaining tokens in the input sequence $\\{\\X_i\\}_{i=1}^n$.\n\\tcbset{colback=white!5!white,colframe=black!5!black,colback=green!1!white}\n\\begin{tcolorbox}[height=2cm]\n\\begin{equation}\\tag{KQ-SVM}\n\\Kb,\\Qb=\\arg\\min_{\\Kb,\\Qb}~\\frac{1}{2} \\left(\\|\\Kb\\|_F^2+\\|\\Qb\\|_F^2\\right)\n \\quad \\text{subj. to}   \\quad \\min_{t\\neq \\op_i}(\\x_{i\\op_i}-\\x_\\itt)^\\top\\Kb\\Qb^\\top\\z_i\\geq 1\\quad \\text{for all} \\quad 1\\leq i\\leq n\\label{eqn:qk:svm}.\n\\end{equation}\n\\end{tcolorbox}\n%\n% A KKT point satisfies certain conditions, known as KKT conditions, which were proven to be necessary for optimality of certen problems that in the case\n% of homogeneous neural networks, these\n% conditions are necessary for optimality. However, they are not sufficient\n% even for local optimality \n% . However, these conditions are not sufficient for local optimality. Therefore,\n\nA KKT point does not always represent an optimal solution for the maximum-margin problem. This is analogous to showing that an unconstrained optimization problem reaches a point where the gradient is zero, without proving whether it is a global minimum, local minimum, or saddle point. While convergence to a KKT point in the \\eqref{eqn:qk:svm} formulation indicates a tendency towards maximizing the margin in the parameter space, it does not provide a guarantee for reaching a maximum-margin solution.\n\nOur objective is to demonstrate that the non-convex optimization problem \\eqref{eqn:erm:kq} at hand is not as daunting as it may initially seem. Specifically, we aim to establish that, apart from the global optimum, all other KKT points  act as saddle points that can be circumvented through gradient descent updates \\ref{GD-QK}. In fact, we identify the implicit regularizer as the \\textit{nuclear norm} and illustrate that, even in the absence of constraints on the factored matrix $\\W=\\Kb\\Qb^\\top$ and employing a full-dimensional factorization, optimization through gradient descent on the $(\\Qb, \\Kb)$ factorization directs us towards the solution of \\eqref{eqn:sattnsvm} with the minimum nuclear norm, i.e. $\\Wm_\\star$.   Our empirical investigation allows us to conjecture that, with appropriate step size,  \\ref{GD-QK} converges to the solution of \\eqref{eqn:sattnsvm} with the minimum nuclear norm, and we provide empirical evidence to support this conjecture, substantiating it within certain restricted settings.\n\\end{comment}\n\n% To provide further intuition on \\eqref{eqn:sattnsvm}, note that, the output token of the attention layer is a convex mixture of input tokens. If the training loss is minimized by selecting the optimal token $\\x_{i\\op_i}$, then, softmax similarities should converge to a one-hot vector that precisely includes $\\x_{i\\op_i}$ (by assigning 1) and discards all other tokens (by assigning 0s). For this to happen, the attention weights $\\W$ should diverge in norm to saturate the softmax probabilities. However, the weights converge in direction to the max-margin SVM thanks to the fact that softmax function is exponentially-tailed. This phenomenon is akin to the inductive bias of logistic regression on separable data \\red{\\cite{}}.\n%Thus, the SVM constraints of \\eqref{eqn:sattnsvm} are of the form $a_{i\\op_i}\\geq a_\\itt+1$. This has the following interpretation: \\eqref{eqn:sattnsvm} is seeking for the most efficient direction that ensures optimal token $\\x_{i\\op_i}$ achieves largest similarity with query $\\z_i$ among all key embeddings.\n%The high-level intuition underlying \\eqref{eqn:sattnsvm} is as follows: \n%what remains unknown is the specific performance metric $\\W$ will optimize. To capture this uncertainty, we introduce a generic norm denoted by $\\diamond$. Several potential candidates for the $\\diamond$ norm include the Frobenius norm $\\|\\cdot\\|_F$, the Nuclear norm $\\|\\cdot\\|_\\star$, the Spectral norm $\\|\\cdot\\|$, and the $\\ell_2$ margin minimizer $\\|\\W\\|_{\\diamond}=\\min_{i\\in[n]} \\tn{\\W\\z_i}$. This leads us to our conjecture.\n%of the attention has to be saturated to guarantee  If the training loss benefits from atte\n%Given the exponentially-decaying tail of the softmax function, we expect attention mechanism to exhibit a bias towards selecting the optimal token that minimizes the empirical risk, assuming the tokens are separable. However, what remains unknown is the specific performance metric $\\W$ will optimize. To capture this uncertainty, we introduce a generic norm denoted by $\\diamond$. Several potential candidates for the $\\diamond$ norm include the Frobenius norm $\\|\\cdot\\|_F$, the Nuclear norm $\\|\\cdot\\|_\\star$, the Spectral norm $\\|\\cdot\\|$, and the $\\ell_2$ margin minimizer $\\|\\W\\|_{\\diamond}=\\min_{i\\in[n]} \\tn{\\W\\z_i}$. This leads us to our conjecture.\n==== END OF /2308.16898/sections/zzz_prelim.tex ====\n==== BEGINNING OF /2308.16898/sections/extensions.tex ====\n\\section{Extending the Theory to Sequential and Causal Predictions}\\label{sec seq extend}\n\n%\\redp{Discuss this extension here as a small self-contained section and include masked attention formulation in the discussion}\n\n%\\noindent\\textbf{Generalization to sequential and causal predictions:} \n% (see \\ref{serm-w} \\& \\ref{serm-kq})\nWhile our formulations (\\ref{eqn:erm:w} \\& \\ref{eqn:erm:kq}) regress a single label $Y_i$ per $(\\X_i,\\z_i)$, we extend in Appendix \\ref{sec multioutput}  our findings to the sequence-to-sequence classification setting, where we output and classify all $T$ tokens per input $\\X_i,\\Z_i\\in\\R^{T\\times d}$. In this scenario, we prove that all of our RP guarantees remain intact after introducing a slight generalization of \\eqref{eqn:sattnsvm}. Concretely, consider the following ERM problems for sequential and causal settings:\n\\begin{align}\n\\Lc^{\\texttt{seq}}(\\W)=\\frac{1}{n}\\sum_{i=1}^n\\sum_{k=1}^T \\ell(Y_\\ik\\cdot h( \\X_i^\\top \\sft{\\X_i\\W\\z_\\ik})),\\quad\\text{and}\\quad%\\label{serm-w}\\tag{SEQ}\\\\\n\\Lc^{\\texttt{csl}}(\\W)=\\frac{1}{n}\\sum_{i=1}^n\\sum_{k=1}^T \\ell(Y_\\ik\\cdot h( \\X_i^\\top \\sftk{\\X_i\\W\\z_\\ik})).%\\label{serm-w}\\tag{CSL}\n\\end{align}\nBoth equations train $T$ tokens per input $(\\X_i,\\Z_i)$ and, as usual, we recover self-attention via $\\Z_i\\gets \\X_i$. For the causal setting, we use the masked attention $\\sftk{\\cdot}$ which calculates the softmax probabilities over the first $k$ entries of its input and sets the remaining $T-k$ entries to zero. This way, the $k$'th prediction of the transformer only utilizes tokens from $1$ to $k$ and not the future tokens.\n\n%score def\nLet $\\bal=(\\alpha_\\ik)\\ikix$ be tokens to be selected by attention (e.g.~locally-optimal indices, see Def.~\\ref{seq loc opt}). Then, the sequential generalization of \\eqref{eqn:sattnsvm} corresponding to $\\Lc^{\\texttt{seq}}(\\W)$ is given by\n\\begin{equation}%\\tag{S\\name}\n \\min_{\\W}\\tf{\\W}\n\\quad \\text{subj. to} \\quad  (\\x_{i\\aik}-\\x_\\itt)^\\top\\W\\z_\\ik\\geq 1\\quad\\textnormal{for all}\\quad {t\\neq \\aik},~~k\\in[T],~~ i\\in[n]~~\\label{seqattnsvm1}.\n\\end{equation}\nWe refer the reader to Appendix \\ref{sec multioutput} which rigorously establishes all RP results for this sequential classification setting. On the other hand, for the causal inference setting SVM should reflect the fact that the model is not allowed to make use of future tokens. Note that the SVM constraints directly arise from softmax calculations. Thus, since attention is masked over the indices $t\\leq k$ and $k\\in[T]$, the SVM constraints should apply over the same mask. Thus, we can consider the straightforward generalization of global-optimality where $\\op_\\ik$ is the token index with highest score over indices $t\\leq k$ and introduce an analogous definition for local-optimality. This leads to the following variation of \\eqref{seqattnsvm1}, which aims to select indices $\\alpha_\\ik\\in [k]$ from the first $k$ tokens\n\\begin{equation*}%\\tag{C\\name}\n \\min_{\\W}\\tf{\\W}\n\\quad \\text{subj. to} \\quad   (\\x_{i\\aik}-\\x_\\itt)^\\top\\W\\z_\\ik\\geq 1\\quad \\textnormal{for all} \\quad  t\\neq \\aik,~~t\\leq k, ~~k \\in[T],~~i\\in[n]\\label{cslattnsvm}.\n\\end{equation*}\n\nCausal attention is a special case of a general attention mask which can restrict softmax to arbitrary subset of the tokens. Such general masking can be handled similar to \\eqref{cslattnsvm} by enforcing SVM constraints over the nonzero support of the mask. Finally, the discussion so far extends our main theoretical results and focuses on selecting single token per sequence. It can further be enriched by the generalized SVM equivalence developed in Section \\ref{sec:multi} to select and compose multiple tokens by generalizing \\eqref{eqn:mattnsvm}.\n==== END OF /2308.16898/sections/extensions.tex ====\n==== BEGINNING OF /2308.16898/sections/experiments.tex ====\n\\section{Experiments}\\label{sec:exprim}\n\n\n\\red{\nTODO:\n\\begin{itemize}\n    \\item \\textbf{Page 2}: visualization using $\\W\\z$ + overparam + ?\n    \\item Low rank experiment: vary $m$ and train $(\\Kb,\\Qb)$\n    \\item Nonlinear head e.g. ReLU function\n    \\item Self attention using $\\V$\n\\end{itemize}\n}\n\n\\red{[Move this part to Experiment section?]}\\ct{agree}\n\n \n%, Above, we assumed that $(\\Kb,\\Qb)$-parameterization is full dimensional i.e. $m=d$. Intuitively, setting $m<d$ should promote a low-rank bias (beyond nuclear norm) as $\\Kb\\Qb^\\top$ is at most rank $m$ by construction.\n%unconstrained solutions are .  \n\n%Also remark that we can capture local geometry as discussed in Sec 3.\\\\(2) Add and discuss experiments on Frobenius and Nuclear norm supporting the conclusions.\\\\(3) Ideally, we should have an experiment with $m<d$ and another with $h(x)=\\onebb^\\top\\texttt{ReLU}(\\Vb\\x)$. The former is easy! Whenever sample size is $n\\leq d$, you automatically have rank-$n$ solution.}\n==== END OF /2308.16898/sections/experiments.tex ====\n==== BEGINNING OF /2308.16898/sections/fig_overparam.tex ====\n\\begin{figure}[t]\n    \\centering\n    % \\hspace{-10pt}\n    \\subfigure[Global convergence for varying $n,d$]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.25\\columnwidth, trim={1.3cm 1.3cm 0 0}, clip]{figs/overparam_diff_n.pdf}};\n        \\node at (0,-2.3) {\\small{Varying $d$}};\n        \\node[rotate=90] at (-3.1,0) {\\small{Prob. of global convergence}};\n        \\node at (1.7,0.3){\\small{{$\\W$}}};\n        \\node at (1.9,-.1){\\small{{$(\\Kb,\\Qb)$}}};\n        \\end{tikzpicture}\n        \\label{fig overparam diff n}\n    }\n    \\hspace{30pt}\n    \\subfigure[Global convergence for varying $T,d$]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.25\\columnwidth, trim={1.3cm 1.3cm 0 0}, clip]{figs/overparam_diff_T.pdf}};\n        \\node at (0,-2.3) {\\small{Varying $d$}};\n        \\node[rotate=90] at (-3.1,0) {\\small{Prob. of global convergence}};\n        \\node at (1.7,0.3){\\small{{$\\W$}}};\n        \\node at (1.9,-.1){\\small{{$(\\Kb,\\Qb)$}}};\n        \\end{tikzpicture}\n        \\label{fig overparam diff T}\n    }\n    \\caption{Global convergence behavior of GD when training cross-attention weights $\\W$ (solid) or $(\\Kb,\\Qb)$ (dashed) with random data. The blue, green, and red curves represent the probabilities of global convergence for \\textbf{(a)}: fixing $T=5$ and varying $n \\in \\{5,10,20\\}$ and \\textbf{(b)}: fixing $n=5$ and varying $T \\in\\{5,10,20\\}$. Results demonstrate that for both attention models, as $d$ increases (due to over-parameterization), attention weights tend to select optimal tokens $(\\opt_i)_{i=1}^n$.\n}\n   \\label{fig overparam}\n\\end{figure}\n==== END OF /2308.16898/sections/fig_overparam.tex ====\n==== BEGINNING OF /2308.16898/supp/toy_dataset.tex ====\n\\section{Toy Dataset for Globally-Convergent Self-Attention}\n\nIn this section, we introduce a toy dataset model for which Assumption \\ref{assum:nabla0} holds with $\\z_i\\gets\\x_{i1}$. Importantly, this dataset captures the spirit of self-attention by allowing the model to retrieve different output tokens depending on the value of the first token. More concretely, in our model, whenever $\\x_1$ admits a value $\\ab_j$ from a dictionary $\\Ac=\\{\\ab_j\\}_{j=1}^r$, self-attention should retrieve a relevant token $\\x_t=\\bb_j$ from a dictionary $\\Bc=\\{\\bb_j\\}_{j=1}^r$.\n\n%All tokens are centered i.e.~$\\E[\\x_t]=0$ for all $t\\in[T]$. \n\\begin{definition} [Toy Distribution for Self-Attn]\\label{def data model} Data $(\\X,Y)$ is generated according to $\\Dc_{\\texttt{data}}$ as follows: Let $\\rho >1$ be the index of $\\x_1$'s \\emph{relevant token} (that is allowed to be random).\n\\begin{itemize}%$\\R^d\\times \\R^d$ with \n\\item \\textbf{Relevant token:} $(\\x_1,\\x_\\rho)$ has a uniform distribution over $r$ values $(\\ab_j,\\bb_j)_{j=1}^r$ with associated labels $(y_j)_{j=1}^r$. That is, whenever $(\\x_1,\\x_\\rho)=(\\ab_j,\\bb_j)$, the output label is $Y=y_j$. \n\\item \\textbf{Fixed score:} For some $\\gamma_1,\\gamma_\\rho$ and all $j\\in[r]$: $y_j \\vb^\\top\\ab_j=\\gamma_1$, $y_j \\vb^\\top\\bb_j=\\gamma_\\rho>0$.\n\\item \\textbf{Other tokens} $t\\not\\in\\{\\rho,1\\}$ are bounded, independent of the rest, and $\\E[\\x_t]=\\E[\\x_t\\x_t^\\top]\\vb=0$.%$\\bSi=\\E[\\x_t\\x_t^\\top]$ obeys $\\bSi\\vb=0$.\n\\end{itemize}\n\\end{definition}\n\n\\redp{Orthogonality condition for concrete example: Suppose vectors $(\\ab_j,\\bb_j)_{j=1}^r$ are all pairwise orthonormal and other tokens have small projections to the subspace $\\text{span}(\\ab_j,\\bb_j)_{j=1}^r$. First, this would provide a simple explicit example to instantiate Theorem \\ref{toy data thm} and Assumption \\ref{assume sep}. Second, based on Def.~\\ref{def data model}, this would also allow us to discuss the setting where \n\\begin{itemize}\n\\item Self-attention selects $\\x_\\rho$ if $\\gamma_\\rho>\\gamma_1$.\n\\item Self-attention selects $\\x_1$ if $\\gamma_1>\\gamma_\\rho$.\n\\end{itemize}\n$\\gamma_\\rho$ vs $\\gamma_1$ is related to the following concern: How can our model highlight ``self'' of self-attention?}\n\n\\begin{assumption}[Separation] \\label{assume sep}Consider the dataset $\\Dc_{\\texttt{data}}$ of Def.~\\ref{def data model}. Let $\\W_\\rho=\\frac{1}{r}\\sum_{j=1}^r\\bb_j\\ab_j^\\top$. Assume $|\\gamma_1|$ is sufficiently small, $T$ is sufficiently large and for an arbitrarily small $\\eps>0$: \n%Scores obey Attention similarities obey \n\\begin{align}\n\\gamma_\\rho-y_j\\vb^\\top \\x_t\\geq\\eps\\quad\\text{and}\\quad (\\bb_j-\\x_t)^\\top\\W_\\rho\\ab_j\\geq \\eps\\quad\\text{for all}~j\\in[r],t\\neq\\rho\\quad\\text{almost surely}.\\label{sep condition}\n\\end{align}\n\\end{assumption}\n\n\n\n\\begin{theorem}\\label{toy data thm} Consider the dataset model $\\Dc_{\\texttt{data}}$ of Def.~\\ref{def data model}. Denote the initial population gradient $\\nabla\\Lc(0):=\\E_{\\Dc_{\\texttt{data}}}[\\nabla\\Lc(0)]$. Let $\\W_1=\\frac{1}{r}\\sum_{j=1}^r\\ab_j\\ab_j^\\top$ and $\\W_\\rho=\\frac{1}{r}\\sum_{j=1}^r\\bb_j\\ab_j^\\top$. We have that%For $\\X\\sim\\Dc_{\\texttt{data}}$, w\n\\begin{align}\n&\\nabla\\Lc(0)=\\frac{\\ell'(0)}{T}(\\gamma_1\\W_1+\\gamma_\\rho\\W_\\rho)-\\frac{\\ell'(0)}{T^2}(\\gamma_1+\\gamma_\\rho)(\\W_1+\\W_\\rho)\\label{eq nablaL0}\n\\end{align}\nAdditionally, suppose Assumption \\ref{assume sep} holds. Then, $\\x_\\rho$ is the optimal token and Assumption \\ref{assum:nabla0} holds almost surely i.e.\n%self-attention at $\\nabla\\Lc(0)$selects $\\x_\\rho$\n\\[\n\\underset{t\\in[T]}{\\min}\\li(\\x_t-\\x_\\rho)^\\top\\nabla\\Lc(0)\\x_1\\ri>0.\n\\]\n\\end{theorem}\n\\begin{proof} We will make use of the simple structure of softmax at $\\W=0$. Let $\\bgam=Y\\cdot\\X\\vb$ and $\\Fb_t=\\x_t\\x_1^\\top$. Using the fact that softmax derivative at $0$ is simply all $1/T$ vector, we have that\n\\begin{align}\n\\nabla\\Lc(0)=\\frac{\\ell'(0)}{T}\\E[\\sum_{t=1}^T \\bgam_t\\Fb_t]-\\frac{\\ell'(0)}{T^2}\\E[\\sum_{t=1}^T \\bgam_t\\sum_{t=1}^T \\Fb_t].\n\\end{align}\n%y_j\\vb^\\top\\ab_j%\\vb^\\top\\bb_j\nLet $\\bSi=\\E[\\x_t\\x_t^\\top]$ for $t\\notin\\{1,\\rho\\}$. To proceed, observe that, for $t\\not\\in\\{1,\\rho\\}$\n\\[\n\\E[\\bgam_t\\Fb_t]=\\E[Y\\vb^\\top\\x_t\\x_t\\x_1^\\top]=\\E[\\vb^\\top\\x_t\\x_t]\\E[Y\\x_1]^\\top=\\bSi\\vb\\E[Y\\x_1]^\\top=0.\n\\]\nSimilarly for $t\\not\\in\\{1,\\rho\\}$, we have $\\E[\\sum_{i=1}^T \\bgam_i\\Fb_t]=0$ by additionally using $\\E[\\x_t]=0$.\n\nWhat remains is $t=1$ and $t=\\rho$. Using fixed score assumption, we find\n\\[\n\\E[\\bgam_1\\Fb_1]=\\E[Y\\vb^\\top\\x_1\\x_1\\x_1^\\top]=\\gamma_1\\W_1,\\quad\\E[\\bgam_\\rho\\Fb_\\rho]=\\E[Y\\vb^\\top\\x_\\rho\\x_\\rho\\x_1^\\top]=\\gamma_\\rho\\W_\\rho.\n\\]\nSimilarly, we obtain\n\\[\n\\E[\\sum_{t=1}^T \\bgam_t\\sum_{t=1}^T \\Fb_t]=\\frac{1}{T^2}(\\gamma_1+\\gamma_\\rho)(\\W_1+\\W_\\rho),\n\\]\nto conclude with \\eqref{eq nablaL0}. To conclude with Assumption \\ref{assum:nabla0}, we use Assumption \\ref{assume sep} and observe that $T\\nabla\\Lc(0)$ is arbitrarily close to $T\\hat{\\nabla}\\Lc(0)$ where $\\hat{\\nabla}\\Lc(0)=\\frac{\\ell'(0)\\gamma_\\rho}{T}\\W_\\rho$. Now, recalling $\\ell'(0)<0$, applying the right hand-side of \\eqref{sep condition} with $\\hat{\\nabla}\\Lc(0)\\propto-\\W_\\rho$, and using the boundedness of tokens and $\\eps>0$ as perturbation buffer, we obtain the desired statement.\n\\end{proof}\n==== END OF /2308.16898/supp/toy_dataset.tex ====\n==== BEGINNING OF /2308.16898/supp/app_exp.tex ====\n\\section{Supporting Experiments}\\label{app supp exp}\n% \\input{sections/fig_overparam_app}\n\\input{sections/fig_nn_main}\n\nIn this section, we introduce implementation details and additional experiments. Code is available at\n\\begin{center}\n\\url{https://github.com/umich-sota/TF-as-SVM}\n\\end{center}\nWe create a 1-layer self-attention using \\texttt{PyTorch}, training it with the SGD optimizer and a learning rate of $\\eta=0.1$. We apply normalized gradient descent to ensure divergence of attention weights. The attention weight $\\W$ is then updated through\n\\[\n\\W(k+1)=\\W(k)-\\eta\\frac{\\nabla\\Lc(\\W(k))}{\\|\\nabla\\Lc(\\W(k))\\|_F}.\n\\]\nIn the setting of $(\\Kb,\\Qb)$-parameterization, we noted that with extended training iterations, the norm of the combined parameter $\\Kb\\Qb^\\top$ consistently rises, despite the gradient being treated as zero due to computational limitations. To tackle this issue, we introduce a minor regularization penalty to the loss function, ensuring that the norms of $\\Kb$ and $\\Qb$ remain within reasonable bounds. This adjustment involves\n\\[\n\\widetilde\\Lc(\\Kb,\\Qb)=\\Lc(\\Kb,\\Qb)+\\lambda(\\|\\Kb\\|^2_F+\\|\\Qb\\|^2_F).\n\\]\nHere, we set $\\lambda$ to be the the smallest representable number, e.g. computed as $1+\\lambda\\neq1$ in \\texttt{Python}, which is around $2.22\\times10^{-16}$. Therefore, $\\Kb,\\Qb$ parameters are updated as follows.\n\\[\n\\Kb(k+1)=\\Kb(k)-\\eta\\frac{\\nabla\\widetilde\\Lc_\\Kb(\\Kb(k),\\Qb(k))}{\\|\\nabla\\widetilde\\Lc_\\Kb(\\Kb(k),\\Qb(k))\\|_F}, \\qquad \\Qb(k+1)=\\Qb(k)-\\eta\\frac{\\nabla\\widetilde\\Lc_\\Qb(\\Kb(k),\\Qb(k))}{\\|\\nabla\\widetilde\\Lc_\\Qb(\\Kb(k),\\Qb(k))\\|_F}.\n\\]\n$\\bullet$ As observed in previous work \\cite{tarzanagh2023margin}, and due to the exponential expression of softmax nonlinearity and computation limitation, \\texttt{PyTorch} has no guarantee to select optimal tokens when the score gap is too small. Therefore in Figures~\\ref{fig overparam W bar}, \\ref{fig overparam bar} and \\ref{fig overparam}, we generate random tokens making sure that $\\min_{i\\in[n],t\\neq\\op_i}\\bgam_{i\\op_i}-\\bgam_{it}\\geq\\underline\\gamma$ and we choose $\\underline\\gamma=0.1$ in our experiments.\n%\n%\n\\paragraph{Rank sensitivity of $(\\Kb,\\Qb)$-parameterization (Figure~\\ref{fig rank m}).} In Figure \\ref{fig rank} and Lemma~\\ref{lem:rank}, we have both theoretically and empirically established that the rank of the SVM solution, denoted as $\\Ws$ in \\eqref{eqn:sattnsvm} or $\\Ws_\\st$ in \\eqref{eqn:sattnsvmst}, is at most rank $\\max(n,d)$. Now, moving to Figure~\\ref{fig rank m}, we delve into GD performance across various dimensions of $\\Kb,\\Qb\\in\\R^{d\\times m}$ while keeping $d=20$ fixed and varying $m$ from $1$ to $10$. In the upper subfigure, we maintain a constant $n=5$ and vary $T$ within $\\{5,10,15\\}$, while in the lower subfigure, $T$ is fixed at $5$ and $n$ changes within $\\{5,10,15\\}$. Results are depicted using blue, green, and red dashed curves, with both $y$-axes representing $1-\\corr{\\W,\\Ws_{\\st,\\bal}}$, where $\\W$ represents the GD solution and $\\Ws_{\\st,\\bal}$ is obtained from \\eqref{eqn:sattnsvmst} by employing token indices $\\bal$ selected via GD and setting the rank limit to $m=d$. Observing both subfigures, we note that a larger $n$ necessitates a larger $m$ for attention weights $\\Kb\\Qb^\\top$ to accurately converge to the SVM solution (Figure~\\ref{fig rank m}(lower)). Meanwhile, performances remain consistent across varying $T$ values (Figure \\ref{fig rank m}(upper)). This observation further validates Lemma \\ref{lem:rank}. Furthermore, the results demonstrate that $\\W$ converges directionally towards $\\Ws_{\\st,\\bal}$ as long as $m\\gtrsim n$, thereby confirming the assertion in our Theorem~\\ref{thm:local:gd}.  \n% Additionally, in Figure \\ref{fig overparam KQ bar}, building on Figure \\ref{fig overparam W bar}, we explore the convergence patterns of the $(\\Kb,\\Qb)$-parameterization. The results show a similar performance trend, indicating that within the $(\\Kb,\\Qb)$-parameterization framework, the attention weights $\\W:=\\Kb\\Qb^\\top$ tend to converge towards the direction of the globally optimal attention SVM solution $\\Ws_{\\st}$.\n%\n%\n\\paragraph{Behavior of GD  with nonlinear nonconvex prediction head and multi-token compositions (Figure~\\ref{fig nn main}).} To better investigate how correlation changes with data dimension $d$, we collect the solid curves in Figure~\\ref{fig nn diff d}(upper) and construct as Figure~\\ref{fig nn itr}. Moreover, Figure \\ref{fig nn sfx zero} displays the average correlation of instances (refer to scatters in Figure~\\ref{fig nn diff d} (lower)), considering masked tokens with softmax probability $<\\Gamma$. Both findings highlight that higher $d$ enhances alignment. For $d\\geq8$ or $\\Gamma\\leq10^{-9}$, the GD solution $\\W$ achieves a correlation of $>0.99$ with the SVM-equivalence $\\W^{\\rfn}$, defined in Section~\\ref{sec:multi}.\n%\n\\input{sections/fig_multi_diff_tau}\n%\n\\paragraph{Investigation of Lemma~\\ref{example dataset} over different $\\tau$ selections (Figure~\\ref{fig multi tau}).}   Consider the setting of Section~\\ref{sec when} and Lemma~\\ref{example dataset}.  Figure~\\ref{fig multi corrs} explores the influence of $\\lambda$ on the count of tokens selected by GD-derived attention weights. As $\\lambda$ increases, the likelihood of selecting more tokens also increases. Shifting focus to Figure~\\ref{fig multi tau}, we examine the effect of $\\tau$. For each outcome, we generate random $\\lambda$ values, retaining pairs $(\\lambda,\\X)$ satisfying $\\tau$ constraints, with averages derived from $100$ successful trials. The results indicate a positive correlation among $\\tau$, $\\lambda$, and the number of selected tokens. Moreover, Figure~\\ref{fig tau prob} provides a precise distribution of selected token counts across various $\\tau$ values (specifically $\\tau\\in\\{3,5,7,9\\}$). The findings confirm that the number of selected tokens remains within the limit of $\\tau$, thus validating the assertion made in Lemma~\\ref{example dataset}.\n\n==== END OF /2308.16898/supp/app_exp.tex ====\n==== BEGINNING OF /2308.16898/supp/supp-roadmap.tex ====\n%\\appendix\n\n%\\part{} \n%\\begin{center}\n%\\Large{Supplementary Materials for \\textquotedblleft \\textquotedblright }\n%\\end{center}\n\\paragraph{Roadmap.} The appendix is organized as follows:\n\n\n\\begin{itemize}\n\\item Appendix \\ref{app sep} provides the proof of Theorem \\ref{thm:separation}. \n\\item Appendix~\\ref{app:sec:aux} provides auxiliary lemmas about the training risk. \n\\item Appendix~\\ref{app:sec:gd:global} presents the proofs for the global convergence of gradient descent (Section \\ref{provable global}). \n\\item Appendix \\ref{app local proofs} presents the proofs for the local convergence of gradient descent (Section \\ref{sec local}). \n\\item Appendix~\\ref{sec multioutput} provides a general regularization path analysis. This analysis addresses the inductive bias of the attention layer for general norm objectives and beyond-linear prediction heads under a sequence-to-sequence classification model. The seq2seq aspect also goes beyond our results in the main body where we predict using single output token (Sections \\ref{sec:bias} and \\ref{sec:local reg path}).\n\\item Appendix \\ref{app supp exp} provides additional experiments and their discussion.\n\n\\end{itemize}\n\n==== END OF /2308.16898/supp/supp-roadmap.tex ====\n==== BEGINNING OF /2308.16898/supp/app-sa-basics.tex ====\n%\\section{Proofs for the Convergence of Gradient Descent}\n\n%\\subsection{Preliminaries on the Training Risk }\n\n% \\begin{table}[t]\n% \t\\centering\n% {\\caption{Summary of the Notations}~\\label{tab:notation}}\n% {\\footnotesize\n% \\begin{tabular}{|l l |}\n% \t\t\\hline\n% \t\t%\\hline\n% \t\t\\bfseries Notation & \\bfseries Description %& \\bfseries Reference\n% \t\t\\\\\n% \t\t\\hline \n% $\\sft{\\cdot}$ &   softmax function \\\\\n% $\\bgam_i$ &  the token's score defined as   $\\bgam_i=Y_i\\cdot \\X_i\\vb$\\\\\n% \t\t\\hline\n% \t\\end{tabular}}\n% \\end{table}\n\n\n\n\\section{Auxiliary Lemmas}\\label{app:sec:aux}\n\n\n\\subsection{Proof of Lemma \\ref{lem:rank}}\\label{app low rank proof}\nLet $\\W^\\svm_\\dm$ denote either solution of \\eqref{eqn:sattnsvm} or \\eqref{eqn:sattnsvmst}. We claim that $\\W^\\svm_\\dm$ is at most rank $n$.\n%\\begin{proof*}\nSuppose the claim is wrong and row space of $\\W^\\svm_\\dm$ does not lie within $\\Sc=\\texttt{span}(\\{\\z_i\\}_{i=1}^n)$. Let $\\W=\\Pi_{\\Sc}(\\W^\\svm_\\dm)$ denote the matrix obtained by projecting the rows of $\\W^\\svm_\\dm$ on $\\Sc$. Observe that $\\W$ satisfies all SVM constraints since $\\W\\z_i=\\W^\\svm_\\dm\\z_i$ for all $i\\in[n]$. For Frobenius norm, using $\\W^\\svm_\\dm\\neq \\W$, we obtain a contradiction via $\\tf{\\W^\\svm_\\dm}^2=\\tf{\\W}^2+\\tf{\\W^\\svm_\\dm-\\W}^2>\\tf{\\W}^2$. For nuclear norm, we can write $\\W=\\Ub\\bSi\\Vb^\\top$ with $\\bSi\\in\\R^{r\\times r}$ where $r$ is dimension of $\\Sc$ and $\\texttt{column\\_span}(\\Vb)= \\Sc$. \n\\\\\nTo proceed, we split the problem into two scenarios.\n%\\ct{$\\texttt{row\\_span}(\\Vb^\\top)$}\n\\\\\n\\noindent\\textbf{Scenario 1:} Let $\\Ub_\\perp,\\Vb_\\perp$ be orthogonal complements of $\\Ub,\\Vb$ -- viewing matrices with orthonormal columns as subspaces. Suppose $\\Ub_\\perp^\\top \\W^\\svm_\\dm\\Vb_\\perp\\neq 0$. Then, singular value inequalities (which were also used in earlier works on nuclear norm analysis \\cite{recht2011null,oymak2010new,oymak2011simplified}) guarantee that $\\tnuc{\\W^\\svm_\\dm}\\geq \\tnuc{\\Ub^\\top \\W^\\svm_\\dm\\Vb}+\\tnuc{\\Ub_\\perp^\\top \\W^\\svm_\\dm\\Vb_\\perp}>\\tnuc{\\W}$.\n\\\\\n\\noindent\\textbf{Scenario 2:} Now suppose $\\Ub_\\perp^\\top \\W^\\svm_\\dm\\Vb_\\perp= 0$. Since $\\W^\\svm_\\dm\\Vb_\\perp\\neq 0$, this implies $\\Ub^\\top \\W^\\svm_\\dm\\Vb_\\perp\\neq 0$. Let $\\W'=\\Ub\\Ub^\\top\\W^\\svm_\\dm$ which is a rank-$r$ matrix. Since $\\W'$ is a subspace projection, we have $\\tnuc{\\W'}\\leq \\tnuc{\\W^\\svm_\\dm}$. Next, observe that $\\tnuc{\\W}=\\texttt{trace}(\\Ub^\\top \\W\\Vb)=\\texttt{trace}(\\Ub^\\top \\W'\\Vb)$. On the other hand, $\\texttt{trace}(\\Ub^\\top \\W'\\Vb)<\\tnuc{\\W'}$ because the equality in \\emph{von Neumann's trace inequality} happens if and only if the two matrices we are inner-producting, namely $(\\W',\\Ub\\Vb^\\top)$, share a joint set of singular vectors \\cite{carlsson2021neumann}. However, this is not true as the row space of $\\W^\\svm_\\dm$ does not lie within $\\Sc$. Thus, we obtain $\\tnuc{\\W}<\\tnuc{\\W'}\\leq \\tnuc{\\W^\\svm_\\dm}$ concluding the proof via contradiction. $\\qed$ \n%\\end{proof*}\n\n\\subsection{Proof of Lemma \\ref{lem min risk}}\nWe first show that  $\\Lc(\\W)>\\Lc_\\st=\\frac{1}{n}\\sum_{i=1}^n \\ell(\\bgam_{i\\op_i})$. The token at the output of the attention layer is given by $\\ab_i=\\X_i^\\top \\s_i$, where $\\sft{\\X_i\\W\\z_i}=\\s_i$. Here, $\\ab_i$ can be written as $\\ab_i=\\sum_{t\\in[T]}\\s_\\itt\\x_\\itt$, where $\\s_\\itt\\geq 0$ and $\\sum_{t\\in[T]}\\s_\\itt=1$. %Note that, for any finite $\\W$, $\\s_\\itt$ as softmax probabilities are strictly positive.\nTo proceed, using the linearity of $h(\\x)=\\vb^\\top\\x$, we find that\n% \\begin{align}\n% \\ell(Y_i\\cdot  h(\\ab_i)) \\red{\\leq} \\ell( Y_i\\cdot  \\sum_{t\\in[T]}c_\\itt h(\\x_\\itt)) \\red{\\leq} \\ell( Y_i\\cdot  h(\\x_{i\\op_i})).\n% \\end{align}\n%\\ct{This whole proof should be written for linear $h$, stated for this anyways, right? I don't see the  first inequality}\n\\begin{align}\\label{eqn:w:low}\n\\nonumber\n  \\Lc(\\W)= \\frac{1}{n} \\sum_{i=1}^n  \\ell(Y_i\\cdot  h(\\ab_i)) &= \\frac{1}{n} \\sum_{i=1}^n  \\ell( Y_i\\cdot  \\sum_{t\\in[T]}\\s_\\itt h(\\x_\\itt))\\\\\n  &\\geq  \\frac{1}{n} \\sum_{i=1}^n  \\ell( Y_i\\cdot  h(\\x_{i\\op_i})) =\\frac{1}{n} \\sum_{i=1}^n  \\ell(\\bgam_{i\\op_i})=\\Lc_\\st. \n\\end{align}\nHere, the inequality follows since  $\\bgam_{it} = Y_i \\cdot h(\\x_\\itt)=  Y_i \n \\cdot \\vb^\\top\\x_{it} \\leq \\bgam_{i\\op_i}$ by Definition \\ref{score def} and strictly-decreasing nature of the loss $\\ell$ due to Assumption~\\ref{assum:loss:prope}.  \n\nOn the other hand, since not all tokens are optimal, there exists a token index $(i,t)$ for which $Y_i\\cdot h(\\x_\\itt)<Y_i\\cdot h( \\x_{i\\op_i})$. Since all softmax entries obey $\\s_\\itt>0$ for finite $\\W$, this implies the strict inequality $\\ell(Y_i\\cdot h(\\ab_i)) >\\ell(Y_i\\cdot h(\\x_{i\\op_i}))$. This leads to the desired conclusion $\\Lc(\\W)>\\Lc_\\st$.\n\nNext, we show that if \\eqref{eqn:sattnsvm} is feasible i.e.~there exists a $\\W$ separating some optimal indices $(\\op_i)_{i=1}^n$ from the other tokens, then  $\\lim_{R\\rightarrow\\infty}\\Lc(R\\cdot\\W)=\\Lc_\\st$. Note that, this assumption does not exclude the existence of other optimal indices. This implies that, letting $\\lim_{R\\rightarrow\\infty} \\sft{\\X_i(R\\cdot\\W)\\z_i}$ saturates the softmax and will be equal to the indicator function at $\\op_i$ for all inputs $i\\in[n]$. Thus, $\\s_\\itt\\rightarrow 0$ for $t\\neq\\op_i$ and $\\s_\\itt\\rightarrow 1$ for $t=\\op_i$. Using $M_1$-Lipschitzness of $\\ell$, we can write \n\\[\n\\left|\\ell(Y_i\\cdot h(\\x_{i\\op_i}))-\\ell( Y_i\\cdot h(\\ab_i))\\right|\\leq M_1 \\left|h(\\ab_i)-h(\\x_{i\\op_i})\\right|.\n\\]\nSince $h$ is linear, it is $\\tn{\\vb}$-Lipschitz implying \n%(where locality is the Euclidean ball with radius $R=\\sup_{i\\in[n],t\\in[T]}\\tn{\\x_\\itt}$) \n\\begin{align*}\n \\left|\\ell(Y_i\\cdot h(\\x_{i\\op_i}))-\\ell(Y_i\\cdot h(\\ab_i))\\right|\\leq M_1\\tn{\\vb}\\cdot\\tn{\\ab_i-\\x_{i\\op_i}}.   \n\\end{align*}\nSince $\\ab_i\\rightarrow\\x_{i\\op_i}$ as $R\\rightarrow\\infty$, \\eqref{eqn:w:low} gives $\\lim_{R\\rightarrow\\infty}\\Lc(R\\cdot\\W)=\\Lc_\\st$.\n%\\end{proof}\n$\\qed$\n\n==== END OF /2308.16898/supp/app-sa-basics.tex ====\n==== BEGINNING OF /2308.16898/supp/reg_path_analysis.tex ====\n%\\subsection{Regularization Path Analysis}\n\nTo proceed with our analysis, let us define the set of optimal solutions $\\Wm_{\\bal}:=\\Ws_{\\dm,\\bal}$ to \\eqref{seqattnsvm}. Let us denote this set by $\\Wcs_{\\bal}:=\\Wcs_\\dm(\\bal)$. Note that, if the $\\dm$-norm is not strongly-convex, $\\Wcs_{\\bal}$ may not be singleton. %Additionally, define the margin $\\Xi=\\frac{1}{\\td{\\Ws}}$.\n\n%To establish the convergence of the \\emph{local regularization path (RP)} to the set $\\Wcs_{\\bal}$, w\n\n\n\n\\subsection{Local regularization path and proof of Theorem \\ref{local RP thm1}}\\label{app:local:RP:thm1}\n\nWe first recall \\emph{local regularization path} which solves the $\\dm$-norm-constrained problem over a conic slice $\\Cc$, namely $\\Wb(R)=\\min_{\\td{\\W}\\leq R,\\W\\in\\Cc}\\Lc(\\W)$. We will show that proper local RP directionally converge to locally-optimal directions. Setting the cone to be the rank-$m$ manifold $\\Rcm$ (and more specifically the set of all matrices $\\R^{d\\times d}$), this will also establish the convergence of global RP to the globally-optimal direction.\n\nOur cone definition $\\con{\\bal}$ is induced by a token selection $\\bal=(\\al_\\ik)\\ikix$ and has a simple interpretation: It prioritizes tokens with lower score than $\\bal$ over tokens with high-score than $\\bal$. This way lower score tokens create a barrier for $\\bal$ and prevents optimization to move towards higher score tokens.\n\\begin{definition}[Low\\&High Score Tokens and Separating Cone]\\label{HL cone def} Given $\\al\\in[T]$, input sequence $\\X$ with label $Y$, $h(\\cdot):\\R^d\\rightarrow\\R$, and score $\\bgam_t=Y\\cdot h(\\x_t)$ for all $t\\in[T]$, define the low and high score tokens as\n\\[\n\\low=\\left\\{t\\in[T]\\bgl \\bgam_t<\\bgam_\\al\\},\\quad \\high=\\{t\\in[T]-\\{\\alpha\\}\\bgl \\bgam_t\\geq \\bgam_\\al \\right\\}.\n\\]\nFor input $\\X_\\ik$ and index $\\alpha_\\ik$, we use the shorthand notations $\\lowi,\\higi$. Finally define $\\con{\\bal}$ as% is defined\n\\begin{align}\n\\con{\\bal}=\\left\\{\\W\\in\\Rcm\\bgl \\min_{i\\in[n]}\\max_{t\\in\\lowi}\\min_{\\tau\\in\\higi} \\inn{\\F_\\ikt-\\F_\\iktt,\\W}\\geq \\eps\\tf{\\W} \\right\\}.\\label{cone alpha eq}\n\\end{align}\n\\end{definition}\n\n\\begin{lemma}\\label{lemma cone} Consider the cone definition of \\eqref{cone alpha eq} and suppose an SVM solution $\\Wma$ exists. If indices $\\bal$ are locally-optimal, $\\Wma\\in \\con{\\bal}$ for all sufficiently small $\\eps>0$. Otherwise, $\\Wma\\not\\in \\con{\\bal}$ for all $\\eps>0$. Additionally, suppose optimal indices $\\op_\\ik\\in\\arg\\max_{t\\in[T]}\\bgam_\\ikt$ are unique. Then, $\\con{\\opt}=\\Rcm$.\n\\end{lemma}\n\\begin{proof} Suppose $\\bal$ is locally optimal. Observe that, thanks to local optimality, $\\Wma$ obeys\n\\[\n\\min_{t\\in\\Tc_\\ik}\\inn{\\F_\\ikt,\\Wma}>\\max_{\\tau\\not\\in\\Tc_\\ik\\cup\\{\\al_\\ik\\}}\\inn{\\F_\\iktt,\\Wma},\n\\]\nfor all $i\\in[n]$. Next, observe that $\\Tc_\\ik\\subseteq \\lowi$ and $\\higi\\subseteq\\Tcb_\\ik=[T]-\\Tc_\\ik-\\{\\al_\\ik\\}$. Thus, the inequality \\eqref{cone alpha eq} holds for small enough $\\eps>0$.\n\nConversely, suppose $\\bal$ is not locally-optimal. Fix \\nei $t\\in\\Tc_\\ik$ with $t\\in \\higi$. Since $t\\in\\Tc_\\ik$, observe that%$\\bgam_\\ik>\\bga_\\itt$.\n\\[\n\\inn{\\F_\\ikt,\\Wma}\\geq \\max_{\\tau\\neq \\al_\\ik}\\inn{\\F_\\iktt,\\Wma}.\n\\]\nIn other words, for this $i\\in[n]$, we found\n\\[\n\\max_{\\tau\\in\\lowi} \\inn{\\F_\\iktt-\\F_\\ikt,\\Wma}\\leq 0,\n\\]\nviolating \\eqref{cone alpha eq} definition for any $\\eps>0$. To show the final claim, observe that, setting $\\bal:=\\op$, we have that $\\higi=\\emptyset$ for all $i\\in[n]$ as $\\op_\\ik$ are unique optimal indices. Thus, there is no constraint enforced on the cone definition in \\eqref{cone alpha eq} making it equal to the rank-$m$ manifold $\\Rcm$.\n\\end{proof}\n\nOur main assumption regarding prediction head is a monotonicity condition which is a strict generalization of linearity: We ask for $h$ to preserve the order of token scores under convex combinations.\n\\begin{assumption}[$h$ preserves the top score]\\label{ass cvx seq} The functions $(h_k)_{k=1}^K$ are $L_h$-Lipschitz in Euclidean distance. Given $\\bal=(\\alpha_\\ik)\\ikix$, there exists a scalar $c:=c_\\bal>0$ such that for all $i\\in[n],k\\in[K]$ the following holds: Consider any convex combination \n\\[ \\x(\\s)=\\sum_{t\\in \\lowi\\cup\\{\\alpha_\\ik\\}}s_t\\cdot\\x_\\itt\\quad\\text{where}\\quad \\sum_{t\\in \\lowi\\cup\\{\\alpha_\\ik\\}}s_t=1,~s_t\\geq 0.\n\\] \nWe have that $Y_\\ik\\cdot h_k(\\x(\\s))\\leq \\bga_\\ik-c(1-s_{\\alpha_\\ik})$ where $\\bga_\\ik=Y_\\ik\\cdot h_k(\\xa_{\\ik})$ is the score of $\\alpha_\\ik$.\n\\end{assumption}\nThis condition states that \\textbf{convex combinations of tokens with scores lower than $\\alpha_\\ik$ cannot achieve a score higher than $\\alpha_\\ik$}. Here, $1-s_{\\alpha_\\ik}$ term denotes the total share of non-optimal tokens. We require this condition to hold over the training dataset rather than the full domain $\\R^d$. Crucially, it is a strict generalization of the linearity assumption: Any linear $h_k$ satisfies Assumption \\ref{ass cvx seq} by setting $c_\\bal>0$ to be the difference between the score of $\\alpha_\\ik$ and the largest score within $\\lowi$ i.e.\n\\begin{align}\nc_\\bal:=\\min_{i\\in[n],k\\in[K]}\\{\\bga_\\ik-\\max_{t\\in\\lowi} \\bgam_\\ikt\\}>0.\\label{c choice}\n\\end{align}\nThis can be seen by writing $h_k(\\x(\\s))=\\sum_{t\\in \\lowi\\cup\\{\\alpha_\\ik\\}}s_t\\cdot \\bgam_\\ikt=\\bga_\\ik+\\sum_{t\\in \\lowi}s_t\\cdot (\\bgam_\\ikt-\\bga_\\ik)\\leq \\bga_\\ik-c_\\bal(1-s_{\\alpha_\\ik})$.\n%\\[ h_k(\\x(\\s))=\\sum_{t\\in \\lowi\\cup\\{\\alpha_\\ik\\}}s_t\\cdot \\bgam_\\ikt=\\bga_\\ik+\\sum_{t\\in \\lowi}s_t\\cdot (\\bgam_\\ikt-\\bga_\\ik)\\leq \\bga_\\ik-c_\\bal(1-s_{\\alpha_\\ik}).\\]\nTo provide a nonlinear example, consider the setting all labels are $Y_\\ik=1$ and $h$ is an arbitrary convex function. Thanks to convexity, we can write $h(\\x(\\s))\\leq \\sum_{t=1}^Ts_t\\cdot h(\\x_t)=\\sum_{t\\in \\lowi\\cup\\{\\alpha_\\ik\\}}s_t\\cdot \\bgam_\\ikt$. Thus, we can use the same choice of $c_\\bal$ in \\eqref{c choice}. \n\nWe remark that, in Section \\ref{sec:multi}, we derive formulae describing general inductive bias of attention without enforcing any assumption on $h$. These formulae allow for arbitrary output tokens generated by the transformer model trained by gradient descent. This includes the setting where gradient descent selects and composes multiple tokens from each sequence rather than a single token $\\alpha_\\ik$.\n\nThe following result is our main theorem regarding the convergence of regularization path to locally-optimal directions when restricted over the cone $\\con{\\bal}$.% whenever $\\bal$ is locally-optimal.  %and $R_0,\\eps>0$.\n\\begin{theorem} [Convergence of Local Regularization Path]\\label{local RP thm} Suppose \\eqref{seqattnsvm} is feasible and $\\bal=(\\al_\\ik)\\ikix$ are locally-optimal token indices. Suppose Assumptions \\ref{assum:loss:prope}\\&\\ref{ass cvx seq} hold. Recall $\\con{\\bal}$ of \\eqref{cone alpha eq} and consider the norm-constrained cone \\[\n\\Ccd:=\\con{\\bal}\\bigcap\\{\\W\\bgl \\td{\\W}\\geq R_0\\}.\n\\]\nDefine the conic regularization path $\\wrb{R}=\\min_{\\Ccd,\\td{\\W}\\leq R}\\Lc(\\W)$. Let $\\Wcs_{\\bal}$ be its set of minima and $\\xdm>0$ be the associated margin i.e.~$\\xdm=1/\\td{\\Wcs_{\\bal}}$. For any sufficiently small $\\eps>0$ and sufficiently large $R_0= \\order{1/\\eps}>0$, $\\lim_{R\\rightarrow\\infty} \\dist{\\frac{\\wrb{R}}{R\\xdm},\\Wcs_{\\bal}}=0$. Additionally, suppose optimal indices $\\op=(\\op_\\ik)\\ikix$ are unique and set $\\bal\\gets\\op$. Then, the same RP convergence guarantee holds with $\\Ccd=\\Rcm$.\n\\end{theorem}\n%$\\eps=R_0=0$ and \n%normalized to have unit $\\dm$-norm\n\\begin{proof} We will prove that $\\wrb{R}$ is the optimal direction and also $\\td{\\wrb{R}}\\rightarrow \\infty$. Define the absolute constant \n\\[\n\\cdm=\\min_{\\td{\\W}=1}\\tf{\\W}.\n\\]\nThis guarantees that for any $\\W$ we have $\\tf{\\W}\\geq \\cdm\\td{\\W}$. Also denote $\\epsd=\\cdm\\eps$. Let us first determine the $\\eps$ parameter: Fix $\\Wma\\in\\Wcs_{\\bal}$. For general $\\bal$, we can choose any $\\eps>0$ that is sufficiently small to guarantee $\\Wma\\in\\con{\\bal}$ based on Lemma \\ref{lemma cone}. For $\\bal=\\op$, our analysis will entirely avoid using $\\eps$, specifically, observe that $\\con{\\bal}=\\Rcm$ based on Lemma \\ref{lemma cone}.\n    \n\\noindent\\textbf{Step 1:} Let us first prove that $\\wrb{R}$ achieves the optimal risk as $R\\rightarrow\\infty$ -- rather than problem having finite optima. Define norm-normalized $\\Wsb=\\xdm\\Wma$. Note that $\\Wma$ separates tokens $\\bal$ from rest of the tokens for each $i,k\\in[n]\\times [K]$. Thus, we have that\n\\begin{align}\n\\lim_{R\\rightarrow\\infty}\\Lc(\\wrb{R})\\leq\\lim_{R\\rightarrow\\infty}\\Lc(R\\cdot\\Wsb):=\\Lc_\\star= \\frac{1}{n}\\sum_{i=1}^n\\sum_{k=1}^K\\ell(\\bga_\\ik).\\label{asymp loss}\n\\end{align}\nOn the other hand, for any choice of $\\W\\in \\con{\\bal}$, set $\\x^{\\W}_\\ik=\\sum_{t=1}^T \\sft{\\X_i\\W\\z_\\ik}_t\\x_t$. Set softmax probabilities $\\sik=\\sft{\\X_i\\W\\z_\\ik}$. Recalling $\\lowi,\\higi$ definitions, we can decompose the attention features as% $\\Tcb_\\ik=[T]-\\Tc_\\ik-\\{\\al_\\ik\\}$,\n\\begin{align}\n\\x^{\\W}_\\ik=\\sik_{\\al_\\ik}\\xa_\\ik+\\sum_{t\\in\\lowi}\\sik_t\\x_\\itt+\\sum_{\\tau\\in\\higi}\\sik_\\tau\\x_{\\ittt}.\n\\end{align}\nWhen $\\bal=\\op$, note that we simply have $\\higi=\\emptyset$. This will be important for setting $R_0=0$ and $\\Ccd=\\Rcm$ in the proof for $\\op$ indices.\n\nSet $\\bgg_\\ikt=\\bgam_\\ikt-\\bga_\\ik=Y_\\ik\\cdot (h_k(\\x_\\itt)-h_k(\\xa_\\ik))$. Building on $L_h$-Lipschitzness of the prediction head $h_k(\\cdot)$, we define\n\\begin{align}\n&B:=\\max_{i\\in[n],k\\in[K]}\\max_{t,\\tau\\in[T]}L_h\\cdot \\tn{\\x_\\itt-\\x_\\ittt}\\geq |\\bgg_\\ikt|.\\label{BB eq}\n\\end{align}\n%Before proceeding further, there are two important observations:\n%\\begin{itemize}\n%\\item Since $\\bal$ is locally-optimal: $\\Tc_\\ik\\subseteq\\lowi$ and $\\higi\\subseteq\\higi=[T]-\\Tc_\\ik-\\{\\al_\\ik\\}$.\n%\\item When $\\bal=\\op$, we have that $\\higi=\\emptyset$. This will be important for setting $R_0=0$ and $\\Ccd=\\R^{d\\times d}$.\n%\\end{itemize}\n% and dropping subscript $i$\n%Setting $\\ab_i=\\Kb_i\\pb$ and $\\s_i=\\sft{\\ab_i}$. \nDefine $P^\\ik:=\\sum_{t\\in\\lowi}\\sik_t$, $Q^\\ik:=\\sum_{t\\in\\higi}\\sik_t$, and $\\bgam^{\\W}_\\ik=Y_\\ik\\cdot h_k(\\x^{\\W}_\\ik)$. Also set temporary variables $\\x'=(\\sik_{\\al_\\ik}+Q^\\ik)\\xa_\\ik+\\sum_{t\\in\\lowi}\\sik_t\\x_\\itt$ and $\\bgam'=Y_\\ik\\cdot h_k(\\x^{\\W}_\\ik)$. \nUsing Assumption \\ref{ass cvx seq} on $\\x'$ and noticing $P^\\ik=1-\\sik_{\\al_\\ik}-Q^\\ik$, observe that\n\\[ \n|\\bgam^{\\W}_\\ik-\\bgam'|\\leq BQ^\\ik\\quad\\text{and}\\quad \\bgam'\\leq \\bga_\\ik-c_\\bal P^\\ik. \n\\] \nRecall from \\eqref{c choice} that, when $h_k$ are linear functions, $c_\\bal$ can be chosen as \n%\\redp{HERE}\\eqref{AA eq}. and, using local-optimality of $\\bal$, we define the global variable $A$ -- independent of $\\W$ choice -- as follows%, for a fixed $(i,k)$ pair, let \n\\begin{align*}\nc_\\bal:=\\min_{i\\in[n],k\\in[K]}\\min_{t\\in\\lowi}-\\bgg_\\ikt>0.%\\label{AA eq}.\n\\end{align*}\n\n\nTo summarize, applying Assumption \\ref{ass cvx seq}, we obtain the following score inequalities\n\\begin{align}\\label{score decomp}\n&\\bgam^{\\W}_\\ik\\leq \\bga_\\ik-c_\\bal P^\\ik+BQ^\\ik,\\\\\n%\\sik_{\\al_\\ik}\\bga_\\ik+\\sum_{t\\in\\lowi}\\sik_t\\bgam_\\ikt+\\sum_{\\tau\\in\\higi}\\sik_\\tau\\bgam_{\\iktt}.\\\\\n&|\\bgam^{\\W}_\\ik-\\bga_\\ik|\\leq L_h\\tn{\\x^{\\W}_\\ik-\\xa_\\ik}\\leq L_h \\sum_{t\\neq \\al_\\ik}\\sik_t\\tn{\\x_\\ikt-\\xa_\\ik}\\leq B(1-\\sik_{\\al_\\ik}).\\label{lip score gap}\n%|1-\\sik_{\\al_\\ik}|\\bga_\\ik+\\sum_{t\\in\\lowi}\\sik_t|\\bgam_\\ikt|+\\sum_{\\tau\\in\\higi}\\sik_\\tau|\\bgam_{\\iktt}|.\n%\\s_{\\al_i}\\bgam_{i\\al_i}+\\sum_{t\\in\\Tc_\\ik}\\s_t\\bgam_\\itt+\\sum_{t\\in\\higi}\\s_t\\bgam_\\itt.\n\\end{align}\nWe will use the $\\bgam^{\\W}_\\ik-\\bga_\\ik$ term in \\eqref{score decomp} to evaluate $\\W$ against the reference loss \\eqref{asymp loss}. Let $\\abik=\\X_i\\W\\z_\\ik$. Now since $\\W\\in \\con{\\bal}$, there exists $t\\in \\lowi$ obeying $\\abik_t-\\max_{\\tau\\in\\higi} \\abik_\\tau\\geq \\eps \\tf{\\W}\\geq \\epsd\\td{\\W}$. Denote $D^\\ik:=(\\sum_{t\\in [T]}e^{\\abik_t})^{-1}$ to be the softmax denominator i.e.~sum of exponentials. We find that,\n\\begin{align}\nQ^\\ik=\\sum_{\\tau\\in\\higi}\\sik_\\tau=D^\\ik\\sum_{\\tau\\in\\higi}e^{\\abik_\\tau}\\leq D^\\ik Te^{\\abik_t-\\eps\\tf{\\W}}\\leq Te^{-\\epsd\\td{\\W}}P^\\ik.\\label{qikeq}\n\\end{align}\nConsequently, the score difference obeys\n%Since above holds for all $i,k\\in[n]\\times [K]$ pairs, we can aggregate them over $(i,k)$ to obtain\n\\[\n\\bgam^{\\W}_\\ik-\\bga_\\ik\\leq BQ^\\ik-c_\\bal P^\\ik\\leq (BTe^{-\\epsd\\td{\\W}}-c_\\bal)P^\\ik.\n\\]\nAbove, the right hand side is strictly negative as soon as $\\td{\\W}\\geq R_0:=\\frac{1}{\\epsd}\\log\\frac{BT}{c_\\bal}$. Note that, this condition applies to all $(i,k)\\in[n]\\times [K]$ pairs uniformly for the same $R_0$. Consequently, for any $\\td{\\W}\\geq R_0$, for all $i,k$ and $\\W\\in \\con{\\bal}$, we have that $\\bgam^{\\W}_\\ik<\\bga_\\ik$. Additionally, when $\\bal=\\op$, note that $Q^\\ik=0$ since $\\higi=\\emptyset$. Thus, $R_0=0$ suffices to ensure $\\bgam^{\\W}_\\ik<\\bga_\\ik$. Using the strictly-decreasing nature of $\\ell$, we conclude with the fact that for all (finite) $\\W\\in \\con{\\bal}$, \n\\[\n\\Lc(\\W)=\\frac{1}{n}\\sum_{i=1}^n\\sum_{k=1}^K\\ell(\\bgam^{\\W}_\\ik)> \\Lc_\\st=\\frac{1}{n}\\sum_{i=1}^n\\sum_{k=1}^K\\ell(\\bga_\\ik),\n\\]\nwhich implies $\\td{\\wrb{R}}\\rightarrow\\infty$.\n\n\\noindent\\textbf{Step 2:} To proceed, we show that $\\wrb{R}$ converges in direction to $\\Wcs_{\\bal}$. Suppose this is not the case i.e.~convergence fails. We will obtain a contradiction by showing that $\\Wsb_R=R\\cdot\\Wsb$ achieves a strictly superior loss compared to $\\wrb{R}$. Also define the normalized parameter $\\wrt{R}=\\frac{\\wrb{R}}{R\\xdm}$ and $\\W'=\\frac{\\wrb{R}}{\\td{\\wrb{R}}\\xdm}$. Note that $\\wrt{R}$ is obtained by scaling down $\\W'$ since $\\td{\\wrb{R}}\\leq R$ and $\\W'$ obeys $\\td{\\W'}=\\td{\\Wma}$.\n\nSince $\\wrt{R}$ fails to converge to $\\Wcs_{\\bal}$, for some $\\delta>0$, there exists arbitrarily large $R>0$ such that $\\dist{\\wrt{R},\\Wcs_{\\bal}}\\geq \\delta$. This translates to the suboptimality in terms of margin constraints as follows: First, distance with respect to the $\\dm$-norm obeys $\\distd{\\wrt{R},\\Wcs_{\\bal}}\\geq \\delta$ for some updated $\\delta\\gets \\cdm\\delta$. Secondly, using triangle inequality,\n\\[ \n\\text{This implies that either~~~}\\td{\\wrt{R}}\\leq \\td{\\Wma}-\\delta/2\\text{~~~or~~~}\\distd{\\W',\\Wcs_{\\bal}}\\geq \\delta/2.\n\\]\nIn either scenario, $\\wrt{R}$ strictly violates one of the margin constraints of \\eqref{seqattnsvm}: If $\\td{\\wrt{R}}\\leq \\td{\\Wma}-\\delta/2$, then, since the optimal SVM objective is $\\td{\\Wma}$, there exists a constraint $(i,k)$ for which $\\inn{\\Fa_{\\ik}-\\F_{\\ikt},\\wrt{R}}\\leq 1-\\frac{\\delta}{2\\td{\\Wma}}$. If $\\distd{\\W',\\Wcs_{\\bal}}\\geq \\delta/2$, then, $\\W'$ has same SVM objective but it is strictly bounded away from the solution set. Thus, for some $\\eps:=\\eps(\\delta)>0$, $\\W'$ and its scaled down version $\\wrt{R}$ strictly violate an SVM constraint achieving margin $\\leq 1-\\eps$. Without losing generality, suppose $\\wrt{R}$ violates the first constraint. Thus, for a properly updated $\\delta>0$ (that is function of the initial $\\delta>0$) and for $(i,k)=(1,1)$ and some \\nei $\\tau\\in \\Tc_\\oo$,% and , we have that\n\\begin{align}\n\\inn{\\Fa_{\\oo}-\\F_{\\oo t},\\wrt{R}}\\leq 1-\\delta.\\label{margin violate}\n%\\pb^\\top(\\kb_{i\\al_1}-\\kb_{i\\tau})\\leq 1-\\delta. \n\\end{align}\nNow, we will argue that this will lead to a contradiction by proving $\\Lc(\\Wsb_R)<\\Lc(\\wrb{R})$ for sufficiently large $R$.\n\nTo obtain the result, we establish a refined softmax probability control as in Step 1 by studying distance to $\\Lc_\\star$. Following \\eqref{score decomp}, denote the score function at $\\wrb{R}$ via $\\bgam^R_\\ik=\\bgam_\\ik^{\\wrb{R}}$ as shorthand notation. Similarly, let $\\sir_\\ik=\\sft{\\abr_\\ik}$ with $\\abr_\\ik=\\X_i\\wrb{R}\\z_\\ik$. Set the corresponding notation for the reference parameter $\\Wsb_R$ as $\\bgam^\\st_\\ik,\\s^\\st_\\ik,\\ab^\\st_\\ik$. \n\nCritically, recall the above inequalities \\eqref{qikeq} that applies to both $\\W\\in\\{\\wrb{R},\\Wsb_R\\}\\subset\\con{\\bal}$ for an index $(i,k)$ and \\nei $t\\in\\Tc_\\ik$\n\\begin{align}\n\\nonumber \nQ^\\ik&=\\sum_{\\tau\\in\\higi}\\s_\\iktt=D^\\ik\\sum_{\\tau\\in\\higi}e^{\\ab_{\\iktt}} \\\\\n&\\leq D^\\ik Te^{\\ab_\\ikt-\\epsd\\td{\\W}}\\leq Te^{-\\epsd\\td{\\W}}P^\\ik\\leq Te^{-\\epsd\\td{\\W}}(1-\\s_{\\ik\\al_\\ik}), \\label{qik bound}\n\\end{align}\nwhere $P^\\ik=\\sum_{\\tau\\in\\lowi}\\s_{\\iktt}$ and $P^\\ik+Q^\\ik= 1-\\s_{\\ik\\al_\\ik}$. \n\n\nNote that, setting $R_0\\geq \\order{1/\\epsd}=\\order{1/\\eps}$, we guarantee that, for any $(i,k)\\in[n]\\times [K]$\n\\begin{align}\nP^\\ik\\geq Q^\\ik\\implies P^\\ik \\geq 0.5(1-\\s_{\\ik\\al_\\ik}). \\label{pik bound}\n\\end{align}\nAdditionally, when $\\bal=\\op$, note that $Q^\\ik=0$ since $\\higi=\\emptyset$. Thus, $R_0=0$ suffices to ensure \\eqref{pik bound}.\n\nTo proceed, recall that $R\\geq \\td{\\wrb{R}}\\geq R_0$ by definition since $\\wrb{R}\\in \\Ccd$ and recall $\\xdm:=1/\\td{\\Wma}$. Equipped with these, we note the following softmax inequalities on the selected tokens $\\al_\\ik$\n\\begin{align}\n&\\s^\\st_{\\ik\\al_\\ik}\\geq \\frac{1}{1+Te^{-R\\xdm}}\\geq 1-Te^{-R\\xdm}\\quad \\text{for all}\\quad (i,k)\\in[n]\\times [K], \\label{salpha bounds}\\\\\n&s^R_{\\ik\\al_\\ik}\\leq \\frac{1}{1+e^{-(1-\\delta)\\td{\\wrb{R}}\\xdm}}\\leq \\frac{1}{1+e^{-(1-\\delta)R\\xdm}}\\quad\\text{for}\\quad (i,k)=(1,1).\\nn\n\\end{align}\nThe former inequality is thanks to $\\Wma$ achieving $\\geq 1$ margins on all tokens $[T]-\\al_\\ik$ and the latter arises from the $\\delta$-margin violation of $\\wrb{R}$ at $(i,k)=(1,1)$ i.e.~Eq.~\\eqref{margin violate}. Since $\\ell$ is strictly decreasing with Lipschitz gradient and the scores are upper/lower bounded by an absolute constant (as tokens are bounded, $(h_k)_{k=1}^K$ are Lipschitz, and both are fixed), we know that $\\cop\\geq -\\ell'(\\bgam_\\ik^{\\W})\\geq \\cdn$ for some constants $\\cop>\\cdn>0$. Thus, following Eq.~\\eqref{BB eq} and the score decomposition \\eqref{score decomp}, and using \\eqref{qik bound},\\eqref{pik bound},\\eqref{salpha bounds} we can write%Below, we use $\\ell'_R,\\ell'_\\st$ to denote the derivatives of $\\ell(x)$ used for first-order Taylor expansion around the scores of $\\wrb{R}$ and $\\Wsb_R$ respectively. \n\\begin{align}\n\\nonumber\n\\Lc(\\wrb{R})-\\Lc_\\star&\\geq \\frac{1}{n}[\\ell(\\bgam_\\oo^{\\wrb{R}})-\\ell(\\bga_\\oo)]\\geq \\frac{\\cdn}{n}(\\bga_{\\oo}-\\bgam_\\oo^{\\wrb{R}})\\\\\n&\\geq \\frac{\\cdn}{n}(c_\\bal P^\\oo_{\\wrb{R}}-BQ^\\oo_{\\wrb{R}})\\label{q11 eq}\\\\\n&\\geq \\frac{\\cdn}{n}(1-\\s_{\\oo\\al_\\oo}^R)(0.5c_\\bal -BTe^{-\\epsd \\td{\\wrb{R}}}) \\nonumber\\\\\n\\nonumber\n&\\geq \\frac{\\cdn}{n}\\frac{1}{1+e^{(1-\\delta)R\\xdm}}(0.5c_\\bal-BTe^{-\\epsd R_0}).\n\\end{align}\nAbove, recalling the choice $R_0\\geq \\order{1/\\epsd}=\\order{1/\\eps}$, $R\\geq R_0$ implies $BTe^{-\\epsd R_0}\\leq c_\\bal/4$ to obtain\n\\begin{align}\n\\Lc(\\wrb{R})-\\Lc_\\star\\geq \\frac{\\cdn\\cdot c_\\bal}{4n}\\frac{1}{1+e^{(1-\\delta)R\\xdm}}.\\label{ineq prl}\n\\end{align}\nAdditionally when $\\bal=\\op$, since $Q^\\oo_{\\wrb{R}}=0$ in \\eqref{q11 eq}, the bound above holds with $R_0=0$ by directly using \\eqref{q11 eq}.\n\nConversely, we upper bound the difference between $\\Lc(\\Wsb_R)$ and $\\Lc_\\star$ as follows. Define the worst-case loss difference for $\\wrb{R}$ as $(i',k')=\\arg\\max_{i\\in[n],k\\in[K]}[\\ell(\\bgam_\\ik^\\st)-\\ell(\\bga_\\ik)]$. Using \\eqref{lip score gap}\\&\\eqref{salpha bounds}, we write\n% and $B_2=\\max_{i\\in[n]}\\{\\bgam_{i\\al_i}-\\min_{t\\in[T]}\\bgam_\\itt\\}$\n\\begin{equation}\n\\begin{aligned}\n\\Lc(\\Wsb_R)-\\Lc_\\star&\\leq \\max_{i\\in[n],k\\in[K]}[\\ell(\\bgam_\\ik^\\st)-\\ell(\\bga_\\ik)]\\leq \\cop\\cdot(\\bga_{i'k'}-\\bgam^\\st_{i'k'})\\\\\n&\\leq \\cop\\cdot(1-\\s_{i'k'\\al_{i'k'}}^\\st)B\\\\\n&\\leq \\cop\\cdot Te^{-R\\xdm}B.\\label{desired Wmm bound}\n\\end{aligned}\n\\end{equation}\nCombining the last inequality and \\eqref{ineq prl}, we conclude that $\\Lc(\\Wsb_R)<\\Lc(\\wrb{R})$ whenever\n\\[\n\\cop T\\cdot e^{-R\\xdm}B<\\frac{\\cdn\\cdot c_\\bal }{4n}\\frac{1}{1+e^{(1-\\delta)R\\xdm}}\\iff \\frac{e^{R\\xdm}}{1+e^{(1-\\delta)R\\xdm}}> \\frac{4\\cop Tn B}{\\cdn c_\\bal }.\n\\]\nThe left hand-side inequality holds for all sufficiently large $R$: Specifically, as soon as $R$ obeys $R>\\frac{1}{\\delta\\xdm}\\log(\\frac{8\\cop Tn B}{\\cdn c_\\bal})$. This completes the proof of the theorem via contradiction as we obtained $\\Lc(\\wrb{R})>\\Lc(\\Wsb_R)$.\n\\end{proof}\n\n\n\\subsection{Global regularization path}\\label{app:global:RP:thm}\n\n\n\n\n\nThe following result is a direct corollary of Theorem \\ref{local RP thm}. Namely, we simply restate the final line of this theorem that applies to optimal tokens.\n\\begin{corollary}[Global Convergence of Regularization Path]\\label{cor gm} Suppose Assumptions \\ref{assum:loss:prope}\\&\\ref{ass cvx seq} hold and the optimal indices $\\op_\\ik=\\arg\\max_{t\\in[T]}\\bgam_\\ikt$ are unique. Consider the global regularization path $\\Wb_{\\dm,R}=\\min_{\\W\\in\\Rcm,\\td{\\W}\\leq R}\\Lc(\\W)$. Let $\\Wcb^\\svm_\\dm$ be the non-empty solution set of \\eqref{seqattnsvm} with $\\bal\\gets\\op$ normalized to have unit $\\dm$-norm. Then\n\\[\n\\lim_{R\\rightarrow\\infty}\\dist{\\frac{\\Wb_{\\dm,R}}{R},\\Wcb^\\svm_\\dm}\n\\]\n\\end{corollary}\n\nThe next corollary directly targets application to \\eqref{serm-w} and \\eqref{serm-kq}. This corollary is also a strict generalization of Theorem \\ref{thm global reg path}. Specifically, we immediately recover Theorem \\ref{thm global reg path} by specializing this to the single-output setting $K\\gets 1$ and full-dimensional parameterization $m\\gets d$.\n\\begin{corollary}\\label{cor global reg path} Suppose Assumptions \\ref{assum:loss:prope}\\&\\ref{ass cvx seq} hold and the optimal indices $\\op_\\ik=\\arg\\max_{t\\in[T]}\\bgam_\\ikt$ are unique. Consider the regularization paths associated to \\eqref{serm-w} and \\eqref{serm-kq}:\n\\begin{align}\n&\\Wb_R=\\underset{\\W\\in\\Rcm,\\tf{\\W}\\leq R}{\\arg\\min}\\Lc(\\W)\\quad\\text{and}\\quad \\Kbb_R,\\Qbb_R=\\underset{\\tf{\\Kb}^2+\\tf{\\Qb}^2\\leq 2R}{\\arg\\min}\\Lc(\\Kb,\\Qb)\n\\end{align}\nSuppose \\eqref{seqattnsvm} is feasible for $\\bal\\gets\\op$. Let $\\Ws$ be the unique solution of \\eqref{seqattnsvm} with Frobenius norm and $\\Wc^{\\svm}_\\star$ be the solution set of \\eqref{seqattnsvm} with nuclear norm and cost function $\\tnuc{\\Wc^{\\svm}_\\star}$. We have that\n%. Denote $\\|\\Wc^{\\svm}\\|_\\star$ to be the associated objective function%nuclear norm normalized to unit nuclear norm\n\\[\n\\lim_{R\\rightarrow\\infty} \\frac{\\Wb_R}{R}=\\frac{\\Ws}{\\tf{\\Ws}},\\quad\\lim_{R\\rightarrow\\infty} \\dist{\\frac{\\Qbb_R\\Kbb_R^\\top}{R},\\frac{\\Wcs_\\star}{\\tnuc{\\Wc^{\\svm}_\\star}}}=0.\n\\]\n\\end{corollary}\n\\begin{proof} We directly apply Corollary \\ref{cor gm} with $\\dm=F$ and $\\dm=\\star$ respectively. To obtain the result on $\\Wb_R$, we note that $\\Ws$ is unique because Frobenius norm-squared is strongly convex. To obtain the result on $(\\Qbb_R,\\Kbb_R)$, we use Lemma \\ref{kqw mapping} and observe that\n\\[\n\\Wb_{\\st,R}:=\\Qbb_R\\Kbb_R^\\top\\in\\underset{\\W\\in\\Rcm,\\tnuc{\\W}\\leq R}{\\arg\\min}\\Lc(\\W).\n\\]\nWe then apply Corollary \\ref{cor gm} with $\\dm=\\star$ to conclude with the convergence of the path $ \\Wb_{\\st,R}$.\n\\end{proof}\n\n\n\\subsubsection{Proof of Theorem \\ref{thm global reg path}}\n\\input{supp/reg_path_glob}\n\n==== END OF /2308.16898/supp/reg_path_analysis.tex ====\n==== BEGINNING OF /2308.16898/supp/reg_path.tex ====\n\n\n\\section{Convergence of Regularization Path for Sequence-to-Sequence Setting}\\label{sec multioutput}\n\n% \\subsection{Global Regularization Path and Proof of Theorem \\ref{thm global reg path}}\\label{app:global:RP:thm}\n\n% \\input{supp/reg_path_glob}\n\n\nIn this section, we provide proofs for the regularization path analysis. We first provide a more general formulation of the optimization problem that allows for regressing multiple token outputs. To distinguish from \\eqref{eqn:erm:w}\\&\\eqref{eqn:erm:kq}, let us call this more general version Sequence Empirical Risk Minimization (SERM).\n\n% and a more general \\emph{cross-attention} rather than \\emph{self-attention}\n\n\\noindent\\textbf{Problem definition:} \n%Note that self-attention maps $T$ tokens to $T$-tokens. Let us consider a general setting where we regress the last $K$ token outputs. \nRather than a single input sequence $\\X$, let us allow for two input sequences $\\X\\in\\R^{T\\times d}$ and $\\Z\\in\\R^{K\\times d}$ with $(\\x_t)_{t=1}^T$ and $(\\z_k)_{k=1}^K$. The cross-attention admits $\\X,\\Z$ and outputs $K$ tokens. We will also allow for $K$ separate prediction heads $(h_k)_{k=1}^K$ for individual cross attention outputs which strictly generalizes the setting where we used single prediction head $h(\\cdot)$. Denote the training labels associated to each token as $\\Y=(Y_\\ik)_{i=1}^K$. Given $n$ samples $(\\Y_i,\\X_i,\\Z_i)_{i=1}^n$, for a decreasing loss function $\\ell(\\cdot)$, minimize the empirical risk by the prediction of first attention output either univariate ($\\W\\in\\R^{d\\times d}$) or bivariate ($\\Kb,\\Qb\\in\\R^{d\\times m}$) fashion:\n\\begin{align}\n\\Lc(\\W)&=\\frac{1}{n}\\sum_{i=1}^n\\sum_{k=1}^K \\ell(Y_\\ik\\cdot h_k( \\X_i^\\top \\sft{\\X_i\\W\\z_\\ik})),\\label{serm-w}\\tag{SERM-W}\\\\\n\\Lc(\\Kb,\\Qb)&=\\frac{1}{n}\\sum_{i=1}^n \\ell(Y_\\ik\\cdot h_k( \\X_i^\\top \\sft{\\X_i\\Kb\\Qb^\\top\\z_\\ik})).\\label{serm-kq}\\tag{SERM-KQ}\n\\end{align}\nIn order to recover the single-output self-attention model, we can simply set $K=1$ and $\\z_{i1}\\gets\\x_{i1}$ and $h_k\\gets h$. To proceed, we introduce the more general version of the \\eqref{eqn:sattnsvm} problem, which we refer to as \\emph{Sequential Cross-Attention SVM} to preserve consistent phrasing. Suppose $\\Kb,\\Qb\\in\\R^{d\\times m}$ with $m\\leq d$ and let $\\Rcm$ denote the set of rank-$m$ matrices in $\\R^{d\\times d}$. Given indices $\\bal=(\\aik)_{\\ik=(1,1)}^{(n,K)}$, consider the SVM with $\\dm$-norm constraint\n%\\nicebox{\n\\begin{equation}\\tag{S\\name}\n \\Wm_{\\dm,\\bal}\\in\\arg\\min_{\\W\\in\\Rcm}\\|\\W\\|_{\\diamond}\n \\quad \\text{s.t.}   \\quad \\min_{t\\neq \\aik}(\\x_{i\\aik}-\\x_\\itt)^\\top\\W\\z_\\ik\\geq 1\\quad\\forall\\quad i\\in[n],k\\in[K]\\label{seqattnsvm}.\n\\end{equation}\n%}\nWhen solution is non-unique, we denote the solution set by $\\Wcs(\\bal)$. In what follows, we denote $\\F_\\ikt:=\\x_\\itt\\z_{\\ik}^\\top$ and given $\\bal$, we denote $\\xa_\\ik=\\x_{i\\alpha_\\ik}$ and $\\Fa_\\ik:=\\x_{i\\alpha_\\ik}\\z_{\\ik}^\\top$. With this notation, we can equivalently write\n%\\nicebox{\n\\begin{equation}\\tag{S\\name'}\n \\Wm_{\\dm,\\bal}\\in\\arg\\min_{\\W\\in\\Rcm}\\|\\W\\|_{\\diamond}\n \\quad \\text{s.t.} \\quad \\min_{t\\neq \\aik}\\inn{\\Fa_\\ik-\\F_\\ikt,\\W}\\geq 1\\quad\\forall~ i\\in[n],k\\in[K]\\label{seqattnsvm2}.\n\\end{equation}\n%}\n\n\\begin{definition}[\\Neis and Locally-Optimal Indices]\\label{seq loc opt} Fix token indices $\\bal=(\\alpha_\\ik)\\ikix$ for which \\eqref{seqattnsvm} is feasible to obtain $\\Wma:= \\Wm_{\\dm,\\bal}$. Define token scores as\n\\[\n\\bgam_\\ikt=Y_\\ik\\cdot h_k(\\x_\\itt),\\quad \\bga_\\ik:=\\bgam_{ik\\alpha_\\ik}=Y_\\ik\\cdot h_k(\\xa_\\ik).\n\\]\nConsider tokens $\\Tc_\\ik\\subset[T]$ such that $\\inn{\\Fa_\\ik-\\F_\\ikt,\\Wma}=1$ for all $t\\in\\Tc_\\ik$. $\\Tc_\\ik$ is allowed to be an empty set. We refer to $\\Tc_\\ik$ as \\neis of $\\Fa_\\ik=\\xa_\\ik\\z_\\ik^\\top$ and define its complement $\\Tcb_\\ik=[T]-\\Tc_\\ik-\\{\\al_\\ik\\}$.  Additionally, token indices $\\bal=(\\alpha_\\ik)\\ikix$ are called locally-optimal if for all $i\\in[n],k\\in[K]$ and  $t\\in\\Tc_\\ik$, token scores obey $\\bga_\\ik>\\bgam_\\ikt$. Associated $\\Wma$ is called a locally-optimal direction. Finally, let $\\op_\\ik\\in\\arg\\max_{t\\in[T]}\\bgam_\\ikt$ be the optimal indices and define the associated $\\Ws(\\op)$ to be a globally-optimal direction.\n\\end{definition}\n\n\\begin{lemma}[Mapping regularization path of $(\\Kb,\\Qb)$ to $\\W$]\\label{kqw mapping} Let $\\Kb,\\Qb\\in\\R^{d\\times m}$ and consider regularization path solutions of \\eqref{serm-w} and \\eqref{serm-kq}\n\\begin{align}\n&\\Wb_R\\in\\underset{\\W\\in\\Rcm:\\tnuc{\\W}\\leq R}{\\arg\\min}\\Lc(\\W)\\label{Wpath}\\\\\n&\\Kbb_R,\\Qbb_R\\in\\underset{\\tf{\\Kb}^2+\\tf{\\Qb}^2\\leq 2R}{\\arg\\min}\\Lc(\\Kb,\\Qb).\\label{KQpath}\n\\end{align}\nFor all $R\\geq 0$, there is a one-to-one map between the set of solutions $\\Wb_R$ of \\eqref{Wpath} and $\\Kbb_R\\Qbb_R^\\top$ of \\eqref{KQpath}.\n%via $\\Wb_R\\leftrightarrow\\Qbb_R\\Kbb_R^\\top$.\n\\end{lemma}\n\\begin{proof} To prove the mapping, first fix a $\\Wb_R$ solution with rank $m$, set $\\Lc_F=\\Lc(\\Wb_R)$ and show the existence of $\\Kb,\\Qb$ with $\\Kb\\Qb^\\top=\\Wb_R$ feasible for \\eqref{KQpath} and $\\Lc(\\Kb,\\Qb)\\leq \\Lc_F$. Use the singular value decomposition $\\Wb_R=\\Ub\\bSi\\Vb^\\top$ with $\\bSi\\in\\R^{m\\times m}$ being diagonal matrix of singular values. Set $\\Kb=\\Ub\\sqrt{\\bSi}$ and $\\Qb=\\Vb\\sqrt{\\bSi}$. Observe that $\\Kb\\Qb^\\top=\\W$ and\n\\[\n\\tf{\\Kb}^2=\\tf{\\Qb}^2=\\sum_{i=1}^m\\sqrt{\\bSi_{ii}}^2=\\tnuc{\\Wb_R}\\leq R.\n\\]\nThus, $\\Kb,\\Qb$ achieves $\\Lc(\\Kb,\\Qb)=\\Lc_F$. Conversely, given $\\Kbb_R,\\Qbb_R$ with $\\Lc_\\st=\\Lc(\\Kbb_R,\\Qbb_R)$, $\\W=\\Kbb_R\\Qbb_R^\\top$ obeys $\\Lc(\\W)=\\Lc_\\st$ and, using the standard nuclear norm inequality, we have\n\\[\n\\tnuc{\\W}=\\tnuc{\\Kbb_R\\Qbb_R^\\top}\\leq \\frac{1}{2}(\\tf{\\Kbb_R}^2+\\tf{\\Qbb_R}^2)=R.\n\\]\nThis shows $\\W$ is feasible for \\eqref{Wpath}. Combining the two findings above, we find that optimal costs are equal ($\\Lc_\\st=\\Lc_F$) and for any $(\\Kbb_R,\\Qbb_R)$ solution there exists a $\\Wb_R$ solution and vice versa.\n\\end{proof}\n\n\n==== END OF /2308.16898/supp/reg_path.tex ====\n==== BEGINNING OF /2308.16898/supp/proof_convergence.tex ====\n%\\subsection{Lemmas for Gradient Analysis}\n\\subsection{Proof of Lemma~\\ref{lem:lip}}\n%\\begin{proof}\nLet\n\\begin{equation*}\n\\bgam_i=Y_i\\cdot \\X_i\\vb, \\quad \n \\hb_i=\\X_i\\W \\z_{i}.\n\\end{equation*}\nFrom Assumption~\\ref{assum:loss:prope}, $\\ell:\\R\\rightarrow\\R$ is differentiable. Hence,  the gradient evaluated at $\\W$ is given by\n\\begin{equation}\\label{grad def}\n\\nabla\\Lc(\\W)=\\frac{1}{n}\\sum_{i=1}^n \\ell' \\left(\\bgam_i^\\top \\sft{\\hb_i}\\right)\\cdot \\X_i^\\top  \\sfp{\\hb_i}  \\bgam_i  \\z_{i}^\\top,\n\\end{equation}\nwhere\n\\begin{equation}\\label{eqn:der:soft}\n\\sfp{\\hb} = \\text{diag}\\left(\\sft{\\hb}\\right) - \\sft{\\hb} \\sft{\\hb}^\\top \\in \\R^{T\\times T}.    \n\\end{equation}\nNote that \n\\begin{equation}\\label{eqn:sprime:bnorm}\n \\| \\sfp{\\hb} \\| \\leq \\| \\sfp{\\hb} \\|_F \\leq  1.  % \\| \\sft{\\hb}\\| +  \\| \\sft{\\hb} \\| ~ \\| \\sft{\\hb}\\| \\leq  2 \\| \\sft{\\hb}\\|  \\leq 2.\n\\end{equation}\n%It follows from \\eqref{grad def} and \\eqref{eqn:der:soft} that  $\\| \\partial \\sft{\\hb_i}/\\partial \\W \\| \\leq \\|\\X_i\\|~\\|\\z_i\\|$.\nHence,  for any $\\W,\\dot{\\W}\\in \\R^{d\\times d}$, $i\\in[n]$,  we have\n\\begin{subequations}\n\\begin{align}\\label{eqn:soft:lipcons1}\n\\left\\|\\sft{\\hb_i}-\\sft{\\dot{\\hb}_i}\\right\\| \\leq \\left\\|\\hb_i-\\dot{\\hb}_i\\right\\| \\leq \\|\\X_i\\|~\\|\\z_i\\|~\\left\\|\\W-\\dot{\\W}\\right\\|_F,\n\\end{align}\nwhere $\\dot{\\hb}_i=\\X_i\\dot{\\W} \\z_{i}$.\n\nSimilarly, \n\\begin{align}\\label{eqn:soft:lipcons2}\n\\nonumber\n\\left\\|\\sfp{\\hb_i}-\\sfp{\\dot{\\hb}_i}\\right\\|_F & \\leq \\left\\|\\sft{\\hb_i} - \\sft{\\dot{\\hb_i}}\\right\\| +   \\left\\|\\sft{\\hb_i} \\sft{\\hb_i}^\\top- \\sft{\\dot{\\hb_i}} \\sft{ \\dot{\\hb_i}}^\\top\\right\\|_F\n\\\\\n & \\leq 3 \\|\\X_i\\|~\\|\\z_i\\|~\\left\\|\\W-\\dot{\\W}\\right\\|_F.\n\\end{align}\n\\end{subequations}\nNext, for any $\\W,\\dot{\\W}\\in\\R^{d\\times d}$, we get\n\\begin{align}\\label{eqn:obj:lipcons}\n  \\nonumber\n \\left\\|\\nabla \\mc{L}(\\W)-\\nabla \\mc{L}(\\dot{\\W})\\right\\|_F\n  &\\leq \\frac{1}{n}  \\sum_{i=1}^n\\left\\| \\ell' \\left(\\bgam_i^\\top \\sft{\\hb_i}\\right) \\cdot \\z_{i}  \\bgam_i^\\top \\sfp{\\hb_i} \\X_i - \\ell' \\left(\\bgam_i^\\top \\sft{\\dot{\\hb}_i}\\right) \\cdot \\z_{i}  \\bgam_i^\\top \\sfp{\\dot{\\hb}_i} \\X_i \\right\\|_F\\\\ \n    \\nonumber\n       & \\le \\frac{1}{n}\\sum_{i=1}^{n}  \\left\\|\\z_{i}  \\bgam_i^\\top \\sfp{\\dot{\\hb}_i} \\X_i \\right\\|_F~\\left| \\ell' \\left(\\bgam_i^\\top \\sft{\\hb_i}\\right) - \\ell' \\left(\\bgam_i^\\top \\sft{\\dot{\\hb}_i}\\right)  \\right| \\\\\n         \\nonumber\n       &+ \\frac{1}{n}\\sum_{i=1}^{n} \\left|  \\ell' \\left(\\bgam_i^\\top \\sft{\\hb_i}\\right)\\right|~\\left\\| \\z_{i}  \\bgam_i^\\top \\sfp{\\hb_i} \\X_i - \\z_{i}  \\bgam_i^\\top \\sfp{\\dot{\\hb}_i} \\X_i \\right\\|_F \\\\\n         \\nonumber\n       & \\le \\frac{1}{n}\\sum_{i=1}^{n}  M_0 ~\\|\\bgam_i\\|^2~\\|\\z_i\\| ~\\|\\X_i\\|~\\left\\|\\sft{\\hb_i}-\\sft{\\dot{\\hb}_i}\\right\\| \\\\\n       & +   \\frac{1}{n}\\sum_{i=1}^{n}  M_1  ~\\|\\bgam_i\\|~\\|\\z_i\\|~\\|\\X_i \\|~\\left\\|\\sfp{\\hb_i}-\\sfp{\\dot{\\hb}_i}\\right\\|_F,\n\\end{align}\nwhere the second inequality follows from the fact that $|ab - cd| \\leq |d||a-c|+ |a||b-d|$ and the third inequality uses Assumption~\\ref{assum:loss:prope} and \\eqref{eqn:sprime:bnorm}.\n\nSubstituting \\eqref{eqn:soft:lipcons1}  and \\eqref{eqn:soft:lipcons2} into \\eqref{eqn:obj:lipcons}, we get\n\\begin{align*}\n\\left\\|\\nabla \\mc{L}(\\W)-\\nabla \\mc{L}(\\dot{\\W})\\right\\|_F &\\leq  \\frac{1}{n}\\sum_{i=1}^{n} \\left( M_0  ~\\|\\bgam_i\\|^2\\|\\z_i\\|^2\\|\\X_i\\|^2+ 3  M_1 \\|\\bgam_i\\|~\\|\\z_i\\|^2~\\|\\X_i \\|^2\\right)  \\|\\W-\\dot{\\W}\\|_F\\\\\n &\\leq  \\frac{1}{n}\\sum_{i=1}^{n} \\left( M_0  ~\\|\\vb\\|^2\\|\\z_i\\|^2\\|\\X_i\\|^4+ 3  M_1 \\|\\vb\\|~\\|\\z_i\\|^2~\\|\\X_i \\|^3\\right)  \\|\\W-\\dot{\\W}\\|_F\\\\\n&\\leq  L_{\\W}~\\|\\W-\\dot{\\W}\\|_F,\n\\end{align*}\nwhere $L_{\\W}$ is defined in \\eqref{eqn:lip:cons:erm}.\n\n\nLet $\\g_{i}=\\X_i\\Kb \\Qb^\\top\\z_{i}$. We have\n\\begin{subequations}\\label{grad def KQ}\n\\begin{align}\n\\nabla_{\\Kb} \\Lc(\\Kb,\\Qb)=\\frac{1}{n}\\sum_{i=1}^n \\ell' \\left(\\bgam_i^\\top \\sft{\\g_i}\\right) \\cdot  \\z_{i}  \\bgam_i^\\top \\sfp{\\g_i} \\X_i \\Qb,\\\\\n\\nabla_{\\Qb} \\Lc(\\Kb,\\Qb)=\\frac{1}{n}\\sum_{i=1}^n \\ell' \\left(\\bgam_i^\\top \\sft{\\g_i}\\right) \\cdot \\X_i^\\top  \\sfp{\\g_i}  \\bgam_i  \\z_{i}^\\top \\Kb.\n\\end{align}\n\\end{subequations}\nBy the similar argument as in \\eqref{eqn:obj:lipcons}, for any $\\Qb$ and $\\dot\\Qb\\in\\R^{d\\times m}$, we have\n\\begin{align}\\label{eqn:objqk:lipcons}\n  \\nonumber\n \\left\\|\\nabla_{\\Qb} \\mc{L}(\\Kb,\\Qb)-\\nabla_{\\Qb} \\mc{L}(\\Kb,\\dot{\\Qb})\\right\\|_F\n  &\\leq \\frac{\\|\\Kb\\|}{n}  \\sum_{i=1}^n\\left\\| \\ell' \\left(\\bgam_i^\\top \\sft{\\hb_i}\\right) \\cdot \\z_{i}  \\bgam_i^\\top \\sfp{\\hb_i} \\X_i - \\ell' \\left(\\bgam_i^\\top \\sft{\\dot{\\hb}_i}\\right) \\cdot \\z_{i}  \\bgam_i^\\top \\sfp{\\dot{\\hb}_i} \\X_i \\right\\|_F\\\\ \n       & \\leq L_{\\W} \\|\\Kb\\| ~\\|\\Qb-\\dot{\\Qb}\\|_F.\n\\end{align}\nSimilarly, for any $\\Kb,\\dot\\Kb\\in\\R^{d\\times m}$, we get   \n$$\n\\left\\|\\nabla_{\\Kb} \\mc{L}(\\Kb,\\Qb)-\\nabla_{\\Kb} \\mc{L}(\\dot{\\Kb},\\Qb)\\right\\|_F \\leq  L_{\\W} \\|\\Qb\\| ~\\|\\Kb-\\dot{\\Kb}\\|_F.\n$$\n$\\qed$\n%\\end{proof}\n%\n\\subsection{A useful lemma for gradient descent analysis}\n\\begin{lemma}\\label{lem:q_reduce} For any $\\X \\in\\R^{T\\times d}$, $\\W,\\V \\in \\R^{d\\times d}$ and $\\z, \\vb \\in \\R^{d}$, let $\\ab= \\X\\V \\z$, $\\s=\\sft{\\X\\W\\z}$, and $\\bgam=\\X\\vb$. Set\n\\begin{equation*}\n\\Gamma=\\sup_{t,\\tau\\in[T]}|\\bgam_t-\\bgam_\\tau|~~~\\textnormal{and}~~~A=\\sup_{t\\in[T]}\\tn{\\ab_t}.\n\\end{equation*}\nWe have that\n  \\[\n    \\left|\\ab^\\top\\textnormal{diag}(\\s) \\bgam-\\ab^\\top\\s\\s^\\top\\bgam-\\sum_{t\\geq 2}^T (\\ab_1-\\ab_t)\\s_t(\\bgam_1-\\bgam_t)\\right|\\leq 2\\Gamma A(1-\\s_1)^2.\n  \\]\n\\end{lemma}\n\n\n \\begin{proof}\nThe proof is similar to \\cite[Lemma~4]{tarzanagh2023margin}, but for the sake of completeness, we provide it here.  Set $\\gamb=\\sum_{t=1}^T \\bgam_t\\s_t$.  We have \n\\begin{align*}\n\\bgam_1-\\gamb=\\sum_{t\\geq 2}^T (\\bgam_1-\\bgam_t)\\s_t,~~\\textnormal{and}~~|\\bgam_1-\\gamb|\\leq \\Gamma (1-\\s_1).\n\\end{align*}    \nThen,\n  \\begin{align} \n  \\nonumber \n    \\ab^\\top\\diag{\\s}\\bgam-\\ab^\\top\\s\\s^\\top\\bgam&=\\sum_{t=1}^T \\ab_t\\bgam_t\\s_t-\\sum_{t=1}^T \\ab_t\\s_t\\sum_{t=1}^T \\bgam_t\\s_t\\\\\n    &=\\ab_1\\s_1(\\bgam_1-\\gamb)-\\sum_{t\\geq 2}^T\\ab_t\\s_t(\\gamb-\\bgam_t). \\label{grad def step3}\n  \\end{align}\nSince \n$$\n\\left|\\sum_{t\\geq 2}^T\\ab_t\\s_t(\\gamb-\\bgam_t)-\\sum_{t\\geq 2}^T\\ab_t\\s_t(\\bgam_1-\\bgam_t)\\right|\\leq A\\Gamma (1-\\s_1)^2,\n$$\nwe obtain\\footnote{For simplicity, we use $\\pm$ on the right hand side to denote the upper and lower bounds.}\n  \\begin{align*}  \n    \\ab^\\top\\diag{\\s}\\bgam-\\ab^\\top\\s\\s^\\top\\bgam&=\\ab_1\\s_1(\\bgam_1-\\gamb)-\\sum_{t\\geq 2}^T\\ab_t\\s_t(\\bgam_1-\\bgam_t)\\pm A\\Gamma (1-\\s_1)^2\\\\\n    &=\\ab_1\\s_1\\sum_{t\\geq 2}^T (\\bgam_1-\\bgam_t)\\s_t-\\sum_{t\\geq 2}^T\\ab_t\\s_t(\\bgam_1-\\bgam_t)\\pm A\\Gamma (1-\\s_1)^2\\\\\n    &=\\sum_{t\\geq 2}^T (\\ab_1\\s_1-\\ab_t)\\s_t(\\bgam_1-\\bgam_t)\\pm A\\Gamma (1-\\s_1)^2\\\\\n    &=\\sum_{t\\geq 2}^T (\\ab_1-\\ab_t)\\s_t(\\bgam_1-\\bgam_t)\\pm 2A\\Gamma (1-\\s_1)^2.\n    %&=\\ab_1\\s_1(\\bgam_1(1-\\s_1)-\\sum_{t\\geq 2} \\bgam_t\\s_t)+\\sum_{t\\geq 2}\\ab_t\\s_t(\\bgam_t(1-\\s_t)-\\sum_{t\\geq 2} \\bgam_t\\s_t)\n  \\end{align*}\nHere,  $\\pm$ on the right handside uses the fact that\n  \\[\n  \\left|\\sum_{t\\geq 2}^T (\\ab_1\\s_1-\\ab_1)\\s_t(\\bgam_1-\\bgam_t)\\right|\\leq (1-\\s_1)A\\Gamma\\sum_{t\\geq 2}^T\\s_t=(1-\\s_1)^2A\\Gamma.\n  \\]\n\\end{proof}\n\n\n\n\n\\section{Global Convergence of Gradient Descent}\\label{app:sec:gd:global}\n\n\\subsection{Divergence of norm of the iterates}\n\nThe next lemma establishes the descent property of gradient descent for $\\mathcal{L}(\\W)$ under Assumption \\ref{assum:loss:prope}. \n\n\\begin{lemma}[Descent Lemma]\\label{lem:grad:descent}\nUnder Assumption \\ref{assum:loss:prope}, if $\\eta \\leq 1/L_{\\W}$, then for any initialization $\\W(0)$, Algorithm~\\ref{GD-W} satisfies:\n\\begin{align}\\label{eq:descent:obj new}\n\\mathcal{L}(\\W(k+1))-\\mathcal{L}(\\W(k))\\leq-\\frac{\\eta}{2} \\tf{\\nabla \\mathcal{L}(\\W(k))}^2,\n\\end{align}\nfor all $k\\ge0$. Additionally, it holds that $\\sum_{k=0}^{\\infty} \\tf{\\nabla\\mathcal{L}\\left(\\W(k)\\right)}^{2}<\\infty$, and $\\lim_{k\\rightarrow \\infty}\n\\tf{\\nabla\\mathcal{L}\\left(\\W\\left(k\\right)\\right)}^{2}=0$.\n\\end{lemma}\n\\begin{proof}\nThe proof is similar to \\cite[Lemma~6]{tarzanagh2023margin}.\n%and , but for the sake of completeness, we provide it.\n\\end{proof}\n\nThe lemma below reveals that the correlation between the training loss's gradient at any arbitrary matrix $\\W$ and the attention SVM solution  $ \\Wm$ is negative. Consequently, for any finite $\\W$, $\\li\\nabla\\Lc(\\W), \\Wm\\ri$ cannot be equal to zero.\n\n\\begin{lemma}\\label{global des lem} \nLet $ \\Wm$ be the SVM solution of \\eqref{eqn:sattnsvm}. Suppose Assumptions \\ref{assum:loss:prope} and \\ref{assum:token} hold.  Then,  for all $\\W\\in\\R^{d\\times d}$, the training loss \\eqref{eqn:erm:w} obeys $\\li\\nabla\\Lc(\\W),\\Wm\\ri<0$. % $\\li\\nabla\\Lc(\\W),\\Wm\\ri \\leq -c <0$, for some constant $c>0$ (see \\eqref{eqn:grad:prod:p:fin}) depending on the data, the head $\\vb$, and a loss derivative bound. \n\\end{lemma}\n%\n\\begin{proof}\nLet\n\\begin{equation}\n\\hbm_i=  \\X_{i} \\Wm \\z_i, ~~~\\bgam_i=Y_i\\cdot \\X_i\\vb,~~~\\textnormal{and}~~~\n \\hb_i=\\X_i\\W \\z_{i}.    \n\\end{equation}\nLet us recall the gradient evaluated at $\\W$ which is given by %\\redp{grad def label was multiply defined, changed its name}\n\\begin{align}\\label{grad def new}\n\\nabla\\Lc(\\W)=\\frac{1}{n}\\sum_{i=1}^n\\ell' \\left(\\bgam_i^\\top \\sft{\\hb_i}\\right) \\cdot \\X_i^\\top  \\sfp{\\hb_i}  \\bgam_i  \\z_{i}^\\top,\n\\end{align}\n which implies that \n\\begin{equation}\\label{eqn:grad:prod:p}\n    \\begin{split}\n\\li\\nabla\\Lc(\\W),\\Wm\\ri&= \\frac{1}{n}\\sum_{i=1}^n \\ell' \\left(\\bgam_i^\\top \\sft{\\hb_i}\\right)  \\cdot \\iprod{ \\X_i^\\top  \\sfp{\\hb_i}  \\bgam_i  \\z_{i}^\\top}{\\Wm}\\\\\n&= \\frac{1}{n}\\sum_{i=1}^n\n\\ell'_i  \\cdot \\tr\\left(  (\\Wm)^\\top  \\X_{i}^\\top \\sfp{\\hb_i} \\bgam_i  \\z_{i}^\\top\\right)\\\\\n%&= \\frac{1}{n}\\sum_{i=1}^n\n%\\ell' \\left(\\bgam_i^\\top \\sft{\\hb_i}\\right)  \\cdot \n%\\z_{i}^\\top (\\Wms)^\\top  \\X_{i}^\\top \\sfp{\\hb_i} \\bgam_i\\\\\n%&= \\frac{1}{n}\\sum_{i=1}^n\n%\\ell' \\left(\\bgam_i^\\top \\sft{\\hb_i}\\right)  \\cdot \n%\\z_{i}^\\top (\\Wms)^\\top  \\X_{i}^\\top \\sfp{\\hb_i} \\bgam_i\\\\\n&= \\frac{1}{n}\\sum_{i=1}^n\n\\ell'_i \\cdot \\hbm_{i}^\\top \\sfp{\\hb_i} \\bgam_i \\\\\n&=  \\frac{1}{n}\\sum_{i=1}^n\n\\ell'_i \\cdot  \\left(\\hbm^\\top_i\\diag{\\s_i}\\bgam_i-\\hbm^\\top_i\\s_i\\s^\\top_i\\bgam_i\\right).        \n    \\end{split}\n\\end{equation}\nHere, let $\\ell'_i:=\\ell'(\\bgam_i^\\top\\sft{\\hb_i})$, $\\s_i=\\sft{\\hb_i}$ and the third equality uses $\\tr\\left(\\bb\\ab^\\top\\right) = \\ab^\\top \\bb$.\n\nIn order to move forward, we will establish the following result, with a focus on the equal score condition (Assumption B.2 [in <a href=\"https://arxiv.org/pdf/2308.16898#Item.5\">original paper</a>]): Let $\\gamma=\\bgam_{t\\geq 2}$ be a constant, and let $\\bgam_1$ and $\\bar{\\hb}_1$ represent the largest indices of vectors $\\bgam$ and $\\hbm$ respectively. For any vector $\\s$ that satisfies $\\sum_{t\\in[T]}\\s_t=1$ and $\\s_t> 0$, we aim to prove that $\\hbm^\\top\\diag{\\s}\\bgam-\\hbm^\\top\\s\\s^\\top\\bgam>0$. To demonstrate this, we proceed by writing the following:\n\\begin{equation}\\label{grad def2}\n\\begin{split}\n\\hbm^\\top\\diag{\\s}\\bgam-\\hbm^\\top\\s\\s^\\top\\bgam&=\\sum_{t=1}^T \\bar{\\hb}_t\\bgam_t \\s_t-\\sum_{t=1}^T  \\bar{\\hb}_t \\s_t\\sum_{t=1}^T \\bgam_t \\s_t\\\\\n&=\\left(\\bar{\\hb}_1\\bgam_1\\s_1+\\gamma\\sum_{t\\geq 2}^T\\bar{\\hb}_t\\s_t\\right)-\\Big(\\bgam_1\\s_1+\\gamma(1-\\s_1)\\Big)\\left(\\bar{\\hb}_1\\s_1+\\sum_{t\\geq 2}^T \\bar{\\hb}_t\\s_t\\right)\\\\\n&=\\bar{\\hb}_1(\\bgam_1-\\gamma) \\s_1(1-\\s_1)-(\\bgam_1-\\gamma)\\s_1\\sum_{t\\geq 2}^T \\bar{\\hb}_t \\s_t\\\\\n&=(\\bgam_1-\\gamma)(1- \\s_1) \\s_1\\left[\\bar{\\hb}_1-\\frac{ \\sum_{t\\geq 2}^T \\bar{\\hb}_t \\s_t}{\\sum_{t\\geq 2}^T\\s_t}\\right]\\\\\n&\\geq(\\bgam_1-\\gamma)(1- \\s_1) \\s_1 (\\bar{\\hb}_1-\\max_{t\\geq 2}\\bar{\\hb}_t).\n\\end{split}\n\\end{equation}\nTo proceed, define\n\\begin{equation*}\n\\bgag^i=\\bgam_{i\\opt_i}-\\max_{t\\neq\\opt_i}\\bgam_{it}~~\\textnormal{and}~~\\bhbg^i=\\bar \\hb_{i\\opt_i}-\\max_{t\\neq\\opt_i}\\bar \\hb_{it}.\n\\end{equation*}\nWith these, we obtain %\\ct{Change $\\opt_i$ to $\\opt_i$? \\dat{ we use general $\\opt_i$ in the lemma statement}}\n% \\begin{equation}\\label{eqn:al:lem}\n% \\hbm^\\top\\diag{\\s}\\bgam-\\hbm^\\top\\s\\s^\\top\\bgam\\geq \\hbg \\bgag \\s_1(1-\\s_1).    \n% \\end{equation}\n\\begin{equation}\\label{eqn:al:lem}\n% \\li\\nabla\\Lc(\\W),\\Wm\\ri\\geq \\frac{1}{n}\\sum_{i=1}^n\\ell'_i\\cdot\n\\hbm^\\top_i\\diag{\\s_i}\\bgam_i-\\hbm^\\top_i\\s_i\\s^\\top_i\\bgam_i\\geq\\bgag^i\\bhbg^i(1-\\s_{i\\opt_i})\\s_{i\\opt_i}.\n\\end{equation}\nNote that \n\\begin{equation*}%\\label{eqn:lower}\n\\begin{split}\n& \\bhbg^i=\\min_{t\\neq \\opt_i}~(\\x_{i \\opt_i}-\\x_{it})^\\top\\Wm \\z_i \\geq1,  \\\\\n&\\bgag^i=\\min_{t\\neq \\opt_i}~\\bgam_{i\\opt_i}-\\bgam_{it} >0,\\\\\n&\\s_{i\\opt_i}(1-\\s_{i\\opt_i}) > 0.    \n\\end{split}\n\\end{equation*}\n\\begin{comment}\n    Hence,\n\\begin{equation}\\label{eqn:lower}\nc_0:=\\min_{i \\in [n]}\\left\\{ \\left(\\min_{t\\neq \\opt_i}~(\\x_{i \\opt_i}-\\x_{it})^\\top\\Wm \\z_i\\right) \\cdot \\left(\\min_{t\\neq \\opt_i}~\\bgam_{i\\opt_i}-\\bgam_{it}\\right) \\cdot \\s_{i\\opt_i}(1-\\s_{i\\opt_i}) \\right\\}>0.\n\\end{equation}\nIt follows from  \\eqref{eqn:al:lem} and \\eqref{eqn:lower} that \n\\begin{equation}\\label{eqn:al:lem:2}\n% \\li\\nabla\\Lc(\\W),\\Wm\\ri\\geq \\frac{1}{n}\\sum_{i=1}^n\\ell'_i\\cdot\n\\min_{i \\in [n]}\\left\\{\\hbm^\\top_i\\diag{\\s_i}\\bgam_i-\\hbm^\\top_i\\s_i\\s^\\top_i\\bgam_i \\right\\} \\geq c_0>0.\n\\end{equation}\nFurther, by our assumption $\\ell'_i<0$.  \nSince by Assumption \\ref{assum:loss:prope}, $\\ell'$ is continuous and the domain is bounded, the maximum is attained and negative, and thus  \n\\begin{equation}\\label{eqn:bound:lprim}\n-c_1= \\max_{x} \\ell'(x), \\qquad  \\textnormal{for some} \\quad c_1>0.     \n\\end{equation}\nHence, using \\eqref{eqn:al:lem:2} and \\eqref{eqn:bound:lprim} in  \\eqref{eqn:grad:prod:p}, we obtain \n\\begin{equation}\\label{eqn:grad:prod:p:fin}\n    \\begin{split}\n\\li\\nabla\\Lc(\\W),\\Wm\\ri \\leq  - c<0, \\quad \\textnormal{where} \\quad c=c_1 \\cdot c_0.\n    \\end{split}\n\\end{equation}\n\n% Using \\eqref{eqn:grad:prod:p} and \\eqref{eqn:al:lem:2}, we obtain \n% \\begin{equation}\\label{eqn:grad:prod:p:fin}\n%     \\begin{split}\n% \\li\\nabla\\Lc(\\W),\\Wm\\ri <0.\n%     \\end{split}\n% \\end{equation}\n\n\nIn the scenario that Assumption~\\ref{assum:token:supp} holds (all tokens are support), $\\hbm_t=\\x_{it}^\\top\\Wm \\z_i $ is constant for all $t\\geq 2$. Hence, following similar steps as in \\eqref{grad def2} completes the proof. \n\\end{comment}\nHence,\n\\begin{equation}\\label{eqn:lower}\n\\min_{i \\in [n]}\\left\\{ \\left(\\min_{t\\neq \\opt_i}~(\\x_{i \\opt_i}-\\x_{it})^\\top\\Wm \\z_i\\right) \\cdot \\left(\\min_{t\\neq \\opt_i}~\\bgam_{i\\opt_i}-\\bgam_{it}\\right) \\cdot \\s_{i\\opt_i}(1-\\s_{i\\opt_i}) \\right\\}>0.\n\\end{equation}\nIt follows from  \\eqref{eqn:al:lem} and \\eqref{eqn:lower} that \n\\begin{equation}\\label{eqn:al:lem:2}\n% \\li\\nabla\\Lc(\\W),\\Wm\\ri\\geq \\frac{1}{n}\\sum_{i=1}^n\\ell'_i\\cdot\n\\min_{i \\in [n]}\\left\\{\\hbm^\\top_i\\diag{\\s_i}\\bgam_i-\\hbm^\\top_i\\s_i\\s^\\top_i\\bgam_i \\right\\}>0.\n\\end{equation}\nFurther, by Assumption \\ref{assum:loss:prope}, $\\ell'_i<0$, $\\ell'$ is continuous and the domain is bounded, the maximum is attained and negative, and thus  \n\\begin{equation}\\label{eqn:bound:lprim}\n\\max_{x} \\ell'(x)<0.    \n\\end{equation}\nHence, using \\eqref{eqn:al:lem:2} and \\eqref{eqn:bound:lprim} in  \\eqref{eqn:grad:prod:p}, we obtain \n\\begin{equation}\\label{eqn:grad:prod:p:fin}\n    \\begin{split}\n\\li\\nabla\\Lc(\\W),\\Wm\\ri <0.\n    \\end{split}\n\\end{equation}\n\n% Using \\eqref{eqn:grad:prod:p} and \\eqref{eqn:al:lem:2}, we obtain \n% \\begin{equation}\\label{eqn:grad:prod:p:fin}\n%     \\begin{split}\n% \\li\\nabla\\Lc(\\W),\\Wm\\ri <0.\n%     \\end{split}\n% \\end{equation}\n\n\nIn the scenario that Assumption~\\ref{assum:token:supp} holds (all tokens are support), $\\hbm_t=\\x_{it}^\\top\\Wm \\z_i $ is constant for all $t\\geq 2$. Hence, following similar steps as in \\eqref{grad def2} completes the proof. \n\n\\end{proof}\n\n\\subsubsection{Proof of Theorem ~\\ref{diverg:norm:w}}\n%\\begin{proof}\nIt follows from Lemma~\\ref{lem:grad:descent} that under Assumption \\ref{assum:loss:prope}, $\\eta \\leq 1/L_{\\W}$, and for any initialization $\\W(0)$, the gradient descent sequence $\\W(k+1)=\\W(k)-\\eta\\nabla \\mathcal{L}(\\W(k))$ satisfies $\\lim_{k\\rightarrow \\infty}\n\\tf{\\nabla\\mathcal{L}\\left(\\W\\left(k\\right)\\right)}^{2}=0$.  \n\nFurther,  it follows from Lemma~\\ref{global des lem} that $\\li\\nabla\\Lc(\\W), \\Wm\\ri <0$  for all $\\W\\in\\R^{d\\times d}$. Hence, for any finite $\\W$, $\\li\\nabla\\Lc(\\W), \\Wm\\ri$ cannot be equal to zero.  Therefore, there are no finite critical points $\\W$, for which $\\nabla \\mc{L} (\\W)=0$ which contradicts Lemma~\\ref{lem:grad:descent}. This\nimplies that $\\left\\Vert \\W\\left(k\\right)\\right\\Vert \\rightarrow\\infty$. \n\\vspace{.1cm}\n%\\\\\n%The proof of the theorem's statement about $\\Kb,\\Qb$ convergence is given in Section \\ref{sec:KQ convergence proof} below (see Theorem~\\ref{diverg:norm:qk}).\n%\\end{proof}\n$\\qed$\n\n\n\n%\\input{supp/QK_max_margin.tex}\n\n\n\n\n\n\n\\subsection{Global convergence under good initial gradient}\\label{app B4} To ensure global convergence, we identify an assumption that prevents GD from getting trapped at suboptimal tokens that offer no scoring advantage compared to other choices. To establish a foundation for providing the convergence of GD to the globally optimal solution $\\Ws$, we present the following definitions.  For parameters $\\mu \\in (0,1)$ and $R>0$, consider the following subset of the sphere and its associated cone:\n\\begin{subequations}\\label{eqn:con:nabla0}\n\\begin{align}\n&\\Scc_{\\mu} (\\Ws):=\\left\\{\\W \\in \\mathbb{R}^{d \\times d}~\\Big|~  \\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\frac{\\W}{\\tf{\\W}}\\ri\\geq \\frac{\\mu}{\\tf{\\Ws}}\\quad \\textnormal{for all}\\quad t\\neq \\op_i, \\quad  i\\in[n]\\right\\},\\\\\n&\\conb_{\\mu,R}(\\Ws):=\\left\\{  \\W\\in\\Scc_\\mu (\\Ws) ~\\Big|~   \\tf{\\W}\\geq R\\right\\}.\n\\end{align}\n\\end{subequations}\nNote that the $\\conb_{\\mu,R}(\\Ws)$ definition is equivalent to the $\\conb_{\\mu,R}$ definition in \\eqref{eqn:con:nabla0:main} with a change of variable $\\mu\\gets\\tf{\\Wm}\\cdot \\mu$.\n%\\red{perturb..}\n\n% %\\red{\n% \\begin{assumption}[First GD step is separating]\\label{assum:nabla0} For\n% all $t\\neq \\op_i,~i\\in[n]$: $ (\\x_{it}-\\x_{i\\op_i})^\\top\\nabla\\Lc(0)\\z_i=:\\iota>0$.\n% \\end{assumption}\n% %}\n% \\begin{theorem}\\label{conv:gd:w:global:nabla0:app}\n% Suppose Assumption~\\ref{assum:loss:prope} on the loss function $\\ell$ and Assumption \\ref{assum:nabla0} on the initial gradient holds.  Then, for any $\\mu \\in  (0,\\min (0.5,\\iota \\tf{\\Ws}/\\tf{\\nabla \\Lc(0)})$, there exists $R>0$ such that  the following holds. \n% \\begin{itemize}\n% %\\item %\\textbf{No stationary points:}\n% %There is no $\\W\\in\\R^{d\\times d}$ satisfying $\\nabla \\Lc(\\W)=0$.\n%   \\item  For any $\\mu>0$, there exists  $R>0$ such  that   $\\conb_{\\mu,R}$ does not contain any  stationary points. \n% \\item Fix any $\\mu \\in  (0, \\iota/\\tf{\\nabla \\Lc(0)})$. Consider GD iterations with $\\W(0)=0$, $\\W(1)=-R/\\tf{\\nabla\\Lc(0)}$, and $\\W(k+1)=\\W(k)-\\eta\\nabla\\Lc(\\W(k))$ for $\\eta\\le 1/L_{\\W}$, $k\\ge 1$, and $R$ sufficiently large. If all iterates remain within $\\conb_{\\mu,R}$, then $\\lim_{k\\rightarrow\\infty} \\tf{\\W(k)}=\\infty$ and $\\lim_{k\\rightarrow\\infty}\\frac{\\W(k)}{\\tf{\\W(k)}}=\\frac{\\Wm}{\\tf{\\Wm}}$.\n%     % \\item  GD iterates $\\W(k+1) = \\W(k) -\\eta(k) \\nabla \\Lc(\\W(k))$ on \\eqref{eqn:erm:w}  with $\\W(0)=0$,  $\\eta(0) \\geq R/\\tf{\\nabla \\Lc(0)}$ and $\\eta(k) \\leq 1 /L_{\\W}$ for all $k \\geq 1$ satisfies $\\lim_{k\\rightarrow\\infty} \\tf{\\W(k)}=\\infty$. \\redp{Is this true? Also where is ``no stationary within cone''} \n% %\\item  Let $D_\\eta(\\W):=(\\W-\\eta\\nabla \\Lc(\\W))/\\tf{\\W}$.  If for all $\\W \\in \\conb_{\\mu,R}(\\Ws)$,   $  \\min_{i \\in [n]}\\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, D_\\eta(\\W)\\ri > \\mu /\\tf{\\Ws}$, then we have  $\\lim_{k\\rightarrow\\infty}\\frac{\\W(k)}{\\tf{\\W(k)}}=\\frac{\\Wm}{\\tf{\\Wm}}$.\n% \\item  Assume for all $\\W \\in \\conb_{\\mu,R}(\\Ws)$, \n% \\begin{align}\\label{assum:extra}\n%    \\min_{i \\in [n]}\\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\W-\\eta\\nabla\\Lc(\\W) \\ri \\geq     \\min_{i \\in [n]}\\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\W\\ri - \\frac{\\eta\\sqrt{\\mu}}{\\tf{\\Wm}^2}\\iprod{\\nabla\\mc{L}(\\W)}{\\Wm}.\n% \\end{align}\n% then all iterates remain within $\\conb_{\\mu,R}$.\n% % \\item  Let $D_\\eta(\\W):=(\\W-\\eta\\nabla \\Lc(\\W))/\\tf{\\W}$ and  $\\bar{\\mu}:=\\sqrt{\\mu}/\\tf{\\W} \\tf{\\Ws}$. Assume for all $\\W \\in \\conb_{\\mu,R}(\\Ws)$, \n% % %\\red{will fix it }\n% % \\begin{align}\\label{assum:extra}\n% %    \\min_{i \\in [n]}\\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, D_\\eta(\\W) \\ri \\geq     \\min_{i \\in [n]}\\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\frac{\\W}{\\tf{\\W}}\\ri + \\bar{\\mu} \\eta \\iprod{-\\nabla\\mc{L}(\\W)}{\\frac{\\Wm}{\\tf{\\Wm}}}.\n% % \\end{align}\n% % Then,  we have  $\\lim_{k\\rightarrow\\infty}\\frac{\\W(k)}{\\tf{\\W(k)}}=\\frac{\\Wm}{\\tf{\\Wm}}$.\n% %\\begin{equation}\n% %\\min_{i \\in [n]}\\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\frac{\\W}{\\tf{\\W}}\\ri > 0,   \n% %\\end{equation}\n% %then \n% %eta^{-1}(min_i <data_i, W-eta*nabla> - min_i <data_i, W>) > 0\n% %\\item %\\red{(Informal, see Thm \\ref{diverg:norm:qk})} \n% %Algorithm~\\ref{GD-QK}, with a proper stepsize schedule and a non-degenerate starting point $(\\Kb(0), \\Qb(0))$, obeys $\\lim_{k \\rightarrow \\infty} \\tf{\\nabla \\Lc_\\Kb(\\Kb(k))}\\vee\\tf{\\nabla\\Lc_\\Qb(\\Qb(k))}=0$, and  $\\lim_{k\\rightarrow\\infty} \\tf{\\Kb(k)}\\wedge\\tf{\\Qb(k)}=\\infty$.\n% % \\item Algorithm~\\ref{GD-QK} with the step size $\\eta \\leq \\mc{O}(1/ \\max(L_\\Qb,L_\\Kb))$ \\redp{Problem: $L_\\Qb,L_\\Kb$ depends on $\\|\\Qb\\|,\\|\\Kb\\|$ which goes to $\\infty$ so $\\eta\\rightarrow 0$} and any starting point $(\\Kb(0), \\Qb(0))$ satisfies  $\\lim_{k \\rightarrow \\infty}\n% % \\left\\|\\nabla \\Lc(\\Kb(k), \\Qb(k))\\right\\|_F=0$, and  $\\lim_{k\\rightarrow\\infty} \\|\\left( \\Kb(k), \\Qb(k)\\right)\\|_F=\\infty$.\n% \\end{itemize}\n% % and step sizes \\red{$\\eta(0)\\propto\\mc{O}(\\frac{1}{\\tf{\\Ws}})$} and  $\\eta(k) \\leq \\mc{O}\\left(1 / L_{\\W}\\right)$ for all $k\\geq 1$. Then, $\\lim_{k\\rightarrow\\infty}\\frac{\\W(k)}{\\tf{\\W(k)}}=\\frac{\\Wm}{\\tf{\\Wm}}$.\n% \\end{theorem}\n\n\\begin{lemma}%[Gradient Condition for Optimal Tokens]\n\\label{glocal cond} \nSuppose Assumption~\\ref{assum:loss:prope} holds and let $\\op=(\\op_i)_{i=1}^n$ be the unique globally-optimal indices with $\\Wm$ denoting the \\ref{eqn:sattnsvm} solution. Define the margin $\\Theta=1/\\tf{\\Ws}$. Let $\\s_{i}=\\sft{\\X_i\\W\\z_i}$. For any $\\mu>0$, there exists a sufficiently large $\\RR_\\mu=\\order{1/\\mu}$ (see \\eqref{R bound2}) such that:\n\\begin{enumerate}[label={\\textnormal{\\textbf{L\\arabic*.}}}, wide, labelwidth=!,itemindent=!, labelindent=5pt]\n\\item \\label{lem:gcond:l1} There is no stationary point within  $ \\conb_{\\mu,\\RR_\\mu}(\\Ws)$, where $\\conb_{\\mu,\\RR_\\mu} (\\Ws)$ is defined in \\eqref{eqn:con:nabla0}. \n\\item\\label{lem:gcond:l2} For all $\\V\\in \\Scc_{\\mu} (\\Ws)$ with $\\tf{\\V}=\\tf{\\Wm}$  and $\\W\\in\\conb_{\\mu,\\RR_\\mu}(\\Wm)$, there exist dataset dependent constants $C,c>0$ such that \n\\begin{subequations}\\label{zero:g:lbound}\n\\begin{align}\n&C\\cdot \\frac{1}{n}\\sum_{i=1}^n \\left(1-\\s_{i\\op_i}\\right) \\geq -\\Big\\langle\\nabla\\Lc(\\W),\\V \\Big\\rangle\\geq c\\cdot \\mu\\cdot  \\frac{1}{n} \\sum_{i=1}^n  \\left(1-\\s_{i\\op_i}\\right)>0, \\label{zero1:g:bound} \\\\\n& -\\li\\frac{\\V}{\\tf{\\V}},\\frac{\\nabla\\Lc(\\W)}{\\tf{\\nabla\\Lc(\\W)}}\\ri \\geq  \\frac{c}{C} \\cdot \\frac{\\Theta}{\\bar{A}}>0, \\label{zero2:g:bound}\\\\\n&\\tf{\\nabla\\Lc(\\W)}\\leq \\bar{A}C \\cdot \\frac{1}{n} \\sum_{i=1}^n  \\left(1-\\s_{i\\op_i}\\right). \\label{zero3:g:bound}\n\\end{align}\n\\end{subequations}\nHere,  $\\s_{i\\opt_i}=(\\sft{\\X_i\\W \\z_{i}})_{\\opt_i}$, $\\bar{A}=\\max_{i\\in[n],t,\\tau\\in[T]}\\tn{\\x_{it}- \\x_{i\\tau}}~\\tn{\\z_i}$, and $\\Theta=1/\\tf{\\Ws}$.\n% Additionally, for all $\\V\\in \\Scc_{\\mu} (\\Ws)$ with $\\tf{\\V}=\\tf{\\Wm}$  and $\\W\\in\\conb_{\\mu,R}$, there is a dataset dependent constant $\\bar{c}=\\frac{c}{C\\bar{A}}$ where $\\bar{A}=\\max_{i\\in[n],t,\\tau\\in[T]}\\tn{\\kb_{it}-\\kb_{i\\tau}}\\cdot\\tn{\\Ws}$ such that correlation to gradient is lower bounded as\n%   \\begin{align}\n%   \\end{align}\n%  and the gradient Frebenious-norm is upper bounded as\n%   \\begin{align}\n \n%   \\end{align}\n%\\item \\label{local cond:l3}  For any $\\pi>0$, there exists $R_\\pi$ such that all $\\pb\\in \\Cc_{\\mu,R_\\pi}(\\Ws)$ obeys\n%\\[\n%\\li\\nabla\\Lc(\\pb),\\frac{\\pb}{\\tn{\\pb}}\\ri\\geq (1+\\pi)\\li\\nabla\\Lc(\\pb), \\frac{\\Ws}{\\tn{\\Ws}}\\ri.\n%\\]\n\\end{enumerate}\n\\end{lemma}\n%\\begin{comment}\n\\begin{proof} For simplicity let $R=\\RR_\\mu$, $\\W\\in\\conb_{\\mu,R}(\\Ws)$ and \n\\begin{equation}\\label{mu choice2}\n\\begin{split}\n%&\\Theta=1/\\tf{\\Wm},\\\\\n&A=\\max_{i\\in[n],t,\\tau\\in[T]} \\frac{(\\tn{\\x_{it}}\\vee\\tn{\\x_{it}-\\x_{i\\tau}})\\cdot\\tn{\\z_i}}{\\Theta}.\n\\end{split}\n\\end{equation}\n%\nThe following inequalities hold for all $\\V\\in \\Scc_{\\mu},~\\tf{\\V}=\\tf{\\Wm}$ and all $i\\in[n], t\\neq \\op_i$:\n\\begin{equation}\\label{cone-A-eq}\n\\begin{split}\n%(\\x_{it}-\\x_{i\\tau})^\\top \\V \\z_i&\\geq \\delta>0,\\\\%\\quad\\text{for all}\\quad \\\\\n%(\\x_{i\\op_i}-\\x_{i\\tau})^\\top \\V \\z_i&\\geq 1+\\delta,\\\\\nA\\geq(\\x_{i\\op_i}-\\x_{it})^\\top \\V \\z_i &\\geq \\mu.\n\\end{split}\n\\end{equation}\n%Here, we used $\\tf{\\V-\\Wm}^2/\\tf{\\Wm}^2\\leq 2\\mu$ which implies $\\tf{\\V-\\Wm}\\leq \\sqrt{2\\mu}/\\Theta$.\n %Now that the choice of local cone is determined, we need to prove the main claims. We will lower bound $-\\hb^\\top \\nabla \\Lc(\\W)$ and establish its strict positivity for $\\tn{\\W}\\geq R$. This will show that there is no stationary point as a by product.\n%Given any $\\W\\in \\conb_{\\mu,R}(\\Wm)$, denote $\\Wb=(\\tf{\\Wm}/\\tf{\\W})\\W$ and recall $\\tf{\\V}=\\tf{\\Wm}$. \nTo proceed, we write the gradient correlation following \\eqref{grad def} and \\eqref{eqn:grad:prod:p}\n\\begin{align}\\label{grad def32}\n\\li\\nabla\\Lc(\\W),\\V\\ri&=\\frac{1}{n}\\sum_{i=1}^n\\ell'_i\\cdot\\li\\hb_i,\\sfp{\\hp_i}\\bgam_i\\ri,\n\\end{align}\nwhere we denoted $\\ell'_i=\\ell'(Y_i\\cdot \\vb^\\top \\X_i^\\top\\sft{\\hp_i})$, $\\hb_i=\\X_i\\V \\z_{i}$, $\\hp_i= \\X_i\\W \\z_{i}$, $\\s_i=\\sft{\\hp_i}$. \n\nIt follows from \\eqref{mu choice2} that $A\\geq \\max_{i\\in[n],t\\in[T]}\\tn{\\hb_{it}}$. Using \\eqref{cone-A-eq}, we can bound the softmax probabilities $\\s_i=\\sft{\\hp_i}$ as follows, for all $i\\in[n]$:\n\\begin{align}\\label{soft prob bound2}\n&S_i:= \\sum_{\\tau\\neq \\op_i}\\s_{i\\tau}\\leq T e^{-R\\mu\\Theta}\\s_{i\\op_i}\\leq T e^{-R\\mu\\Theta}.\n\\end{align}\n%(over all $i,t$) \nRecall scores $\\bgam_{it}=Y_i\\cdot\\vb^\\top \\x_{it}$. Define the score gaps:\n\\begin{equation*}\n \\bgg_i=\\bgam_{i\\op_i}-\\max_{t\\neq\\op_i}\\bgam_{it},~~~ \\bgm_i=\\bgam_{i\\op_i}-\\min_{t\\neq\\op_i}\\bgam_{it},~~~\\textnormal{and}~~~\\Gamma=\\sup_{i\\in[n],t,\\tau\\in[T]}|\\bgam_{it}-\\bgam_{i\\tau}|. \n\\end{equation*}\nLet us focus on a fixed datapoint $i\\in[n]$, assume (without losing generality) $\\op_i=1$, and drop subscripts $i$.\n%, that is,~$\\op:=\\op_i$, $\\X:=\\X_i$, $Y:=Y_i$, $\\Kb:=\\Kb_i$, $\\hp=\\Kb\\pb$, $\\hb=\\Kb\\V$, $\\s=\\sft{\\Kb\\pb}$, $\\bgam=Y\\cdot\\X\\vb$, and $\\bgg:=\\bgg_i$.\nDirectly applying Lemma \\ref{lem:q_reduce}, we obtain\n\\[\n  \\big|\\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam-\\sum_{t\\geq 2}^T (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2\\Gamma A(1-\\s_1)^2.\n\\]\n\\noindent To proceed, let us upper/lower bound the gradient correlation. Since $A\\geq \\hb_1-\\hb_t\\geq \\mu>0$ from \\eqref{cone-A-eq}, setting $S:=\\sum_{t\\neq\\op_i}\\s_t=1-\\s_1$, we find\n\\begin{equation}\n A\\cdot S\\cdot \\bgm  \\geq\\sum_{t\\neq\\op} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\geq \\mu\\cdot S\\cdot \\bgg.\\label{aggregate2}\n\\end{equation}\nNext we show that $S=1-\\s_1$ dominates $(1-\\s_1)^2=S^2$ for large $R$. Specifically, we wish for \n\\begin{align}\\label{wishfor2}\n\\mu S \\bgg/2\\geq 2\\Gamma A(1-\\s_1)^2\\iff S\\geq \\frac{4}{\\mu}\\frac{\\Gamma A}{\\bgg}S^2\\iff S\\leq \\frac{\\mu\\bgg}{4\\Gamma A}.\n\\end{align}\nUsing \\eqref{soft prob bound2}, what we wish is ensured for all $i\\in[n]$, by guaranteeing $Te^{-R\\mu\\Theta}\\leq \\frac{\\mu\\bgg}{4\\Gamma A}$. That is, by choosing\n\\begin{align}\\label{R bound2}\nR\\geq \\frac{1}{\\mu\\Theta}\\log\\left(\\frac{4T\\Gamma A}{\\mu\\bggm}\\right),\n\\end{align}\nwhere $\\bggm=\\min_{i\\in[n]}\\bgg_i$ is the global scalar corresponding to the worst case score gap over all inputs. \n\nWith the above choice of $R$, we guaranteed\n\\[\n  2 A(1-\\s_1)\\cdot \\bgm\\geq 2A\\cdot S\\cdot \\bgm \\geq \\sum_{t\\neq\\op} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\geq\\frac{\\mu\\cdot S\\cdot \\bgg}{2}\\geq\\frac{\\mu(1-\\s_1) \\bgg}{2},\n\\]\nvia \\eqref{wishfor2} and \\eqref{aggregate2}. \n\n\nSince this holds over all inputs, going back to the gradient correlation \\eqref{grad def32} and averaging above over all inputs $i\\in[n]$ and plugging back the indices $i$, we obtain the advertised bound % by setting $q_i=1-\\s_{i\\op_i}$ (where we have used $\\op_i=1$ above, without losing generality)\n\\begin{align}\\label{pbb corr2}\n  % \\frac{2A}{n}\\sum_{i\\in [n]} -\\ell'_i\\cdot q_i\\cdot \\bgm_i\\geq -\\li\\nabla\\Lc(\\W),\\V\\ri\\geq \\frac{\\mu}{2n}\\sum_{i\\in [n]} -\\ell'_i\\cdot q_i\\cdot \\bgg_i.\n  \\frac{2A}{n}\\sum_{i\\in [n]} -\\ell'_i\\cdot S_i\\cdot \\bgm_i\\geq -\\li\\nabla\\Lc(\\W),\\V\\ri\\geq \\frac{\\mu}{2n}\\sum_{i\\in [n]} -\\ell'_i\\cdot S_i\\cdot \\bgg_i.\n\\end{align}\nLet $-\\ell'_{\\min/\\max}$ be the min/max values negative loss derivative admits over the ball $[-A,A]$ and note that $\\max_{i\\in[n]}\\bgm_i>0$ and $\\min_{i\\in[n]}\\bgg_i>0$ are dataset dependent constants. Then, we declare the constants $C=-2A\\ell'_{\\max}\\cdot \\max_{i\\in[n]}\\bgm_i>0,c=-(1/2)\\ell'_{\\min}\\cdot \\min_{i\\in[n]}\\bgg_i>0$ to obtain the bound \\eqref{zero1:g:bound}. \n\nThe proof of  \\eqref{zero2:g:bound} and \\eqref{zero3:g:bound} follows similarly as the proof of Lemma \\ref{local cond}. \n\n% For general $\\tf{\\V}=\\tf{\\Ws}$, using \\eqref{wishwish2}, this choice of $R$ similarly guarantees \n% $$\n% \\frac{\\bar{A}}{2\\Theta }(1-\\s_1) \\bgm\\geq \\ab^\\top\\diag{\\s}\\bgam-\\ab^\\top\\s\\s^\\top\\bgam,\n% $$\n% for fixed input. Going back to the gradient correlation \\eqref{grad def3} and averaging above over all inputs $i\\in[n]$, with the same definition of $C>0$, we obtain\n% \\[ \n% \\frac{ \\bar{A} C}{  \\Theta n}\\sum_{i\\in [n]} q_i\\geq -\\li\\nabla\\Lc(\\W),\\V\\ri.\n% \\]\n% To proceed, setting $\\V=\\frac{\\tf{\\Ws}}{\\tf{\\nabla\\Lc(\\W)}}\\cdot \\nabla\\Lc(\\W)$, this implies that\n% \\[ \n% \\li\\nabla\\Lc(\\W),\\V\\ri \\leq \\tf{\\nabla\\Lc(\\W)}\\cdot \\tf{\\Ws}\\leq \\frac{\\bar{A} C}{\\Theta \n%  n}\\sum_{i\\in [n]} q_i.\n% \\]\n% This gives \\eqref{local2:g:bound}. Combining this inequality with \\eqref{pbb corr2}, we obtain that for all $\\V,\\W\\in\\Sc_{\\mu}(\\Ws)$\n% \\[ \n% \\li\\frac{\\V}{\\tf{\\V}},\\frac{\\nabla\\Lc(\\W)}{\\tn{\\nabla\\Lc(\\W)}}\\ri\\geq \\frac{c \\Theta }{C\\bar{A}},\n% \\]\n% which gives \\eqref{local3:g:bound}.\n\n\n\\end{proof}\n\n\n\n\nThe following lemma shows that as $\\pi$ approaches zero, the negative gradient of the loss function at $ \\W\\in \\conb_{\\mu,R}(\\Wm)$  becomes more correlated with the max-margin solution ($\\Ws$) than with $\\W$ itself.\n\n\\begin{lemma}%[Gradient Correlation Condition for Optimal Tokens]\n\\label{lem:glocal:corr} \nSuppose Assumption~\\ref{assum:loss:prope} holds and let $\\op=(\\op_i)_{i=1}^n$ be the unique optimal tokens with $ \\Wm$ denoting the SVM solution. Fix any $\\mu>0$ (per Lemma \\ref{glocal cond}). For any choice of $\\pi>0$, there exists $R:=R_{\\pi} \\geq \\bar{R}_\\mu$ such that, for any $ \\W\\in \\conb_{\\mu,R}(\\Wm)$, we have\n\\[\n \\li \\nabla\\Lc(\\W), \\frac{\\W}{\\tf{\\W}} \\ri\\geq (1+\\pi)\\li \\nabla\\Lc(\\W), \\frac{\\Wm}{\\tf{\\Wm}}\\ri.\n\\]\nHere, $\\conb_{\\mu,R}(\\Wm)$ is the cone defined at  \\eqref{eqn:con:nabla0}.\n\\end{lemma}\n\\begin{proof}\n %Set notations $\\hb_i=\\Kb_i\\Wb$, $\\hbm_i=\\Kb_i\\Ws$, and $\\s_i=\\sft{\\Kb_i\\pb}$. \nLet  $\\Wb= \\tf{\\Wm} \\W/\\tf{\\W}$, $\\hb_i=\\X_i\\Wb \\z_{i}$, $\\hbm_i= \\X_i\\Ws \\z_{i}$, and $\\s_i=\\sft{\\X_i\\W \\z_{i}}$. To establish the result, we will prove that, for sufficiently large $R$ and for any $\\W\\in \\conb_{\\mu,R}(\\Wm)$:\n\\begin{align}\\label{main local cond2}\n\\nonumber \n\\li -\\nabla\\Lc(\\W),\\frac{\\W}{\\tf{\\W}}\\ri&= -\\frac{1}{n}\\sum_{i=1}^n\\ell'_i \\cdot  \\li \\hb_i, \\sfp{\\X_i\\W \\z_{i}}\\bgam_i\\ri\\\\\n&\\leq - \\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot  \\li\\hbm_i, \\sfp{\\X_i\\W \\z_{i}}\\bgam_i\\ri=(1+\\pi)\\li-\\nabla\\Lc(\\W), \\frac{\\ps}{\\tf{\\Ws}}\\ri.\n\\end{align}\nDirectly applying Lemma \\ref{lem:q_reduce}, for all $\\V\\in \\Scc_\\mu$ with $\\tf{\\V}=\\tf{\\Wm}$ and $\\hp_i=\\X_i\\V \\z_i$, we have found\n\\begin{align}\n  \\big|\\hp^\\top_i\\diag{\\s_i}\\bgam_i-\\hp^\\top_i\\s_i\\s^\\top_i\\bgam_i-\\sum_{t\\neq\\op_i} (\\hp_{i1}-\\hp_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\big|\\leq 2\\Gamma A(1-\\s_{i1})^2.\n\\end{align}\nRecalling $\\hbm_{i1}-\\hbm_{it}\\geq 1$, we note that $\\sum_{t\\neq\\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq \\sum_{t\\neq\\op_i} (\\hbm_{i1}-\\hbm_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})$. Now plugging in $\\hb,\\hbm$ in the bound above and assuming $\\pi\\leq 1$ (w.l.o.g.), \\eqref{main local cond2} is implied by the following stronger inequality\n\\begin{align*}\n-\\frac{1}{n}&\\sum_{i=1}^n\\ell'_i \\cdot \\left(6\\Gamma A(1-\\s_{i1})^2+ \\sum_{t\\neq \\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it}) \\right)\\\\\n&\\leq -\\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i  \\cdot \\sum_{t\\neq \\op_i} (\\hbm_{i1}-\\hbm_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n&\\leq-\\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\neq \\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align*}\nFirst, we claim that $0.5\\pi\\sum_{t\\in \\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\geq 6\\Gamma A(1-\\s_{i1})^2$ for all $i \\in [n]$.  \nThe proof of this claim directly follows the argument in Lemma~\\ref{glocal cond}, %\\ct{minor: reference to Lemma 12 that comes after} argument, \n(namely following \\eqref{soft prob bound2}, \\eqref{wishfor2}, \\eqref{R bound2}) \nwe have that $1-\\s_{i1}\\leq Te^{-R\\mu\\Theta}$ and $\\bgam_{i1}-\\bgam_{it}\\geq \\bggm$ for all $i \\in [n]$. This leads to the choice (for $D_0\\geq 12$)\n\\begin{align}\n  R\\geq R_\\pi =\\frac{1}{\\mu\\Theta}\\log\\left(\\frac{D_{0}\\cdot T\\Gamma A}{\\pi\\bggm}\\right).\\label{Rpi choice2}\n\\end{align}\nWe shall choose $D_0$ sufficiently large such that $R_{\\pi}\\geq \\bar{R}_{\\mu}$, where $\\bar{R}_{\\mu}$ is defined in Lemma \\ref{glocal cond}.\n\nFollowing this control over the perturbation term $6\\Gamma A(1-\\s_{i1})^2$, to conclude with the result, what remains is proving the comparison\n\\begin{align}\\label{desired comp2}\n-\\frac{1}{n} \\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\neq\\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq - \\frac{1+0.5\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\neq\\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align}\n\\noindent\\textbf{Scenario 1:} $\\tf{\\Wb-\\Wm}\\leq \\eps=\\frac{\\pi}{4A\\Theta}$ for some $\\eps>0$.  In this scenario, for any $ t\\neq \\op_i$ and $i\\in [n ]$, we have\n\\[\n|\\hb_{it}-\\hbm_{it}|=|\\x_{it}^\\top (\\Wb-\\Wm)  \\z_{i}|\\leq A\\Theta\\eps=\\frac{\\pi}{4}.\n\\]\nConsequently, we obtain \n\\[\n\\hb_{i1}-\\hb_{it}\\leq \\hbm_{i1}-\\hbm_{it}+2A\\Theta\\eps= 1+0.5\\pi.\n\\] \nSimilarly, $\\hb_{i1}-\\hb_{it}\\geq 1-0.5\\pi\\geq 0.5$. Since all terms $\\hb_{i1}-\\hb_{it},\\s_{it},\\bgam_{i1}-\\bgam_{it}$ in \\eqref{desired comp2} are nonnegative, we  obtain \\eqref{desired comp2}. \n\n% \\noindent\\textbf{Scenario 2:}  $\\tf{\\Wb-\\Wm}\\geq \\eps=\\frac{\\pi}{4A\\Theta}$.  Since $\\Wb$ is not (locally) max-margin, in this scenario, for some $i \\in  [n]$, $\\nu=\\nu(\\eps)>0$, and $\\tau\\neq\\op_i$, we have that\n% \\begin{align*}\n% \\hb_{i1}-\\hb_{i\\tau}\\leq 1-2\\nu.\n% \\end{align*}\n% Here $\\tau=\\arg\\max_{\\tau\\neq\\op_i} \\x_{i\\tau}\\Wb \\z_i$ denotes the nearest point to $\\hb_{i1}$ (along the $\\Wb$ direction). Note that a non-neighbor $t\\in\\Tcb_i$ cannot be nearest because $\\Wb\\in \\cone_{\\mu}(\\ps)$ and \\eqref{cone-non-nei} holds. Recall that $\\s_i=\\sft{\\RR\\hb_i}$ where $\\RR=\\tf{\\W}\\Theta \\geq R\\Theta$. To proceed, let $ \\underline{\\hb}_i:=\\min_{t \\in\\mc{T}_i}\\hb_{i1}-\\hb_{it}$,\n% \\begin{align*}\n% \\mc{I}:=\\left\\{ i\\in[n]: \\underline{\\hb}_i \\leq 1-2\\nu \\right\\}, \\qquad [n]-\\mc{I}:=\\left\\{ i\\in[n]:  1-2\\nu  <  \\underline{\\hb}_i\\right\\}.\n% %\\\\\n% %\\mc{I}_3&=\\left\\{ i\\in[n]:  1 +\\frac{\\pi}{8} < \\underline{\\hb}_i\\right\\},\n% \\end{align*}\n% }\n% % Let  $\\Theta=1/\\tf{\\Ws}$ and \n% % \\begin{equation}\\label{a choice}\n% % \\begin{split}\n% % %&\\Theta=1/\\tf{\\Wm},\\\\\n% % &A=\\max_{i\\in[n],t,\\tau\\in[T]} \\frac{(\\tn{\\x_{it}}\\vee\\tn{\\x_{it}-\\x_{i\\tau}})\\cdot\\tn{\\z_i}}{\\Theta}.\n% % \\end{split}\n% % \\end{equation}\n% To establish the result, we will prove that, for sufficiently large $R=R_\\pi$, for any $\\W\\in \\conb_{\\mu,R}(\\Wm)$ and any $i\\in[n]$, \n% \\begin{align}\\label{main local cond2}\n% \\li\\hb_i,\\sfp{\\X_i\\W\\z_{i}}\\bgam_i\\ri\\leq (1+\\pi)\\li\\hbm_i,\\sfp{\\X_i\\W \\z_{i}}\\bgam_i\\ri.\n% \\end{align}\n% %\\ct{minor: Quantities like $A,\\Theta$ are defined later in Lemma 12}\n% Once \\eqref{main local cond2} holds for all $i$, the same conclusion will hold for the gradient correlations via \\eqref{grad def32}. Moving forward, we shall again focus on a single point $i\\in[n]$ and drop all subscripts $i$, i.e. we will use vectors $\\hb,\\hbm,\\s$. Also assume $\\op=\\op_i=1$ without losing generality as above. Following \\eqref{aggregate2}, for all $\\V\\in \\Scc_\\mu$ with $\\tf{\\V}=\\tf{\\Wm}$ and $\\hp=\\X\\V \\z$, we have found\n% \\begin{align}\n%   \\big|\\hp^\\top\\diag{\\s}\\bgam-\\hp^\\top\\s\\s^\\top\\bgam-\\sum_{t\\neq\\op} (\\hp_1-\\hp_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2\\Gamma A(1-\\s_1)^2. %\\label{aggregate2}\n% \\end{align}\n% Recalling $\\hbm_1-\\hbm_t\\geq 1$, we note that $\\sum_{t\\neq\\op}\\s_t(\\bgam_1-\\bgam_t)\\leq \\sum_{t\\neq\\op} (\\hbm_1-\\hbm_t)\\s_t(\\bgam_1-\\bgam_t)$. Now plugging in $\\hb,\\hbm$ in the bound above and assuming $\\pi\\leq 1$ (w.l.o.g.), \\eqref{main local cond2} is implied by the following stronger inequality\n% \\begin{align*}\n% 6\\Gamma A(1-\\s_1)^2+ \\sum_{t\\neq\\op} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t) &\\leq (1+\\pi)\\sum_{t\\neq\\op}\\s_t(\\bgam_1-\\bgam_t)\\\\\n% &\\leq (1+\\pi) \\sum_{t\\neq\\op} (\\hbm_1-\\hbm_t)\\s_t(\\bgam_1-\\bgam_t).\n% \\end{align*}\n% First, we claim that $0.5\\pi\\sum_{t\\neq\\op}\\s_t(\\bgam_1-\\bgam_t)\\geq 6\\Gamma A(1-\\s_1)^2$. The proof of this claim directly follows the argument in Lemma~\\ref{glocal cond}, %\\ct{minor: reference to Lemma 12 that comes after} argument, \n% (namely following \\eqref{wishfor2}, \\eqref{R bound2}, \\eqref{soft prob bound2}) \n% we have that $1-\\s_1\\leq Te^{-R\\mu\\Theta}$ and $\\bgam_1-\\bgam_t\\geq \\bggm$. This leads to the choice (for $D_0\\geq 12$)\n% \\begin{align}\n%   R\\geq R_\\pi =\\frac{1}{\\mu\\Theta}\\log\\left(\\frac{D_{0}\\cdot T\\Gamma A}{\\pi\\bggm}\\right).\\label{Rpi choice2}\n% \\end{align}\n% We shall choose $D_0$ sufficiently large such that $R_{\\pi}\\geq \\bar{R}_{\\mu}$, where $\\bar{R}_{\\mu}$ is defined in Lemma \\ref{glocal cond}.\n\n% Following this control over the perturbation term $6\\Gamma A(1-\\s_1)^2$, to conclude with the result, what remains is establishing the comparison\n% \\begin{align}\\label{desired comp2}\n% \\sum_{t\\neq\\op} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\leq (1+0.5\\pi) \\sum_{t\\neq\\op}\\s_t(\\bgam_1-\\bgam_t).\n% \\end{align}\n% To proceed, we split the problem into two scenarios. \n% \\noindent\\textbf{Scenario 1:} $\\tf{\\Wb-\\Wm}\\leq \\eps=\\frac{\\pi}{4A\\Theta}$ for some $\\eps>0$. In this scenario, for any token, we find that\n% \\[\n% |\\hb_t-\\hbm_t|\\leq A\\Theta\\eps=\\pi/4.\n% \\]\n% Consequently, we obtain \n% \\[\n% \\hb_1-\\hb_t\\leq \\hbm_1-\\hbm_t+2A\\Theta\\eps= 1+0.5\\pi.\n% \\] \n% %\\ct{Equality above uses $\\hb_1-\\hb_t=1$? why is this the case?}\n% Since $\\bgam_1-\\bgam_t\\geq 0$, for all $t\\neq 1$, we find $(\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\leq (1+0.5\\pi)\\s_t(\\bgam_1-\\bgam_t)$. This implies the desired result \\eqref{desired comp2}.\\smallskip\n% \\noindent\\textbf{Scenario 2:} \n% {\\color{blue}\n% $\\tf{\\Wb-\\Wm}\\geq \\eps=\\frac{\\pi}{4A\\Theta}$. Since $\\Wb$ is not the max-margin solution, in this scenario, for some $\\nu=\\nu(\\eps)>0$ and $\\tau\\neq 1$, we have that $\\hb_1-\\hb_\\tau\\leq 1-2\\nu$. Here $\\tau=\\arg\\max_{\\tau\\neq 1} \\x_\\tau\\Wb \\z$ denotes the nearest point to $\\hb_1$ (along the $\\Wb$ direction). Recall that $\\s=\\sft{  C^\\svm\\hb}$, where  $C^\\svm=\\tf{\\W}/\\tf{\\Wm}$.  To proceed, split the tokens into two groups: Let $\\Nc$ be the group of tokens obeying $(\\x_1- \\x_\\tau) \\W \\z \\leq 1-\\nu$ and $[T]-\\{1\\}-\\Nc$ be the rest of the non-optimal tokens. \n\n\\noindent\\textbf{Scenario 2:} $\\tf{\\Wb-\\Wm}\\geq \\eps=\\frac{\\pi}{4A\\Theta}$.  Since $\\Wb$ is not max-margin solution, in this scenario, for some $i \\in  [n]$, $\\nu=\\nu(\\eps)>0$, and $\\tau\\neq\\op_i$, we have that\n\\begin{align*}\n\\hb_{i1}-\\hb_{i\\tau}\\leq 1-2\\nu.\n\\end{align*}\nHere $\\tau=\\arg\\max_{\\tau\\neq\\op_i} \\x_{i\\tau}\\Wb \\z_i$ denotes the nearest point to $\\hb_{i1}$ (along the $\\Wb$ direction). Recall that $\\s=\\sft{  \\RR\\hb}$, where  $\\RR=R\\Theta=\\tf{\\W}/\\tf{\\Wm}$. To proceed, let $ \\underline{\\hb}_i:=\\min_{t \\neq\\op_i}\\hb_{i1}-\\hb_{it}$,\n\\begin{align*}\n\\mc{I}:=\\left\\{ i\\in[n]: \\underline{\\hb}_i \\leq 1-2\\nu \\right\\}, \\qquad [n]-\\mc{I}:=\\left\\{ i\\in[n]:  1-2\\nu  <  \\underline{\\hb}_i\\right\\}.\n%\\\\\n%\\mc{I}_3&=\\left\\{ i\\in[n]:  1 +\\frac{\\pi}{8} < \\underline{\\hb}_i\\right\\},\n\\end{align*}\nFor all $ i \\in [n]-\\mc{I}$,\n\\begin{equation}\\label{eqn:grad:difff0}\n\\begin{split}\n      \\sum_{t\\neq \\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it}) &- (1+0.5\\pi) \\sum_{t\\neq \\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n      & \\leq  \\left(2A - (1+0.5\\pi)\\right)\\Gamma\\sum_{t\\neq \\op_i,~\\hb_{i1}-\\hb_{it} \\geq 1+\\frac{\\pi}{2} } \\s_{it} \\\\\n      & \\leq  \\left(2A - (1+0.5\\pi)\\right)\\Gamma Te^{-\\RR(1+\\frac{\\pi}{2})} \\\\\n      &\\leq   2A\\Gamma  T e^{-\\RR(1+\\frac{\\pi}{2})}.\n      % & \\leq  \\left(2A\\Gamma - (1+0.5\\pi)\\bggm \\right)\\sum_{t\\in \\op_i,~\\hb_{i1}-\\hb_{it} \\geq 1+\\frac{\\pi}{2} } \\s_{it} \\\\\n      % &\\leq   \\left(2A\\Gamma - (1+0.5\\pi) \\bggm\\right)  T \\Gamma e^{-\\RR(1+\\frac{\\pi}{2})}\\\\\n      % &\\leq   2A\\Gamma^2  T e^{-\\RR(1+\\frac{\\pi}{2})}.    \n\\end{split}\n\\end{equation}\n% For all $ i \\in \\mc{I}_2$,  split the tokens into two groups: Let $\\Nc_i$ be the group of tokens obeying $\\Wb^\\top (\\kb_{i1}-\\kb_{it})\\leq 1 +\\frac{\\pi}{4}$ and $\\Tc_i-\\Nc_i$ be the rest of the tokens.  Observe that\n% \\[\n% \\frac{\\sum_{t\\in \\Tc_i-\\Nc}\\s_{it}}{\\sum_{t\\neq\\op_i}\\s_{it}}\\leq  T\\frac{e^{\\frac{-\\pi}{4} \\RR}}{e^{\\frac{-\\pi}{8}\\RR}}=Te^{-\\RR\\frac{\\pi}{8}}.\n% \\]\n% Thus, using $|\\hb_{i1}-\\hb_{it}|\\leq 2A$ and recalling the definition of $\\bgg$, observe that \n% \\[\n% \\sum_{t\\neq\\op_i-\\Nc} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq \\frac{2\\Gamma A Te^{-\\RR\\pi/8}}{\\bgg} \\sum_{t\\in \\Tc_i} \\s_{it}(\\bgam_{i1}-\\bgam_{it}).%\\leq \\frac{3A\\Gamma Te^{-\\RR\\nu}}{\\bgg} \\sum_{t\\in \\Nc} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n% \\]\n\n% \\begin{align*}\n%   \\sum_{t\\in \\Tc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})&= \\sum_{t\\in \\Nc} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})+\\sum_{t\\neq\\op_i-\\Nc} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\nonumber\\\\\n% %  &\\leq \\sum_{t\\in \\Nc} (1-\\nu)\\s_{it}(\\bgam_{i1}-\\bgam_{it})+\\sum_{t\\neq\\op_i-\\Nc} 2A\\Gamma Te^{-\\RR\\nu}\\nonumber\\\\\n%   &\\leq \\sum_{t\\in \\Nc} (1+\\pi/4)\\s_{it}(\\bgam_{i1}-\\bgam_{it})+\\frac{2\\Gamma A Te^{-\\RR\\pi/8}}{\\bgg} \\sum_{t\\in \\Tc_i} \\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n%   &\\leq \\left(1+\\frac{\\pi}{4}+\\frac{2\\Gamma A Te^{-\\RR\\pi/8}}{\\bgg}\\right)\\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\n% \\end{align*}\n\nFor all $ i \\in \\mc{I}$, split the tokens into two groups: Let $\\Nc_i$ be the group of tokens obeying $ \\hb_{i1}-\\hb_{it} \\leq 1-\\nu$ and $\\bar\\Nc_i:=[T]-\\{\\op_i\\}-\\Nc_i$ be the rest of the neighbors. Observe that\n\\[\n\\frac{\\sum_{t\\in\\bar{\\Nc}_i}\\s_{it}}{\\sum_{t\\neq\\op_i}\\s_{it}}\\leq  T\\frac{e^{\\nu \\RR}}{e^{2\\nu\\RR}}=Te^{-\\RR\\nu}.\n\\]\nUsing $|\\hb_{i1}-\\hb_{it}|\\leq 2A$ and  $\\bggm=\\min_{i\\in[n]}\\bgg_i =\\min_{i\\in[n]} (\\bgam_{i1}-\\max_{t\\neq\\op_i}\\bgam_{it})$, observe that \n\\[\n\\sum_{t\\in\\bar\\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq \\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm} \\sum_{t\\neq\\opt_i} \\s_{it}(\\bgam_{i1}-\\bgam_{it}).%\\leq \\frac{3A\\Gamma Te^{-\\RR\\nu}}{\\bgg} \\sum_{t\\in \\Nc} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\]\nThus, \n\\begin{align*}\n  \\sum_{t\\neq \\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})&= \\sum_{t\\in \\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})+\\sum_{t\\in\\bar\\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\nonumber\\\\\n%  &\\leq \\sum_{t\\in \\Nc} (1-\\nu)\\s_{it}(\\bgam_{i1}-\\bgam_{it})+\\sum_{t\\neq\\op_i-\\Nc} 2A\\Gamma Te^{-\\RR\\nu}\\nonumber\\\\\n  &\\leq \\sum_{t\\in \\Nc_i} (1-\\nu)\\s_{it}(\\bgam_{i1}-\\bgam_{it})+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm} \\sum_{t\\neq \\op_i} \\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n  &\\leq \\left(1-\\nu+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}\\right)\\sum_{t\\neq \\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n &\\leq \\left(1+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}\\right)\\sum_{t\\neq \\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align*}\nHence, choosing \n\\begin{align}\nR\\geq\\frac{1}{\\nu\\Theta}\\log\\left(\\frac{8\\Gamma AT}{\\bggm\\pi}\\right)\\label{R bound pi}\n\\end{align}\nresults in that\n\\begin{equation}\\label{eqn:grad:difff1}\n    \\begin{split}\n     &\\sum_{t\\neq\\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})  - \\left(1+\\frac{\\pi}{2}\\right) \\sum_{t\\neq\\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}) \\\\\n   &\\leq\\left(\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}-\\frac{\\pi}{2}\\right)\\sum_{t\\neq\\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n   &\\leq -\\frac{\\pi}{4}\\sum_{t\\neq\\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n   &\\leq-\\frac{\\pi}{4T}\\bggm  e^{-\\bar{R} (1-2\\nu)}.      \n    \\end{split}\n\\end{equation}\nHere, the last inequality follows from the fact that $\\sum_{t\\neq\\op_i}\\s_{it}\\geq \\max_{t\\neq\\op_i}\\s_{it}\\geq\\frac{e^{-\\bar{R}(1-2\\nu)}}{\\sum_{t=1}^Te^{-\\bar{R}(\\hb_{i1}-\\hb_{it})}}\\geq e^{-\\bar{R}(1-2\\nu)}/T$.\n\n%Consequently, the proof boils down to ensuring the perturbation term $\\frac{2\\Gamma A Te^{-R\\Theta\\nu}}{\\bgg}\\leq 0.5\\pi$. \nFrom Assumption~\\ref{assum:loss:prope}, we have $c_{\\min}\\leq-\\ell'\\leq c_{\\max}$ for some positive constants $c_{\\min}$ and $c_{\\max}$. It follows from  \\eqref{eqn:grad:difff0} and \\eqref{eqn:grad:difff1} that \n\\begin{align*}\n-\\frac{1}{n}\\sum_{i}^n \\ell_i' \\cdot&\\left(\n      \\sum_{t\\neq\\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})- \\sum_{t\\neq\\op_i} (1+0.5\\pi)\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\right)\\\\\n      & \\leq    c_{\\max}2A\\Gamma  T \\Gamma e^{-\\RR(1 +\\frac{\\pi}{2})}-\\frac{c_{\\min}}{nT}\\cdot\\frac{\\pi\\bggm}{4}e^{-\\bar{R} (1-2\\nu)}\\\\\n      % & \\leq    c_{\\max}\\left(2A\\Gamma - (1+0.5\\pi) \\bggm\\right)  T \\Gamma e^{-\\RR(1 +\\frac{\\pi}{2})}-\\frac{c_{\\min}}{n}\\left(\\nu+0.5\\pi-\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}\\right)\\\\\n      & \\leq 0.\n\\end{align*}\nCombing with \\eqref{R bound pi}, this is guaranteed by %for all inputs $i\\in[n]$ by recalling $\\bggm=\\min_{i\\in[n]}\\bgg_i$ and \nchoosing \n% \\[\n%   R\\geq ... \\frac{1}{\\nu\\Theta}\\log\\left(\\frac{4\\Gamma AT}{\\bggm\\pi}\\right),\n% \\]\n\\[\n  R\\geq \\max\\left\\{\\frac{1}{\\nu\\Theta}\\log\\left(\\frac{8\\Gamma AT}{\\bggm\\pi}\\right),\\frac{1}{(2\\nu+\\pi/2)\\Theta}\\log\\left(\\frac{8n\\Gamma AT^2 c_{\\max}}{c_{\\min}\\bggm\\pi}\\right)\\right\\},\n\\]\nwhere $\\nu=\\nu(\\frac{\\pi}{4A\\Theta})$ depends only on $\\pi$ and global problem variables. \n\nCombining this with the prior $R$ choice \\eqref{Rpi choice2} (by taking maximum), we conclude with the statement.\n\\end{proof}\n\n% The proof is similar to \\cite[Theorem 2]{tarzanagh2023margin}.  Given any $\\epsilon\\in(0,1)$, let $\\pi=\\epsilon/(1-\\epsilon)$. It follows from  Theorem~\\ref{diverg:norm:w} that $\\lim_{k\\to\\infty}\\tf{\\W(k)}=\\infty$. Hence,  we can choose $k_\\eps$ such that for any $k\\ge k_\\eps$, it holds that $\\tf{\\W(k)}>  R_\\epsilon \\vee 1/2 $ for some parameter $R_\\epsilon$. \n\n\n\n\n\n\\subsubsection{Proof of Theorem~\\ref{conv:gd:w:global:nabla0}}\\label{app:glob:nab0}\nThe following theorem is a restatement of Theorem \\ref{conv:gd:w:global:nabla0}. Two minor differences are: (1) We state with the change of variable $\\mu\\gets\\tf{\\Wm}\\cdot \\mu$; see discussions below \\eqref{eqn:con:nabla0}. (2) We also include \\eqref{lem:zglob:l3} in the statement of the theorem.\n\\begin{theorem}\\label{conv:gd:w:global:nabla0:app}\nSuppose Assumption~\\ref{assum:loss:prope} on the loss function $\\ell$ and Assumption \\ref{assum:nabla0} on the initial gradient hold.  %Then, for any $\\mu \\in  (0,\\min (0.5,\\iota \\tf{\\Ws}/\\tf{\\nabla \\Lc(0)})$, there exists $R>0$ such that the following holds. \n\\begin{enumerate}[label={\\textnormal{\\textbf{L\\arabic*.}}}, wide, labelwidth=!,itemindent=!, labelindent=5pt]\n  \\item   \\label{lem:zglob:l1}For any $\\mu>0$, there exists  $R>0$ such  that   $\\conb_{\\mu,R}(\\Ws)$ defined in \\eqref{eqn:con:nabla0} does not contain any  stationary points. \n\\item \\label{lem:zglob:l2} Fix any  $\\mu \\in  (0,\\min (1,\\iota \\tf{\\Ws}/\\tf{\\nabla \\Lc(0)})$. Consider GD iterations with $\\W(0)=0$, $\\W(1)=-R\\nabla\\Lc(0)/\\tf{\\nabla\\Lc(0)}$, and $\\W(k+1)=\\W(k)-\\eta\\nabla\\Lc(\\W(k))$ for $\\eta\\le 1/L_{\\W}$, $k\\ge 1$, and $R$ sufficiently large. If all iterates remain within $\\conb_{\\mu,R}$, then $\\lim_{k\\rightarrow\\infty} \\tf{\\W(k)}=\\infty$ and $\\lim_{k\\rightarrow\\infty}\\frac{\\W(k)}{\\tf{\\W(k)}}=\\frac{\\Wm}{\\tf{\\Wm}}$.\n\\item  \\label{lem:zglob:l3} Assume $\\eta\\leq 1/L_{\\W}$ and for all $\\W \\in \\conb_{\\mu,R}(\\Ws)$ with sufficiently large $R$?\n\\begin{align}\\label{assum:extra}\n   \\min_{i \\in [n]}\\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\W-\\eta\\nabla\\Lc(\\W) \\ri \\geq     \\min_{i \\in [n]}\\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\W\\ri - \\frac{2\\eta\\mu}{\\tf{\\Wm}^2}\\iprod{\\nabla\\mc{L}(\\W)}{\\Wm},\n\\end{align}\nthen all GD iterations remain within $\\conb_{\\mu,R}(\\Wm)$.\n\\end{enumerate}\n\\end{theorem}\n\n\n\\begin{proof}\nNote that \\ref{lem:zglob:l1} is a direct corollary of Lemma~\\ref{glocal cond}. We proceed with the proof of \\ref{lem:zglob:l2} and \\ref{lem:zglob:l3}.  \n%Without loss of generality assume $\\Theta=1/\\tf{\\Wm} \\leq 1$.  \nWe provide the proof in four steps:\\\\\n\\textbf{Step~1: $\\conb_{\\mu,R^0_\\mu}(\\Wm)$ construction}.  Let us denote the initialization lower bound as $R^0_\\mu:=R$, where $R$ is given in the Theorem~\\ref{conv:gd:w:global:nabla0:app}'s statement. Consider an arbitrary value of  $\\epsilon \\in (0, \\mu/2)$ and let $1/(1+\\pi)=1-\\epsilon$. We additionally denote $R_\\eps\\gets R_\\pi\\vee 1/2$ where $R_\\pi$ was defined in Lemma~\\ref{lem:glocal:corr}. At initialization $\\W(0)$, we set $\\eps=\\mu/2$ to obtain $R^0_\\mu= R_{\\mu/2}$. \n\nWe proceed to show  $\\mu \\in  (0,\\min (1,\\iota \\tf{\\Ws}/\\tf{\\nabla \\Lc(0)})$.  It follows from  Assumption~\\ref{assum:nabla0}  and under zero initialization for GD ($\\W(0)=0$) that\n$$\n\\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top,   - \\nabla \\mc{L}(\\W(0)) \\right\\rangle =\\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top,   - \\nabla \\mc{L}(0) \\right\\rangle\\ge \\iota  > 0, \n$$\nfor some positive constant $\\iota $. Hence, for any initial step size $\\eta (0)>0$ and $\\W(1)=-\\eta(0) \\nabla\\Lc(0)$, \n\\begin{equation}\\label{eqn:decpath:zinit}\n\\begin{split}\n    \\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\frac{\\W (1)}{\\tf{\\W(1)}}\\ri &=  \\frac{\\eta (0) }{\\tf{\\W(1)}}  \\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top,   - \\nabla \\mc{L}(0) \\right\\rangle \\\\\n     & \\geq  \\frac{\\iota  \\eta (0) }{\\tf{\\W(1)}}=  \\frac{\\iota }{ \\tf{\\nabla \\mc{L}(0)}}\\\\\n     &\\geq  \\frac{\\mu}{\\tf{\\Wm}}.\n\\end{split}\n\\end{equation}\nHere, the last inequality follows from our choice of $\\mu$ in the theorem statement, i.e.\n\\begin{align}\\label{eqn:mu:zero}\n \\mu \\in \\left(0, \\min\\left(1, \\frac{\\iota \\tf{\\Wm}}{\\tf{\\nabla \\mc{L}(0)}}\\right)\\right).\n\\end{align}\n%\\redp{Add a explanation on $c$ vs $\\nabla\\Lc(0)$ relation.}\nThis $\\mu$ choice induces the conic set $\\conb_{\\mu,R^0_\\mu}(\\Wm)$ with  $R^0_\\mu= R_{\\mu/2}$, where $R_{\\mu/2}$ was defined in Lemma~\\ref{lem:glocal:corr}. %We note that the result of Lemma~\\ref{glocal cond} holds for any $\\mu >0$. Hence, \nNow, given the parameter  $ \\mu $ satisfying \\eqref{eqn:mu:zero}, we can choose $\\eta (0)$ such that $\\tf{\\W (1)} \\geq R^0_\\mu$ and $\\W(1)\\in\\conb_{\\mu, R^0_\\mu}(\\Wm)$. To achieve this, since $\\W(0)=0$, we obtain  %for any $R^0_\\mu \\geq \\RR_{\\mu} \\vee 1/2$, where $\\RR_{\\mu}$ is defined in Lemma~\\ref{glocal cond}.\n\\begin{equation}\\label{eqn:stepeta0}\n    \\eta (0) =\\frac{R^0_\\mu}{\\tf{\\nabla \\mc{L}(0) }}. % \\Rightarrow  \\W(1)=-\\frac{R^0_\\mu\\nabla\\Lc(0)}{\\tf{\\nabla\\Lc(0)}}.\n    %\\geq \\frac{1}{2\\tf{\\nabla \\mc{L}(0) }}.\n\\end{equation}\nSince by our definition, $R^0_\\mu \\leftarrow R$,  \\eqref{eqn:stepeta0} gives $\\W(1)$ in the theorem's statement.\n\n%Hence, for this choice of stepsize $ \\eta (0)$, $ \\W (1) \\in \\conb_{\\mu,R_{\\mu}}$ defined in \\eqref{eqn:con:nabla0}.\n% \\textbf{Step~1: There are no stationary points within $\\Cc_{\\mu,R^0_\\mu}(\\Ws)$.} We begin by proving that there are no stationary points within  $\\Cc_{\\mu,R^0_\\mu}(\\Ws)$. Let $(\\Tc_i)_{i=1}^n$ denote  the sets of \\neis as defined in Definition \\ref{def loc opt}. We define $\\Tcb_i=[T]-\\Tc_i-\\{\\alpha_i\\}$ as the tokens that are non-\\neis. Additionally, let $\\mu$ be defined as in \\eqref{mu choice}. Then, since $R^0_\\mu\\geq \\RR_\\mu$ per Lemma \\ref{lem:local:corr}, we can apply Lemma~\\ref{local cond} to find that: For all $\\V,\\W\\in  \\Sc_{\\mu}(\\Ws)$ with $\\tf{\\W} \\neq 0$ and $\\tf{\\W} \\geq R^0_\\mu$,  we have that $-\\V^\\top \\nabla \\Lc(\\W)$ is strictly positive.\n\\noindent \\textbf{Step~2: There are no stationary points within $\\conb_{\\mu,R_\\mu^0}(\\Wm)$.} \nThis step follows from \\ref{lem:zglob:l1}. Specifically, \n%We begin by proving that there are no stationary points within  $\\conb_{\\mu,R^0_\\mu}(\\Ws)$. \n% Let $(\\Tc_i)_{i=1}^n$ denote  the sets of \\neis as defined in Definition \\ref{def loc opt}. We define $\\Tcb_i=[T]-\\Tc_i-\\{\\alpha_i\\}$ as the tokens that are non-\\neis. \n%Then,  \nwe can apply Lemma~\\ref{glocal cond} to find that: For all $\\V,\\W\\in  \\bar{\\Sc}_{\\mu}(\\Ws)$ with $\\tf{\\W} \\neq 0$ and $\\tf{\\W} \\geq R^0_\\mu$,  we have that $-\\li\\V, \\nabla \\Lc(\\W)\\ri$ is strictly positive.\n%This step directly follows from Lemma~\\ref{glocal cond}: for all $\\V,\\W\\in \\Scc_{\\mu}(\\Wm)$ with $\\tf{\\V}=\\tf{\\Wm}$ and $\\tf{\\W}\\geq R_\\mu$, it is guaranteed that there exists a value of $R_\\mu$ such that $\\iprod{\\V}{ -\\nabla \\Lc(\\W)}$ is strictly positive.\n\\\\\n\\emph{Gradient correlation holds for large parameter norm.}  %Consider an arbitrary value of  $\\epsilon \\in (0, \\mu/2)$. Let $1/(1+\\pi)=1-\\epsilon$. It follows from Lemma~\\ref{lem:glocal:corr} that, there exists $R_\\epsilon$ such that all  $ \\W \\in \\conb_{\\mu,R_\\epsilon}(\\Wm)$ satisfy\n% \\begin{align}\\label{eqn:neg:corr:local:nabla0}\n% \\iprod{-\\nabla\\mc{L}(\\W)}      {\\frac{\\Wm}{\\tf{\\Wm}}} \\geq (1-\\epsilon)    \\iprod{-\\nabla \\mc{L}(\\W)}{\\frac{\\W}{\\tf{\\W}}}.\n% \\end{align}\nIt follows from  Lemma~\\ref{lem:glocal:corr} that, there exists $ R_\\epsilon\\geq \\bar{R}_\\mu\\vee 1/2$ such that all  $ \\W \\in \\conb_{\\mu,R_\\epsilon}(\\Wm)$ satisfy%\\vspace{-2pt}\n\\begin{align}\\label{eqn:neg:corr:0}\n\\iprod{-\\nabla\\mc{L}(\\W)}      {\\frac{\\Wm}{\\tf{\\Wm}}} \\geq (1-\\epsilon)    \\iprod{-\\nabla \\mc{L}(\\W)}{\\frac{\\W}{\\tf{\\W}}}.\n\\end{align}\nThe following argument applies to a general $\\eps\\in(0,\\mu/2)$. However, at initialization $\\W(0)=0$, we have set $\\eps=\\mu/2$ and defined the initialization radius as $R^0_\\mu= R_{\\mu/2}$. To proceed, we will prove the main statements \\eqref{lem:zglob:l2} and \\eqref{lem:zglob:l3} as follows.\n\\begin{itemize}\n\\item Proving \\ref{lem:zglob:l3}: In \\textbf{Step 3}, we will assume Condition \\eqref{assum:extra} to prove that gradient iterates remain within $\\conb_{\\mu,R_\\eps}(\\Ws)$. Concretely, for any $\\epsilon \\in (0, \\mu/2)$, we will show that after gradient descent enters the conic set $\\conb_{\\mu,R_\\eps}(\\Ws)$ for the first time, it will never leave the set under Condition \\eqref{assum:extra} of the theorem statement and \\eqref{eqn:neg:corr:0}. In what follows, let us denote $k_\\eps$ to be the first time gradient descent enters $\\conb_{\\mu,R_\\eps}(\\Ws)$. Note that for $\\eps\\gets\\mu/2$, $k_\\eps=0$ i.e.~the point of initialization.\n\n\\item Proving \\ref{lem:zglob:l2}: In \\textbf{Step 4}, assuming iterates within $\\conb_{\\mu,R_\\eps}(\\Ws)$, we will prove that the norm diverges (as a result such $k_\\eps$ is guaranteed to exist) and, additionally, the gradient updates asymptotically aligns with $\\Ws$. \n\\end{itemize}\n%\n%\n\n% \\textbf{Step~4: Updates remain inside the cone $\\conb_{\\mu,R_\\mu}(\\Wm)$.}  By leveraging the results from \\textbf{Step 1} and \\textbf{Step 2}, we demonstrate that the gradient iterates remain within this cone.  Choose $R$  such that \n% \\begin{align}\\label{eqn:bound:R:nabla0}\n% R \\geq R_\\mu \\vee R_\\epsilon \\vee 1/2.\n% \\end{align}\n\\textbf{Step~3 (Proof of \\ref{lem:zglob:l3}): Updates remain inside the cone $\\conb_{\\mu,R_\\eps}(\\Ws)$.}   Note that if $\\W(k) \\in \\conb_{\\mu,R_\\eps}(\\Ws)$ for all $k \\geq 1$, the required condition in \\ref{lem:zglob:l2} holds, and we proceed to \\textbf{Step 4}. In this step, we show \\ref{lem:zglob:l3}. Specifically, we show that under Condition \\eqref{assum:extra} and using  \\eqref{eqn:neg:corr:0}, all iterates $\\W(k) \\in \\conb_{\\mu,R_\\eps}(\\Ws)$ remain within $\\conb_{\\mu,R_\\eps}(\\Ws)$.\n\nTo proceed, by leveraging the results from \\textbf{Step 1} and \\textbf{Step 2}, we demonstrate that the gradient iterates, with an appropriate constant step size, starting from $\\W(k_\\eps) \\in \\conb_{\\mu,R_\\eps}(\\Ws)$, remain within this set. We proceed by induction.  Suppose that the claim holds up to iteration $k \\geq k_\\eps$. This implies that $ \\W(k) \\in \\conb_{\\mu,R_\\eps}(\\Ws)$. Hence, recalling $\\conb_{\\mu,R_\\eps}(\\Ws)$ defined in \\eqref{eqn:con:nabla0}, there exists scalar $\\mu=\\mu(\\bal) \\in (0,1)$  and $R_\\eps$ such that  $\\tf{\\W(k)}\\geq R_\\eps$, and\n% \\begin{equation*}\n% \\begin{split}\n% \\left\\langle \\frac{\\W(k)}{\\tf{\\W(k)}},\\frac{\\Wm}{\\tf{\\Wm}} \\right\\rangle  \\geq 1-\\mu. \n% \\end{split}\n% \\end{equation*}\n\\begin{equation*}\n\\begin{split}\n\\left\\langle (\\x_{i\\op_i}-\\x_{it})\\z_i^\\top,\\frac{\\W(k)}{\\tf{\\W(k)}} \\right\\rangle  \\geq \\mu\\Theta,\n\\end{split}\n\\end{equation*}\n%We provide the proof by induction.  It follows from \\textbf{Step 1} that $\\W(1) \\in \\conb_{\\mu,R_\\eps}(\\Wm)$ using \\eqref{eqn:stepeta0}. Let $\\W(k) \\in \\conb_{\\mu,R_\\eps}(\\Wm)$. \n%for some positive constants $\\bar{\\rho}(k)$ and $\\rho$. \n%\\ct{Regarding this last statement ``for all $k\\geq1$\": (i) this is NOT part of the induction right? So then, I would state before to make clear. (ii) sorry I don't see: why do these hold?}\nwhere $\\Theta=1/\\tf{\\Wm}$. \n\nLet \n\\begin{subequations}\\label{eqn:rho:def:nabla0}\n\\begin{align}\n\\frac{1}{1-\\epsilon} \\iprod{ {\\frac{\\Wm}{\\tf{\\Wm}}}}{-\\nabla\\mc{L}(\\W(k))} =:\\rho(k)>0.\n\\end{align}\n\\end{subequations}\nUsing \\eqref{assum:extra}, we have \n\\begin{equation}\\label{eqn:localgd:1:nabla0}\n    \\begin{split}\n   \\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top,    \\frac{\\W(k+1)}{\\tf{\\W(k)}} \\right\\rangle &=   \\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top,    \\frac{\\W(k)}{\\tf{\\W(k)}} -\\frac{\\eta}{\\tf{\\W(k)}}\\nabla \\mc{L}(\\W(k)) \\right\\rangle\\\\   \n%   &\\ge \\mu \\Theta- \\frac{\\eta}{\\tf{\\W(k)}} \\left\\langle   (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top, \\nabla \\mc{L}(\\W(k)) \\right\\rangle  \\\\\n      & \\geq \\mu \\Theta +\\frac{ 2\\eta  (1-\\epsilon)\\mu \\Theta \\rho(k) }{\\tf{\\W(k)}}.\n    \\end{split}\n\\end{equation}\n%for some $\\bar{\\rho}(k)>0$.\n%Here, the first inequality follows from the induction assumption $\\W(k) \\in \\conb_{\\mu,R_\\eps}(\\Wm)$, and the second inequality uses \\eqref{eqn:bar:rho}.\nFrom Lemma~\\ref{glocal cond},   we have $\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle<0$~ which implies that $\\tf{\\W(k+1)} \\geq \\tf{\\W(k)}$.  This together with  $R_\\eps$ definition and $\\tf{\\W(k)}\\geq 1/2$ implies that  \n\\begin{align*}\n\\tf{\\W(k+1)}&\\leq\\frac{1}{{2\\tf{\\W(k)}}} \\left(\\tf{\\W(k+1)}^2+\\tf{\\W(k)}^2\\right)\\\\\n& = \\frac{1}{2\\tf{\\W(k)}} \\left(2\\tf{\\W(k)}^2-2\\eta\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle+\\eta^2\\tf{\\nabla \\Lc(\\W(k))}^2\\right)\\\\\n       &\\leq  \\tf{\\W(k)}- \\frac{\\eta}{\\tf{\\W(k)}}\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle + \\eta^2 \\|\\nabla \\Lc(\\W(k))\\|_F^2.\n\\end{align*}\n%where the last inequality follows from $\\tf{\\W(k)} \\geq R \\geq 1/2 $. \nThus,\n\\begin{equation}\\label{eqn:localgd:2:nabla0}\n\\begin{split}\n  \\frac{\\tf{\\W(k+1)}}{\\tf{\\W(k)}}& \\leq  1- \\frac{\\eta}{\\tf{\\W(k)}}\n       \\left\\langle \\nabla \\Lc(\\W(k)),\\frac{\\W(k)}{\\tf{\\W(k)}} \\right\\rangle + \\eta^2 \\frac{\\|\\nabla \\mc{L}(\\W(k))\\|_F^2}{\\tf{\\W(k)}}\\\\\n& \\leq 1- \\frac{\\eta}{(1-\\epsilon)\\tf{\\W(k)}}  \\iprod{\\nabla\\mc{L}(\\W(k))}\n     {\\frac{\\Wm}{\\tf{\\Wm}}}+ \\eta^2 \\frac{\\|\\nabla \\mc{L}(\\W(k))\\|_F^2}{\\tf{\\W(k)}}\\\\\n      & \\leq  1 + \\frac{\\eta \\rho(k)}{\\tf{\\W(k)}} + \\frac{\\eta^2\\|\\nabla \\mc{L}(\\W(k))\\|_F^2}{\\tf{\\W(k)}}=:C_1(\\rho(k),\\eta).\n\\end{split}\n\\end{equation}\nHere, the second inequality uses \\eqref{eqn:neg:corr:0}. \n\nNow, it follows from \\eqref{eqn:localgd:1:nabla0} and \\eqref{eqn:localgd:2:nabla0} that \n\\begin{equation}\\label{eqn:localgd:3:nabla0}\n\\begin{split}\n\\min_{t\\neq \\op_i,~i\\in[n]} ~~  \\left\\langle  (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top, \\frac{\\W(k+1)}{\\tf{\\W(k+1)}}\\right\\rangle   &\\geq \\frac{1}{C_1({\\rho}(k),\\eta)} \\left(\\mu \\Theta+\\frac{2\\eta (1-\\epsilon)\\mu \\Theta  {\\rho}(k)}{\\tf{\\W(k)}}\\right)\\\\\n& = \\mu \\Theta+\\frac{\\eta\\mu \\Theta}{C_1({\\rho}(k),\\eta)} \\left(\\frac{ \\big(2(1-\\epsilon) -1 \\big)  \\rho(k)}{\\tf{\\W(k)}}\n-   \\eta \\frac{\\tf{\\nabla \\mc{L}(\\W(k))}^2 }{\\tf{\\W(k)}}\\right)\\\\\n& = \\mu \\Theta+\\frac{\\eta\\mu \\Theta}{C_1({\\rho}(k),\\eta)} \\left(\\frac{ (1-2\\epsilon)   \\rho(k)}{\\tf{\\W(k)}}\n-   \\eta  \\frac{\\tf{\\nabla \\mc{L}(\\W(k))}^2 }{\\tf{\\W(k)}}\\right)\\\\\n%& \\geq \\mu \\Theta+\\frac{\\eta}{C_1({\\rho}(k),\\eta)} \\left(\\frac{ (1-\\mu) \\mu\\Theta  \\rho(k)}{\\tf{\\W(k)}}\n%-   \\eta \\mu \\Theta \\frac{\\|\\nabla \\mc{L}(\\W(k))\\|^2 }{\\tf{\\W(k)}}\\right)\\\\\n& \\geq \\mu \\Theta,\n\\end{split}\n\\end{equation}\n where the last inequality uses our choice of stepsize $\\eta\\leq 1/L_W$ in Theorem~\\ref{conv:gd:w:global:nabla0}'s statement. Specifically, we need $\\eta$ to be small to ensure the last inequality. We will guarantee this by choosing a proper $R_\\eps$ in Lemma \\ref{lem:glocal:corr}. Specifically, Lemma \\ref{lem:glocal:corr} leaves the choice of $D_0$ in $R_\\eps$ lower bound of \\eqref{Rpi choice2} open (it can always be chosen larger). Here, by choosing $D_0\\gtrsim 1/L_{\\W}$ will ensure $\\eta\\leq 1/L_{\\W}$ works well.\n% \\begin{align}\\label{eqn:zeta:mu}\n% \\nonumber\n% \\eta &\\leq \\frac{\\mu}{2(1-\\mu)(1-\\frac{\\mu}{2})}\n%  \\frac{c}{C}  \\frac{\\Theta}{\\bar{A}}   \\frac{1}{\\bar{A}C T}    e^{R_\\mu^0\\Theta/2} \\\\\n% & \\leq \\frac{\\mu-\\epsilon}{1-\\mu} \\cdot  \\frac{1}{1-\\epsilon} \\cdot \\frac{c}{C} \\cdot \\frac{\\Theta}{\\bar{A}} \\cdot  \\frac{1}{\\bar{A}C T}  e^{R_\\mu^0\\Theta/2} \\leq \\frac{(\\mu-\\epsilon)}{1-\\mu}  \\frac{\\rho(k) } { \\|\\nabla \\mc{L}(\\W(k))\\|^2_F}.\n% \\end{align}\n% Let \n% \\begin{equation}\\label{eqn:C2}\n% C_2 = (1-\\mu)  \\frac{  \\bar{A}^2 C^2 }{ \\Theta c}.\n% \\end{equation}\n% Next, we will demonstrate that the choice of $\\eta$ in \\eqref{eqn:zeta:mu} does indeed meet our step size condition as stated in the theorem, i.e., $\\eta \\leq 1/L_{\\W}$. Recall that $1/(1+\\pi)=1-\\epsilon$, which implies that $\\pi =\\epsilon/(1-\\epsilon)$. Combining this with \\eqref{R boundC0}, we obtain:\n% \\begin{align*}\n% R_\\pi &\\geq\\frac{\\max(2,\\delta^{-1})}{\\Theta}\\log\\left(\\frac{C_0 T\\Gamma A}{\\pi\\bggm}\\right), \\quad \\textnormal{where} \\quad C_0 \\geq 64 \\pi\\\\\n% & \\Rightarrow R_\\epsilon \\geq\\frac{\\max(2,\\delta^{-1})}{\\Theta}\\log\\left(\\frac{ (1-\\epsilon)C_0 T\\Gamma A}{\\epsilon\\bggm}\\right),\\quad  \\textnormal{where}   \\quad C_0 \\geq 64  \\frac{\\epsilon}{1-\\epsilon}\n% \\end{align*}\n% On the other hand, at the initialization, we have  $\\epsilon=\\mu/2$ which implies that \n% \\begin{align}\\label{eqn:rmu:c0}\n%   R_{\\mu}^0 \\geq \\frac{\\max(2,\\delta^{-1})}{\\Theta}\\log\\left(\\frac{ (2-\\mu)C_0 T\\Gamma A}{\\mu\\bggm}\\right),  \\quad  \\textnormal{where} \\quad  C_0 \\geq 64  \\frac{\\mu}{2(1-\\frac{\\mu}{2})}\n% \\end{align}\n% In the following, we will determine  a lower bound on  $C_0$ such that our step size condition in Theorem~\\ref{thm:local:gd}'s statement, i.e., $\\eta \\leq 1/L_{\\W}$, is satisfied. Note that for the choice of $\\eta$ in \\eqref{eqn:zeta:mu} to meet the condition $\\eta \\leq 1/L_{\\W}$, the following condition must hold:\n% \\begin{equation}\n%  \\frac{1}{L_{\\W}}\\leq \n% \\frac{\\mu}{ (2-\\mu)} \\frac{1}{C_2T} e^{R_\\mu^0\\Theta/2}  \\Rightarrow R_\\mu^0 \\geq  \\frac{2}{\\Theta} \\log  \\left(\\frac{1}{L_{\\W}}   \\frac{2-\\mu}{\\mu} C_2 T\\right).\n% \\end{equation}\n% where $C_2$ is given in \\eqref{eqn:C2}.\n% This together with \\eqref{eqn:rmu:c0} implies that \n% \\begin{align*}\n% \\frac{C_0 \\Gamma A}{\\bggm}  & \\geq (1-\\mu) \\frac{C_2}{L_{\\W}}   \\Rightarrow  C_0 \\geq  \\max \\left( \\frac{(1-\\mu)C_2}{L_{\\W}}    \\frac{\\bggm }{\\Gamma A},  \\frac{64\\mu}{2-\\mu} \\right). \n% \\end{align*}\n% Therefore, with this lower bound on $C_0$, the step size bound in \\eqref{eqn:zeta:mu} is sufficiently large to ensure that $\\eta \\leq 1/L_{\\W}$ guarantees \\eqref{eqn:wt+1:cone}. Hence,  $\\pb(t+1)$ remains within the cone, i.e., $\\pb(t+1) \\in \\Cc_{\\mu,R_\\eps}(\\ps)$.\n% Hence,  it follows from \\eqref{eqn:wt+1:cone} that $\\W(k+1) \\in \\Cc_{\\mu,R_\\eps}(\\Ws)$.\n% \\\\\n\\begin{equation}\\label{eqn:zeta:mu:0}\n\\begin{split}\n    \\eta &\\leq    \\big( 1-\\mu\\big)\\mu\n \\frac{c}{C}  \\frac{\\Theta}{\\bar{A}}   \\frac{1}{\\bar{A}C T}    e^{R_\\mu^0\\Theta/2}\\\\\n %&\\leq     \\frac{( 1-2\\epsilon)  }{1-\\epsilon} \n %\\frac{c}{C}  \\frac{\\Theta}{\\bar{A}}   \\frac{1}{\\bar{A}C T}    e^{R_\\mu^0\\Theta/2}\\\\\n &\\leq      \\frac{1-2\\epsilon  }{1-\\epsilon}   \\frac{c \\mu}{C}   \\frac{\\Theta}{\\bar{A}}    \\frac{1}{\\bar{A}C T}  e^{R_\\mu^0\\Theta/2} \\\\\n &\\leq   \\big(1-2\\epsilon \\big)   \\frac{\\rho(k) } { \\|\\nabla \\mc{L}(\\W(k))\\|^2_F}.\n\\end{split}    \n\\end{equation}\n% \\begin{align}\n% \\nonumber\n% \\eta &\\leq    \\big( (1-\\frac{\\mu}{2}) \\sqrt{\\mu}-\\mu \\big)\n%  \\frac{c}{C}  \\frac{\\Theta}{\\bar{A}}   \\frac{1}{\\bar{A}C T}    e^{R_\\mu^0\\Theta/2}\\\\\n%  &\\leq    \\big( (1-\\epsilon) \\sqrt{\\mu}  -\\mu \\big)\n%  \\frac{c}{C}  \\frac{\\Theta}{\\bar{A}}   \\frac{1}{\\bar{A}C T}    e^{R_\\mu^0\\Theta/2}\\\\\n%  &\\leq   \\frac{ \\big( (1-\\epsilon) \\sqrt{\\mu}  -\\mu  \\big) \\Theta }{ \\mu \\Theta}  \\cdot  \\frac{1}{1-\\epsilon} \\cdot \\frac{c \\mu}{C} \\cdot \\frac{\\Theta}{\\bar{A}} \\cdot  \\frac{1}{\\bar{A}C T}  e^{R_\\mu^0\\Theta/2} \\\\\n%  &\\leq \\frac{ \\big( (1-\\epsilon) \\sqrt{\\mu} -\\mu \\big) \\Theta  }{\\mu \\Theta}  \\frac{\\rho(k) } { \\|\\nabla \\mc{L}(\\W(k))\\|^2_F}.\n% \\end{align}\nHere, the first inequality follows since $\\epsilon \\in (0, \\mu/2)$ (as seen in \\textbf{Step 2}). Also,  $\\mu < 1$ implies that $1-\\mu > 0$, we obtain $\\eta > 0$. The last inequality is obtained from Lemma~\\ref{glocal cond}:\n\\begin{align*}\n    \\frac{\\rho(k) } { \\tf{\\nabla \\mc{L}(\\W(k))}} &= - \\frac{1}{1-\\epsilon} \\iprod{ \\frac{\\nabla\\mc{L}(\\W(k))}{\\tf{\\nabla \\mc{L}(\\W(k))}}}\n     {\\frac{\\Wm}{\\tf{\\Wm}}}  \\geq \\frac{1}{1-\\epsilon} \\cdot \\frac{c \\mu}{C} \\cdot \\frac{\\Theta}{\\bar{A}},\\\\\n         \\frac{1} { \\tf{\\nabla \\mc{L}(\\W(k))}} &{\\geq \\frac{1}{\\bar{A}C \\cdot \\frac{1}{n} \\sum_{i=1}^n  \\left(1-\\s_{i\\op_i}\\right)} \\geq     \\frac{1}{ \\bar{A} C T e^{-R_\\mu^{0}\\Theta/2}} }\n\\end{align*}\nfor some data dependent constrants $c$, $C$, $\\bar{A}=\\max_{i\\in[n],t,\\tau\\in[T]}\\tn{(\\x_{it}- \\x_{i\\tau})}~\\tn{\\z_i}$, and $\\Theta=1/\\tf{\\Ws}$.\n\nThe remainder of the proof of this step is identical to \\eqref{eqn:pitoC0}--\\eqref{eqn:pitoC02}, with the replacement of $C_0$ by $D_0$ and the tracking of changes. Specifically, Lemma \\ref{lem:glocal:corr} leaves the choice of $D_0$ in $R_\\eps$ lower bound of \\eqref{Rpi choice2} open (it can always be chosen larger). Hence,  for sufficiently large $D_0$, we have\n\\begin{align}\n\\eta \\leq \\frac{1}{L_{\\W}}\\leq   \\big( 1-\\mu\\big)\\mu\n \\frac{c}{C}  \\frac{\\Theta}{\\bar{A}}   \\frac{1}{\\bar{A}C T}    e^{R_\\mu^0\\Theta/2}.\n\\end{align}\n% $   \\big( (1-\\frac{\\mu}{2}) \\sqrt{\\mu}-\\mu \\big)\n%  \\frac{c}{C}  \\frac{\\Theta}{\\bar{A}}   \\frac{1}{\\bar{A}C T}    e^{R_\\mu^0\\Theta/2} \\geq \\frac{1}{L_\\W}$ for a sufficiently large $D_0$.  \nThis implies \\eqref{eqn:localgd:3:nabla0} and  $\\W(k+1) \\in\\conb_{\\mu,R_\\eps}(\\Ws)$. %\\yl{[Why using both $R_\\mu^0$ and $R_\\eps$?]}\n% Here, we leverage the fact that the parameter $\\mu$ can be chosen arbitrarily, and then we select the step size $\\eta$ to satisfy the following condition: \\ct{this step-size not reflected in theorem.}\n% $$\\eta \\leq \\frac{  \\big(c_4-1\\big)\\mu \\Theta \\rho(k) }{\\mu\\Theta} \\frac{1} {\\|\\nabla \\mc{L}(\\W(k))\\|^2}.\n% $$\n% ...\n% \\begin{align*}\n% \\eta \\leq  \\zeta(\\mu) &:=  \\frac{\\mu}{2(1-\\mu)(1-\\frac{\\mu}{2})}\n% \\cdot \\frac{c}{C} \\cdot \\frac{\\Theta}{\\bar{A}} \\cdot  \\frac{1}{\\bar{A}C } \\\\\n% & \\leq \\frac{\\mu-\\epsilon}{1-\\mu} \\cdot  \\frac{1}{1-\\epsilon} \\cdot \\frac{c}{C} \\cdot \\frac{\\Theta}{\\bar{A}} \\cdot  \\frac{1}{\\bar{A}C } \\leq \\frac{(\\mu-\\epsilon)}{1-\\mu}  \\frac{\\rho(k) } { \\|\\nabla \\mc{L}(\\W(k))\\|^2_F}.\n% \\end{align*}\n\n% Here, the first inequality uses our choice of $\\epsilon \\in (0, \\mu/2)$ (see \\textbf{Step 2}), and the last inequality is obtained from Lemma~\\ref{local cond} since\n% \\begin{align*}\n%     \\frac{\\bar{\\rho}(k)} { \\tf{\\nabla \\mc{L}(\\W(k))}}&=  \\Big\\langle  (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top, - \\frac{\\nabla \\mc{L}(\\W(k))}{ \\tf{\\nabla \\mc{L}(\\W(k))}} \\Big\\rangle, \n%     \\\\\n%     \\frac{\\rho(k) } { \\tf{\\nabla \\mc{L}(\\W(k))}} &= - \\frac{1}{1-\\epsilon} \\iprod{ \\frac{\\nabla\\mc{L}(\\W(k))}{\\tf{\\nabla \\mc{L}(\\W(k))}}}\n%      {\\frac{\\Wm}{\\tf{\\Wm}}}  \\geq \\frac{1}{1-\\epsilon} \\cdot \\frac{c}{C} \\cdot \\frac{\\Theta}{\\bar{A}},\\\\\n%          \\frac{1} { \\tf{\\nabla \\mc{L}(\\W(k))}} &\\geq \\frac{1}{\\bar{A}C \\cdot \\frac{1}{n} \\sum_{i=1}^n  \\left(1-\\s_{i\\alpha_i}\\right)} \\geq  \\frac{1}{\\bar{A}C}, \n% \\end{align*}\n% for some data dependent constrants $c$ and $C$, $\\bar{A}=\\max_{i\\in[n],t,\\tau\\in[T]}\\tn{(\\x_{it}- \\x_{i\\tau})}~\\tn{\\z_i}$, and $\\Theta=1/\\tf{\\Ws}$.\n\n\n\n\\noindent\\textbf{Step 4 (Proof of \\ref{lem:zglob:l2}): $\\W(k)$ and $\\Wm$ perfectly align over time.} \nBy theorem statement (alternatively via \\textbf{Step 3}), we have that all iterates remain within the initial conic set i.e.~$\\W(k)\\in\\conb_{\\mu,R^0_\\mu}(\\Ws)$ for all $k\\geq 0$. Note that it follows from Lemma~\\ref{glocal cond}  that  $\\li\\nabla\\Lc(\\W), \\Ws/\\tf{\\Ws}\\ri<0$, for any finite $\\W \\in \\conb_{\\mu,R^0_\\mu}(\\Ws)$. Hence, there are no finite critical points $\\W \\in \\conb_{\\mu,R^0_\\mu}(\\Ws)$, for which $\\nabla \\mc{L} (\\W)=0$. Now, based on Lemma~\\ref{lem:grad:descent}, which guarantees that $\\nabla\\Lc(\\W(k))\\rightarrow 0$, this\nimplies that $\\left\\Vert \\W\\left(k\\right)\\right\\Vert \\rightarrow\\infty$. Consequently, for any choice of $\\eps\\in (0,\\mu/2)$ there is an iteration $k_\\eps$ such that, for all $k\\geq k_\\eps$, $\\W(k)\\in\\conb_{\\mu,R_\\eps}(\\Ws)$. Once within $\\conb_{\\mu,R_\\eps}(\\Ws)$,  multiplying both sides of \\eqref{eqn:neg:corr:0} by the stepsize $\\eta$ and using the gradient descent update, we get\n\\begin{equation*}%\\label{eqn:decpath:1}\n\\begin{split}\n     \\left\\langle \\W(k+1)-\\W(k),\\frac{ \\Wm}{\\tf{\\Wm}} \\right\\rangle &\\geq  (1-\\epsilon) \\left\\langle \\W(k+1)-\\W(k), \\frac{\\W(k)}{\\tf{\\W(k)}}\\right\\rangle\\\\\n     &= \\frac{(1-\\epsilon)}{2\\tf{\\W(k)}}\\left(\\tf{\\W(k+1)}^2- \\tf{\\W(k)}^2-\\tf{\\W(k+1)-\\W(k)}^2\\right) \\\\\n     & \\geq (1-\\epsilon)\\left( \\frac{1}{2\\tf{\\W(k)}} \\left(\\tf{\\W(k+1)}^2- \\tf{\\W(k)}^2\\right)-\\tf{\\W(k+1)-\\W(k)}^2\\right) \\\\\n     & \\geq (1-\\epsilon)\\left(\\tf{\\W(k+1)}- \\tf{\\W(k)}-\\tf{\\W(k+1)-\\W(k)}^2\\right) \\\\\n          & \\geq (1-\\epsilon)\\Big(\\tf{\\W(k+1)}- \\tf{\\W(k)}- 2\\eta  \\left(\\mc{L}(\\W(k))-\\mc{L}(\\W(k+1))\\right) \\Big).\n\\end{split}\n\\end{equation*}\n% \\ct{Check inequality in third line above? I see how to jump from second line to fourth but not third. {\\color{cyan}D: fixed, please see below. Thanks}}\nHere, the second inequality is obtained from  $\\tf{\\W(k)}\\geq 1/2$; the third inequality follows since  for any $a, b >0$, we have $  (a^2-b^2)/(2b) -  (a-b) \\geq 0$; and the last inequality  uses Lemma~\\ref{lem:grad:descent}.\n\n%Here, the last inequality uses Lemma~\\ref{lem:grad:descent}.\n\nSumming the above inequality over $k\\geq k_\\eps$ gives \n% \\begin{align*}\n%       \\left\\langle\\frac{\\W(k)}{\\tf{\\W(k)}}, \\frac{ \\Wm}{\\tf{\\Wm}} \\right\\rangle \\ge1-\\epsilon+ \\frac{C(\\epsilon,\\eta)}{\\tf{\\W(k)}},\n% \\end{align*}\n% for some finite constant $C(\\epsilon,\\eta)$ defined as\n% \\begin{equation}\\label{eqn:decpath:22}\n% C(\\epsilon,\\eta):= \\left\\langle \\W(k_\\eps), \\frac{ \\Wm}{\\tf{\\Wm}}\\right\\rangle-(1-\\epsilon)\\tf{\\W(k_\\eps)} -2\\eta (1-\\epsilon) (\\mc{L}(\\W(k_\\eps))-\\mathcal{L}_{\\star}),\n% \\end{equation}\n% where $\\mathcal{L}_{\\star}\\leq\\mathcal{L}\\left(\\W\\left(k\\right)\\right)$ for all $k\\geq 0$.\n\n% Since $\\left\\Vert  \\W\\left(k\\right)\\right\\Vert \\rightarrow\\infty$, we get\n%     \\begin{align}\\label{eqn:eps:final}\n%       \\liminf_{k\\to\\infty}\\iprod{\\frac{\\W(k)}{\\tf{\\W(k)}}}{\\frac{ \\Wm}{\\tf{\\Wm}}}\\ge1-\\epsilon.\n%     \\end{align}\n% Given that $\\epsilon$ is arbitrary, we can consider the limit as $\\epsilon$ approaches zero. Thus, $\\W(k)/\\tf{\\W(k)}\\to   \\Wm/\\tf{\\Wm}$.  $\\qed$ \n% %\\end{proof}\n\n\\begin{align*}\n      \\left\\langle\\frac{\\W(k)}{\\tf{\\W(k)}}, \\frac{\\Wm}{\\tf{\\Wm}} \\right\\rangle \\ge1-\\epsilon+ \\frac{C(\\epsilon,\\eta)}{\\tf{\\W(k)}}, \\qquad \\W(k)\\in\\conb_{\\mu,R_\\eps}(\\Ws),   \n\\end{align*}\nwhere $\\mathcal{L}_{\\star}\\leq\\mathcal{L}\\left(\\W\\left(k\\right)\\right)$ for all $k\\geq k_\\eps$, and \n\\begin{equation*}%\\label{eqn:decpath:22}\nC(\\epsilon,\\eta)= \\left\\langle \\W(k_\\eps), \\frac{ \\Wm}{\\tf{\\Wm}}\\right\\rangle-(1-\\epsilon)\\tf{\\W(k_\\eps)} -2\\eta (1-\\epsilon) (\\mc{L}(\\W(k_\\eps))-\\mathcal{L}_{\\star}).\n\\end{equation*}\nConsequently,\n    \\begin{align*}\n      \\liminf_{k\\to\\infty}\\iprod{\\frac{\\W(k)}{\\tf{\\W(k)}}}{\\frac{\\Wm}{\\tf{\\Wm}}}\\ge1-\\epsilon, \\qquad \\W(k)\\in\\conb_{\\mu,R_\\eps}(\\Ws).  \n    \\end{align*}\nSince  $\\epsilon \\in (0, \\mu/2)$  is arbitrary, this implies $\\W(k)/\\tf{\\W(k)}\\to  \\Wm/\\tf{\\Wm}$.\n\\end{proof}\n\n\n\n% It follows from Lemma~\\ref{lem:lip} that, under Assumption~\\ref{assum:loss:prope}, the gradient Lipschitz constant is on the order of the token's score; see \\eqref{eqn:obj:lipcons}. Consequently, there exists $\\Delta_0$ such that the following statement holds:\n% \\begin{align}\\label{eqn:hess:var}\n% \\left\\|\\Hc(\\W)- \\tilde{\\Hc}(\\tilde{\\W})\\right\\|\\leq \\Delta_0(\\delta,R),  \n% \\end{align}\n% where $\\Hc$ and $\\tilde{\\Hc}$ denote the (tensor) Hessian of  $\\Lc$ and $\\tilde{\\Lc}$ objectives, respectively. \n\n% Let\n% \\begin{align*}\n% \\Hc_{\\tau} &:=\\Hc \\left(\\W(\\tau+1),\\W(\\tau)\\right)=\\int_{0}^1 \\Hc\\left(t \\W(\\tau+1)+(1-t)\\W(\\tau)\\right)dt, \\\\\n% \\tilde{\\Hc}_{\\tau}&:=\\tilde{\\Hc}\\left(\\tilde{\\W}(\\tau+1),\\tilde{\\W}(\\tau)\\right)=\\int_{0}^1 \\tilde{\\Hc}\\left(t \\tilde{\\W}(\\tau+1)+(1-t)\\tilde{\\W}(\\tau)\\right)dt.    \n% \\end{align*}\n% It follows from \\eqref{eqn:gd:orig:prox}  that \n% \\begin{align*}\n% \\Rb_{\\tau+1}:=\\nabla \\Lc(\\W(\\tau+1))= \\nabla \\Lc(\\W(\\tau)-\\eta\\nabla \\Lc (\\W(\\tau)) &=\\nabla  \\Lc (\\W(\\tau))-\\eta \\Hc_{\\tau} \\nabla \\Lc(\\W(\\tau))\\\\\n% &=\\left(\\Ic-\\eta \\Hc_{\\tau}\\right)\\Rb_\\tau,\\\\\n% \\tilde{\\Rb}_{\\tau+1}:=\\nabla \\tilde{\\Lc}(\\tilde{\\W}(\\tau+1))= \\nabla \\tilde{\\Lc}(\\tilde{\\W}(\\tau)-\\eta\\nabla \\Lc {\\tilde{\\W}(\\tau)})&=\\nabla \\tilde{\\Lc}(\\tilde{\\W}(\\tau))-\\eta \\tilde{\\Hc}_{\\tau} \\nabla \\tilde{\\Lc} (\\tilde{\\W}(\\tau))\\\\\n% &=\\left(\\mc{I}-\\eta \\tilde{\\Hc}_{\\tau}\\right)\\tilde{\\Rb}_\\tau.\n% \\end{align*}\n% Let $r_{\\tau} :=\\tn{\\Rb_\\tau-\\tilde{\\Rb}_\\tau}$. We have\n% \\begin{equation}\\label{eqn:grad:seq}\n% \\begin{split}\n% r_{\\tau+1}\\leq\\tn{\\Rb_\\tau-\\tilde{\\Rb}_\\tau}+ \\eta \\tn{\\Hc_{\\tau}\\Rb_\\tau -\\tilde{\\Hc}_\\tau \\tilde{\\Rb}_\\tau} &\\leq r_\\tau+\\eta \\tn{\\Hc_\\tau-\\tilde{\\Hc}_\\tau}~\\tn{\\tilde{\\Rb}_\\tau} + \\eta\\tn{\\Hc_\\tau}r_\\tau \\\\\n% &\\leq r_\\tau+\\eta\\tn{\\Hc_\\tau-\\tilde{\\Hc}_\\tau}~\\tn{\\tilde{\\Rb}_\\tau} + \\eta\\tn{\\Hc_\\tau}r_\\tau \\\\\n% &\\leq  r_\\tau (1+ \\eta L_{\\W} ) + \\eta \\Delta_0(\\delta,R) M_g.   \n% \\end{split}\n% \\end{equation}\n% Here, the last inequality uses $\\tn{\\tilde{\\Rb}_\\tau}  \\leq M_g $ and $\\tn{\\Hc_\\tau} \\leq L_{\\W}$ for all $\\|\\W\\| \\leq R $ due to Lemma~\\ref{lem:lip} under Assumption~\\ref{assum:loss:prope}.\n% Hence, it follows from \\eqref{eqn:grad:seq} that\n% \\begin{align*}\n% r_{\\tau}&\\leq  r_{\\tau-1} (1+ \\eta L_{\\W}) +\\eta \\Delta_0(\\delta,R) M_g, \\qquad \\textnormal{for all} \\qquad  0\\le \\tau\\le \\underline{k}.\n% \\end{align*}\n% Let $\\Delta(\\delta, R)$ be defined such that $r_0 \\leq \\Delta(\\delta, R) < 1$ for some specific values of $\\delta$ and $R$. For all $0\\leq \\tau\\leq \\underline{k}$, we claim \n% \\begin{align}\\label{eqn:K0}\n% \\underline{k} =  \\frac{1}{ \\eta  \\left(\\Delta_0(\\delta,R) M_g+ L_{\\W} \\Delta(\\delta,R)\\right)}, \\qquad \\textnormal{implies that}  \\qquad  r_{\\tau} \\leq \\Delta(\\delta,R).\n% \\end{align} \n% We prove the result by induction. Suppose the above holds for all $k\\leq \\tau-1$. Consequently, we have\n% \\begin{align*}\n% r_k&\\leq r_{k-1} (1+ \\eta L_{\\W}) +\\eta  M_g \\Delta_0(\\delta,R) , \\\\\n% &\\leq r_{k-1}+\\eta \\left( M_g \\Delta_0(\\delta,R) + L_{\\W}  \\Delta(\\delta,R) \\right).\n% %&=r_{t-1}+\\eta M_1(\\Delta_0(\\delta,R)).\n% \\end{align*}\n% Summing up for $0\\leq k\\leq \\tau-1$, we obtain\n% \\begin{align*}\n% r_{\\tau}=\\sum_{k=0}^{\\tau-1} r_{k+1}-r_k &\\leq  \\eta  \\tau  \\left( M_g\\Delta_0(\\delta,R) + L_{\\W} \\Delta(\\delta,R)\\right)\\\\\n% &\\leq  \\eta  \\underline{k}  \\left( M_g \\Delta_0(\\delta,R) + L_{\\W} \\Delta(\\delta,R)\\right)\\\\\n% %&\\leq \\eta T M_1\\\\\n% &\\leq \\Delta(\\delta,R),\n% \\end{align*}\n% where the last inequality uses  \\eqref{eqn:K0}.\n% \\\\\n% \\textbf{Step 2}.  It follows from Theorem~\\ref{conv:gd:kq:global} for equally-scored problem that for all $\\mu \\in (0,1)$, there exist $R_{\\mu} \\geq R$ such that \n% \\begin{equation}\\label{eqn:localgd:1 new}\n% \\begin{split}\n%      \\left\\langle \\frac{\\W(k)}{\\tf{\\tilde{\\W}(k)}},\\frac{\\Wm}{\\tf{\\Wm}} \\right\\rangle  &=    \\left\\langle \\frac{\\tilde{\\W}(k)}{\\tf{\\tilde{\\W}(k)}} -\\frac{\\W(k) -\\tilde{\\W}(k)}{\\tf{\\tilde{\\W}(k)}}, \\frac{\\Wm}{\\tf{\\Wm}} \\right\\rangle\\\\\n%       &\\geq 1-\\mu  + \\left(\\frac{\\mu}{2}- \\eta\\frac{ \\Delta(\\delta,R)}{\\tf{\\tilde{\\W}(k)}}\\right).\n% \\end{split}\n% \\end{equation}\n% Further, %\\redp{multiply defined label, name changed}\n% \\begin{equation}\\label{eqn:localgd:2 new}\n% \\begin{split}\n%   \\frac{\\tf{\\W(k)}}{\\tf{\\tilde{\\W}(k)}}& \\leq 1+  \\frac{\\tf{ \\tilde{\\W}(k)-\\W(k)}}{\\tf{\\tilde{\\W}(k)}}\\leq  1+\\frac{\\eta \\Delta(\\delta,R)}{\\tf{\\tilde{\\W}(k)}}. \n% \\end{split}\n% \\end{equation}\n% It follows from \\eqref{eqn:localgd:1 new} and \\eqref{eqn:localgd:2 new} that   \n% \\begin{equation}\\label{eqn:w:in:cone}\n% \\begin{split}\n% \\left\\langle \\frac{\\W(k)}{\\tf{\\W(k)}},\\frac{\\Wm}{\\tf{\\Wm}} \\right\\rangle   %&\\geq \\frac{1}{c_3(\\Delta,\\eta)} \\left( 1-\\frac{\\mu}{2} - \\eta\\frac{ r_k}{\\tf{\\W(k)}\n% %}\\right)\\\\\n% &\\geq \\frac{1}{1+\\frac{\\eta \\Delta(\\delta,R)}{\\tf{\\tilde{\\W}(k)}}} \\left( 1-\\mu +\\frac{\\mu}{2} - \\eta\\frac{ \\Delta(\\delta,R)}{\\tf{\\W(k)}\n% }\\right)\\\\\n% &\\geq 1-\\mu+\\frac{\\eta}{1+\\frac{\\eta \\Delta(\\delta,R)}{\\tf{\\tilde{\\W}(k)}}} \\left(\\frac{\\mu}{2} - (1-\\mu) \\frac{\\Delta(\\delta,R)}{\\tf{\\tilde{\\W}(k)}}- \\frac{ \\Delta(\\delta,R)}{\\tf{\\tilde{\\W}(k)}}\\right)\\\\\n% & \\geq 1-\\mu. \n% \\end{split}\n% \\end{equation}\n% Here, the last inequality is obtained for sufficiently small $\\delta$, which also implies a sufficiently small $\\Delta(\\delta,R)$.\n% \\\\\n% \\textbf{Step 3.} Let $\\mu \\in (0,1)$ be the correlation constant such that $\\Cc_{\\mu,R}(\\Wm)$ does not contain any stationary points of the original problem. It follows from \\eqref{eqn:w:in:cone} that $\\W(k) \\in \\Cc_{\\mu,R}(\\Wm)$. The remainder of the proof follows by applying Theorem~\\ref{thm:local:gd} to the original problem.\n% %\n% Let $\\Lc$ and $\\tilde{\\Lc}$ denote the objectives of the original and equally-scored (equal-score) problems, respectively. \n% Fix $\\W(0)=\\tilde{\\W}(0)$. Algorithm  \\ref{GD-W} applied to these problems implies that  \n% \\begin{equation}\\label{eqn:gd:orig:prox}\n%  \\W(\\tau+1)=\\W(\\tau)-\\eta\\nabla \\Lc(\\W(\\tau)), \\quad \\text{and} \\quad \\tilde{\\W}(\\tau+1)=\\tilde{\\W}(\\tau)-\\eta \\nabla \\tilde{\\Lc}(\\tilde{\\W}(\\tau)).   \n% \\end{equation}\n% Let  $\\hb_i=\\X_i\\W \\z_{i}$ and  $\\tilde{\\hb}_i= \\tilde{\\X}_i \\tilde{\\W} \\z_{i}$.  Following Lemma~\\ref{lem:lip} and under Assumption~\\ref{assum:loss:prope},  we have\n% \\begin{align}\\label{eqn:obj:wvstilde}\n%   \\nonumber\n%  \\left\\|\\nabla \\mc{L}(\\W)- \\nabla \\tilde{\\mc{L}}(\\tilde{\\W})\\right\\|\n%   &\\leq \\frac{1}{n}  \\sum_{i=1}^n\\left\\| \\ell' \\left(\\bgam_i^\\top \\sft{\\hb_i}\\right) \\cdot \\z_{i}  \\bgam_i^\\top \\sfp{\\hb_i} \\X_i - \\ell' \\left(\\tilde{\\bgam}_i^\\top \\sft{\\tilde{\\hb}_i}\\right) \\cdot \\z_{i}  \\tilde{\\bgam}_i^\\top \\sfp{\\tilde{\\hb}_i}  \\tilde{\\X}_i \\right\\|\\\\ \n%        & \\le \\frac{1}{n}\\sum_{i=1}^{n} M_0~ \\left\\|  \\z_{i}  \\tilde{\\bgam}_i^\\top \\sfp{\\tilde{\\hb}_i} \\tilde{\\X}_i \\right\\| \\left\\| \\bgam_i^\\top \\sft{\\hb_i} -  \\tilde{\\bgam}_i^\\top \\sft{\\tilde{\\hb}_i}\\right\\| \\\\\n%          \\nonumber\n%        &+ \\frac{1}{n}\\sum_{i=1}^{n}  M_1~\\left\\| \\z_{i}  \\bgam_i^\\top \\sfp{\\hb_i} \\X_i - \\z_{i}  \\tilde{\\bgam}_i^\\top \\sfp{\\tilde{\\hb}_i} \\tilde{\\X}_i \\right\\|, \n%        %\\\\\n%         % \\nonumber\n%        % & \\le \\frac{1}{n}\\sum_{i=1}^{n}  M_0 ~\\|\\bgam_i\\|^2~\\|\\z_i\\| ~\\|\\X_i\\|~\\left\\|\\sft{\\hb_i}-\\sft{\\dot{\\hb}_i}\\right\\| \\\\\n%        % & +   \\frac{1}{n}\\sum_{i=1}^{n}  M_1  ~\\|\\bgam_i\\|~\\|\\z_i\\|~\\|\\X_i \\|~\\left\\|\\sfp{\\hb_i}-\\sfp{\\dot{\\hb}_i}\\right\\|,\n% \\end{align}\n% where the second inequality follows from the fact that $|ab - cd| \\leq |d||a-c|+ |a||b-d|$ .\n% %and the third inequality uses Assumption~\\ref{assum:loss:prope}.\n\n% It follows from \\eqref{eqn:obj:wvstilde} that  for any $\\W,\\tilde{\\W}$ with $\\tf{\\W} \\leq R$ and $\\tf{\\tilde{\\W}} \\leq R$, there exists $\\Delta_0$ such that the following statement holds:\n% \\begin{align}\\label{eqn:grad:var:0}\n%  \\left\\|\\nabla \\mc{L}(\\W)- \\nabla \\tilde{\\mc{L}}(\\tilde{\\W})\\right\\| \\leq \\Delta_0(\\delta,R),  \n% \\end{align}\n% %= \\nabla \\Lc(\\W(\\tau)-\\eta\\nabla \\Lc (\\W(\\tau)) &=\\nabla  \\Lc (\\W(\\tau))-\\eta \\Hc_{\\tau} \\nabla \\Lc(\\W(\\tau))\n% % \\\\\n% % &=\\left(\\Ic-\\eta \\Hc_{\\tau}\\right)\\Rb_\\tau,\\\\\n% %\\tilde{\\Rb}_{\\tau+1}:=\n% % \\qquad \n% % \\tilde{\\nabla}_{\\tau+1}:=\\nabla \\tilde{\\Lc}(\\tilde{\\W}(\\tau)-\\eta\\nabla \\Lc {\\tilde{\\W}(\\tau)})\n% %&=\\nabla \\tilde{\\Lc}(\\tilde{\\W}(\\tau))-\\eta \\tilde{\\Hc}_{\\tau} \\nabla \\tilde{\\Lc} (\\tilde{\\W}(\\tau))\\\\\n% %&=\\left(\\mc{I}-\\eta \\tilde{\\Hc}_{\\tau}\\right)\\tilde{\\Rb}_\\tau.\n% Now, we track the difference between $\\W$ and  $\\tilde{\\W}$ as follows\n% \\begin{equation}\\label{eqn:grad:var:del}\n%     \\begin{split}\n% \\tf{ \\tilde{\\W}(k)-\\W(k)} = \\eta \\sum_{\\tau=0}^{k-1} \\left\\|\\nabla \\mc{L}(\\W(\\tau))- \\nabla \\tilde{\\mc{L}}(\\tilde{\\W}(\\tau))\\right\\| \n% &\\leq \\eta \\sum_{\\tau=0}^{k-1}\\Delta_0(\\delta,R), \\\\\n% &\\leq  \\eta k\\Delta_0(\\delta,R), \\\\\n% &\\leq  \\eta \\Delta_0(\\delta,R), \\\\       \n%     \\end{split}\n% \\end{equation}\n% where the last inequality uses $k \\leq 1/\\Delta_0(\\delta,R)$.\n% \\begin{align}%\\label{eqn:K0}\n% \\underline{k} =  \\frac{1}{ \\eta  \\left(\\Delta_0(\\delta,R) M_g+ L_{\\W} \\Delta(\\delta,R)\\right)}, \n% %\\qquad \\textnormal{implies that}  \\qquad  r_{\\tau} \\leq \\Delta(\\delta,R).\n% \\end{align} \n\n% \\begin{align}\\label{eqn:step:loc}\n% \\eta \\leq  \\min\\left(\\frac{1}{L_{\\W}}, \\zeta(\\mu)\\right),    \n% %\\eta \\leq \\mc{O}  \\left(\\min\\left(\\frac{1}{M_0 \\smax^4 +M_1\\smax^3}, \\zeta(\\mu)\\right),    \n% \\end{align}\n% satisfies $\\lim_{t\\rightarrow\\infty}\\tn{\\pb(t)}=\\infty$ and $\\lim_{t\\rightarrow\\infty}\\pb(t)/\\tn{\\pb(t)}=\\ps /\\tn{\\ps}$. {\\color{red}Here,  $\\zeta (\\mu)$ is a correlation  dependent parameter; see Eq. \\eqref{eqn:zeta:mu} in the Appendix.} \n% \\end{theorem} \n\n\\section{Local Convergence of Gradient Descent}\\label{app local proofs}\nTo provide a basis for discussing local convergence of GD, we establish a cone centered around $\\Wma$  using the following construction. For parameters $\\mu \\in (0,1)$ and $R>0$, we define $\\Cc_{\\mu,R}(\\Wma)$ as the set of matrices $\\W \\in\\R^{d\\times d}$ such that $\\tf{\\W}\\geq R$ and  the correlation coefficient between $\\W$ and $\\Wma$ is at least $1-\\mu$:\n% \n%\n\\begin{subequations}\n\\begin{align}\\label{eqn:coneofw:r}\n\\Sc_{\\mu}(\\Wma)&:= \\left\\{\\W\\in\\R^{d\\times d}~:~\\left\\langle\\frac{\\W}{\\tf{\\W}},\\frac{{\\Wma}}{\\tf{{\\Wma}}} \\right\\rangle \\geq 1-\\mu\\right\\}, \\\\\n\\Cc_{\\mu,R}({\\Wma})&:= \\Sc_{\\mu}(\\Wma) \\cap  \\left\\{\\W\\in\\R^{d\\times d}~:~\\tf{\\W}\\geq R\\right\\}.\n\\end{align}\n\\end{subequations}\n\\begin{lemma}%[Local Gradient Condition]\n\\label{local cond} \nSuppose Assumption~\\ref{assum:loss:prope} on the loss function $\\ell$ holds, and let $\\bal=(\\alpha_i)_{i=1}^n$ be locally optimal tokens according to Definition \\ref{def loc opt}. Let $ \\Wm= \\Wm_\\bal$ denote the SVM solution obtained via \\eqref{eqn:sattnsvm} by applying the Frobenius norm and replacing $(\\opt_i)_{i=1}^n$ with $\\boldsymbol{\\alpha} = (\\alpha_i)_{i=1}^n$. %For parameters $\\mu >0$ and $R>0$, let\n%\n%\n% \\begin{align*}%\\label{eqn:coneofw:r}\n% \\Sc_{\\mu}(\\Wm)&= \\left\\{\\W\\in\\R^{d\\times d}~:~\\left\\langle\\frac{\\W}{\\tf{\\W}},\\frac{{\\Wm}}{\\tf{{\\Wm}}} \\right\\rangle \\geq 1-\\mu\\right\\}, \\\\\n% \\Cc_{\\mu,R}({\\Wm})&= \\Sc_{\\mu}(\\Wm) \\cap  \\left\\{\\W\\in\\R^{d\\times d}~:~\\tf{\\W}\\geq R\\right\\}.\n% \\end{align*}\nThere exists a scalar $\\mu=\\mu(\\bal)>0$ such that for sufficiently large $\\RR_\\mu$:\n\\begin{enumerate}[label={\\textnormal{\\textbf{L\\arabic*.}}}, wide, labelwidth=!,itemindent=!, labelindent=5pt]\n\\item \\label{lem:cond:l1} There is no stationary point within  $ \\Cc_{\\mu,\\RR_\\mu} (\\Wm)$.\n%, where $\\Sc_{\\mu}(\\Wm)$ is defined in \\eqref{eqn:def:cone}. \n\\item\\label{lem:cond:l2} For all $\\V\\in \\Sc_{\\mu}(\\Wm)$ with $\\tf{\\V}=\\tf{\\Wm}$  and $\\W\\in\\Cc_{\\mu,\\RR_\\mu}(\\Wm)$, there exist dataset dependent constants $C,c>0$ such that %\\redp{Where is the $\\mu$ below (right hand side) coming from?}\n\\begin{subequations}\\label{local:g:lbound}\n\\begin{align}\n&C\\cdot \\frac{1}{n}\\sum_{i=1}^n \\left(1-\\s_{i\\alpha_i}\\right) \\geq -\\Big\\langle\\nabla\\Lc(\\W),\\V \\Big\\rangle\\geq c\\cdot  \\frac{1}{n} \\sum_{i=1}^n  \\left(1-\\s_{i\\alpha_i}\\right)>0, \\label{local1:g:bound} \\\\\n&\\tf{\\nabla\\Lc(\\W)}\\leq \\bar{A}C \\cdot \\frac{1}{n} \\sum_{i=1}^n  \\left(1-\\s_{i\\alpha_i}\\right), \\label{local2:g:bound}\\\\\n& -\\li\\frac{\\V}{\\tf{\\V}},\\frac{\\nabla\\Lc(\\W)}{\\tf{\\nabla\\Lc(\\W)}}\\ri \\geq  \\frac{c}{C} \\cdot \\frac{\\Theta}{\\bar{A}}>0. \\label{local3:g:bound}\n\\end{align}\n\\end{subequations}\nHere, $\\s_{i\\alpha_i}= (\\sft{\\X_i\\W \\z_{i}})_{\\alpha_i}$, $\\bar{A}=\\max_{i\\in[n],t,\\tau\\in[T]}\\tn{(\\x_{it}- \\x_{i\\tau})}~\\tn{\\z_i}$, and $\\Theta=1/\\tf{\\Ws}$.\n%where $C = \\frac{2}{n}\\sum_{i\\in [n]} \\ell'_i \\cdot \\max_{t\\in\\Tc_i}Y_i\\cdot(\\x_{i\\alpha_i}-\\x_{it})^\\top\\vb$ and c=$\\frac{1}{8n}\\sum_{i\\in [n]} \\ell'_i\\cdot \\bgg_i$.\n% \\item  \\label{lem:cond:l3}   For any choice of $\\pi>0$, there exists $R:=R_\\pi$ such that, for any $ \\W\\in \\Cc_{\\mu,R}(\\Wm)$ we have\n% \\[\n% \\li\\nabla\\Lc(\\W), \\frac{\\W}{\\tf{\\W}_F} \\ri\\geq (1+\\pi)\\li\\nabla\\Lc(\\W), \\frac{\\Wm}{\\tf{\\Wm}}\\ri.\n% \\]\n\\end{enumerate}\n\\end{lemma}\n\n% Define $\\Sc_{\\mu}(\\Wm)$ to be the set of vectors obeying $\\corr{\\W,\\Wm}\\geq 1-\\mu$. \n% we have that% as follows: % and $\\tn{\\W}\\geq R$. \n% \\begin{itemize}\n%   \\item There is no stationary point within $\\Sc_{\\mu}(\\Wm)\\bigcap \\{\\W\\bgl\\tn{\\W}\\geq R\\}$, where $\\Sc_{\\mu}(\\Wm)$ is defined in \\eqref{eqn:def:cone}. \n%   %Additionally, for any $\\W\\in \\Cc_{\\mu,R}(\\Wm)$, we have $\\W^\\top \\nabla\\Lc(\\W)<0$.\n%   \\item Let $q_i=1-\\sft{\\Kb_i\\W}_{\\alpha_i}$, $\\ell'_i=\\ell'(Y_i\\cdot \\vb^\\top \\X_i^\\top\\sft{\\Kb_i\\W})<0$, $\\bgg_i=\\min_{t\\in\\Tc_i}Y_i\\cdot(\\x_{i\\alpha_i}-\\x_{it})^\\top\\vb$, and $\\bgm_i=\\max_{t\\in\\Tc_i}Y_i\\cdot(\\x_{i\\alpha_i}-\\x_{it})^\\top\\vb$. For all $\\hb,\\W\\in \\Sc_{\\mu}(\\Wm)$ with $\\tn{\\hb}=\\tn{\\Wm}$, we have%the gradient correlation obeys\n%   \\begin{align}\\label{ps corr}\n%   \\frac{2}{n}\\sum_{i\\in [n]} \\ell'_i\\cdot q_i\\cdot \\bgm_i\\geq \\li\\nabla\\Lc(\\W),\\hb\\ri\\geq \\frac{1}{8n}\\sum_{i\\in [n]} \\ell'_i\\cdot q_i\\cdot \\bgg_i.\n%   \\end{align}\n%   Note that above $-\\ell'_i$ and $\\bgg_i,\\bgm_i$ are upper/lower bounded by positive dataset-dependent constants. The only term that can vanish (as $\\tn{\\W}\\rightarrow\\infty$) is $q_i$. Consequently, there exists constants $C,c>0$ such that,% for all $\\W\\in \\Cc_{\\mu,R}(\\Wm)$,\n%   \\begin{align}\n%   C\\cdot\\max_{i\\in[n]}q_i \\geq -\\li\\nabla\\Lc(\\W),\\hb\\ri\\geq c\\cdot\\min_{i\\in[n]}q_i>0.\\label{local simplified bound}\n%   \\end{align}\n% Note that, the identical bound holds by setting $\\hb=\\Wm$ or $\\hb=\\tn{\\Wm}\\W/\\tn{\\W}$.\n%   \\item Denote $\\Wb=\\tn{\\Wm}\\W/\\tn{\\W}$. For any $\\pi>0$, there exists $R:=R_\\pi$ such that all $\\W\\in \\Sc_{\\mu}(\\Wm)$ with $\\tn{\\W}\\geq R$ obeys\n% \\[\n% \\li\\nabla\\Lc(\\W),\\Wb\\ri\\geq (1+\\pi)\\li\\nabla\\Lc(\\W),\\Wm\\ri.\n% \\]\n% \\end{itemize}\n\n\n% Theorem \\ref{conv:gd:global} on the global convergence of gradient descent serves as a prelude to the general behavior of the optimization. Once we relax Assumption \\ref{assum:opt:token} by allowing for arbitrary token scores, we will show that $\\W$ can converge (in direction) to a locally-optimal solution. However, this locally-optimal solution is still characterized in terms of \\eqref{attnsvm} which separates \\emph{locally-optimal} tokens from the rest. Our theory builds on two new concepts: locally-optimal tokens and neighbors of these tokens. \n% is a special case of the general optimization landscape\n% direction still selects locallIn this section, we discuss the local convergence of gradient descent methods towards the max margin solution in the context of Attention SVM.\n% \\begin{theorem}\\label{local det thm} Fix a vector $\\vb\\in\\R^d$. Define scores $\\bgam_i=\\X_i\\vb$. Suppose gradient descent on $\\W$ achieves a limit $\\W^\\star=\\W_t/\\tn{\\W_t}$. Suppose $\\lim_{t\\rightarrow\\infty}\\sft{\\Kb_i\\W_t}$ are one-hot vectors (selects only one token). Then, there exists indices $(\\alpha_i)_{i=1}^n$ such that, $\\W^\\star=\\Wm/\\tn{\\Wm}$ where $\\Wm$ is the SVM solution\n% \\begin{align}\n% \\Wm=\\arg\\min_{\\W\\in\\R^d} \\tn{\\W}\\quad\\text{subject to}\\quad \\W^\\top (\\kb_{i\\alpha_i}-\\kb_{it})\\geq 1\\label{svm-local}.\n% \\end{align}\n% Additionally, in this SVM, all support vectors $\\x_{it}$ obeying $\\W^\\top (\\x_{i\\alpha_i}-\\x_{it})=1$ satisfy $\\bgam_{it}<\\bgam_{i\\alpha_i}$.\n% \\end{theorem}\n% \\begin{assumption} [\\neis have same score]\\label{assum:regular} Fix token indices $\\bal=(\\alpha_i)_{i=1}^n$ and let $\\Tc_i\\subset[T]$ be their \\neis for $i\\in[n]$. For all $i\\in[n]$ and $t_1,t_2\\in\\Tc_i$, the scores are same, i.e., $\\x_{it_1}^\\top\\vb=\\x_{it_1}^\\top \\vb$.\n% % Relaxing this assumption is tricky because there are $\\W$ directions with gradients subpar correlations with $\\Wm$. The reason is that $\\W$ can select the lowest score neighbor to get high-corr whereas $\\Wm$ will be $\\Omega(1)$ less corr.\n% \\end{assumption}\n%\n\n%\\ct{off-support tokens} \\ct{SVM support-tokens}\n\\begin{proof}\nLet $R=\\RR_\\mu$, $(\\Tc_i)_{i=1}^n$ be the set of all \\neis per Definition \\ref{def loc opt}. Let $\\Tcb_i=[T]-\\Tc_i-\\{\\alpha_i\\}$ be the non-\\neis. Let\n\\begin{equation}\\label{mu choice}\n\\begin{split}\n&\\Theta=1/\\tf{\\Wm},\\\\\n&\\delta= \\frac{1}{2}\\min_{i\\in[n]}\\min_{t\\in\\Tc_i,\\tau\\in\\Tcb_i}(\\x_{it}-\\x_{i\\tau})^\\top \\Wm \\z_{i},\\\\\n&A=\\max_{i\\in[n],t\\in[T]} \\frac{\\tf{\\x_{it} \\z_i^\\top}}{\\Theta},\\\\\n& \\mu\\leq \\mu(\\delta)=\\frac{1}{8}\\left(\\frac{\\min(0.5,\\delta)}{A}\\right)^2.\n\\end{split}\n\\end{equation}\n%\\ct{}\n%\nSince $\\Wm$ is the max-margin model ensuring $(\\x_{i\\alpha_i}-\\x_{it})^\\top\\Wm \\z_i\\geq 1$, the following inequalities hold for all $\\W\\in \\cone_\\mu(\\Wm),~\\tf{\\W}=\\tf{\\Wm}$ and all $i\\in[n], t\\in\\Tc_i,\\tau\\in\\Tcb_i$:\n\\begin{equation}\\label{cone-non-nei}\n\\begin{split}\n(\\x_{it}-\\x_{i\\tau})^\\top \\W \\z_i&\\geq \\delta>0,\\\\%\\quad\\text{for all}\\quad \\\\\n(\\x_{i\\alpha_i}-\\x_{i\\tau})^\\top \\W \\z_i&\\geq 1+\\delta,\\\\\n\\frac{3}{2}\\geq(\\x_{i\\alpha_i}-\\x_{it})^\\top \\W \\z_i &\\geq \\frac{1}{2}.\n\\end{split}\n\\end{equation}\nHere, we used $\\tf{\\W-\\Wm}^2/\\tf{\\Wm}^2\\leq 2\\mu$ which implies $\\tf{\\W-\\Wm}\\leq \\sqrt{2\\mu}/\\Theta$.\n\n\n%\\noindent \\ref{lem:cond:l1}\n%Now that the choice of local cone is determined, we need to prove the main claims. We will lower bound $-\\hb^\\top \\nabla \\Lc(\\W)$ and establish its strict positivity for $\\tn{\\W}\\geq R$. This will show that there is no stationary point as a by product.\n%Given any $\\W\\in \\Cc_{\\mu,R}(\\Wm)$, denote $\\Wb=(\\tf{\\Wm}/\\tf{\\W})\\W$ and recall $\\tf{\\V}=\\tf{\\Wm}$. \nTo proceed, we write the gradient correlation following \\eqref{grad def} and \\eqref{grad def2}\n\\begin{align}\\label{grad def3}\n\\li\\nabla\\Lc(\\W),\\V\\ri&=\\frac{1}{n}\\sum_{i=1}^n\\ell'_i\\cdot\\hb_i^\\top\\sfp{\\hp_i}\\bgam_i,\n\\end{align}\nwhere we denoted $\\ell'_i=\\ell'(Y_i\\cdot \\vb^\\top \\X_i^\\top\\sft{\\hp_i})$, $\\hb_i=\\X_i\\V \\z_{i}$, $\\hp_i= \\X_i\\W \\z_{i}$, and $\\s_i=\\sft{\\hp_i}$.  \n\nUsing \\eqref{cone-non-nei}, for all $t\\in\\Tc_i,\\tau\\in \\Tcb_i$, for all $\\W\\in \\Cc_{\\mu,R}(\\Wm)$, we have that\n\\begin{align*}\n&\\hp_{it}-\\hp_{i\\tau}\\geq R\\Theta\\delta,\\\\\n&\\hp_{i\\alpha_i}-\\hp_{i\\tau}\\geq R\\Theta(1+\\delta),\\\\\n&\\hp_{i\\alpha_i}-\\hp_{it}\\geq R\\Theta/2.    \n\\end{align*}\nConsequently, we can bound the softmax probabilities $\\s_i=\\sft{\\hp_i}$ over non-\\neis as follows: For all $i\\in[n]$ and any $t_i\\in \\Tc_i$\n\\begin{subequations}\n\\begin{align}\\label{soft prob bound}\n&S_i:=\\sum_{\\tau\\in\\Tc_i}\\s_{i\\tau} \n%\\leq \\sum_{\\tau\\neq \\alpha_i}\\s_{i\\tau}\n\\leq T e^{-R\\Theta/2}\\s_{i\\alpha_i}\\leq T e^{-R\\Theta/2},\\\\\n&Q_i:=\\sum_{\\tau\\in\\Tcb_i}\\s_{i\\tau} \\leq T e^{-R\\Theta\\delta}\\s_{it_i}\\leq T e^{-R\\Theta\\delta}S_i.\n\\end{align}\n\\end{subequations}\n%(over all $i,t$) \nRecall scores $\\bgam_{it}=Y_i\\cdot\\vb^\\top \\x_{it}$. Define the score gaps over \\neis:\n\\begin{equation*}\n \\bgg_i=\\bgam_{i\\alpha_i}-\\max_{t\\in\\Tc_i}\\bgam_{it}~~~ \\textnormal{and}~~~ \\bgm_i=\\bgam_{i\\alpha_i}-\\min_{t\\in\\Tc_i}\\bgam_{it}. \n\\end{equation*}\nIt follows from \\eqref{mu choice} that \n\\begin{align*}\n&A=\\max_{i\\in[n],t\\in[T]} \\frac{\\tf{\\x_{it} \\z_i^\\top}}{\\Theta}\\geq \\max_{i\\in[n],t\\in[T]}\\tn{\\hb_{it}}.%=\\tn{\\kb_{it}\\hb}  \n\\end{align*}\nDefine the $\\bal$-dependent global scalar $\\Gamma=\\sup_{i\\in[n],t,\\tau\\in[T]}|\\bgam_{it}-\\bgam_{i\\tau}|$.\n\n\nLet us focus on a fixed datapoint $i\\in[n]$, assume (without losing generality) $\\alpha_i=1$, and drop subscripts $i$.\n%, that is,~$\\alpha:=\\alpha_i$, $\\X:=\\X_i$, $Y:=Y_i$, $\\Kb:=\\Kb_i$, $\\hp=\\Kb\\pb$, $\\hb=\\Kb\\V$, $\\s=\\sft{\\Kb\\pb}$, $\\bgam=Y\\cdot\\X\\vb$, and $\\bgg:=\\bgg_i$.\nDirectly applying Lemma \\ref{lem:q_reduce}, we obtain\n\\[\n  \\big|\\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam-\\sum_{t\\geq 2}^T (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2\\Gamma A(1-\\s_1)^2.\n\\]\nTo proceed, let us decouple the non-\\neis within $\\sum_{t\\geq 2}^T (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)$ via\n\\[\n\\big|\\sum_{t\\in\\Tcb} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2Q\\Gamma A.\n\\]\nAggregating these, we found\n\\begin{align}\n  \\big|\\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam-\\sum_{t\\in \\Tc} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2\\Gamma A((1-\\s_1)^2+Q).\\label{aggregate}\n\\end{align}\n%\\noindent \\ref{lem:cond:l2} \nTo proceed, let us upper/lower bound the gradient correlation.  We use two bounds depending on $\\V\\in\\Sc_{\\mu}(\\Ws)$ (\\textbf{Case 1}) or general $\\V\\in\\R^{d\\times d}$ (\\textbf{Case 2}).\n\n%\\red{\n\\noindent$\\bullet$ \\textbf{Case 1:  $\\V\\in\\Sc_{\\mu}(\\Ws)$.} Since $1.5\\geq \\hb_1-\\hb_t\\geq 0.5$ following \\eqref{cone-non-nei}, we find\n\\[\n 1.5\\cdot S\\cdot \\bgm  \\geq\\sum_{t\\in \\Tc} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\geq 0.5\\cdot S\\cdot \\bgg,\n\\]\nwhere recall the definition of $S$ (having dropped subscripts) in \\eqref{soft prob bound}. \n\n\\noindent$\\bullet$ \\textbf{Case 2: $\\Vb\\in\\R^{d\\times d}$ and $\\tf{\\V}=\\tf{\\Wm}$.}  Define $\\bar{A}=\\max_{i\\in[n],t,\\tau\\in[T]}\\tn{\\x_{it}-\\x_{i\\tau}}~\\tn{\\z_i}$. For any $\\tf{\\V}=\\tn{\\Ws}$, we use the fact that\n$$\\tn{\\hb_1-\\hb_t}\\leq \\tf{(\\x_{it}-\\x_{i\\tau}) \\z_i^\\top}\\cdot\\tf{\\V}\\leq \\frac{\\bar{A}}{\\Theta}.$$\nNote that by definition $ \\frac{\\bar{A}}{\\Theta} \\geq 1$. To proceed, we can upper bound\n\\begin{align}\n\\frac{\\bar{A}}{\\Theta}\\cdot S\\cdot \\bgm  \\geq\\sum_{t\\in \\Tc} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t).\\label{wishwish2}\n\\end{align}\n\nNext we claim that for both cases, $S$ dominates $((1-\\s_1)^2+Q)$ for large $R$. Specifically, we wish for \n\\begin{align}\\label{wishfor}\n\\frac{S\\cdot \\bgg}{4}\\geq 4\\Gamma A\\max((1-\\s_1)^2,Q)\\iff S\\geq 16\\frac{\\Gamma A}{\\bgg}\\max((1-\\s_1)^2,Q).\n\\end{align}\nNow choose $R\\geq \\delta^{-1}\\log(T)/\\Theta$  to ensure $Q\\leq S$ since $Q\\leq Te^{-R\\Theta\\delta}S$ from \\eqref{soft prob bound}. Consequently\n\\[\n(1-\\s_1)^2=(Q+S)^2\\leq 4S^2\\leq 4STe^{-R\\Theta/2}.\n\\]\nCombining these, what we wish is ensured by guaranteeing\n\\begin{align}\\label{s bound}\n  S\\geq 16\\frac{\\Gamma A}{\\bgg}\\max(4STe^{-R\\Theta/2},Te^{-R\\Theta\\delta}S).\n\\end{align}\nThis in turn is ensured for all inputs $i\\in[n]$ by choosing \n\\begin{align}\\label{R bound}\nR\\geq \\frac{\\max(2,\\delta^{-1})}{\\Theta}\\log\\left(\\frac{64T\\Gamma A}{\\bggm}\\right),\n\\end{align}\nwhere $\\bggm=\\min_{i\\in[n]}\\bgg_i$ is the global scalar which is the worst case score gap over all inputs. \n\\\\\n$\\bullet$ \\textbf{Case 1: $\\V\\in\\Sc_{\\mu}(\\Ws)$}. With the above choice of $R$, we guaranteed\n\\[\n  2 (1-\\s_1)\\cdot \\bgm\\geq 2\\cdot S\\cdot \\bgm \\geq \\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam\\geq\\frac{S\\cdot \\bgg}{4}\\geq\\frac{(1-\\s_1) \\bgg}{8}.\n\\]\nvia \\eqref{wishfor} and \\eqref{aggregate}. \n\nSince this holds over all inputs, going back to the gradient correlation \\eqref{grad def3} and averaging above over all inputs $i\\in[n]$ and plugging back the indices $i$, we obtain the advertised bound %by setting $q_i=1-\\s_{i\\alpha_i}$ (where we set $\\alpha_i=1$ above without losing generality)\n\\begin{align}\\label{pbb corr}\n% \\frac{2}{n}\\sum_{i\\in [n]} -\\ell'_i\\cdot q_i\\cdot \\bgm_i\\geq -\\li\\nabla\\Lc(\\W),\\V\\ri\\geq \\frac{1}{8n}\\sum_{i\\in [n]} -\\ell'_i\\cdot q_i\\cdot \\bgg_i.\n\\frac{2}{n}\\sum_{i\\in [n]} -\\ell'_i\\cdot S_i\\cdot \\bgm_i\\geq -\\li\\nabla\\Lc(\\W),\\V\\ri\\geq \\frac{1}{8n}\\sum_{i\\in [n]} -\\ell'_i\\cdot S_i\\cdot \\bgg_i.\n\\end{align}\nLet $-\\ell'_{\\min/\\max}$ be the min/max values negative loss derivative admits over the ball $[-A,A]$ and note that $\\max_{i\\in[n]}\\bgm_i>0$ and $\\min_{i\\in[n]}\\bgg_i>0$ are dataset dependent constants. Then, we declare the constants $C=-2\\ell'_{\\max}\\cdot \\max_{i\\in[n]}\\bgm_i>0,c=-(1/8)\\ell'_{\\min}\\cdot \\min_{i\\in[n]}\\bgg_i>0$ to obtain the bound \\eqref{local1:g:bound}. \n\\vspace{.2cm}\n\\\\\n\\noindent$\\bullet$ \\textbf{Case 2: $\\Vb\\in\\R^{d\\times d}$ and $\\tf{\\V}=\\tf{\\Wm}$.} Next, we show \\eqref{local2:g:bound} and \\eqref{local3:g:bound}. For any $\\V \\in \\mathbb{R}^{d \\times d}$ satisfying $\\tf{\\V}=\\tf{\\Ws}$, using \\eqref{wishwish2} and the  choice of $R$ in \\eqref{R bound} similarly guarantees \n$$\n\\frac{2\\bar{A}}{\\Theta }(1-\\s_1) \\bgm\\geq \\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam,\n$$\nfor fixed input. Going back to the gradient correlation \\eqref{grad def3} and averaging above over all inputs $i\\in[n]$, with the same definition of $C>0$, we obtain\n\\begin{align}\n\\frac{ \\bar{A} C}{  \\Theta n}\\sum_{i\\in [n]} (1-\\s_{i\\alpha_i})\\geq -\\li\\nabla\\Lc(\\W),\\V\\ri.\\label{local lamma general upper}\n\\end{align}\nTo proceed, since \\eqref{local lamma general upper} holds for any $\\V\\in\\R^{d\\times d}$, we observe that when setting $\\V=\\frac{\\tf{\\Ws}}{\\tf{\\nabla\\Lc(\\W)}}\\cdot \\nabla\\Lc(\\W)$, this implies that\n\\[ \n\\li\\nabla\\Lc(\\W),\\V\\ri = \\tf{\\nabla\\Lc(\\W)}\\cdot \\tf{\\Ws}\\leq \\frac{\\bar{A} C}{\\Theta \n n}\\sum_{i\\in [n]} (1-\\s_{i\\alpha_i}).\n\\]\nSimplifying $\\Theta=1/\\tf{\\Ws}$ on both sides gives \\eqref{local2:g:bound}. \n\\\\\nCombining the above inequality with \\eqref{pbb corr}, we obtain that for all $\\V,\\W\\in\\Sc_{\\mu}(\\Ws)$\n\\[ \n-\\li\\frac{\\V}{\\tf{\\V}},\\frac{\\nabla\\Lc(\\W)}{\\tf{\\nabla\\Lc(\\W)}}\\ri\\geq \\frac{c \\Theta }{C\\bar{A}},\n\\]\nwhich gives \\eqref{local3:g:bound}.\n\n\n\n%\\ct{To conclude the $c$ lower bound, need a lower bound on $\\max_{i\\in[n]}-\\ell'_i$?}\n\\end{proof}\n\n\\begin{lemma}%[Local Gradient Condition]\n\\label{lem:local:corr} \nSuppose Assumption~\\ref{assum:loss:prope} on the loss function $\\ell$ holds, and let $\\bal=(\\alpha_i)_{i=1}^n$ be locally optimal tokens according to Definition \\ref{def loc opt}. Let $ \\Wm= \\Wm_\\bal$ denote the SVM solution obtained via \\eqref{eqn:sattnsvm} by replacing $(\\opt_i)_{i=1}^n$ with $\\boldsymbol{\\alpha} = (\\alpha_i)_{i=1}^n$. Let $\\mu=\\mu(\\bal)>0$ and $\\bar{R}_{\\mu}$ be defined as in Lemma~\\ref{local cond}. For any choice of $\\pi>0$, there exists $R_\\pi \\geq \\bar{R}_{\\mu}$ such that, for any $ \\W\\in \\Cc_{\\mu,R_\\pi}(\\Wm)$, we have\n\\[\n \\li \\nabla\\Lc(\\W), \\frac{\\W}{\\tf{\\W}} \\ri\\geq (1+\\pi)\\li \\nabla\\Lc(\\W), \\frac{\\Wm}{\\tf{\\Wm}}\\ri.\n\\]\n\\end{lemma}\n\\begin{proof}\nLet  $R=R_{\\pi}$, $\\Wb=\\tf{\\Wm} \\W/\\tf{\\W} $, $\\hb_i=\\X_i\\Wb \\z_{i}$, and $\\hbm_i= \\X_i \\Wm \\z_{i}$.   To establish the result, we will prove that, for sufficiently large $R$ and for any $\\W\\in \\Cc_{\\mu,R}(\\Wm)$:\n\\begin{align}\\label{main local cond}\n\\nonumber \n\\li -\\nabla\\Lc(\\W),\\frac{\\W}{\\tf{\\W}}\\ri&= -\\frac{1}{n}\\sum_{i=1}^n\\ell'_i \\cdot  \\li \\hb_i, \\sfp{\\X_i\\W \\z_{i}}\\bgam_i\\ri\\\\\n&\\leq - \\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot  \\li\\hbm_i, \\sfp{\\X_i\\W \\z_{i}}\\bgam_i\\ri=(1+\\pi)\\li-\\nabla\\Lc(\\W), \\frac{\\ps}{\\tf{\\Ws}}\\ri.\n\\end{align}\n\nFollowing \\eqref{aggregate}, for all $\\W\\in \\Sc_{\\mu}(\\Wm)$ with $\\tf{\\W}=\\tf{\\Wm}$, $\\hp=\\X\\W \\z$, and $\\s=\\sft{\\hp}$, we have found\n\\begin{align}\n  \\big|\\hp^\\top_i\\diag{\\s_i}\\bgam_i-\\hp^\\top_i\\s_i\\s^\\top_i\\bgam_i-\\sum_{t\\in \\Tc_i} (\\hp_{i1}-\\hp_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\big|\\leq 2\\Gamma A((1-\\s_{i1})^2+Q_i), %\\label{aggregate}\n\\end{align}\nwhere $\\Tc_i$ is the set of support indices. \n\n\nPlugging in $\\hb,\\hbm$ in the bound above and assuming $\\pi\\leq 1$ (w.l.o.g.), \\eqref{main local cond} is implied by the following stronger inequality\n\\begin{align*}\n-\\frac{1}{n}&\\sum_{i=1}^n\\ell'_i \\cdot \\left(6\\Gamma A((1-\\s_{i1})^2+Q_i)+ \\sum_{t\\in \\Tc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it}) \\right)\\\\\n&\\leq -\\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i  \\cdot \\sum_{t\\in \\Tc_i} (\\hbm_{i1}-\\hbm_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n&=-\\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align*}\nFirst, we claim that $0.5\\pi\\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\geq 6\\Gamma A((1-\\s_{i1})^2+Q_i)$ for all $i \\in [n]$.  The proof of this claim directly follows the earlier argument, namely, following \\eqref{wishfor}, \\eqref{s bound}, and \\eqref{R bound}  which leads to the choice \n\\begin{equation}\\label{R boundC0}\nR \\ge\\frac{\\max(2,\\delta^{-1})}{\\Theta}\\log\\left(\\frac{C_0\\cdot T\\Gamma A}{\\pi\\bggm}\\right),    \n\\end{equation}\nfor some constant $C_0>0$. Using \\eqref{R bound}, we choose $C_0 \\geq 64 \\pi$ to guarantee $R=R_\\pi \\geq \\bar{R}_{\\mu}$.\n\nFollowing this control over the perturbation term $6\\Gamma A((1-\\s_{i1})^2+Q_i)$, to conclude with the result, what remains is proving the comparison\n\\begin{align}\\label{desired comp}\n-\\frac{1}{n} \\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\in \\Tc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq - \\frac{1+0.5\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align}\nTo proceed, we split the problem into two scenarios. \n\n\\noindent\\textbf{Scenario 1:} $\\tf{\\Wb-\\Wm}\\leq \\eps=\\frac{\\pi}{4A\\Theta}$ for some $\\eps>0$.  In this scenario, for any $ t\\in \\Tc_i$ and $i\\in [n ]$, we have\n\\[\n|\\hb_{it}-\\hbm_{it}|=|\\x_{it}^\\top (\\Wb-\\Wm)  \\z_{it}|\\leq A\\Theta\\eps=\\frac{\\pi}{4}.\n\\]\nConsequently, we obtain \n\\[\n\\hb_{i1}-\\hb_{it}\\leq \\hbm_{i1}-\\hbm_{it}+2A\\Theta\\eps= 1+0.5\\pi.\n\\] \nSimilarly, $\\hb_{i1}-\\hb_{it}\\geq 1-0.5\\pi\\geq 0.5$. Since all terms $\\hb_{i1}-\\hb_{it},\\s_{it},\\bgam_{i1}-\\bgam_{it}$ in \\eqref{desired comp} are nonnegative and $(\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq (1+0.5\\pi)\\s_{it}(\\bgam_{i1}-\\bgam_{it})$, above implies the desired result in \\eqref{desired comp}.\n\n\\vspace{3pt}\n\\noindent\\textbf{Scenario 2:} $\\tf{\\Wb-\\Wm}\\geq \\eps=\\frac{\\pi}{4A\\Theta}$.  Since $\\Wb$ is not (locally) max-margin, in this scenario, for some $i \\in  [n]$, $\\nu=\\nu(\\eps)>0$, and $\\tau\\in\\Tc_i$, we have that\n\\begin{align*}\n\\hb_{i1}-\\hb_{i\\tau}\\leq 1-2\\nu.\n\\end{align*}\nHere $\\tau=\\arg\\max_{\\tau\\in\\Tc_i} \\x_{i\\tau}\\Wb \\z_i$ denotes the nearest point to $\\hb_{i1}$ (along the $\\Wb$ direction). Note that a non-neighbor $t\\in\\Tcb_i$ cannot be nearest because $\\Wb\\in \\cone_{\\mu}(\\ps)$ and \\eqref{cone-non-nei} holds. Recall that $\\s_i=\\sft{\\RR\\hb_i}$ where $\\RR=\\tf{\\W}\\Theta \\geq R\\Theta$. To proceed, let $ \\underline{\\hb}_i:=\\min_{t \\in\\mc{T}_i}\\hb_{i1}-\\hb_{it}$,\n\\begin{align*}\n\\mc{I}:=\\left\\{ i\\in[n]: \\underline{\\hb}_i \\leq 1-2\\nu \\right\\}, \\qquad [n]-\\mc{I}:=\\left\\{ i\\in[n]:  1-2\\nu  <  \\underline{\\hb}_i\\right\\}.\n%\\\\\n%\\mc{I}_3&=\\left\\{ i\\in[n]:  1 +\\frac{\\pi}{8} < \\underline{\\hb}_i\\right\\},\n\\end{align*}\nFor all $ i \\in [n]-\\mc{I}$,\n\\begin{equation}\\label{eqn:grad:difff2}\n\\begin{split}\n      \\sum_{t\\in \\Tc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it}) &- (1+0.5\\pi) \\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n      & \\leq  \\left(2A - (1+0.5\\pi)\\right)\\Gamma\\sum_{t\\in \\Tc_i,~\\hb_{i1}-\\hb_{it} \\geq 1+\\frac{\\pi}{2} } \\s_{it} \\\\\n      & \\leq  \\left(2A - (1+0.5\\pi)\\right)\\Gamma Te^{-\\RR(1+\\frac{\\pi}{2})} \\\\\n      &\\leq   2A\\Gamma  T e^{-\\RR(1+\\frac{\\pi}{2})}.\n      % & \\leq  \\left(2A\\Gamma - (1+0.5\\pi)\\bggm \\right)\\sum_{t\\in \\Tc_i,~\\hb_{i1}-\\hb_{it} \\geq 1+\\frac{\\pi}{2} } \\s_{it} \\\\\n      % &\\leq   \\left(2A\\Gamma - (1+0.5\\pi) \\bggm\\right)  T \\Gamma e^{-\\RR(1+\\frac{\\pi}{2})}\\\\\n      % &\\leq   2A\\Gamma^2  T e^{-\\RR(1+\\frac{\\pi}{2})}.    \n\\end{split}\n\\end{equation}\n% For all $ i \\in \\mc{I}_2$,  split the tokens into two groups: Let $\\Nc_i$ be the group of tokens obeying $\\Wb^\\top (\\kb_{i1}-\\kb_{it})\\leq 1 +\\frac{\\pi}{4}$ and $\\Tc_i-\\Nc_i$ be the rest of the tokens.  Observe that\n% \\[\n% \\frac{\\sum_{t\\in \\Tc_i-\\Nc}\\s_{it}}{\\sum_{t\\in\\Tc_i}\\s_{it}}\\leq  T\\frac{e^{\\frac{-\\pi}{4} \\RR}}{e^{\\frac{-\\pi}{8}\\RR}}=Te^{-\\RR\\frac{\\pi}{8}}.\n% \\]\n% Thus, using $|\\hb_{i1}-\\hb_{it}|\\leq 2A$ and recalling the definition of $\\bgg$, observe that \n% \\[\n% \\sum_{t\\in\\Tc_i-\\Nc} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq \\frac{2\\Gamma A Te^{-\\RR\\pi/8}}{\\bgg} \\sum_{t\\in \\Tc_i} \\s_{it}(\\bgam_{i1}-\\bgam_{it}).%\\leq \\frac{3A\\Gamma Te^{-\\RR\\nu}}{\\bgg} \\sum_{t\\in \\Nc} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n% \\]\n\n% \\begin{align*}\n%   \\sum_{t\\in \\Tc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})&= \\sum_{t\\in \\Nc} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})+\\sum_{t\\in\\Tc_i-\\Nc} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\nonumber\\\\\n% %  &\\leq \\sum_{t\\in \\Nc} (1-\\nu)\\s_{it}(\\bgam_{i1}-\\bgam_{it})+\\sum_{t\\in\\Tc_i-\\Nc} 2A\\Gamma Te^{-\\RR\\nu}\\nonumber\\\\\n%   &\\leq \\sum_{t\\in \\Nc} (1+\\pi/4)\\s_{it}(\\bgam_{i1}-\\bgam_{it})+\\frac{2\\Gamma A Te^{-\\RR\\pi/8}}{\\bgg} \\sum_{t\\in \\Tc_i} \\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n%   &\\leq \\left(1+\\frac{\\pi}{4}+\\frac{2\\Gamma A Te^{-\\RR\\pi/8}}{\\bgg}\\right)\\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\n% \\end{align*}\n\nFor all $ i \\in \\mc{I}$, split the tokens into two groups: Let $\\Nc_i$ be the group of tokens obeying $ \\hb_{i1}-\\hb_{it} \\leq 1-\\nu$ and $\\Tc_i-\\Nc_i$ be the rest of the neighbors. Observe that\n\\[\n\\frac{\\sum_{t\\in \\Tc_i-\\Nc_i}\\s_{it}}{\\sum_{t\\in\\Tc_i}\\s_{it}}\\leq  T\\frac{e^{\\nu \\RR}}{e^{2\\nu\\RR}}=Te^{-\\RR\\nu}.\n\\]\nUsing $|\\hb_{i1}-\\hb_{it}|\\leq 2A=2 \\max_{i\\in[n],t\\in[T]}\\tn{\\kb_{it}}/\\Theta$ and  $\\bggm=\\min_{i\\in[n]}\\bgg_i =\\min_{i\\in[n]} (\\bgam_{i1}-\\max_{t\\in\\Tc_i}\\bgam_{it})$, observe that \n\\[\n\\sum_{t\\in\\Tc_i-\\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq \\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm} \\sum_{t\\in \\Tc_i} \\s_{it}(\\bgam_{i1}-\\bgam_{it}).%\\leq \\frac{3A\\Gamma Te^{-\\RR\\nu}}{\\bgg} \\sum_{t\\in \\Nc} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\]\nThus, \n\\begin{align*}\n  \\sum_{t\\in \\Tc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})&= \\sum_{t\\in \\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})+\\sum_{t\\in\\Tc_i-\\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\nonumber\\\\\n%  &\\leq \\sum_{t\\in \\Nc} (1-\\nu)\\s_{it}(\\bgam_{i1}-\\bgam_{it})+\\sum_{t\\in\\Tc_i-\\Nc} 2A\\Gamma Te^{-\\RR\\nu}\\nonumber\\\\\n  &\\leq \\sum_{t\\in \\Nc_i} (1-\\nu)\\s_{it}(\\bgam_{i1}-\\bgam_{it})+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm} \\sum_{t\\in \\Tc_i} \\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n  &\\leq \\left(1-\\nu+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}\\right)\\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n &\\leq \\left(1+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}\\right)\\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align*}\nHence, choosing \n\\begin{align}\nR\\geq\\frac{1}{\\nu\\Theta}\\log\\left(\\frac{8\\Gamma AT}{\\bggm\\pi}\\right)\\label{R bound pi 1}\n\\end{align}\nresults in that\n\\begin{equation}\\label{eqn:grad:difff3}\n    \\begin{split}\n     &\\sum_{t\\in \\Tc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})  - (1+\\frac{\\pi}{2}) \\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}) \\\\\n   &\\leq\\left(\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}-\\frac{\\pi}{2}\\right)\\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n   &\\leq -\\frac{\\pi}{4}\\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n   &\\leq-\\frac{\\pi}{4T}\\bggm  e^{-\\bar{R} (1-2\\nu)}.      \n    \\end{split}\n\\end{equation}\nHere, the last inequality follows from the fact that $\\sum_{t\\in \\Tc_i}\\s_{it}\\geq \\max_{t\\in\\Tc_i}s_{it}\\geq\\frac{e^{-\\bar{R}(1-2\\nu)}}{\\sum_{t=1}^Te^{-\\bar{R}(\\hb_{i1}-\\hb_{it})}}\\geq e^{-\\bar{R}(1-2\\nu)}/T$.\n\n%Consequently, the proof boils down to ensuring the perturbation term $\\frac{2\\Gamma A Te^{-R\\Theta\\nu}}{\\bgg}\\leq 0.5\\pi$. \nFrom Assumption~\\ref{assum:loss:prope}, we have $c_{\\min}\\leq-\\ell'\\leq c_{\\max}$ for some positive constants $c_{\\min}$ and $c_{\\max}$. It follows from  \\eqref{eqn:grad:difff2} and \\eqref{eqn:grad:difff3} that \n\\begin{align*}\n-\\frac{1}{n}\\sum_{i}^n \\ell_i' \\cdot&\\left(\n      \\sum_{t\\in \\Tc_i} (\\hb_{i1}-_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})- \\sum_{t\\in \\Tc_i} (1+0.5\\pi)\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\right)\\\\\n      & \\leq    c_{\\max}2A\\Gamma  T \\Gamma e^{-\\RR(1 +\\frac{\\pi}{2})}-\\frac{c_{\\min}}{nT}\\cdot\\frac{\\pi\\bggm}{4}e^{-\\bar{R} (1-2\\nu)}\\\\\n      % & \\leq    c_{\\max}\\left(2A\\Gamma - (1+0.5\\pi) \\bggm\\right)  T \\Gamma e^{-\\RR(1 +\\frac{\\pi}{2})}-\\frac{c_{\\min}}{n}\\left(\\nu+0.5\\pi-\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}\\right)\\\\\n      & \\leq 0.\n\\end{align*}\nCombing with \\eqref{R bound pi 1}, this is guaranteed by %for all inputs $i\\in[n]$ by recalling $\\bggm=\\min_{i\\in[n]}\\bgg_i$ and \nchoosing \n% \\[\n%   R\\geq ... \\frac{1}{\\nu\\Theta}\\log\\left(\\frac{4\\Gamma AT}{\\bggm\\pi}\\right),\n% \\]\n\\[\n  R\\geq \\max\\left\\{\\frac{1}{\\nu\\Theta}\\log\\left(\\frac{8\\Gamma AT}{\\bggm\\pi}\\right),\\frac{1}{(2\\nu+\\pi/2)\\Theta}\\log\\left(\\frac{8n\\Gamma AT^2 c_{\\max}}{c_{\\min}\\bggm\\pi}\\right)\\right\\},\n\\]\nwhere $\\nu=\\nu(\\frac{\\pi}{4A\\Theta})$ depends only on $\\pi$ and global problem variables. \n\nCombining this with the prior $R$ choice \\eqref{R boundC0} (by taking maximum), we conclude with the statement.\n\n\n\\end{proof}\n\n\n\n\\subsection{Proof of Theorem~\\ref{thm:local:gd}}\nThe proof of this theorem follows the proof of \\cite[Theorem 3]{tarzanagh2023margin}. Let us denote the initialization lower bound as $R^0_\\mu:=R$, where $R$ is given in the Theorem~\\ref{thm:local:gd}'s statement. Consider an arbitrary value of  $\\epsilon \\in (0, \\mu/2)$ and let $1/(1+\\pi)=1-\\epsilon$. We additionally denote $R_\\eps\\gets R_\\pi\\vee 1/2$ where $R_\\pi$ was defined in Lemma~\\ref{lem:local:corr}.  At initialization $\\W(0)$, we set $\\eps=\\mu/2$ to obtain $R^0_\\mu= R_{\\mu/2}$, and provide the proof in four steps:\n\\\\\n\\textbf{Step~1: There are no stationary points within $\\Cc_{\\mu,R^0_\\mu}(\\Ws)$.} We begin by proving that there are no stationary points within  $\\Cc_{\\mu,R^0_\\mu}(\\Ws)$. Let $(\\Tc_i)_{i=1}^n$ denote  the sets of \\neis as defined in Definition \\ref{def loc opt}. We define $\\Tcb_i=[T]-\\Tc_i-\\{\\alpha_i\\}$ as the tokens that are non-\\neis. Additionally, let $\\mu$ be defined as in \\eqref{mu choice}. Then, since $R^0_\\mu\\geq \\RR_\\mu$ per Lemma \\ref{lem:local:corr}, we can apply Lemma~\\ref{local cond} to find that: For all $\\V,\\W\\in  \\Sc_{\\mu}(\\Ws)$ with $\\tf{\\W} \\neq 0$ and $\\tf{\\W} \\geq R^0_\\mu$,  we have that $- \\li\\V, \\nabla \\Lc(\\W)\\ri$ is strictly positive.\n\\\\\n\\textbf{Step~2:}  It follows from Lemma~\\ref{lem:local:corr} that, there exists $ R_\\epsilon\\geq \\bar{R}_\\mu\\vee 1/2$ such that all  $ \\W \\in \\Cc_{\\mu,R_\\epsilon}(\\Wm)$ satisfy%\\vspace{-2pt}\n\\begin{align}\\label{eqn:neg:corr:local}\n\\iprod{-\\nabla\\mc{L}(\\W)}      {\\frac{\\Wm}{\\tf{\\Wm}}} \\geq (1-\\epsilon)    \\iprod{-\\nabla \\mc{L}(\\W)}{\\frac{\\W}{\\tf{\\W}}}.\n\\end{align}\nThe argument below applies to a general $\\eps\\in(0,\\mu/2)$. However, at initialization $\\W(0)$, we set $\\eps=\\mu/2$ and, recalling above, initialization lower bound was defined as $R^0_\\mu:= R_{\\mu/2}$. To proceed, for any $\\epsilon \\in (0, \\mu/2)$, we will show that after gradient descent enters the conic set $\\Cc_{\\mu,R_\\eps}(\\Ws)$ for the first time, it will never leave the set. Let $t_\\eps$ be the first time gradient descent enters $\\Cc_{\\mu,R_\\eps}(\\Ws)$. In \\textbf{Step 4}, we will prove that such $t_\\eps$ is guaranteed to exist. Additionally, for $\\eps\\gets\\mu/2$, note that $t_\\eps=0$ i.e.~the point of initialization.\n%\n\\\\\n%\\textbf{Step 2: Updates remain inside the cone }.\n\\textbf{Step~3: Updates remain inside the cone $\\Cc_{\\mu,R_\\eps}(\\Ws)$.} \nBy leveraging the results from \\textbf{Step 1} and \\textbf{Step 2}, we demonstrate that the gradient iterates, with an appropriate constant step size, starting from $\\W(k_\\eps) \\in \\Cc_{\\mu,R_\\eps}(\\Ws)$, remain within this cone. \n\nWe proceed by induction. Suppose that the claim holds up to iteration $k \\geq k_\\eps$. This implies that $ \\W(k) \\in \\Cc_{\\mu,R_\\eps}(\\Ws)$. Hence, recalling cone definition, there exists scalar $\\mu=\\mu(\\bal) \\in (0,1)$  and $R $ such that  $\\tf{\\W(k)}\\geq R$, and\n\\begin{equation*}\n\\begin{split}\n\\left\\langle \\frac{\\W(k)}{\\tf{\\W(k)}},\\frac{\\Wm}{\\tf{\\Wm}} \\right\\rangle  \\geq 1-\\mu. \n\\end{split}\n\\end{equation*}\nFor all $k\\geq 1$, let\n\\begin{align}\\label{eqn:rho:def}\n\\rho (k) := - \\frac{1}{1-\\epsilon} \\iprod{\\nabla\\mc{L}(\\W(k))}\n     {\\frac{\\Wm}{\\tf{\\Wm}}}.\n\\end{align}\nNote that $\\rho (k) >0$ due to \\textbf{Step 1}. This together with the gradient descent update rule gives \n\\begin{subequations}\n\\begin{equation}\\label{eqn:localgd:1}\n\\begin{split}\n     \\left\\langle \\frac{\\W(k+1)}{\\tf{\\W(k)}},\\frac{\\Wm}{\\tf{\\Wm}} \\right\\rangle  &=    \\left\\langle \\frac{\\W(k)}{\\tf{\\W(k)}} -\\frac{\\eta}{\\tf{\\W(k)}}\\nabla \\mc{L}(\\W(k)), \\frac{\\Wm}{\\tf{\\Wm}} \\right\\rangle\\\\\n      &\\ge 1-\\mu- \\frac{\\eta}{\\tf{\\W(k)}}\\iprod{\\nabla \\mc{L}(\\W(k))} {\\frac{\\Wm}{\\tf{\\Wm}}} \\\\\n      & \\geq 1-\\mu +\\frac{\\eta \\rho (k)  (1-\\epsilon)}{\\tf{\\W(k)}}.\n\\end{split}\n\\end{equation}\nNote that from Lemma~\\ref{local cond}, we have $\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle<0$~ which implies that $\\tf{\\W(k+1)} \\geq \\tf{\\W(k)}$.  This together with  $R_\\eps$ definition and $\\tf{\\W(k)}\\geq 1/2$ implies that\n\\begin{align*}\n\\tf{\\W(k+1)}&\\leq\\frac{1}{{2\\tf{\\W(k)}}} \\left(\\tf{\\W(k+1)}^2+\\tf{\\W(k)}^2\\right)\\\\\n& = \\frac{1}{2\\tf{\\W(k)}} \\left(2\\tf{\\W(k)}^2-2\\eta\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle+\\eta^2\\tf{\\nabla \\Lc(\\W(k))}^2\\right)\\\\\n       &\\leq  \\tf{\\W(k)}- \\frac{\\eta}{\\tf{\\W(k)}}\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle + \\eta^2 \\tf{\\nabla \\Lc(\\W(k))}^2, \n\\end{align*}\n% \\tf{\\W(k+1)} \\leq \n% $$ \nwhich gives\n\\begin{equation}\\label{eqn:localgd:2}\n\\begin{split}\n  \\frac{\\tf{\\W(k+1)}}{\\tf{\\W(k)}}& \\leq  1- \\frac{\\eta}{\\tf{\\W(k)}}\n       \\left\\langle \\nabla \\Lc(\\W(k)),\\frac{\\W(k)}{\\tf{\\W(k)}} \\right\\rangle + \\eta^2 \\frac{\\|\\nabla \\mc{L}(\\W(k))\\|^2}{\\tf{\\W(k)}}\\\\\n& \\leq 1- \\frac{\\eta}{(1-\\epsilon)\\tf{\\W(k)}}  \\iprod{\\nabla\\mc{L}(\\W(k))}\n     {\\frac{\\Wm}{\\tf{\\Wm}}}+ \\eta^2 \\frac{\\|\\nabla \\mc{L}(\\W(k))\\|^2}{\\tf{\\W(k)}}\\\\\n      & \\leq  1 + \\frac{\\eta \\rho (k) }{\\tf{\\W(k)}} + \\frac{\\eta^2\\|\\nabla \\mc{L}(\\W(k))\\|^2}{\\tf{\\W(k)}}=:C_1(\\rho (k) ,\\eta).\n\\end{split}\n\\end{equation}\n\\end{subequations}\nHere, the second inequality follows from \\eqref{eqn:neg:corr:local} and \\eqref{eqn:rho:def}.\n\nNow, it follows from \\eqref{eqn:localgd:1} and \\eqref{eqn:localgd:2} that   \n\\begin{equation}\\label{eqn:wt+1:cone}\n\\begin{split}\n\\left\\langle \\frac{\\W(k+1)}{\\|\\W(k+1)\\|},\\frac{\\Ws}{\\|\\Ws\\|} \\right\\rangle   &\\geq \\frac{1}{C_1(\\rho(k),\\eta)} \\left(1-\\mu +\\frac{\\eta \\rho(k)(1-\\epsilon)}{\\tf{\\W(k)}}\\right)\\\\\n& = 1-\\mu+ \\frac{1}{C_1(\\rho(k),\\eta)} \\left((1-\\mu)(1-C_1(\\rho(k),\\eta)) +\\frac{\\eta \\rho(k)(1-\\epsilon)}{\\tf{\\W(k)}}\\right)\\\\\n& = 1-\\mu+ \\frac{\\eta}{C_1(\\rho(k),\\eta)} \\left((\\mu-1)(\\frac{ \\rho(k)}{\\tf{\\W(k)}} + \\frac{\\eta\\|\\nabla \\mc{L}(\\W(k))\\|^2}{\\tf{\\W(k)}}) +\\frac{ \\rho(k)(1-\\epsilon)}{\\tf{\\W(k)}}\\right)\\\\\n& = 1-\\mu+\\frac{\\eta}{C_1(\\rho(k),\\eta)} \\left(\\frac{\\rho(k)(\\mu -\\epsilon)}{\\tf{\\W(k)}}\n-   \\eta (1-\\mu) \\frac{\\|\\nabla \\mc{L}(\\W(k))\\|^2 }{\\tf{\\W(k)}}\\right)\\\\\n& \\geq 1-\\mu, \n\\end{split}\n\\end{equation}\nwhere the last inequality uses our choice of stepsize $\\eta\\leq 1/L_W$ in Theorem~\\ref{thm:local:gd}'s statement. Specifically, we need $\\eta$ to be small to ensure the last inequality. We will guarantee this by choosing a proper $R_\\eps$ in Lemma \\ref{lem:local:corr}. Specifically, Lemma \\ref{lem:local:corr} leaves the choice of $C_0$ in $R_\\eps$ lower bound of \\eqref{R boundC0} open (it can always be chosen larger). Here, by choosing $C_0\\gtrsim 1/L_{\\W}$ will ensure $\\eta\\leq 1/L_W$ works well.\n\\begin{align}\\label{eqn:zeta:mu}\n\\nonumber\n\\eta &\\leq \\frac{\\mu}{2(1-\\mu)(1-\\frac{\\mu}{2})}\n \\frac{c}{C}  \\frac{\\Theta}{\\bar{A}}   \\frac{1}{\\bar{A}C T}    e^{R_\\mu^0\\Theta/2} \\\\\n& \\leq \\frac{\\mu-\\epsilon}{1-\\mu} \\cdot  \\frac{1}{1-\\epsilon} \\cdot \\frac{c}{C} \\cdot \\frac{\\Theta}{\\bar{A}} \\cdot  \\frac{1}{\\bar{A}C T}  e^{R_\\mu^0\\Theta/2} \\leq \\frac{(\\mu-\\epsilon)}{1-\\mu}  \\frac{\\rho(k) } { \\|\\nabla \\mc{L}(\\W(k))\\|^2_F}.\n\\end{align}\nHere, the first inequality uses our choice of $\\epsilon \\in (0, \\mu/2)$ (see \\textbf{Step 2}), and the last inequality is obtained from Lemma~\\ref{local cond} since\n\\begin{align*}\n    \\frac{\\rho(k) } { \\tf{\\nabla \\mc{L}(\\W(k))}} &= - \\frac{1}{1-\\epsilon} \\iprod{ \\frac{\\nabla\\mc{L}(\\W(k))}{\\tf{\\nabla \\mc{L}(\\W(k))}}}\n     {\\frac{\\Wm}{\\tf{\\Wm}}}  \\geq \\frac{1}{1-\\epsilon} \\cdot \\frac{c}{C} \\cdot \\frac{\\Theta}{\\bar{A}},\\\\\n         \\frac{1} { \\tf{\\nabla \\mc{L}(\\W(k))}} &\\geq \\frac{1}{\\bar{A}C \\cdot \\frac{1}{n} \\sum_{i=1}^n  \\left(1-\\s_{i\\alpha_i}\\right)}  \\geq    \\frac{1}{ \\bar{A} C T e^{-R_\\mu^{0}\\Theta/2}} \n\\end{align*}\nfor some data dependent constrants $c$ and $C$, $\\bar{A}=\\max_{i\\in[n],t,\\tau\\in[T]}\\tn{(\\x_{it}- \\x_{i\\tau})}~\\tn{\\z_i}$, and $\\Theta=1/\\tf{\\Ws}$.\n\nNext, we will demonstrate that the choice of $\\eta$ in \\eqref{eqn:zeta:mu} does indeed meet our step size condition as stated in the theorem, i.e., $\\eta \\leq 1/L_{\\W}$. Recall that $1/(1+\\pi)=1-\\epsilon$, which implies that $\\pi =\\epsilon/(1-\\epsilon)$. Combining this with \\eqref{R boundC0}, we obtain:\n\\begin{align}\\label{eqn:pitoC0}\nR_\\pi &\\geq\\frac{\\max(2,\\delta^{-1})}{\\Theta}\\log\\left(\\frac{C_0 T\\Gamma A}{\\pi\\bggm}\\right), \\quad \\textnormal{where} \\quad C_0 \\geq 64 \\pi.\\\\\n& \\Rightarrow R_\\epsilon \\geq\\frac{\\max(2,\\delta^{-1})}{\\Theta}\\log\\left(\\frac{ (1-\\epsilon)C_0 T\\Gamma A}{\\epsilon\\bggm}\\right),\\quad  \\textnormal{where}   \\quad C_0 \\geq 64  \\frac{\\epsilon}{1-\\epsilon}.\n\\end{align}\nOn the other hand, at the initialization, we have  $\\epsilon=\\mu/2$ which implies that \n\\begin{align}\\label{eqn:rmu:c0}\n  R_{\\mu}^0 \\geq \\frac{\\max(2,\\delta^{-1})}{\\Theta}\\log\\left(\\frac{ (2-\\mu)C_0 T\\Gamma A}{\\mu\\bggm}\\right),  \\quad  \\textnormal{where} \\quad  C_0 \\geq 64  \\frac{\\mu}{2(1-\\frac{\\mu}{2})}.\n\\end{align}\nIn the following, we will determine  a lower bound on  $C_0$ such that our step size condition in Theorem~\\ref{thm:local:gd}'s statement, i.e., $\\eta \\leq 1/L_{\\W}$, is satisfied. Note that for the choice of $\\eta$ in \\eqref{eqn:zeta:mu} to meet the condition $\\eta \\leq 1/L_{\\W}$, the following condition must hold:\n\\begin{equation}\n \\frac{1}{L_{\\W}}\\leq \n\\frac{\\mu}{ (2-\\mu)} \\frac{1}{C_2T} e^{R_\\mu^0\\Theta/2}  \\Rightarrow R_\\mu^0 \\geq  \\frac{2}{\\Theta} \\log  \\left(\\frac{1}{L_{\\W}}   \\frac{2-\\mu}{\\mu} C_2 T\\right).\n\\end{equation}\nwhere\n$C_2 = (1-\\mu)  \\frac{  \\bar{A}^2 C^2 }{ \\Theta c}$.\n\n \nThis together with \\eqref{eqn:rmu:c0} implies that \n\\begin{align}\\label{eqn:pitoC02}\n\\frac{C_0 \\Gamma A}{\\bggm}  & \\geq (1-\\mu) \\frac{C_2}{L_{\\W}}   \\Rightarrow  C_0 \\geq  \\max \\left( \\frac{(1-\\mu)C_2}{L_{\\W}}    \\frac{\\bggm }{\\Gamma A},  \\frac{64\\mu}{2-\\mu} \\right). \n\\end{align}\nTherefore, with this lower bound on $C_0$, the step size bound in \\eqref{eqn:zeta:mu} is sufficiently large to ensure that $\\eta \\leq 1/L_{\\W}$ guarantees \\eqref{eqn:wt+1:cone}. \n\nHence,  it follows from \\eqref{eqn:wt+1:cone} that $\\W(k+1) \\in \\Cc_{\\mu,R_\\eps}(\\Ws)$.\n\\\\\n\\textbf{Step 4: The correlation of $\\W(k)$ and $\\Wm$ increases over $k$.} \nFrom Step 3, we have that all iterates remain within the initial conic set i.e.~$\\W(k)\\in\\Cc_{\\mu,R^0_\\mu}(\\Ws)$ for all $k\\geq 0$. Note that it follows from Lemma~\\ref{local cond} that  $\\li\\nabla\\Lc(\\W), \\Ws/\\tf{\\Ws}\\ri<0$, for any finite $\\W \\in \\Cc_{\\mu,R^0_\\mu}(\\Ws)$. Hence, there are no finite critical points $\\W \\in \\Cc_{\\mu,R^0_\\mu}(\\Ws)$, for which $\\nabla \\mc{L} (\\W)=0$. Now, based on Lemma~\\ref{lem:grad:descent}, which guarantees that $\\nabla\\Lc(\\W(k))\\rightarrow 0$, this\nimplies that $\\left\\Vert \\W\\left(t\\right)\\right\\Vert_F\\rightarrow\\infty$. Consequently, for any choice of $\\eps\\in (0,\\mu/2)$ there is an iteration $k_\\eps$ such that, for all $k\\geq k_\\eps$, $\\W(k)\\in\\Cc_{\\mu,R_\\eps}(\\Ws)$. Once within $\\Cc_{\\mu,R_\\eps}(\\Ws)$, multiplying both sides \\eqref{eqn:neg:corr:local} by the stepsize $\\eta$ and using the gradient descent update, we get\n\\begin{equation*}%\\label{eqn:decpath:1}\n\\begin{split}\n     \\left\\langle \\W(k+1)-\\W(k),\\frac{ \\Wm}{\\tf{\\Wm}} \\right\\rangle &\\geq  (1-\\epsilon) \\left\\langle \\W(k+1)-\\W(k), \\frac{\\W(k)}{\\tf{\\W(k)}}\\right\\rangle\\\\\n     &= \\frac{(1-\\epsilon)}{2\\tf{\\W(k)}}\\left(\\tf{\\W(k+1)}^2- \\tf{\\W(k)}^2-\\tf{\\W(k+1)-\\W(k)}^2\\right) \\\\\n     & \\geq (1-\\epsilon)\\left( \\frac{1}{2\\tf{\\W(k)}} \\left(\\tf{\\W(k+1)}^2- \\tf{\\W(k)}^2\\right)-\\tf{\\W(k+1)-\\W(k)}^2\\right) \\\\\n     & \\geq (1-\\epsilon)\\left(\\tf{\\W(k+1)}- \\tf{\\W(k)}-\\tf{\\W(k+1)-\\W(k)}^2\\right) \\\\\n          & \\geq (1-\\epsilon)\\Big(\\tf{\\W(k+1)}- \\tf{\\W(k)}- 2\\eta  \\left(\\mc{L}(\\W(k))-\\mc{L}(\\W(k+1))\\right) \\Big).\n\\end{split}\n\\end{equation*}\n% \\ct{Check inequality in third line above? I see how to jump from second line to fourth but not third. {\\color{cyan}D: fixed, please see below. Thanks}}\nHere, the second inequality is obtained from  $\\tf{\\W(k)}\\geq 1/2$; the third inequality follows since  for any $a, b >0$, we have $  (a^2-b^2)/(2b) -  (a-b) \\geq 0$; and the last inequality  uses Lemma~\\ref{lem:grad:descent}.\n\n%Here, the last inequality uses Lemma~\\ref{lem:grad:descent}.\n\nSumming the above inequality over $k\\geq k_\\eps$ gives \n\\begin{align*}\n      \\left\\langle\\frac{\\W(k)}{\\tf{\\W(k)}}, \\frac{\\Wm}{\\tf{\\Wm}} \\right\\rangle \\ge1-\\epsilon+ \\frac{C(\\epsilon,\\eta)}{\\tf{\\W(k)}},    \\qquad \\W(k)\\in\\Cc_{\\mu,R_\\eps}(\\Ws),   \n\\end{align*}\nwhere $\\mathcal{L}_{\\star}\\leq\\mathcal{L}\\left(\\W\\left(k\\right)\\right)$ for all $k\\geq 0$, and \n\\begin{equation*}%\\label{eqn:decpath:22}\nC(\\epsilon,\\eta)= \\left\\langle \\W(k_\\eps), \\frac{ \\Wm}{\\tf{\\Wm}}\\right\\rangle-(1-\\epsilon)\\tf{\\W(k_\\eps)} -2\\eta (1-\\epsilon) (\\mc{L}(\\W(k_\\eps))-\\mathcal{L}_{\\star}).\n\\end{equation*}\n% Since $\\left\\Vert  \\W\\left(k\\right)\\right\\Vert \\rightarrow\\infty$, we get\n%     \\begin{align}\\label{eqn:eps:final}\n%       \\liminf_{k\\to\\infty}\\iprod{\\frac{\\W(k)}{\\tf{\\W(k)}}}{\\frac{ \\Wm}{\\tf{\\Wm}}}\\ge1-\\epsilon.\n%     \\end{align}\n% Given that $\\epsilon$ is arbitrary, we can consider the limit as $\\epsilon$ approaches zero. Thus, $\\W(k)/\\tf{\\W(k)}\\to   \\Wm/\\tf{\\Wm}$.  $\\qed$ \n% %\\end{proof}\n% \\begin{align*}\n%       \\left\\langle\\frac{\\W(k)}{\\tf{\\W(k)}}, \\frac{\\Wm}{\\tf{\\Wm}} \\right\\rangle \\ge1-\\epsilon+ \\frac{C(\\epsilon,\\eta)}{\\tf{\\W(k)}},    \\qquad \\W(k)\\in\\Cc_{\\mu,R_\\eps}(\\Ws),   \n% \\end{align*}\n% for some finite constant $C(\\epsilon,\\eta)$ (that depends only on $\\eta,\\eps,\\tf{\\W(k_\\eps)}$).\nConsequently, as $k\\rightarrow\\infty$\n    \\begin{align*}\n      \\liminf_{k\\to\\infty}\\iprod{\\frac{\\W(k)}{\\tf{\\W(k)}}}{\\frac{\\Wm}{\\tf{\\Wm}}}\\ge1-\\epsilon, \\qquad \\W(k)\\in\\Cc_{\\mu,R_\\eps}(\\Ws).\n    \\end{align*}\nSince  $\\epsilon \\in (0, \\mu/2)$  is arbitrary, we get $\\W(k)/\\tf{\\W(k)}\\to  \\Wm/\\tf{\\Wm}$.\n%\\end{proof} \n $\\qed$ \n\n\n==== END OF /2308.16898/supp/proof_convergence.tex ====\n==== BEGINNING OF /2308.16898/supp/great_leap_forward.tex ====\n\\section{Global Convergence with Good Initial Gradient}\\label{sec one step}\n\n\\begin{lemma}[Gradient Condition for Optimal Tokens]\\label{glocal cond} \nSuppose Assumption~\\ref{assum:loss:prope} holds and let $\\op=(\\op_i)_{i=1}^n$ be the unique globally-optimal indices with $\\Wm$ denoting the SVM solution. Define the margin $\\Theta=1/\\tf{\\Ws}$. \n%\\begin{align}\\label{one step margin}\n%\\Theta=\\min_{i\\in[n],t\\neq\\op_i}(\\x_{i\\op_i}-\\x_\\itt)^\\top\\frac{\\Wm}{\\tf{\\Wm}}\\z_i\n%\\end{align}\nGiven $\\mu\\geq 0$, consider the following subset of the sphere and its associated cone\n% \\begin{align}\n% &\\Sc_{\\mu}=\\{\\tf{\\W}=1\\bgl \\underset{i\\in[n],t\\neq \\op_i}{\\min}\\li(\\x_{i\\op_i}-\\x_\\itt)^\\top\\W\\z_i\\ri\\geq \\mu\\cdot\\Theta\\},\\\\\n% &\\conb_{\\mu,R}=\\{\\tf{\\W}\\geq R\\bgl \\W/\\tf{\\W}\\in\\Sc_\\mu\\}.\n% \\end{align}\n\\begin{subequations}\\label{eqn:con:nabla0}\n\\begin{align}\n&\\Sc_{\\mu}=\\left\\{\\W~\\Big|~  \\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\frac{\\W}{\\tf{\\W}}\\ri\\geq \\mu\\cdot\\Theta\\quad \\textnormal{for all}\\quad t\\neq \\op_i, \\quad  i\\in[n]\\right\\},\\\\\n&\\conb_{\\mu,R}=\\left\\{  \\W\\in\\Sc_\\mu ~\\Big|~   \\tf{\\W}\\geq R\\right\\}.\n\\end{align}\n\\end{subequations}\n\nFor any $\\mu>0$, there exists sufficiently large $R=R_\\mu=\\order{1/\\mu}$ (see \\eqref{R bound2}) such that:\n\\begin{enumerate}[label={\\textnormal{\\textbf{L\\arabic*.}}}, wide, labelwidth=!,itemindent=!, labelindent=5pt]\n\\item \\label{lem:gcond:l1} There is no stationary point within  $ \\conb_{\\mu,R}$.\n%, where $\\cone_{\\mu}(\\Wm)$ is defined in \\eqref{eqn:def:cone}. \n\\item\\label{lem:gcond:l2}  Let $s_i=\\sft{\\X_i\\W\\z_i}$. For all $\\V\\in \\Sc_{\\mu},\\W\\in\\conb_{\\mu,R}$, there exist $C,c>0$ such that \n\\begin{align*}\nC\\cdot\\max_{i\\in[n]}  \\left(1-s_{i\\op_i}\\right) \\geq -\\li\\nabla\\Lc(\\W),\\V\\ri\\geq c\\cdot\\mu\\cdot\\min_{i\\in[n]}  \\left(1-s_{i\\op_i}\\right).\n\\end{align*}\n\\end{enumerate}\n\\end{lemma}\n%\\begin{comment}\n\\begin{proof} Let us introduce the norm upper bound\n\\begin{equation}\\label{mu choice2}\n\\begin{split}\n%&\\Theta=1/\\tf{\\Wm},\\\\\n&A=\\max_{i\\in[n],t,\\tau\\in[T]} \\frac{(\\tn{\\x_{it}}\\vee\\tn{\\x_{it}-\\x_{i\\tau}})\\cdot\\tn{\\z_i}}{\\Theta}.\n\\end{split}\n\\end{equation}\n%\nThe following inequalities hold for all $\\V\\in \\Sc_{\\mu},~\\tf{\\V}=\\tf{\\Wm}$ and all $i\\in[n], t\\neq \\op_i$:\n\\begin{equation}\\label{cone-A-eq}\n\\begin{split}\n%(\\x_{it}-\\x_{i\\tau})^\\top \\V \\z_i&\\geq \\delta>0,\\\\%\\quad\\text{for all}\\quad \\\\\n%(\\x_{i\\op_i}-\\x_{i\\tau})^\\top \\V \\z_i&\\geq 1+\\delta,\\\\\nA\\geq(\\x_{i\\op_i}-\\x_{it})^\\top \\V \\z_i &\\geq \\mu.\n\\end{split}\n\\end{equation}\n%Here, we used $\\tf{\\V-\\Wm}^2/\\tf{\\Wm}^2\\leq 2\\mu$ which implies $\\tf{\\V-\\Wm}\\leq \\sqrt{2\\mu}/\\Theta$.\n %Now that the choice of local cone is determined, we need to prove the main claims. We will lower bound $-\\hb^\\top \\nabla \\Lc(\\W)$ and establish its strict positivity for $\\tn{\\W}\\geq R$. This will show that there is no stationary point as a by product.\n%Given any $\\W\\in \\conb_{\\mu,R}(\\Wm)$, denote $\\Wb=(\\tf{\\Wm}/\\tf{\\W})\\W$ and recall $\\tf{\\V}=\\tf{\\Wm}$. \nTo proceed, we write the gradient correlation following \\eqref{grad def} and \\eqref{grad def2}\n\\begin{align}\\label{grad def32}\n\\li\\nabla\\Lc(\\W),\\V\\ri&=\\frac{1}{n}\\sum_{i=1}^n\\ell'_i\\cdot\\li\\hb_i,\\sfp{\\hp_i}\\bgam_i\\ri,\n\\end{align}\nwhere we denoted $\\ell'_i=\\ell'(Y_i\\cdot \\vb^\\top \\X_i^\\top\\sft{\\hp_i})$, $\\hb_i=\\X_i\\V \\z_{i}$, $\\hp_i= \\X_i\\W \\z_{i}$, $\\s_i=\\sft{\\hp_i}$. It follows from \\eqref{mu choice2} that $A\\geq \\max_{i\\in[n],t\\in[T]}\\tn{\\hb_{it}}$. Using \\eqref{cone-A-eq}, we can bound the softmax probabilities $\\s_i=\\sft{\\hp_i}$ as follows, for all $i\\in[n]$:\n\\begin{align}\\label{soft prob bound2}\n&S_i:= \\sum_{\\tau\\neq \\op_i}\\s_{i\\tau}\\leq T e^{-R\\mu\\Theta}\\s_{i\\op_i}\\leq T e^{-R\\mu\\Theta}.\n\\end{align}\n%(over all $i,t$) \nRecall scores $\\bgam_{it}=Y_i\\cdot\\vb^\\top \\x_{it}$. Define the score gaps:\n% over neighbors:\n\\begin{equation*}\n \\bgg_i=\\bgam_{i\\op_i}-\\max_{t\\neq\\op_i}\\bgam_{it},~~~ \\bgm_i=\\bgam_{i\\op_i}-\\min_{t\\neq\\op_i}\\bgam_{it},~~~\\textnormal{and}~~~\\Gamma=\\sup_{i\\in[n],t,\\tau\\in[T]}|\\bgam_{it}-\\bgam_{i\\tau}|. \n\\end{equation*}\nLet us focus on a fixed datapoint $i\\in[n]$, assume (without losing generality) $\\op_i=1$, and drop subscripts $i$.\n%, that is,~$\\op:=\\op_i$, $\\X:=\\X_i$, $Y:=Y_i$, $\\Kb:=\\Kb_i$, $\\hp=\\Kb\\pb$, $\\hb=\\Kb\\V$, $\\s=\\sft{\\Kb\\pb}$, $\\bgam=Y\\cdot\\X\\vb$, and $\\bgg:=\\bgg_i$.\nDirectly applying Lemma \\ref{lem:q_reduce}, we obtain\n\\[\n  \\big|\\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam-\\sum_{t\\geq 2}^T (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2\\Gamma A(1-\\s_1)^2.\n\\]\n\\noindent To proceed, let us upper/lower bound the gradient correlation. Since $A\\geq \\hb_1-\\hb_t\\geq \\mu>0$ from \\eqref{cone-A-eq}, setting $S:=\\sum_{t\\neq\\op_i}\\s_t=1-\\s_1$, we find\n\\begin{equation}\n A\\cdot S\\cdot \\bgm  \\geq\\sum_{t\\neq\\op} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\geq \\mu\\cdot S\\cdot \\bgg.\\label{aggregate2}\n\\end{equation}\nNext we show that $S=1-\\s_1$ dominates $(1-\\s_1)^2=S^2$ for large $R$. Specifically, we wish for \n\\begin{align}\\label{wishfor2}\n\\mu S \\bgg/2\\geq 2\\Gamma A(1-\\s_1)^2\\iff S\\geq \\frac{4}{\\mu}\\frac{\\Gamma A}{\\bgg}S^2\\iff S\\leq \\frac{\\mu\\bgg}{4\\Gamma A}.\n\\end{align}\nUsing \\eqref{soft prob bound2}, what we wish is ensured for all $i\\in[n]$, by guaranteeing $Te^{-R\\mu\\Theta}\\leq \\frac{\\mu\\bgg}{4\\Gamma A}$. That is, by choosing\n\\begin{align}\\label{R bound2}\nR:=R_\\mu= \\frac{1}{\\mu\\Theta}\\log\\big(\\frac{4T\\Gamma A}{\\mu\\bggm}\\big),\n\\end{align}\nwhere $\\bggm=\\sup_{i\\in[n]}\\bgg_i$ is the global scalar corresponding to the worst case score gap over all inputs. \n\nWith the above choice of $R$, we guaranteed\n\\[\n  2 A(1-\\s_1)\\cdot \\bgm\\geq 2A\\cdot S\\cdot \\bgm \\geq \\sum_{t\\neq\\op} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\geq\\frac{\\mu\\cdot S\\cdot \\bgg}{2}\\geq\\frac{\\mu(1-\\s_1) \\bgg}{2}.\n\\]\nvia \\eqref{wishfor2} and \\eqref{aggregate2}. Since this holds over all inputs, going back to the gradient correlation \\eqref{grad def32} and averaging above over all inputs $i\\in[n]$ and plugging back the indices $i$, we obtain the advertised bound by setting $q_i=1-\\s_{i\\op_i}$ (where we have used $\\op_i=1$ above, without losing generality)\n\\begin{align}\\label{pbb corr2}\n  \\frac{2A}{n}\\sum_{i\\in [n]} \\ell'_i\\cdot q_i\\cdot \\bgm_i\\geq \\li\\nabla\\Lc(\\W),\\V\\ri\\geq \\frac{\\mu}{2n}\\sum_{i\\in [n]} \\ell'_i\\cdot q_i\\cdot \\bgg_i.\n\\end{align}\n\\end{proof}\n%\\end{comment}\n\n\\begin{lemma}[Gradient Condition for Optimal Tokens]\\label{lem:glocal:corr} \nSuppose Assumption~\\ref{assum:loss:prope} holds and let $\\op=(\\op_i)_{i=1}^n$ be the unique optimal tokens with $ \\Wm$ denoting the SVM solution. Fix any $\\mu>0$ (per Lemma \\ref{glocal cond}). For any choice of $\\pi>0$, there exists $R:=R_{\\pi,\\mu}$ such that, for any $ \\W\\in \\conb_{\\mu,R}(\\Wm)$ we have\n\\[\n \\li \\nabla\\Lc(\\W), \\frac{\\W}{\\tf{\\W}} \\ri\\geq (1+\\pi)\\li \\nabla\\Lc(\\W), \\frac{\\Wm}{\\tf{\\Wm}}\\ri.\n\\]\n\\end{lemma}\n\\begin{proof}\n %Set notations $\\hb_i=\\Kb_i\\pbb$, $\\hbm_i=\\Kb_i\\ps$, and $\\s_i=\\sft{\\Kb_i\\pb}$. \nLet  $\\Wb= \\tf{\\W} \\W/\\tf{\\Wm}$, $\\hb_i=\\X_i\\Wb \\z_{i}$, $\\hbm_i= \\X_i\\Ws \\z_{i}$, and $\\s_i=\\sft{\\X_i\\W \\z_{i}}$. To establish the result, we will prove that, for sufficiently large $R=R_\\pi$, for any $\\W\\in \\conb_{\\mu,R}(\\Wm)$ and any $i\\in[n]$, \n\\begin{align}\\label{main local cond2}\n\\li\\hb_i,\\sfp{\\X_i\\W\\z_{i}}\\bgam_i\\ri\\leq (1+\\pi)\\li\\hbm_i,\\sfp{\\X_i\\W \\z_{i}}\\bgam_i\\ri.\n\\end{align}\nOnce \\eqref{main local cond2} holds for all $i$, the same conclusion will hold for the gradient correlations via \\eqref{grad def32}. Moving forward, we shall again focus on a single point $i\\in[n]$ and drop all subscripts $i$ i.e.~we will use vectors $\\hb,\\hbm,\\s$. Also assume $\\op=\\op_i=1$ without losing generality as above. Following \\eqref{aggregate2}, for all $\\V\\in \\Sc_\\mu$ with $\\tf{\\V}=\\tf{\\Wm}$ and $\\hp=\\X\\V \\z$, we have found\n\\begin{align}\n  \\big|\\hp^\\top\\diag{\\s}\\bgam-\\hp^\\top\\s\\s^\\top\\bgam-\\sum_{t\\neq\\op} (\\hp_1-\\hp_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2\\Gamma A(1-\\s_1)^2. %\\label{aggregate2}\n\\end{align}\nRecalling $\\hbm_1-\\hbm_t\\geq 1$, we note that $\\sum_{t\\neq\\op}\\s_t(\\bgam_1-\\bgam_t)\\leq \\sum_{t\\neq\\op} (\\hbm_1-\\hbm_t)\\s_t(\\bgam_1-\\bgam_t)$. Now plugging in $\\hb,\\hbm$ in the bound above and assuming $\\pi\\leq 1$ (w.l.o.g.), \\eqref{main local cond2} is implied by the following stronger inequality\n\\begin{align*}\n6\\Gamma A(1-\\s_1)^2+ \\sum_{t\\neq\\op} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t) &\\leq (1+\\pi)\\sum_{t\\neq\\op}\\s_t(\\bgam_1-\\bgam_t)\\\\\n&\\leq (1+\\pi) \\sum_{t\\neq\\op} (\\hbm_1-\\hbm_t)\\s_t(\\bgam_1-\\bgam_t).\n\\end{align*}\nFirst, we claim that $0.5\\pi\\sum_{t\\neq\\op}\\s_t(\\bgam_1-\\bgam_t)\\geq 6\\Gamma A(1-\\s_1)^2$. The proof of this claim directly follows the earlier argument, namely following \\eqref{wishfor2}, \\eqref{R bound2}, \\eqref{soft prob bound2} we have that $1-\\s_1\\leq Te^{-R\\mu\\Theta}$ and $\\bgam_1-\\bgam_t\\geq \\bggm$. This leads to the choice (for $C=12$)\n\\begin{align}\nR_\\pi =\\frac{1}{\\mu\\Theta}\\log\\big(\\frac{C\\cdot T\\Gamma A}{\\pi\\bggm}\\big).\\label{Rpi choice2}\n\\end{align}\n\nFollowing this control over the perturbation term $6\\Gamma A(1-\\s_1)^2$, to conclude with the result, what remains is establishing the comparison\n\\begin{align}\\label{desired comp2}\n\\sum_{t\\neq\\op} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\leq (1+0.5\\pi) \\sum_{t\\neq\\op}\\s_t(\\bgam_1-\\bgam_t).\n\\end{align}\nTo proceed, we split the problem into two scenarios. \n\n\\noindent\\textbf{Scenario 1:} $\\tf{\\Wb-\\Wm}\\leq \\eps=\\frac{\\pi}{4A\\Theta}$ for some $\\eps>0$. In this scenario, for any token, we find that\n\\[\n|\\hb_t-\\hbm_t|\\leq A\\Theta\\eps=\\pi/4.\n\\]\nConsequently, we obtain \n\\[\n\\hb_1-\\hb_t\\leq \\hbm_1-\\hbm_t+2A\\Theta\\eps= 1+0.5\\pi.\n\\] \nSince $\\bgam_1-\\bgam_t\\geq 0$, for all $t\\neq 1$, we find $(\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\leq (1+0.5\\pi)\\s_t(\\bgam_1-\\bgam_t)$. This implies the desired result \\eqref{desired comp2}.\\smallskip\n\n\\noindent\\textbf{Scenario 2:} $\\tf{\\Wb-\\Wm}\\geq \\eps=\\frac{\\pi}{4A\\Theta}$. Since $\\Wb$ is not the max-margin solution, in this scenario, for some $\\nu=\\nu(\\eps)>0$ and $\\tau\\neq 1$, we have that $\\hb_1-\\hb_\\tau\\leq 1-2\\nu$. Here $\\tau=\\arg\\max_{\\tau\\neq 1} \\x_\\tau\\Wb \\z$ denotes the nearest point to $\\hb_1$ (along the $\\Wb$ direction). Recall that $\\s=\\sft{  C^\\svm\\hb}$, where  $C^\\svm=\\tf{\\W}/\\tf{\\Wm}$.  To proceed, split the tokens into two groups: Let $\\Nc$ be the group of tokens obeying $(\\x_1- \\x_\\tau) \\W \\z \\leq 1-\\nu$ and $[T]-\\{1\\}-\\Nc$ be the rest of the non-optimal tokens. Observe that\n\\[\n\\frac{\\sum_{t\\not\\in\\{\\op\\}\\cup\\Nc}\\s_t}{\\sum_{t\\neq\\op}\\s_t}\\leq\\frac{\\sum_{t\\not\\in\\{\\op\\}\\cup\\Nc}\\s_t}{\\sum_{t=\\tau}\\s_t}\\leq  T\\frac{e^{\\nu C^\\svm}}{e^{2\\nu C^\\svm}}=Te^{-C^\\svm\\nu}.\n\\]\nThus, using $|\\hb_1-\\hb_t|\\leq 2A$ and recalling the definition of $\\bgg$, observe that \n\\[\n\\sum_{t\\not\\in\\{\\op\\}\\cup\\Nc} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\leq \\frac{2\\Gamma A Te^{-C^\\svm\\nu}}{\\bgg} \\sum_{t\\neq\\op} \\s_t(\\bgam_1-\\bgam_t).%\\leq \\frac{3A\\Gamma Te^{-C^\\svm\\nu}}{\\bgg} \\sum_{t\\in \\Nc} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t).\n\\]\nPlugging this into \\eqref{desired comp2}, we obtain\n\\begin{align*}\n  \\sum_{t\\neq\\op} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)&= \\sum_{t\\in \\Nc} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)+\\sum_{t\\not\\in\\{\\op\\}\\cup\\Nc} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\nonumber\\\\\n  &\\leq \\sum_{t\\in \\Nc} (1-\\nu)\\s_t(\\bgam_1-\\bgam_t)+\\sum_{t\\not\\in\\{\\op\\}\\cup\\Nc} 2A\\Gamma Te^{-C^\\svm\\nu}\\nonumber\\\\\n  &\\leq \\left(1-\\nu+\\frac{2\\Gamma A Te^{-C^\\svm\\nu}}{\\bgg}\\right)\\sum_{t\\neq\\op}\\s_t(\\bgam_1-\\bgam_t)\\\\\n  &\\leq \\left(1+\\frac{2\\Gamma A Te^{-C^\\svm\\nu}}{\\bgg}\\right)\\sum_{t\\neq\\op}\\s_t(\\bgam_1-\\bgam_t).\\\\\n\\end{align*}\nConsequently, the proof boils down to ensuring the perturbation term $\\frac{2\\Gamma A Te^{-C^\\svm\\nu}}{\\bgg}\\leq 0.5\\pi$. Since $C^\\svm\\geq R\\Theta$, this is guaranteed for all inputs $i\\in[n]$ by recalling $\\bggm=\\min_{i\\in[n]}\\bgg_i$ and choosing \n\\[\n  R\\geq R_\\pi= \\frac{1}{\\nu\\Theta}\\log(\\frac{4\\Gamma AT}{\\bggm\\pi}),\n\\]\nwhere $\\nu=\\nu(\\frac{\\pi}{4A\\Theta})$ depends only on $\\pi$ and global problem variables. \n\nCombining this with the prior $R_\\pi$ choice of \\eqref{Rpi choice2} (by taking their maximum), we conclude with the statement.   \n\\end{proof}\n\\subsection{Proof of Theorem~\\ref{conv:gd:w:global:nabla0}}\n\\begin{proof}\nWe provide the proof in four steps:\\\\\n\\textbf{Step~1: There are no stationary points within $\\conb_{\\mu,R_\\mu}(\\Wm)$ for sufficiently large $R_\\mu$.} \nThis step directly follows from Lemma~\\ref{glocal cond}: for all $\\V,\\W\\in \\Sc_{\\mu}(\\Wm)$ with $\\tf{\\V}=\\tf{\\Wm}$ and $\\tf{\\W}\\geq R_\\mu$, it follows from Lemma~\\ref{glocal cond} that there exists  $R_\\mu$ such that $\\iprod{\\V}{ -\\nabla \\Lc(\\W)}$ is strictly positive.\n\\\\\n\\textbf{Step~2:}  Consider an arbitrary value of  $\\epsilon \\in (0, \\mu/2)$ and let $1/(1+\\pi)=1-\\epsilon$. It follows from Lemma~\\ref{lem:glocal:corr} that, there exists $R_\\epsilon$ such that all  $ \\W \\in \\conb_{\\mu,R_\\epsilon}(\\Wm)$ satisfy%\\vspace{-2pt}\n\\begin{align}\\label{eqn:neg:corr:local:nabla0}\n\\iprod{-\\nabla\\mc{L}(\\W)}      {\\frac{\\Wm}{\\tf{\\Wm}}} \\geq (1-\\epsilon)    \\iprod{-\\nabla \\mc{L}(\\W)}{\\frac{\\W}{\\tf{\\W}}}.\n\\end{align}\n%\n%\n\\\\\n\\textbf{Step~3: Updates remain inside the cone $\\conb_{\\mu,R_\\mu}(\\Wm)$.}  By leveraging the results from \\textbf{Step 1} and \\textbf{Step 2}, we demonstrate that the gradient iterates remain within this cone. Note that by our assumption and for sufficiently large $\\eta_0$, we get $\\W(1) \\in \\conb_{\\mu,R}(\\Wm)$. Here, $R$ is chosen such that \n\\begin{align}\\label{eqn:bound:R:nabla0}\nR \\geq R_\\mu \\vee R_\\epsilon \\vee 1/2.\n\\end{align}\n We provide the proof by induction. By the above argument, $\\W(1) \\in \\conb_{\\mu,R}(\\Wm)$. Let $\\W(k) \\in \\conb_{\\mu,R}(\\Wm)$. For all $k\\geq 1$, we have that \n% \\begin{align}\\label{eqn:rho:def:nabla0}\n% \\bar{\\rho} := \\min_{t\\neq \\op_i,~i\\in[n]} ~~  (1-\\epsilon) \\left\\langle  (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top, - \\nabla \\mc{L}(\\W(k)) \\right\\rangle  \\leq  \\left \\langle \\Wm, - \\nabla \\mc{L}(\\W(k))\\right \\rangle .\n% \\end{align}\n\\begin{subequations}\\label{eqn:rho:def:nabla0}\n\\begin{align}\n& \\min_{t\\neq \\op_i,~i\\in[n]} ~~   \\left\\langle  (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top, - \\nabla \\mc{L}(\\W(k)) \\right\\rangle \\geq \\bar{\\rho} , \n\\label{eqn:bar:rho}\n\\\\\n &\\frac{1}{1-\\epsilon} \\iprod{ {\\frac{\\Wm}{\\tf{\\Wm}}}}{-\\nabla\\mc{L}(\\W(k))} \\geq \\rho.\n\\end{align}\n\\end{subequations}\nfor some positive constants $\\bar{\\rho}$ and $\\rho$.  \n\n\nTo proceed, for all $t\\neq \\op_i,~i\\in[n]$, we obtain \n\\begin{equation}\\label{eqn:localgd:1:nabla0}\n    \\begin{split}\n   \\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top,    \\frac{\\W(k+1)}{\\tf{\\W(k)}} \\right\\rangle &=   \\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top,    \\frac{\\W(k)}{\\tf{\\W(k)}} -\\frac{\\eta}{\\tf{\\W(k)}}\\nabla \\mc{L}(\\W(k)) \\right\\rangle\\\\   \n   &\\ge \\mu \\Theta- \\frac{\\eta}{\\tf{\\W(k)}} \\left\\langle   (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top, \\nabla \\mc{L}(\\W(k)) \\right\\rangle  \\\\\n%           &\\ge \\mu \\Theta- \\frac{\\eta}{\\tf{\\W(k)}}\\iprod{\\frac{\\Wm}{\\tf{\\Wm}} } {\\nabla \\mc{L}(\\W(k))} \\\\\n      & \\geq \\mu \\Theta +\\frac{\\eta\\bar{\\rho}}{\\tf{\\W(k)}}.\n    \\end{split}\n\\end{equation}\nHere, the first inequality follows from the induction assumption $\\W(k) \\in \\conb_{\\mu,R}(\\Wm)$, and the second inequality uses \\eqref{eqn:bar:rho}.\n\nFrom Lemma~\\ref{glocal cond}, we have $\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle<0$~ which implies that $\\tf{\\W(k+1)} \\geq \\tf{\\W(k)}$. Hence, \n\\begin{align*}\n\\tf{\\W(k+1)}&\\leq\\frac{1}{{2\\tf{\\W(k)}}} \\left(\\tf{\\W(k+1)}^2+\\tf{\\W(k)}^2\\right)\\\\\n& = \\frac{1}{2\\tf{\\W(k)}} \\left(2\\tf{W(t)}^2-2\\eta\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle+\\eta^2\\tf{\\nabla \\Lc(\\W(k))}^2\\right)\\\\\n       &\\leq  \\tf{\\W(k)}- \\frac{\\eta}{\\tf{\\W(k)}}\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle + \\eta^2 \\|\\nabla \\Lc(\\W(k))\\|^2, \n\\end{align*}\nwhere the last inequality follows from $\\tf{\\W(k)} \\geq R \\geq 1/2 $. \n\nThus,\n\\begin{equation}\\label{eqn:localgd:2:nabla0}\n\\begin{split}\n  \\frac{\\tf{\\W(k+1)}}{\\tf{\\W(k)}}& \\leq  1- \\eta\n       \\left\\langle \\nabla \\Lc(\\W(k)),\\frac{\\W(k)}{\\tf{\\W(k)}} \\right\\rangle + \\eta^2 \\frac{\\|\\nabla \\mc{L}(\\W(k))\\|^2}{\\tf{\\W(k)}}\\\\\n& \\leq 1- \\frac{\\eta}{1-\\epsilon}  \\iprod{\\nabla\\mc{L}(\\W(k))}\n     {\\frac{\\Wm}{\\tf{\\Wm}}}+ \\eta^2 \\frac{\\|\\nabla \\mc{L}(\\W(k))\\|^2}{\\tf{\\W(k)}}\\\\\n      & \\leq  1 + \\frac{\\eta \\rho}{\\tf{\\W(k)}} + \\frac{\\eta^2\\|\\nabla \\mc{L}(\\W(k))\\|^2}{\\tf{\\W(k)}}=C_1(\\rho,\\eta).\n\\end{split}\n\\end{equation}\nNow, it follows from \\eqref{eqn:localgd:1:nabla0} and \\eqref{eqn:localgd:2:nabla0} that   \n\\begin{equation}\\label{eqn:localgd:3:nabla0}\n\\begin{split}\n\\min_{t\\neq \\op_i,~i\\in[n]} ~~  \\left\\langle  \\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top, \\frac{\\W(k+1)}{\\tf{\\W(k+1)}}\\right\\rangle   &\\geq \\frac{1}{C_1(\\bar{\\rho},\\eta)} \\left(\\mu \\Theta+\\frac{\\eta \\bar{\\rho}}{\\tf{\\W(k)}}\\right)\\\\\n& \\geq \\mu \\Theta+\\frac{\\eta}{C_1(\\bar{\\rho},\\eta)} \\left(\\frac{\\bar{\\rho}-\\rho\\mu \\Theta}{\\tf{\\W(k)}}\n-   \\eta \\mu \\Theta \\frac{\\|\\nabla \\mc{L}(\\W(k))\\|^2 }{\\tf{\\W(k)}}\\right)\\\\\n& \\geq \\mu \\Theta.\n\\end{split}\n\\end{equation}\nHere, we leverage the fact that the parameter $\\mu$ can be chosen arbitrarily, and then we select the step size $\\eta$ to satisfy the following condition:\n$$\\eta \\leq \\frac{\\bar{\\rho}-\\rho\\mu \\Theta }{\\mu\\Theta} \\frac{1} {\\|\\nabla \\mc{L}(\\W(k))\\|^2}.\n$$\nHence, \\eqref{eqn:localgd:3:nabla0} gives $\\W(k+1) \\in \\conb_{\\mu,R}$.\n\\\\\n\\textbf{Step 4: The correlation of $\\W(k)$ and $\\Wm$ increases over $k$.} \nIt follows from Theorem~\\ref{diverg:norm:w} that \n$\\left\\Vert \\W\\left(k\\right)\\right\\Vert \\rightarrow\\infty$. Hence, we can choose $k_0$ such that for any $k\\ge k_0$, it holds that $\\tf{\\W(k)}>  R$ for some $R \\geq R_\\mu \\vee R_\\epsilon \\vee 1/2$. Now, following similar steps in \\eqref{eqn:decpath:1} and \\eqref{eqn:decpath:22},  for  some constant $C(\\epsilon,\\eta)$, we obtain\n\\begin{align*}\n      \\left\\langle\\frac{\\W(k)}{\\tf{\\W(k)}}, \\frac{\\Wm}{\\tf{\\Wm}} \\right\\rangle \\ge1-\\epsilon+ \\frac{C(\\epsilon,\\eta)}{\\tf{\\W(k)}}.      \n\\end{align*}\nConsequently,\n    \\begin{align*}\n      \\liminf_{t\\to\\infty}\\iprod{\\frac{\\W(k)}{\\tf{\\W(k)}}}{\\frac{\\Wm}{\\tf{\\Wm}}}\\ge1-\\epsilon.\n    \\end{align*}\nSince  $\\epsilon \\in (0, \\mu/2)$  is arbitrary, we get $\\W(k)/\\tf{\\W(k)}\\to  \\Wm/\\tf{\\Wm}$.\n\\end{proof} \n==== END OF /2308.16898/supp/great_leap_forward.tex ====\n==== BEGINNING OF /2308.16898/supp/QK_max_margin.tex ====\n\\subsection{Convergence of Gradient Descent  for (K,Q)-Parameterization}\\label{sec:KQ convergence proof}\n\nIn the following, our objective is to establish analogous assurances for Algorithm \\ref{GD-QK} when applied to \\eqref{eqn:erm:kq}. Nevertheless, unlike Theorem~\\ref{diverg:norm:w}, where a fixed step size of $O(1/L_{\\W})$ is utilized, we now necessitate an adaptive step size configuration for the $(\\Kb, \\Qb)$ decomposition.  This is because $(1/L_\\Kb, 1/L_\\Qb)$ tends to  $0$ as the number of iterations approaches infinity. To proceed, for any $R > 0$, we introduce the set $\\mc{S}(R)$ as follows:\n\\begin{equation}\\label{eqn:kq:set}\n \\mc{S}(R):= \\left\\{(\\Kb,\\Qb)~~\\big|~~\\tf{\\Qb} \\leq R,~~\\tf{\\Kb} \\leq R \\right\\}.   \n\\end{equation}\nThe following lemma demonstrates that given any $R>0$, the number of iterations taken by Algorithm~\\ref{GD-QK} within $\\Sc(R)$ is finite. \n\\begin{lemma}\\label{lem:out:S}\nSuppose Assumptions~\\ref{assum:loss:prope} and \\ref{assum:token} hold. Assume the gradient at the initialization $(\\Kb(0), \\Qb(0))$ is nonzero and $\\Lc(\\Kb(0), \\Qb(0)) \\leq \\Lc(0, 0)$. Consider Algorithm~\\ref{GD-QK} with a step size $\\eta=1/L(R)$, where $L(R):=RL_{\\W}$. Then for any $R>0$, there exists an iteration index $k$ at which $(\\Kb(k),\\Qb(k)) \\notin \\mc{S} (R)$.\n\\end{lemma}\n\\begin{proof}\nLet us select a value for $R$ and fix the step size to $\\eta=1/L(R)$. Assuming that both $(\\Kb({k+1}), \\Qb(k+1))$ and $(\\Kb({k}), \\Qb(k))$ reside within the set $\\mc{S}(R)$, we can establish a descent of objective analogous to Lemma \\ref{lem:grad:descent}. Specifically, we obtain the following result: \n\\begin{align}\\label{eq:descent:obj}\n\\nonumber\n\\mathcal{L}(\\Kb({k+1}), \\Qb({k+1}))-\\mathcal{L}(\\Kb({k}),\\Qb(k)) &\\leq-\\frac{1}{2 L(R)} \\tf{\\nabla \\mathcal{L}(\\Kb({k}), \\Qb(k))}^2 \\\\\n& =-\\frac{\\eta}{2} \\tf{\\nabla \\mathcal{L}(\\Kb({k}), \\Qb(k))}^2.\n\\end{align}\nBy our assumption, $\\tf{\\nabla \\Lc (\\Kb(0), \\Qb(0))} \\neq 0$, which implies that  %for all $k \\geq 1$, \n\\begin{equation}\\label{eqn:obj:less0}\n    \\mathcal{L}(\\Kb({1}),\\Qb(1)) \\leq  \\mathcal{L}(\\Kb({0}),\\Qb(0)) - \\frac{1}{ 2 L(R)} \\tf{\\nabla \\mathcal{L}(\\Kb({0}), \\Qb(0))}^2 <    \\mathcal{L}(\\Kb(0),\\Qb(0)) \\leq  \\mathcal{L}(0,0).\n\\end{equation}\nNext, we claim that for any $R>0$, there exists a constant $\\epsilon(R)>0$, such that \n\\begin{align}\\label{eqn:grad:low:eps}\n\\forall k\\ge1\\,:\\,  \\quad (\\Kb (k),\\Qb(k)) \\in \\mathcal{S}, \\qquad \\tf{\\nabla_{\\Qb} \\Lc(\\Kb(k),\\Qb(k))}\\geq \\epsilon(R). \n\\end{align}\nFix an arbitrary $R>0$. If \\eqref{eqn:grad:low:eps} is not true, then for any $\\epsilon>0$, there exists some $k\\ge1$ such that $\\|\\Kb(k)\\|_F\\le R$ and  $\\tf{\\Qb(k)} \\leq R$ while  \\eqref{grad def KQ} gives\n\\begin{align}\\label{eqn:qgrad}\n\\nonumber\n\\left\\|\\nabla_{\\Qb} \\Lc(\\Kb (k),\\Qb(k))\\right\\|_F^2 & =\\left\\|\\frac{1}{n}\\sum_{i=1}^n \\ell' \\left(\\bgam_i^\\top \\sft{\\g_i}\\right) \\cdot \\X_i^\\top  \\sfp{\\g_i}  \\bgam_i  \\z_{i}^\\top \\Kb(k)\\right\\|_F^2 \\\\\n&=  \\tf{\\nabla \\Lc (\\W(k))}^2 \\left\\|\\Kb(k)\\right\\|^2_F \\leq \\epsilon^2,   \n\\end{align}\nwhere $\\g_{i}=\\X_i\\Kb(k) \\Qb(k)^\\top\\z_{i}=\\X_i\\W(k)\\z_{i}$.\n\nIt follows from Lemma~\\ref{global des lem} that $\\li\\nabla\\Lc(\\W (k)),\\Wm/\\|\\Wm\\|_F\\ri \\leq  - c <0$ for some positive constants $c$.  %\\ct{should this be $\\langle{\\nabla \\Lc (\\W (k))},{-\\Wm/\\|\\Wm\\|_F}\\rangle \\geq Mc$} \n%Let $-M= \\max_{x} \\ell'(x)$. Since $\\ell'$ is continuous and the domain is bounded, the maximum is attained and negative. \nHence, $\\tf{\\nabla \\Lc (\\W(k))}  \\geq c$ and $\\tf{\\Kb(k)} \\leq \\epsilon/c$.  This together with $\\tf{\\Qb(k)} \\leq R$ implies that   $\\tf{ \\Kb(k) \\Qb(k)^\\top} \\leq  \\epsilon R/ c$.  In other words,  after $k=1$, $\\tf{ \\Kb(k) \\Qb(k)^\\top}$  may be arbitrarily small, which implies $\\Lc(\\Kb(k), \\Qb(k))$ can be arbitrarily close to $ \\mathcal{L}(\\Kb(0),\\Qb(0))$. % \\ct{Does this need $K(0)=Q(0)=0$}.\nThis is a contradiction to \\eqref{eqn:obj:less0}. Hence, \\eqref{eqn:grad:low:eps} holds.\n\nNow,  it follows from \\eqref{eq:descent:obj} and \\eqref{eqn:grad:low:eps}  that \n\\begin{align*}\n  \\mathcal{L}(\\Kb(0),\\Qb(0))\n         \\ge   \\mathcal{L}(\\Kb(0),\\Qb(0)) -\\mathcal{L}_{\\star} \\geq  \\frac{1}{2 L(R)}\\sum_{k=0}^{\\infty} \\left\\|\\nabla_{\\Qb} \\Lc(\\Kb(k),\\Qb(k)) \\right\\|_{F}^2\n         %& =\\sum_{t=0}^{\\infty}\\|W_L\\cdots W_2\\|^2\\|\\nR(\\wnn)\\|^2\\dif t \\\\\n        % & %\\ge\\sum_{t=0}^{\\infty}\\tf{\\Kb(k)}^2~~\\tf{\\nabla \\mc{L}(\\W)}^2\\\\\n          \\geq \\infty,\n    \\end{align*}\nwhich is a contradiction. Hence,  $(\\Kb(k),\\Qb(k))$ must go out of $\\mc{S} (R)$.\n\\end{proof}\n\nNote that in Lemma~\\ref{lem:out:S},  we assume that the gradients at the initialization $(\\Kb(0), \\Qb(0))$ are nonzero. Requiring the initialization not to be a critical point is reasonable because if it were, gradient descent would be unable to make any progress.  From Lemma~\\ref{lem:out:S}, we see that $(\\Kb(k),\\Qb(k))$ can go out of the set $\\mc{S} (R)$.  However, we can address this matter by dynamically increasing $R$ while proportionally decreasing the step sizes, as formalized in the following theorem:\n%\n\\begin{theorem}\\label{diverg:norm:qk}%\\redp{TO FIX}\nSuppose Assumption~\\ref{assum:loss:prope} on the loss function $\\ell$ and Assumption \\ref{assum:token} on the tokens hold. Assume the initialization $(\\Kb(0), \\Qb(0))$ satisfies $\\nabla \\Lc (\\Kb(0),\\Qb(0)) \\neq 0$. Let $\\eta_k=\\min\\{1/L(R_k),1\\}$, where $R_k$ is chosen such that $(\\Kb(k),\\Qb(k))\\in \\mc{S}(R_k-1)$, and if $(\\Kb(k+1),\\Qb(k+1))\\in \\mc{S} (R_k-1)$, then $R_{k+1}=R_k$.  Then, the following statements hold:\n\\begin{itemize}\n\\item %\\textbf{No stationary points:} \nThere is no $\\Kb,\\Qb\\in\\R^{d\\times m}$ satisfying $\\nabla \\Lc(\\Kb,\\Qb)=0$.\n\\item Algorithm~\\ref{GD-QK} with the step size $\\eta_k$  satisfies  $\\lim_{k \\rightarrow \\infty} \\tf{\\nabla \\Lc_\\Kb(\\Kb(k), \\Qb(k))}\\vee\\tf{\\nabla\\Lc_\\Qb(\\Kb(k),\\Qb(k))}=0$, and  $\\lim_{k\\rightarrow\\infty} \\tf{\\Kb(k)}\\wedge\\tf{\\Qb(k)}=\\infty$.\n\\end{itemize}\n\\end{theorem}\n\n\\begin{proof}\nSince $(\\Kb(k),\\Qb(k))\\in \\mc{S}(R_k)$, %\\ct{$S(R_{k}-1)$? But I didn't understand why in Thm. 10 you need $R_{K}-1$}\nwe have \n    \\begin{align}\n        \\|\\Qb(k+1)\\|_F \\le\\|\\Qb(k)\\|_F+\\eta_k\\tf{ \\nabla_{\\Qb} \\Lc(\\Kb(k),\\Qb(k))} \\nonumber    & \\le\\|\\Qb(k)\\|_F+\\frac{1}{L(R_k)}\\tf{\\nabla_{\\Qb} \\Lc(\\Kb(k),\\Qb(k))} \\nonumber \\\\\n%         & \\le\\|\\Qb(k)\\|_F+\\frac{1}{L(R_k)}R_kG \\nonumber \\\\\n         & \\le\\|\\Qb(k)\\|_F+1. \\label{eq:gd_inc}\n    \\end{align}\n%Here the last inequality follows since \n    \nSince $R_k\\to\\infty$ by Lemma~\\ref{lem:out:S} and $R_{k+1}=R_k$ as long as $ (\\Kb(k+1),\\Qb(k+1))\\in \\mc{S}(R_k-1)$, we obtain\n\\begin{equation}\\label{eqn:c1}\n \\max\\{ \\|\\Qb(k)\\|_F,  \\|\\Kb(k)\\|_F \\} \\quad    \\textnormal{is unbounded}. \\tag{C1}\n\\end{equation}\n\n    \nIt then follows that for any $k$, by Cauchy-Schwarz,\n\\begin{align*}\n\\left(\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}\\right) \\left(\\sum_{\\tau=0}^{k-1}\\eta_{\\tau} \\|\\nabla \\Lc(\\Kb(\\tau),\\Qb(\\tau))\\|^2_F\\right) & \n\\ge \\left(\\sum_{\\tau=0}^{k-1}\\eta_{\\tau} \\|\\nabla \\Lc(\\Kb(\\tau),\\Qb(\\tau))\\|^2_F\\right)^2\\to\\infty.\n\\end{align*}\nSince by  \\eqref{eq:descent:obj},\n\\begin{align*}\n        \\sum_{\\tau=0}^{k-1}\\eta_{\\tau} \\left\\|\\nabla \\Lc(\\Kb(\\tau),\\Qb(\\tau))\\right\\|^2_F\\le 2\\Lc(\\Kb(0),\\Qb(0))-2\\Lc(\\Kb(k),\\Qb(k) )\\le2 \\Lc(\\Kb(0),\\Qb(0)),\n\\end{align*}\nwe have $\\sum_{t=0}^{\\infty}\\eta_k=\\infty$.\n\nSince gradient descent never increases the risk,  for $(\\Kb(k),\\Qb(k))\\in \\mc{S}(R)$, $\\|\\partial\\Lc/\\partial \\Qb(k)\\|_F\\ge\\epsilon(R)$ for some constant $\\epsilon(R)>0$.  Following similar steps  as the proof of \\eqref{eqn:grad:low:eps},   we get that $\\sum_{k:(\\Kb(k),\\Qb(k))\\in \\mc{S}(R)}^{}\\eta_k<\\infty$.  \n\n\nFor simplicity, we  replace $\\Qb^\\top$ with $\\Qb$ in \\eqref{eqn:erm:kq}. %Then, for any $(\\Kb,\\Qb)$, we have \n%     \\begin{equation}\\label{eq:gd_align_tmp1}\n%        \\Qb \\nabla_{\\Qb} \\Lc(\\Kb,\\Qb)^\\top= \\nabla_{\\Kb} \\Lc(\\Kb,\\Qb)   \\Kb^{\\top} .\n%     \\end{equation}\n For gradient descent iterates \\ref{GD-QK}, summing from $0$ to $k-1$, we get\n\\begin{align}\\label{eqn:qk:recur}\n& \\quad\\Qb(k)^\\top\\Qb(k)-\\Qb(0)^\\top\\Qb(0)+\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\nabla_{\\Qb} \\Lc(\\Kb(\\tau),\\Qb(\\tau))^\\top \\nabla_{\\Qb} \\Lc(\\Kb(\\tau),\\Qb(\\tau)) \\nonumber \\\\\n= & \\quad \\Kb(k)\\Kb^{\\top}(k)-\\Kb(0)\\Kb^{\\top}(0)+\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2  \\nabla_{\\Kb} \\Lc(\\Kb(\\tau),\\Qb(\\tau)) \\nabla_{\\Kb} \\Lc(\\Kb(\\tau),\\Qb(\\tau))^\\top.\n\\end{align}\nLet\n\\begin{subequations}\n    \\begin{align*}\n       P_{\\Qb}&=\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\nabla_{\\Qb} \\Lc(\\Kb,\\Qb) \\nabla_{\\Qb} \\Lc(\\Kb(\\tau), \\Qb (\\tau)))^\\top,\\\\\n   P_{\\Kb}&:=\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\nabla_{\\Kb} \\Lc(\\Kb,\\Qb)  \\nabla_{\\Qb} \\Lc(\\Kb(\\tau), \\Kb (\\tau)))^\\top,\n    \\end{align*}\n    and\n\\begin{align*}\nS_{\\Qb}(k)=\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\nabla_{\\Qb} \\Lc(\\Kb(\\tau), \\Qb (\\tau)))^\\top \\nabla_{\\Qb} \\Lc(\\Kb(\\tau), \\Qb (\\tau))),\\\\\nS_{\\Kb}(k)=\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\nabla_{\\Kb} \\Lc(\\Kb(\\tau), \\Qb (\\tau)))^\\top \\nabla_{\\Kb} \\Lc(\\Kb(\\tau), \\Qb (\\tau))) .\n    \\end{align*}\n\\end{subequations}\nWe obtain\n\\begin{subequations}\n    \\begin{align}\\label{eq:qk:tr:b1}\n           \\tr(P_{\\Kb}(k))+ \\tr(P_{\\Qb}(k))  & =\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\|\\nabla \\Lc(\\Kb(\\tau), \\Qb (\\tau))\\|^2_F \\nonumber \\\\\n         & \\le \\sum_{\\tau=0}^{k-1}\\eta_{\\tau} \\|\\nabla \\Lc(\\Kb(\\tau), \\Qb (\\tau))\\|^2_F  \\nonumber \\\\\n         &  \\leq 2 \\Lc(\\W(0)) -2 \\Lc(\\W(k))  \\nonumber\\\\\n            &  \\leq 2 \\Lc(\\W(0)) -2 \\Lc_\\star.\n    \\end{align}\nIt follows from \\eqref{eqn:qk:recur} that\n\\begin{align}\\label{eq:qk:tr:b2}\n\\|\\Kb(k)\\|_F^2=\\|\\Qb(k)\\|_F^2+\\|\\Kb(0)\\|_F^2-\\|\\Qb(0)\\|_F^2-\\tr(P_{\\Kb}(k))+\\tr(S_{\\Qb}(k)).\n\\end{align}    \n\\end{subequations}\nIn other words, the difference between the squares of Frobenius norms of $(\\Kb,\\Qb)$ is still bounded.\n% It shows that gradient descent spends a finite amount of time in $\\mc{S}$ for any $R>0$.\n\nNow, combining \\eqref{eq:qk:tr:b1} and \\eqref{eq:qk:tr:b2}, we get\n \\begin{align*}\n   2 \\Lc(\\W(0))\n           \\ge\\sum_{t=0}^{\\infty} \\eta_k \\left\\|\\nabla_{\\Qb} \\Lc(\\Kb (k),\\Qb(k))\\right\\|_F^2 %\\\\\n          %& =\\sum_{t=0}^{\\infty}\\|W_L\\cdots W_2\\|^2\\|\\nR(\\wnn)\\|^2\\dif t \\\\\n          %& \\ge\\sum_{t=0}^{\\infty}\\|\\Kb(k)\\|~~\\|\\nabla \\mc{L}(\\W) \\|\\\\\n           \\geq \\infty,\n     \\end{align*}\n     which is a contradiction. This implies $\\tf{\\Kb(k)}\\to\\infty$, since $\\Lc(\\Kb,\\Qb)$ has no finite optimum.\n\\end{proof}\n\nTheorem~\\ref{diverg:norm:qk} is the formal version of the $(\\Kb,\\Qb)$-statement in Theorem \\ref{diverg:norm:w}. It provides similar guarantees to the $\\W$-statements of Theorem \\ref{diverg:norm:w}, namely the absence of finite stationary points and the divergence of the parameter norm to infinity. It is important to mention that the step size can be appropriately determined using methods such as a line search. The line search ensures that the \\ref{GD-QK} update is not excessively aggressive, thus allowing the boundary $R$ to be increased as needed.\n\n%\n%\n%In the next two sections, we prove Lemma~\\ref{lem:out:S} \\& Theorem \\ref{diverg:norm:qk} and present an analysis of gradient descent applied to equation \\eqref{eqn:erm:kq} with slight simplification. To improve readability, we substitute $\\Qb^\\top$ with $\\Qb$ in \\eqref{eqn:erm:kq}.\n\n%\\subsubsection{Proof of Lemma~\\ref{lem:out:S}}\n% we can address this issue by dynamically adjusting the value of $R$ and correspondingly reducing the step sizes. This is formalized in the following assumption:\n% From Lemma~\\ref{lem:out:S}, we see that $(\\Kb(k),\\Qb(k))  $ can go out of the set $\\mc{S} (R)$.  However, we can address this matter by dynamically increasing $R$ while proportionally decreasing the step sizes, as formalized below:\n% % \\begin{assumption}\\label{ass:step_size}\n% % The step size $\\eta_k=\\min\\{1/L(R_k),1\\}$, where $R_k$ is chosen such that $(\\Kb(k),\\Qb(k))\\in \\mc{S}(R_k-1)$, and if $(\\Kb(k+1),\\Qb(k+1))\\in \\mc{S} (R_k-1)$, then $R_{t+1}=R_k$.\n% % \\end{assumption}\n% Note that this condition can be satisfied, for example, by employing a line search, which ensures that the gradient descent update is not overly aggressive and allows the boundary $R$ to be increased appropriately.\n% \\begin{lemma}\\label{lem:gd_unbounded}\n% Under same assumptions as in Theorem~\\ref{diverg:norm:qk}, gradient descent ensures that\n%     \\begin{enumerate}\n%         \\item \n%         \\item $ \\max\\{ \\|\\Qb(k)\\|_F,  \\|\\Kb(k)\\|_F \\}$   is unbounded.\n%         \\item $\\sum_{k=0}^{\\infty}\\eta_k=\\infty$.\n%         \\item For any $R>0$, $\\sum_{k: (\\Kb(k),\\Qb(k)) \\in \\mc{S}(R)}^{}\\eta_k<\\infty$. % $\\W(k):=(\\Kb(k),\\Qb(k))$\n%     \\end{itemize}\n% \\end{lemma}\n\n% \\begin{proof}\n\n%\\end{proof}\n\n%\\subsubsection{Proof of Theorem~\\ref{diverg:norm:qk}}\n%\n\n% \\subsection{Convergence of $(Q,K)$ parameterization to max-margin direction}\n% \\red{working}\n\n% Recall that:\n%     \\begin{equation}\\label{eqn:qk:joint}\n%       \\Kb^{\\top}(k)\\Kb(k)-\\Kb^{\\top}(0)\\Kb(0) + Q_{\\Qb}(k)=\\Qb(k)\\Qb^{\\top}(k)-\\Qb(0)\\Qb^{\\top}(0) + P_{\\Kb}(k).\n%     \\end{equation}\n%     Let $A=\\Qb(0)\\Qb^{\\top}(0)-\\Kb^{\\top}(0)\\Kb(0)$. By \\eqref{eqn:qk:joint} and the definition of singular vectors and singular values, we have\n%     \\begin{align}\n%         \\sigma_{\\Qb}^2 & \\geq v_{\\Kb}^{\\top}\\Qb\\Qb^{\\top}v_{\\Kb} \\nonumber \\\\\n%          & =v_{\\Kb}^{\\top}\\Kb^{\\top}\\Kb v_{\\Kb}+v_{\\Kb}^{\\top} A v_{\\Kb} \\nonumber \\\\\n%          & =\\sigma_{\\Kb}^2+v_{\\Kb}^{\\top}A v_{\\Kb} - \\nonumber \\\\\n%          & \\ge\\sigma_{\\Kb}^2-\\|A\\| - \\|P_{\\Qb}(k)\\|_2 \\nonumber\\\\\n%                & \\ge\\sigma_{\\Kb}^2-\\|A\\| - \\tr(P_{\\Qb}(k))\n%          . \\label{eqn:svd:qk}\n%     \\end{align}\n% Moreover, by taking the trace on both sides of \\eqref{eqn:qk:joint}, we have\n%     \\begin{align}\n%         % \\|\\Qb\\|_F^2=\\tr\\left(\\Qb\\Qb^{\\top}\\right) & =\\tr(\\Kb^{\\top}\\Kb)+\\tr(\\Qb(0)\\Qb^{\\top}(0))-\\tr(\\Kb^{\\top}(0)\\Kb(0)) \\nonumber \\\\\n%         %  & =\\|\\Kb\\|_F^2+\\|\\Qb(0)\\|_F^2-\\|\\Kb(0)\\|_F^2.\n% \\|\\Qb(k)\\|_F^2=\\|\\Kb(k)\\|_F^2+\\|\\Qb(0)\\|_F^2-\\|\\Kb(0)\\|_F^2-\\tr(P_{\\Qb}(k))+\\tr(Q_{\\Kb}(k))\n%          \\label{eq:ind_2}\n%     \\end{align}\n% Hence, \n%     \\begin{equation}\\label{eq:partial_sum}\n%         \\|\\Qb\\|_F^2-\\|\\Qb\\|_2^2\\le\\|\\Qb(0)\\|_F^2-\\|\\Kb(0)\\|_F^2+\\|A\\|\\le D.\n%     \\end{equation}\n% Next, we prove that singular vectors get aligned. Consider $u_{\\Qb}^{\\top}\\Kb^{\\top}\\Kb u_{\\Qb}$. On one hand, similar to \\eqref{eqn:svd:qk}, we can get that\n%     \\begin{align}\\label{eqn:svd:qk2}\n%         u_{\\Qb}^{\\top}\\Kb^{\\top}\\Kb u_{\\Qb} & =u_{\\Qb}^{\\top}\\Qb\\Qb^{\\top}u_{\\Qb}-u_{\\Qb}^{\\top}\\Qb(0)\\Qb^{\\top}(0)u_{\\Qb}+u_{\\Qb}^{\\top}\\Kb^{\\top}(0)\\Kb(0)u_{\\Qb}-\\tn{Q_{\\Kb}(k)} \\nonumber \\\\\n%          & \\ge u_{\\Qb}^{\\top}\\Qb\\Qb^{\\top}u_{\\Qb}-u_{\\Qb}^{\\top}\\Qb(0)\\Qb^{\\top}(0)u_{\\Qb} -\\tn{Q_{\\Kb}(k)} \\nonumber \\\\\n%          & \\ge\\sigma_{\\Qb}^2-\\|\\Qb(0)\\|^2 - \\tn{Q_{\\Kb}(k)}.\n%     \\end{align}\n% On the other hand, it follows from the definition of singular vectors and \\eqref{eq:partial_sum} that\n% \\begin{align}\n% u_{\\Qb}^{\\top}\\Kb^{\\top}\\Kb u_{\\Qb} & =\\langle u_{\\Qb},v_{\\Kb}\\rangle^2\\sigma_{\\Kb}^2+u_{\\Qb}^{\\top}\\del{\\Kb^{\\top}\\Kb-v_{\\Kb}\\sigma_{\\Kb}^2v_{\\Kb}^{\\top}}u_{\\Qb} + 2\\Lc(\\Kb(0),\\Qb(0))\\nonumber \\\\\n% & \\le \\langle u_{\\Qb},v_{\\Kb}\\rangle^2\\sigma_{\\Kb}^2+\\|\\Kb\\|_F^2-\\|\\Kb\\|^2 + 2\\Lc(\\Kb(0),\\Qb(0)) \\nonumber \\\\\n% & \\le \\langle u_{\\Qb},v_{\\Kb}\\rangle^2\\sigma_{\\Kb}^2+D + 2 \\Lc(\\Kb(0),\\Qb(0)). \\label{eqn:svd:qk3}\n% \\end{align}\n% Combining \\eqref{eqn:svd:qk2} and \\eqref{eqn:svd:qk3}, we get\n%     \\begin{align}\n%         \\sigma_{\\Qb}^2 & \\le \\langle u_{\\Qb},v_{\\Kb}\\rangle^2\\sigma_{\\Kb}^2+D+\\|\\Qb(0)\\|^2 +2 \\Lc((\\Kb(0),\\Qb(0))). \\label{eq:dp_prelim}\n%     \\end{align}\n%     Similar to \\eqref{eqn:svd:qk2}, we can get\n%     \\begin{align*}\n%         \\sigma_{\\Qb}^2\\ge v_{\\Kb}^{\\top}\\Qb\\Qb^{\\top}v_{\\Kb}\\ge\\sigma_{\\Kb}^2-\\|\\Kb(0)\\|^2 -\\|P_{\\Qb}(k)\\|_2\n%     \\end{align*}\n%     Therefore\n%     \\begin{equation}\\label{eqn:ratio}\n%         \\frac{\\sigma_{\\Qb}^2}{\\sigma_{\\Kb}^2}\\ge1-\\frac{\\|\\Kb(0)\\|^2 +\\|P_{\\Qb}(k)\\|}{\\sigma_{\\Kb}^2}.\n%     \\end{equation}\n%     Combining \\eqref{eq:dp_prelim} and \\eqref{eqn:ratio}, we finally get\n%     \\begin{equation*}\n%         \\langle u_{\\Qb},v_{\\Kb}\\rangle^2\\ge1-\\frac{D+\\|\\Qb(0)\\|^2+\\|\\Kb(0)\\|^2 +3\\Lc((\\Kb(0),\\Qb(0)))}{\\sigma_{\\Kb}^2} \\rightarrow 0.\n%     \\end{equation*}\n==== END OF /2308.16898/supp/QK_max_margin.tex ====\n==== BEGINNING OF /2308.16898/supp/separation.tex ====\n% \\section{Proof of Theorem \\ref{separation thm}: Separability Under Mild Over-Parameterization}\\label{app sep}\n\\section{Proof of Theorem \\ref{thm:separation}: Separability Under Mild Over-Parameterization}\\label{app sep}\n\n\n%\\begin{proof} \n\nWe denote Kronecker product of two matrices via $\\kron$. Additionally, given $\\W\\in\\R^{d\\times d}$, let us denote its vectorization $\\w=\\text{vec}(\\W)\\in\\R^{d^2}$. We first note that separation is implied by the linear independence of the constraints. Specifically, we are interested in guaranteeing\n\\[\n\\li\\w,\\fb_\\itt\\ri\\geq 1\\quad \\text{for all}\\quad i\\in[n],~~t\\neq\\bal_i, ~~ \\textnormal{where} ~~\\fb_\\itt:=(\\x_{i\\bal_i}-\\x_\\itt)\\kron \\z_i.\n\\]\nNote that, the inequality constraints above are feasible as soon as $\\fb_\\itt$'s are linearly independent. Thus, we will instead prove linear independence of the vectors $(\\fb_\\itt)_{i\\in[n],t\\neq\\bal_i}$. Also note that, since there are finitely many $\\bal$ choices, if we show almost sure separation for a fixed but arbitrary $\\bal$ choice, through union bound, we recover the result for all $\\bal$. Thus, we prove the result for a fixed $\\bal$ choice.\n\n\nWe will prove this result inductively.  Let $\\M_{n-1}\\in\\R^{(n-1)(T-1)\\times d^2}$ denote the matrix whose rows are given by the features $(\\fb_\\itt)_{i\\in[n-1],t\\neq\\bal_i}$. Suppose the result is correct for $n-1$, thus, $\\M_{n-1}$ is full row-rank almost surely (post random Gaussian perturbation). Now, fix $\\M_{n-1}$ and, conditioned on $\\M_{n-1}$ being full row-rank, let us show that $\\M_n$ is also full row-rank almost surely. To prove this, consider the $n$'th example $(\\X_n,\\z_n)$. Let $(\\g_t)_{t=1}^T,\\hb\\in\\R^d$ be random vectors with i.i.d.~$\\Nn(0,\\sigma^2)$ entries. Consider the perturbed input $\\X'_n \\in\\R^{T\\times d}$ with tokens $\\x'_{nt}=\\x_{nt}+\\g_t$ and $\\z'_n=\\z_n+\\hb$. Note that for self-attention, we set $\\z_n=\\x_{n1}$ and $\\hb=\\g_1$. From these, create the matrix $\\tilde{\\M}_n \\in \\R^{(T-1)\\times d^2}$ with rows $(\\fb'_{nt})_{t\\neq\\bal_n}$ where $\\fb'_{nt}=(\\x'_{n\\bal_n}-\\x'_{nt})\\kron \\z'_n$. Observe that $\\M_n=\\begin{bmatrix}\\tilde{\\M}_n\\\\\\M_{n-1}\\end{bmatrix}$. To conclude with the result, we will apply Lemma \\ref{lem add up}. To apply this lemma, we have two claims.\n\n\\noindent\\textbf{Claim 1:} Let $\\bar{\\z}_n$ be the projection of $\\z'_n$ on the orthogonal complement of $(\\z_i)_{i=1}^{n-1}$. Consider the matrix $\\bar{\\M}_n$ with rows $\\bar{\\fb}_{nt}=(\\x'_{n\\bal_n}-\\x'_{nt})\\kron \\bar{\\z}_n$ for $t\\neq \\bal_n$. $\\bar{\\M}_n$ is rank $T-1$ almost surely whenever $d\\geq \\max(T-1,n)$.\n\nTo see this claim, first denote the orthogonal complement of the span of the vectors $(\\z_i)_{i=1}^{n-1}$ by $Z_{n-1}$. The span of the vectors $(\\z_i)_{i=1}^{n-1}$ is at most $n-1$ dimensional and, since $d\\geq n$, $\\text{dim}(Z_{n-1})\\geq 1$. Consequently, $\\bar{\\z}_n\\neq 0$ almost surely because the Gaussian variable $\\z_n+\\hb$ will have nonzero projection on $Z_{n-1}$ almost surely. Secondly, let $\\bar{\\X}\\in\\R^{(T-1)\\times d}$ be the matrix whose rows are equal to $\\x'_{n\\bal_n}-\\x'_{nt}$ for $t\\neq \\bal_n$. $\\bar{\\X}$ is full row-rank almost surely, this is because conditioned on $\\g_{\\bal_n}$, the matrix $\\bar{\\X}$ is written as $\\bar{\\X}=\\tilde{\\X}+\\Gb$ where $\\tilde{\\X}$ is deterministic and $\\Gb$ is i.i.d. Gaussian. The latter perturbation ensures full row-rank almost surely whenever $T-1\\leq d$. Finally, note that $\\bar{\\M}_n=\\bar{\\X}\\kron \\bar{\\z}_n$. Since the rank of the Kronecker product is multiplicative, we conclude with the claim.\n\n\\noindent\\textbf{Claim 2:} Let $S_{n-1}\\subset\\R^{d^2}$ be the null space of $\\M_{n-1}$. There exists a subspace $P\\subseteq S_{n-1}$ such that rows of $\\bar{\\M}_n$ are projections of the rows of $\\M_n$ on $P$, that is, $\\bar{\\fb}_{nt}=\\Pi_{P}(\\fb'_{nt})$ where $\\Pi$ denotes set projection.\n\nTo show this claim, let us consider the matrix forms of the vectorized features i.e.~let us work with $\\R^{d\\times d}$ rather than $\\R^{d^2}$. Denote the notation change as $\\Fb_\\itt=(\\x_{i\\bal_i}-\\x_\\itt) \\z_i^\\top\\leftrightarrow \\fb_\\itt=(\\x_{i\\bal_i}-\\x_\\itt)\\kron \\z_i$. Recall that $Z_{n-1}$ denotes the orthogonal complement of $(\\z_i)_{i=1}^{n-1}$. Define $Q$ to be the set of matrices in $\\R^{d\\times d}$ whose column space lies in $Z_{n-1}$ and $P$ to be the vectorization of $Q$. We first show that $P$ is a subset of the null space of $S_{n-1}$. To see this, fix any matrix $\\A\\in P$ and a row $\\fb_\\itt$ from $\\M_{n-1}$. Matricized $\\fb_\\itt$ can be written as $\\Fb_\\itt=\\ab\\z_i^\\top$ for $\\z_i\\in Z_{n-1}^\\perp$. Since $\\A\\in Q$, this implies $\\li\\Fb_\\itt,\\A\\ri=\\ab^\\top\\A\\z_i=0$ as $\\A\\z_i=0$. This holds for all $\\Fb_\\itt$, thus, $\\texttt{vectorized}(\\A)\\in\\texttt{null}(S_{n-1})$.\n\nNext, we need to show that $\\bar{\\fb}_{nt}$ is the projection of $\\fb'_{nt}$ on $P$. To see this, we will show that $\\bar{\\fb}_{nt}\\in P$ whereas $\\fb'_{nt}-\\bar{\\fb}_{nt}\\in P^\\perp$ for all $t$. Write $\\Fb'_{nt}=\\texttt{matricized}(\\fb'_{nt})=\\ab{\\z'}_n^\\top$. We have that $\\bar{\\Fb}_{nt}=\\texttt{matricized}(\\bar{\\fb}_{nt})=\\ab\\bar{\\z}_n^\\top$ where $\\bar{\\z}_n=\\Pi_{Z_{n-1}}(\\z'_n)$. This implies $\\bar{\\Fb}_{nt}\\in Q$ and $\\bar{\\fb}_{nt}\\in P$. Similarly, since $\\z'_n-\\bar{\\z}_n\\in Z_{n-1}^\\perp$ which implies $\\Fb'_{nt}-\\bar{\\Fb}_{nt}\\in Q^\\perp$.  %Now pick a matrix $\\A\\in Q$. Since, we have \n\nTo conclude with the proof, observe that, through Claims 1 and 2, $\\M_n=\\begin{bmatrix}\\tilde{\\M}_n\\\\\\M_{n-1}\\end{bmatrix}$ satisfies the requirements of Lemma \\ref{lem add up} almost surely, namely, projection of $\\tilde{\\M}_n$ onto a subset of the null space of $\\M_{n-1}$ being full rank. Thus, $\\M_n$ is full rank almost surely.\n%\\end{proof}\n $\\qed$ \n %\n\\begin{lemma}\\label{lem add up}Let $\\A\\in\\R^{n\\times p},\\B\\in\\R^{m\\times p}$. Suppose $n+m\\leq p$ and $\\A$ is full row-rank. Denote the null space of $\\A$ by $S_\\A^\\perp$. Let $P$ be a subspace that is its subset i.e.~$P\\subseteq S_\\A^\\perp$. Let $\\B'$ be the matrix obtained by projecting each of row of $\\B$ on $P$ and suppose $\\B'$ is full rank. Then, the concatenation $\\Cb= [\\A; \\B ]$\n%\\begin{bmatrix}\\A\\\\\\B\\end{bmatrix}$\nis full row-rank.\n\\end{lemma}\n\\begin{proof} Let $(\\ab_i)_{i=1}^n$, $(\\bb_i)_{i=1}^m$, $(\\bb'_i)_{i=1}^m$ be the rows of $\\A,\\B,\\B'$, respectively. Suppose the set of rows of $\\A$ and $\\B$ are linearly dependent. Then, for some $(c_i)_{i=1}^{n},(c'_i)_{i=1}^{m}$ (which are not all-zeros), we have that\n\\begin{align}\n\\sum_{i=1}^n c_i\\ab_i+\\sum_{i=1}^m c'_i\\bb_i=0.\\label{cs are nonzero}\n\\end{align}\nWe now rewrite this as follows to decouple $P$ and $P^\\perp$:\n\\[\n\\sum_{i=1}^n c_i\\ab_i+\\sum_{i=1}^m c'_i\\bb'_i+\\sum_{i=1}^m c'_i(\\bb'_i-\\bb_i)=0.\n\\]\n%\\red{The two terms on the left side lie in $P^\\perp$.}\nProjecting above inequality to $P$, we find that $\\sum_{i=1}^m c'_i\\bb'_i=0$. Since $(\\bb'_i)_{i=1}^m$ are linearly independent, we find $c'_i=0$ for all $i\\in[m]$. This implies $\\sum_{i=1}^n c_i\\ab_i=0$. Since $(\\ab_i)_{i=1}^n$ are linearly independent, this implies $c_i=0$ for all $i\\in[n]$. Thus, \\eqref{cs are nonzero} can only hold if all coefficients are zero which is a contradiction.\n\\end{proof}\n\n\n%\\begin{lemma}\\label{lem add up}Let $\\A\\in\\R^{n\\times p},\\B\\in\\R^{m\\times p}$. Suppose $n+m\\leq p$ and $\\A$ is full row-rank. Let $\\B'$ be the matrix obtained by projecting each of row of $\\B$ on the orthogonal complement of the $n$-dimensional row-space of $\\A$. Suppose $\\B'$ is full rank. Then, the concatenation $\\Cb=\\begin{bmatrix}\\A\\\\\\B\\end{bmatrix}$ is full row-rank.\n%\\end{lemma}\n%\\begin{proof} Let $(\\ab_i)_{i=1}^n$, $(\\bb_i)_{i=1}^m$, $(\\bb'_i)_{i=1}^m$ be the rows of $\\A,\\B,\\B'$ respectively, Suppose the rows of $\\A,\\B$ are linearly dependent. Then, for some $(c_i)_{i=1}^{n},(c'_i)_{i=1}^{m}$ (which are not all-zeros), we have that\n%\\begin{align}\n%\\sum_{i=1}^n c_i\\ab_i+\\sum_{i=1}^m c'_i\\bb_i=0.\\label{cs are nonzero}\n%\\end{align}\n%We can rewrite this as\n%\\[\n%\\sum_{i=1}^n c_i\\ab_i+\\sum_{i=1}^m c'_i\\bb'_i+\\sum_{i=1}^m c'_i(\\bb'_i-\\bb_i)=0.\n%\\]\n%Denote the row space of $\\A$ by $S_A$. Projecting above inequality to $S_A^\\perp$, we find that $\\sum_{i=1}^m c'_i\\bb'_i=0$. Since $(\\bb'_i)_{i=1}^m$ are linearly independent, we find $c'_i=0$ for all $i\\in[m]$. This implies $\\sum_{i=1}^n c_i\\ab_i=0$. Since $(\\ab_i)_{i=1}^n$ are linearly independent, this implies $c_i=0$. Thus, \\eqref{cs are nonzero} can only hold if all coefficients are zero which is a contradiction.\n%\\end{proof}\n\n\n% \\begin{proof} \n% We denote kronecker product of two matrices via $\\kron$. Additionally, given $\\W\\in\\R^{d\\times d}$, let us denote its vectorization $\\w=\\text{vec}(\\W)\\in\\R^{d^2}$. We first note that separation is implied by the linear independence of the constraints. Specifically, let $\\fb_\\itt=(\\xa_i-\\x_\\itt)\\kron \\z_i$. We are interested in guaranteeing\n% \\[\n% \\li\\w,\\fb_\\itt\\ri\\geq 1\\quad \\text{for all}\\quad i\\in[n],t\\neq\\bal_i.\n% \\]\n% Note that, the constraints above are feasible as soon as $\\fb_\\itt$'s are linearly independent. Thus, we will instead prove linear independence of the vectors $(\\fb_\\itt)_{i\\in[n],t\\neq\\alpha_i}$. Also note that, since there are finitely many $\\bal$ choices, if we show almost sure separation for a fixed but arbitrary $\\bal$ choice, through union bound, we recover the result for all $\\bal$. Thus, we prove the result for a fixed $\\bal$ choice.\n\n\n% We will prove this result inductively.  Let $\\M_{n-1}\\in\\R^{(n-1)(T-1)\\times d^2}$ denote the matrix whose rows are given by the features $(\\fb_\\itt)_{i\\in[n-1],t\\neq\\alpha_i}$. Suppose the result is correct for $n-1$, thus, $\\M_{n-1}$ is full row-rank almost surely (post random gaussian perturbation). Now, fix $\\M_{n-1}$ and, conditioned on $\\M_{n-1}$ being full row-rank, let us show that $\\M_n$ is also full row-rank almost surely. To prove this, consider the $n$'th example $(\\X_n,\\z_n)$. Let $(\\g_t)_{t=1}^n,\\hb\\in\\R^d$ be random vectors with i.i.d.~$\\Nn(0,\\sigma^2)$ entries. Consider the perturbed input $\\X'_n$ with tokens $\\x'_{nt}=\\x_{nt}+\\g_t$ and $\\z'_n=\\z_n+\\hb$. Note that for self-attention, we set $\\z_n=\\x_{n1}$ and $\\hb=\\g_1$. From these, create the matrix $\\tilde{\\M}_n$ with rows $(\\fb'_{nt})_{t\\neq\\alpha_i}$ where $\\fb'_{nt}=\\x'_{nt}\\kron \\z'_n$. Observe that $\\M_n=\\begin{bmatrix}\\tilde{\\M}_n\\\\\\M_{n-1}\\end{bmatrix}$. To conclude with the result, we will apply Lemma \\ref{lem add up}. To apply this lemma, we have two claims.\n\n% \\noindent\\textbf{Claim 1:} Let $\\bar{\\z}_n$ be the projection of $\\z'_n$ on the orthogonal complement of $(\\z_i)_{i=1}^{n-1}$. Consider the matrix $\\bar{\\M}_n$ with rows $\\bar{\\fb}_{nt}=(\\x'_{n\\alpha_n}-\\x'_{nt})\\kron \\bar{\\z}_n$ for $t\\neq \\alpha_n$. $\\bar{\\M}_n$ is rank $T-1$ almost surely whenever $d\\geq \\max(T-1,n)$.\n\n% To see this claim, first denote the orthogonal complement of the span of the vectors $(\\z_i)_{i=1}^{n-1}$ by $Z_{n-1}$. The span is at most $n-1$ dimensional and, since $d\\geq n$, $\\text{dim}(Z_{n-1})\\geq 1$. Consequently, $\\bar{\\z}_n\\neq 0$ almost surely because the gaussian variable $\\z_n+\\hb$ will have nonzero projection on $Z_{n-1}$ almost surely. Secondly, let $\\bar{\\X}\\in\\R^{(T-1)\\times d}$ be the matrix whose rows are equal to $\\x'_{n\\alpha_n}-\\x'_{nt}$ for $t\\neq \\alpha_n$. $\\bar{\\X}$ is full row-rank almost surely, this is because: Conditioned on $\\g_{\\alpha_n}$, the matrix $\\bar{\\X}$ is written as $\\bar{\\X}=\\tilde{\\X}+\\Gb$ where $\\tilde{\\X}$ is deterministic and $\\Gb$ is i.i.d. gaussian. The latter perturbation ensures full row-rank almost surely whenever $T-1\\leq d$. Finally, note that $\\bar{\\M}_n=\\bar{\\X}\\kron \\bar{\\z}_n$. Since the rank of the Kronecker product is multiplicative, we conclude with the claim.\n\n% \\noindent\\textbf{Claim 2:} Let $S_{n-1}\\subset\\R^{d^2}$ be the null space of $\\M_{n-1}$. There exists a subspace $P\\subseteq S_{n-1}$ such that rows of $\\bar{\\M}_n$ are projections of the rows of $\\M_n$ on $P$, that is, $\\bar{\\fb}_{nt}=\\Pi_{P}(\\fb'_{nt})$ where $\\Pi$ denotes set projection.\n\n% To show this claim, let us consider the matrix forms of the vectorized features i.e.~let us work with $\\R^{d\\times d}$ rather than $R^{d^2}$. Denote the notation change as $\\Fb_\\itt=(\\xa_i-\\x_\\itt) \\z_i^\\top\\leftrightarrow \\fb_\\itt=(\\xa_i-\\x_\\itt)\\kron \\z_i$. Recall that $Z_{n-1}$ denotes the orthogonal complement of $(\\z_i)_{i=1}^{n-1}$. Define $Q$ to be the set of matrices in $\\R^{d\\times d}$ whose column space lies in $Z_{n-1}$ and $P$ to be the vectorization of $Q$. We first show that $P$ is a subset of the null space of $S_{n-1}$. To see this, fix any matrix $\\A\\in P$ and a row $\\fb_\\itt$ from $\\M_{n-1}$. Matricized $\\fb_\\itt$ can be written as $\\Fb_\\itt=\\ab\\z_i^\\top$ for $\\z_i\\in Z_{n-1}^\\perp$. Since $\\A\\in Q$, this implies $\\li\\Fb_\\itt,\\A\\ri=\\ab^\\top\\A\\z_i=0$ as $\\A\\z_i=0$. This holds for all $\\Fb_\\itt$, thus, $\\texttt{vectorized}(\\A)\\in\\texttt{null}(S_{n-1})$.\n\n% Next, we need to show that $\\bar{\\fb}_{nt}$ is the projection of $\\fb'_{nt}$ on $P$. To see this, we will show that $\\bar{\\fb}_{nt}\\in P$ whereas $\\fb'_{nt}-\\bar{\\fb}_{nt}\\in P^\\perp$ for all $t$. Write $\\Fb'_{nt}=\\texttt{matricized}(\\fb'_{nt})=\\ab{\\z'}_n^\\top$. We have that $\\bar{\\Fb}_{nt}=\\texttt{matricized}(\\bar{\\fb}_{nt})=\\ab\\bar{\\z}_n^\\top$ where $\\bar{\\z}_n=\\Pi_{Z_{n-1}}(\\z'_n)$. This implies $\\bar{\\Fb}_{nt}\\in Q$ and $\\bar{\\fb}_{nt}\\in P$. Similarly, since $\\z'_n-\\bar{\\z}_n\\in Z_{n-1}^\\perp$ which implies $\\Fb'_{nt}-\\bar{\\Fb}_{nt}\\in Q^\\perp$.  %Now pick a matrix $\\A\\in Q$. Since, we have \n\n% To conclude with the proof, observe that, through Claims 1 and 2, $\\M_n=\\begin{bmatrix}\\tilde{\\M}_n\\\\\\M_{n-1}\\end{bmatrix}$ satisfies the requirements of Lemma \\ref{lem add up} almost surely, namely, projection of $\\tilde{\\M}_n$ onto a subset of the null space of $\\M_{n-1}$ being full rank. Thus, $\\M_n$ is full rank almost surely.\n% \\end{proof}\n\n% \\begin{lemma}\\label{lem add up}Let $\\A\\in\\R^{n\\times p},\\B\\in\\R^{m\\times p}$. Suppose $n+m\\leq p$ and $\\A$ is full row-rank. Denote the null space of $\\A$ by $S_A^\\perp$. Let $P$ be a subspace that is its subset i.e.~$P\\subseteq S_A^\\perp$. Let $\\B'$ be the matrix obtained by projecting each of row of $\\B$ on $P$ and suppose $\\B'$ is full rank. Then, the concatenation $\\Cb= [\\A; \\B ]$\n% %\\begin{bmatrix}\\A\\\\\\B\\end{bmatrix}$\n% is full row-rank.\n% \\end{lemma}\n% \\begin{proof} Let $(\\ab_i)_{i=1}^n$, $(\\bb_i)_{i=1}^m$, $(\\bb'_i)_{i=1}^m$ be the rows of $\\A,\\B,\\B'$ respectively, Suppose the set of rows of $\\A$ and $\\B$ are linearly dependent. Then, for some $(c_i)_{i=1}^{n},(c'_i)_{i=1}^{m}$ (which are not all-zeros), we have that\n% \\begin{align}\n% \\sum_{i=1}^n c_i\\ab_i+\\sum_{i=1}^m c'_i\\bb_i=0.\\label{cs are nonzero}\n% \\end{align}\n% We now rewrite this as follows to decouple $P$ and $P^\\perp$:\n% \\[\n% \\sum_{i=1}^n c_i\\ab_i+\\sum_{i=1}^m c'_i\\bb'_i+\\sum_{i=1}^m c'_i(\\bb'_i-\\bb_i)=0.\n% \\]\n% The two terms on the left side lie in $P^\\perp$. Projecting above inequality to $P$, we find that $\\sum_{i=1}^m c'_i\\bb'_i=0$. Since $(\\bb'_i)_{i=1}^m$ are linearly independent, we find $c'_i=0$ for all $i\\in[m]$. This implies $\\sum_{i=1}^n c_i\\ab_i=0$. Since $(\\ab_i)_{i=1}^n$ are linearly independent, this implies $c_i=0$. Thus, \\eqref{cs are nonzero} can only hold if all coefficients are zero which is a contradiction.\n% \\end{proof}\n\n\n% %\\begin{lemma}\\label{lem add up}Let $\\A\\in\\R^{n\\times p},\\B\\in\\R^{m\\times p}$. Suppose $n+m\\leq p$ and $\\A$ is full row-rank. Let $\\B'$ be the matrix obtained by projecting each of row of $\\B$ on the orthogonal complement of the $n$-dimensional row-space of $\\A$. Suppose $\\B'$ is full rank. Then, the concatenation $\\Cb=\\begin{bmatrix}\\A\\\\\\B\\end{bmatrix}$ is full row-rank.\n% %\\end{lemma}\n% %\\begin{proof} Let $(\\ab_i)_{i=1}^n$, $(\\bb_i)_{i=1}^m$, $(\\bb'_i)_{i=1}^m$ be the rows of $\\A,\\B,\\B'$ respectively, Suppose the rows of $\\A,\\B$ are linearly dependent. Then, for some $(c_i)_{i=1}^{n},(c'_i)_{i=1}^{m}$ (which are not all-zeros), we have that\n% %\\begin{align}\n% %\\sum_{i=1}^n c_i\\ab_i+\\sum_{i=1}^m c'_i\\bb_i=0.\\label{cs are nonzero}\n% %\\end{align}\n% %We can rewrite this as\n% %\\[\n% %\\sum_{i=1}^n c_i\\ab_i+\\sum_{i=1}^m c'_i\\bb'_i+\\sum_{i=1}^m c'_i(\\bb'_i-\\bb_i)=0.\n% %\\]\n% %Denote the row space of $\\A$ by $S_A$. Projecting above inequality to $S_A^\\perp$, we find that $\\sum_{i=1}^m c'_i\\bb'_i=0$. Since $(\\bb'_i)_{i=1}^m$ are linearly independent, we find $c'_i=0$ for all $i\\in[m]$. This implies $\\sum_{i=1}^n c_i\\ab_i=0$. Since $(\\ab_i)_{i=1}^n$ are linearly independent, this implies $c_i=0$. Thus, \\eqref{cs are nonzero} can only hold if all coefficients are zero which is a contradiction.\n% %\\end{proof}\n\n==== END OF /2308.16898/supp/separation.tex ====\n==== BEGINNING OF /2308.16898/supp/reg_path_glob.tex ====\n%\\subsection{Proof of Global Regularization Path Theorem \\ref{thm global reg path}}\\label{app:global:RP:thm}\nCorollary \\ref{cor global reg path} already proves Theorem \\ref{thm global reg path} through the more general Theorem \\ref{local RP thm}. Below, we provide a self contained proof of Theorem \\ref{thm global reg path} for clarity.\n\n\\begin{proof} Throughout $\\dm$ denotes either Frobenius norm or nuclear norm. We will prove that $\\wrb{R}$ asymptotically aligns with the set of globally-optimal directions and also $\\td{\\wrb{R}}\\rightarrow \\infty$. $\\Rcm\\subseteq\\R^{d\\times d}$ denote the manifold of rank $\\leq$$m$ matrices.\n\\\\\n\\noindent\\textbf{Step 1:} Let us first prove that $\\wrb{R}$ achieves the optimal risk as $R\\rightarrow\\infty$ -- rather than problem having finite optima. Define $\\xdm=1/\\td{\\Wm}$ and norm-normalized $\\Wsb=\\xdm\\Wm$. Note that $\\Wm$ separates tokens $\\op$ from rest of the tokens for each $i \\in[n]$. Thus, we have that\n\\begin{align}\n\\lim_{R\\rightarrow\\infty}\\Lc(\\wrb{R})\\leq\\lim_{R\\rightarrow\\infty}\\Lc(R\\cdot\\Wsb):=\\Lc_\\star= \\frac{1}{n}\\sum_{i=1}^n \\ell(\\bgam^{\\op}_i).\\label{glob:asymp loss}\n\\end{align}\nOn the other hand, for any $\\W\\in \\Rcm$, define the softmax probabilities $\\s^{(i)}=\\sft{\\X_i\\W\\z_i}$ and attention features $\\x^{\\W}_i=\\sum_{t=1}^T \\s^{(i)}_t\\x_t$. Decompose $\\x^{\\W}_i$ as \n$\n\\x^{\\W}_i=\\s^{(i)}_{\\op_i}\\x_{i\\opt_i}+\\sum_{t\\neq \\op_i}\\s^{(i)}_t\\x_\\itt.\n$ Set $\\bgg_\\itt=\\bgam^{\\op}_i-\\bgam_\\itt=Y_i\\cdot \\vb^\\top(\\x_{i\\opt_i}-\\x_\\itt)>0$, and define\n\\begin{align}\n&B:=\\max_{i\\in[n]}\\max_{t,\\tau\\in[T]}\\tn{\\vb}\\cdot \\tn{\\x_\\itt-\\x_\\ittt}\\geq \\bgg_\\itt.\\label{glob:BB eq}\n\\end{align}\n%Before proceeding further, there are two important observations:\n%\\begin{itemize}\n%\\item Since $\\op$ is locally-optimal: $\\Tc_i\\subseteq\\lowi$ and $\\higi\\subseteq\\higi=[T]-\\Tc_i-\\{\\op_i\\}$.\n%\\item When $\\op=\\op$, we have that $\\higi=\\emptyset$. This will be important for setting $R_0=0$ and $\\Ccd=\\R^{d\\times d}$.\n%\\end{itemize}\n% and dropping subscript $i$\n%Setting $\\ab_i=\\Kb_i\\pb$ and $\\s_i=\\sft{\\ab_i}$. \nDefine $c_\\op=\\min_{i\\in[n],t\\neq\\op_i}\\bgg_\\itt>0$ and $\\bgam^{\\W}_i=Y_i\\cdot \\vb^\\top\\x^{\\W}_i$. We obtain the following score inequalities\n\\begin{align}\\label{glob:score decomp}\n&\\bgam^{\\W}_i\\leq \\bgam^{\\op}_i-c_\\op (1-\\s^{(i)}_{\\op_i})<\\bgam^{\\op}_i,\\\\\n%\\s^{(i)}_{\\op_i}\\bgam^{\\op}_i+\\sum_{t\\neq \\op_i}\\s^{(i)}_t\\bgam_\\itt+\\sum_{\\tau\\in\\higi}\\s^{(i)}_\\tau\\bgam_{\\ittt}.\\\\\n&|\\bgam^{\\W}_i-\\bgam^{\\op}_i|\\leq \\tn{\\vb}\\cdot\\tn{\\x^{\\W}_i-\\xa_i}\\leq \\tn{\\vb} \\sum_{t\\neq \\op_i}\\s^{(i)}_t\\tn{\\x_\\itt-\\xa_i}\\leq B (1-\\s^{(i)}_{\\op_i}).\\nonumber%\\label{lip score gap}\n%|1-\\s^{(i)}_{\\op_i}|\\bgam^{\\op}_i+\\sum_{t\\neq \\op_i}\\s^{(i)}_t|\\bgam_\\itt|+\\sum_{\\tau\\in\\higi}\\s^{(i)}_\\tau|\\bgam_{\\ittt}|.\n%\\s_{\\op_i}\\bgam_{i\\op_i}+\\sum_{t\\in\\Tc_i}\\s_t\\bgam_\\itt+\\sum_{t\\in\\higi}\\s_t\\bgam_\\itt.\n\\end{align}\nWe will use the $\\bgam^{\\W}_i-\\bgam^{\\op}_i$ term in \\eqref{glob:score decomp} to evaluate $\\W$ against the reference loss $\\Lc_\\star$ of \\eqref{glob:asymp loss}. %Let $\\a^{(i)}=\\X_i\\W\\z_i$. \n%Now since $\\W\\in \\Rcm$, there exists $t\\neq\\op_i$ obeying $\\abik_t-\\max_{\\tau\\in\\higi} \\abik_\\tau\\geq \\eps \\tf{\\W}\\geq \\epsd\\td{\\W}$. Denote $D^i:=(\\sum_{t\\in [T]}e^{\\abik_t})^{-1}$ to be the softmax denominator i.e.~sum of exponentials. We find that,\n%\\begin{align}\n%Q^i=\\sum_{\\tau\\in\\higi}\\s^{(i)}_\\tau=D^i\\sum_{\\tau\\in\\higi}e^{\\abik_\\tau}\\leq D^i Te^{\\abik_t-\\eps\\tf{\\W}}\\leq Te^{-\\epsd\\td{\\W}}P^i.\\label{qikeq}\n%\\end{align}\n%Above, the right hand side is strictly negative as soon as $\\td{\\W}\\geq R_0:=\\frac{1}{\\epsd}\\log\\frac{BT}{c_\\op}$. Note that, this condition applies to all $(i,k)\\in[n]\\times [K]$ pairs uniformly for the same $R_0$. Consequently, for any $\\td{\\W}\\geq R_0$, for all $i,k$ and $\\W\\in \\Rcm$, we have that $\\bgam^{\\W}_i<\\bgam^{\\op}_i$. Additionally, when $\\op=\\op$, note that $Q^i=0$ since $\\higi=\\emptyset$. Thus, $R_0=0$ suffices to ensure $\\bgam^{\\W}_i<\\bgam^{\\op}_i$. \nUsing the strictly-decreasing nature of $\\ell$, we conclude with the fact that for all (finite) $\\W\\in \\Rcm$, \n\\[\n\\Lc(\\W)=\\frac{1}{n}\\sum_{i=1}^n \\ell(\\bgam^{\\W}_i)> \\Lc_\\st=\\frac{1}{n}\\sum_{i=1}^n \\ell(\\bgam^{\\op}_i),\n\\]\nwhich implies $\\td{\\wrb{R}}\\rightarrow\\infty$ together with \\eqref{glob:asymp loss}.\n\n\\noindent\\textbf{Step 2:} To proceed, we show that $\\wrb{R}$ converges in direction to $\\Wcs$, which denotes the set of SVM minima. Suppose this is not the case and~convergence fails. We will obtain a contradiction by showing that $\\Wsb_R=R\\cdot\\Wsb$ achieves a strictly superior loss compared to $\\wrb{R}$. Let us introduce the normalized parameters $\\wrt{R}=\\frac{\\wrb{R}}{R\\xdm}$ and $\\W'=\\frac{\\wrb{R}}{\\td{\\wrb{R}}\\xdm}$. Note that $\\wrt{R}$ is obtained by scaling down $\\W'$ since $\\td{\\wrb{R}}\\leq R$ and $\\W'$ obeys $\\td{\\W'}=\\td{\\Wm}$.\nSince $\\wrt{R}$ fails to converge to $\\Wcs$, for some $\\delta>0$, there exists arbitrarily large $R>0$ such that $\\dist{\\wrt{R},\\Wcs}\\geq \\delta$. This translates to the suboptimality in terms of the margin constraints as follows: First, since nuclear norm dominates Frobenius, distance with respect to the $\\dm$-norm obeys $\\distd{\\wrt{R},\\Wcs}\\geq \\delta$. Secondly, using triangle inequality,\n\\[ \n\\text{this implies that either~~~}\\td{\\wrt{R}}\\leq \\td{\\Wm}-\\delta/2\\text{~~~or~~~}\\distd{\\W',\\Wcs}\\geq \\delta/2.\n\\]\nIn either scenario, $\\wrt{R}$ strictly violates one of the margin constraints of \\eqref{seqattnsvm}: If $\\td{\\wrt{R}}\\leq \\td{\\Wm}-\\delta/2$, then, since the optimal SVM objective is $\\td{\\Wm}$, there exists a constraint $i,t\\neq\\op_i$ for which $\\inn{(\\x^\\op_i-\\x_\\itt)\\z_i^\\top,\\wrt{R}}\\leq 1-\\frac{\\delta}{2\\td{\\Wm}}$. If $\\distd{\\W',\\Wcs}\\geq \\delta/2$, then, $\\W'$ has the same SVM objective but it is strictly bounded away from the solution set. Thus, for some $\\eps:=\\eps(\\delta)>0$, $\\W'$ and its scaled down version $\\wrt{R}$ strictly violate an SVM constraint achieving margin $\\leq 1-\\eps$. Without losing generality, suppose $\\wrt{R}$ violates the first constraint $i=1$. Thus, for a properly updated $\\delta>0$ (that is function of the initial $\\delta>0$) and for $i=1$ and some \\nei $\\tau\\in \\Tc_1$,% and , we have that\n\\begin{align}\n\\inn{ (\\x^\\op_1-\\x_{1t})\\z_1^\\top,\\wrt{R}}\\leq 1-\\delta.\\label{margin violate:glob}\n%\\pb^\\top(\\kb_{i\\al_1}-\\kb_{i\\tau})\\leq 1-\\delta. \n\\end{align}\nNow, we will argue that this leads to a contradiction by proving $\\Lc(\\Wsb_R)<\\Lc(\\wrb{R})$ for sufficiently large $R$.\n\nTo obtain the result, we establish a refined softmax probability control as in Step 1 by studying distance to $\\Lc_\\star$. Following \\eqref{glob:score decomp}, denote the score function at $\\wrb{R}$ via $\\bgam^R_i:=\\bgam_i^{\\wrb{R}}$. Similarly, let $\\sir_i=\\sft{\\abr_i}$ with $\\abr_i=\\X_i\\wrb{R}\\z_i$. Set the corresponding notation for the reference parameter $\\Wsb_R$ as $\\bgam^\\st_i,\\s^\\st_i,\\ab^\\st_i$.  Recall that $R\\geq \\td{\\wrb{R}}$ and $\\xdm:=1/\\td{\\Wm}$. We note the following softmax inequalities \n\\begin{align}\n&\\s^\\st_{i\\op_i}\\geq \\frac{1}{1+Te^{-R\\xdm}}\\geq 1-Te^{-R\\xdm}\\quad \\text{for all}\\quad i\\in[n], \\label{glob:salpha bounds}\\\\\n&s^R_{i\\op_i}\\leq \\frac{1}{1+e^{-(1-\\delta)\\td{\\wrb{R}}\\xdm}}\\leq \\frac{1}{1+e^{-(1-\\delta)R\\xdm}}\\quad\\text{for}\\quad i=1.\\nn\n\\end{align}\nThe former inequality is thanks to $\\Wm$ achieving $\\geq$$1$ margins on all tokens $[T]-\\op_i$ and the latter arises from the $\\delta$-margin violation of $\\wrb{R}$ at $i=1$ i.e.~Eq.~\\eqref{margin violate:glob}. Since $\\ell$ is strictly decreasing with Lipschitz derivative and the scores are upper/lower bounded by an absolute constant (as tokens are bounded and fixed), we have that $\\cop\\geq -\\ell'(\\bgam_i^{\\W})\\geq \\cdn$ for some constants $\\cop>\\cdn>0$. Thus, following Eq.~\\eqref{glob:BB eq}, the score decomposition \\eqref{glob:score decomp}, and \\eqref{glob:salpha bounds} we can write%Below, we use $\\ell'_R,\\ell'_\\st$ to denote the derivatives of $\\ell(x)$ used for first-order Taylor expansion around the scores of $\\wrb{R}$ and $\\Wsb_R$ respectively. \n\\begin{align}\\label{glob:ineq prl}\n\\Lc(\\wrb{R})-\\Lc_\\star&\\geq \\frac{1}{n}[\\ell(\\bgam_1^{\\wrb{R}})-\\ell(\\bgam^{\\op}_1)]\\geq \\frac{\\cdn}{n}(\\bgam^{\\op}_{1}-\\bgam_1^{\\wrb{R}})\\\\\n\\nonumber\n&\\geq \\frac{\\cdn}{n}c_\\op (1-\\s^R_{1\\op_1}).%\\label{glob:q11 eq}\n\\\\\n\\nonumber \n&\\geq \\frac{\\cdn c_\\op}{n}\\frac{1}{1+e^{(1-\\delta)R\\xdm}}.\n\\end{align}\nConversely, we upper bound the difference between $\\Lc(\\Wsb_R)$ and $\\Lc_\\star$ as follows. Define the worst-case loss difference for $\\wrb{R}$ as $j=\\arg\\max_{i\\in[n]}[\\ell(\\bgam_i^\\st)-\\ell(\\bgam^{\\op}_i)]$. Using \\eqref{glob:score decomp}\\&\\eqref{glob:salpha bounds}, we write\n% and $B_2=\\max_{i\\in[n]}\\{\\bgam_{i\\op_i}-\\min_{t\\in[T]}\\bgam_\\itt\\}$\n\\begin{equation*}\n\\begin{aligned}\n\\Lc(\\Wsb_R)-\\Lc_\\star&\\leq \\max_{i\\in[n]}[\\ell(\\bgam_i^\\st)-\\ell(\\bgam^{\\op}_i)]\\leq \\cop\\cdot(\\bgam^{\\op}_{j}-\\bgam^\\st_{j})\\\\\n&\\leq \\cop\\cdot(1-\\s^\\st_{j\\op_j})B\\\\\n&\\leq \\cop\\cdot Te^{-R\\xdm}B.%\\label{desired Wmm bound}\n\\end{aligned}\n\\end{equation*}\nCombining the last inequality and \\eqref{glob:ineq prl}, we conclude that $\\Lc(\\Wsb_R)<\\Lc(\\wrb{R})$ whenever\n\\[\n\\cop T\\cdot e^{-R\\xdm}B<\\frac{\\cdn\\cdot c_\\op }{n}\\frac{1}{1+e^{(1-\\delta)R\\xdm}}\\iff \\frac{e^{R\\xdm}}{1+e^{(1-\\delta)R\\xdm}}> \\frac{\\cop Tn B}{\\cdn c_\\op }.\n\\]\nThe left hand-side inequality holds for all sufficiently large $R$: Specifically, as soon as $R$ obeys $R>\\frac{1}{\\delta\\xdm}\\log(\\frac{2\\cop Tn B}{\\cdn c_\\op})$. This completes the proof of the theorem by contradiction since we obtained $\\Lc(\\wrb{R})>\\Lc(\\Wsb_R)$.\n\\end{proof}\n\n\n\n\n\n==== END OF /2308.16898/supp/reg_path_glob.tex ====",
            "processed_original_tex": "==== BEGINNING OF /2308.16898/notation.tex ====\n\n\\usepackage[utf8]{inputenc} \n\\usepackage[T1]{fontenc}    \n\\usepackage{hyperref}       \n\\usepackage{url}            \n\\usepackage{booktabs}       \n\\usepackage{amsfonts,amsmath,amssymb}       \n\\usepackage{nicefrac}       \n\\usepackage{microtype}      \n\\usepackage{xcolor}         \n\\usepackage{txfonts}\n\\usepackage[inline]{enumitem}\n\n\n\\newcommand{\\RSIS}{RSI$+$Smooth}\n\\usepackage{graphicx}\n\\usepackage{subfigure}\n\n\\usepackage{wrapfig,bm,comment,color}\n\\usepackage{breakurl,epsfig,epsf,fmtcount,semtrans,multirow,boldline}\n\\usepackage{tcolorbox}\n\\usepackage{xcolor}\n\\tcbuselibrary{skins}\n\\usepackage{tikz}\n\\definecolor{darkred}{RGB}{150,0,0}\n\\definecolor{darkgreen}{RGB}{0,150,0}\n\\definecolor{darkblue}{RGB}{0,0,200}\n\\hypersetup{colorlinks=true, linkcolor=darkred, citecolor=darkgreen, urlcolor=darkblue}\n\n\n\\newtheorem{theorem}{Theorem}\n\\newtheorem{result}{Result}\n\\newtheorem{fact}{Fact}\n\\newtheorem{problem}{Problem}\n\\newtheorem{claim}{Claim}\n\\newtheorem{assumption}{Assumption}\n\\renewcommand*{\\theassumption}{\\Alph{assumption}}\n\\newtheorem{question}{Question}\n\\newtheorem{lemma}{Lemma}\n\\newtheorem{corollary}{Corollary}\n\\newtheorem{proposition}{Proposition}\n\\newtheorem{definition}{Definition}\n\\newtheorem{condition}{Condition}\n\\newtheorem{conjecture}{Conjecture}\n\\newtheorem{remark}{Remark}\n\\newtheorem{remarks}{Remarks}\n\\newtheorem{example}{Example}\n\n\n\n\\newenvironment{assbis}[1]\n  {\\renewcommand{\\theassumption}{\\ref{#1}$'$}\n   \\addtocounter{assumption}{-1}\n   \\begin{assumption}}\n  {\\end{assumption}}\n  \n\\def \\endprf{\\hfill {\\vrule height6pt width6pt depth0pt}\\medskip}\n\n\\newenvironment{proof}{\\noindent {\\bf Proof.} }{\\endprf\\par}\n\\newenvironment{proofsk}{\\noindent {\\bf Proof sketch.} }{\\endprf\\par}\n\n\\newcommand{\\qed}{{\\unskip\\nobreak\\hfil\\penalty50\\hskip2em\\vadjust{}\n           \\nobreak\\hfil$\\Box$\\parfillskip=0pt\\finalhyphendemerits=0\\par}}\n\n\n\\newcommand{\\goodbox}[1]{\\begin{center} \n\\begin{tcolorbox}[boxsep=1pt,left=7pt,right=7pt,top=0pt,bottom=2pt,enhanced, width=14cm,colframe=white!3!black,colback=black!2!white,colbacktitle=orange!5!yellow!10!white,\nfonttitle=\\bfseries,coltitle=black,attach boxed title to top center=\n{yshift=-0.25mm-\\tcboxedtitleheight/2,yshifttext=2mm-\\tcboxedtitleheight/2},\nboxed title style={boxrule=0.2mm,\nframe code={ \\path[tcb fill frame] ([xshift=-4mm]frame.west)\n-- (frame.north west) -- (frame.north east) -- ([xshift=4mm]frame.east)\n-- (frame.south east) -- (frame.south west) -- cycle; },\ninterior code={ \\path[tcb fill interior] ([xshift=-2mm]interior.west)\n-- (interior.north west) -- (interior.north east)\n-- ([xshift=2mm]interior.east) -- (interior.south east) -- (interior.south west)\n-- cycle;} }] #1 \\end{tcolorbox}\\end{center}\n}\n\\newcommand{\\nicebox}[1]{\\begin{center} \\begin{tcolorbox}[boxsep=1pt,left=7pt,right=7pt,top=0pt,bottom=2pt,enhanced, width=14cm,colframe=green!3!black,colback=green!3!white,colbacktitle=orange!5!yellow!10!white,\nfonttitle=\\bfseries,coltitle=black,attach boxed title to top center=\n{yshift=-0.25mm-\\tcboxedtitleheight/2,yshifttext=2mm-\\tcboxedtitleheight/2},\nboxed title style={boxrule=0.2mm,\nframe code={ \\path[tcb fill frame] ([xshift=-4mm]frame.west)\n-- (frame.north west) -- (frame.north east) -- ([xshift=4mm]frame.east)\n-- (frame.south east) -- (frame.south west) -- cycle; },\ninterior code={ \\path[tcb fill interior] ([xshift=-2mm]interior.west)\n-- (interior.north west) -- (interior.north east)\n-- ([xshift=2mm]interior.east) -- (interior.south east) -- (interior.south west)\n-- cycle;} }] #1 \\end{tcolorbox}\\end{center} }\n\n\n\\newcommand{\\LM}{\\texttt{LMM}\\xspace}\n\\newcommand{\\GM}{\\texttt{GMM}\\xspace}\n\\newcommand{\\oo}{{11}}\n\\newcommand{\\red}{\\textcolor{red}}\n\\newcommand{\\blue}{\\textcolor{blue}}\n\\newcommand{\\redp}[1]{\\textcolor{red}{[#1]}}\n\\newcommand{\\grn}{\\textcolor{darkgreen}}\n\\newcommand{\\so}[1]{\\textcolor{darkblue}{#1}}\n\\newcommand{\\SO}[1]{\\textcolor{red}{[SO: #1]\\\\}}\n\\newcommand{\\dat}[1]{\\textcolor{darkgreen}{#1}}\n\\newcommand{\\DAT}[1]{\\textcolor{green}{[DAT: #1]}}\n\\newcommand{\\yl}[1]{\\textcolor{orange}{#1}}\n\\newcommand{\\YL}[1]{\\textcolor{orange}{[YL: #1]}}\n\\newcommand{\\ct}[1]{\\textcolor{magenta}{[CT: #1]}}\n\\newcommand{\\todo}[1]{\\textcolor{darkred}{TODO: #1}}\n\\newcommand{\\clr}[1]{\\red{#1}}\n\\newcommand{\\cln}[1]{\\red{}}\n\\newcommand{\\som}[1]{\\marginpar{\\color{darkblue}\\tiny\\ttfamily SO: #1}}\n\n\n\n\\newcommand{\\xat}{\\ob}\n\\newcommand{\\rfn}{\\texttt{SVMeq}}\n\\newcommand{\\ont}{\\texttt{1token}}\n\\newcommand{\\xast}{\\xat^\\st}\n\\newcommand{\\Wf}{{\\mtx{W}^\\tsc{fin}}}\n\\newcommand\\tr{{{\\operatorname{trace}}}}\n\\newcommand{\\noi}{\\noindent}\n\\newcommand{\\new}{\\text{new}}\n\\newcommand{\\prm}{\\text{prompt}}\n\\newcommand{\\TF}{{\\texttt{TF}}}\n\\newcommand{\\outr}[1]{\\text{outer\\_update}(#1)}\n\n\\newcommand{\\eps}{\\epsilon}\n\\newcommand{\\epsd}{\\varepsilon_\\dm}\n\\newcommand{\\dv}{\\rho}\n\\newcommand{\\beps}{\\boldsymbol{\\eps}}\n\\newcommand{\\bota}{\\boldsymbol{\\iota}}\n\\newcommand{\\ept}{\\eps_{\\TF}}\n\\newcommand{\\cz}{c_0}\n\\newcommand{\\cc}[1]{\\Cc(#1)}\n\\newcommand{\\kz}{\\nu}\n\\newcommand{\\bxi}{\\boldsymbol{\\xi}}\n\\newcommand{\\vsp}{\\vspace}\n\\newcommand{\\fnn}{f_{\\text{nn}}}\n\\newcommand{\\ff}{f_{\\text{nn}}}\n\\newcommand{\\wi}{k_\\st}\n\\newcommand{\\hbm}{\\vct{\\bar{h}}}\n\\newcommand{\\fF}[1]{f_{\\text{nn},#1}}\n\\newcommand{\\vc}{\\text{vec}}\n\\newcommand{\\flin}{f_{\\text{lin}}}\n\\newcommand{\\bhbg}{{\\bar h}_{\\tsc{gap}}}\n\\newcommand{\\fln}[1]{f_{\\text{lin},#1}}\n\\newcommand{\\rest}{\\text{rest}}\n\\newcommand{\\hp}{\\tilde{\\mtx{h}}}\n\\newcommand{\\exc}[1]{\\mathcal{R}_{\\tsc{gap}}(#1)}\n\\newcommand{\\Mb}{\\bar{\\M}}\n\\newcommand{\\Nb}{\\bar{N}}\n\\newcommand{\\epsr}{\\varepsilon_R}\n\\newcommand{\\mur}{\\mu_R}\n\\newcommand{\\bp}{\\bar{p}}\n\\newcommand{\\zig}{\\nu}\n\\newcommand{\\bh}{\\bar{h}}\n\\newcommand{\\hb}{\\vct{h}}\n\\newcommand{\\al}{\\alpha}\n\\newcommand{\\als}{\\alpha}\n\\newcommand{\\fs}{f^{\\Dc}}\n\\newcommand{\\Ncov}{\\mathcal{N}_\\eps(\\Bal)}\n\\newcommand{\\ft}{h}\n\\newcommand{\\ftv}{f^{\\Tc\\cup\\Vc}}\n\\newcommand{\\deff}{h_{\\text{eff}}}\n\\newcommand{\\defz}{\\bar{h}_{\\text{eff}}}\n\\newcommand{\\deft}{\\tilde{h}_{\\text{eff}}}\n\\newcommand{\\defg}{\\bar{h}^{\\nabla}_{\\text{eff}}}\n\\newcommand{\\defF}{\\bar{h}^{\\FB}_{\\text{eff}}}\n\\newcommand{\\bC}{\\bar{C}}\n\\newcommand{\\st}{\\star}\n\\newcommand{\\dm}{{\\diamond}}\n\\newcommand{\\nt}{n_\\mathcal{T}}\n\\newcommand{\\h}{h}\n\\newcommand{\\nv}{n_\\mathcal{V}}\n\\newcommand{\\distas}{\\overset{\\text{i.i.d.}}{\\sim}}\n\n\\newcommand{\\pleq}{\\overset{{P}}{\\leq}}\n\\newcommand{\\np}{{n\\wedge p}}\n\\newcommand{\\KB}{\\bar{K}}\n\\newcommand{\\bG}{\\boldsymbol{\\Gamma}}\n\\newcommand{\\rP}{\\stackrel{{P}}{\\longrightarrow}}\n\n\\newcommand{\\xv}{\\x_{\\vb}}\n\\newcommand{\\xa}{\\x^\\bal}\n\\newcommand{\\xw}{\\x'_{\\vb}}\n\\newcommand{\\xu}{\\x_{\\ub}}\n\\newcommand{\\xvt}[1]{\\x_{\\vb_{#1}}}\n\\newcommand{\\xvtt}[1]{\\x_{\\vb,#1}}\n\\newcommand{\\xwt}[1]{\\x_{#1,\\w}}\n\\newcommand{\\xws}[1]{\\x_{#1,\\ws}}\n\\newcommand{\\xs}{\\x_{\\ws}}\n\\newcommand{\\yst}{\\hat{\\y}}\n\\newcommand{\\yb}{\\bar{\\y}}\n\\newcommand{\\link}[1]{\\lnk(#1)}\n\\newcommand{\\lnk}{\\psi}\n\\newcommand{\\eig}{\\text{eigvec}}\n\\newcommand{\\hu}{h}\n\\newcommand{\\ru}{r}\n\\newcommand{\\pbar}{{{\\bar{p}}}}\n\\newcommand{\\pbd}{p}\n\\newcommand{\\lay}[1]{{\\vct{k}}^{(#1)}}\n\\newcommand{\\lah}[1]{{\\vct{\\hat{k}}}^{(#1)}}\n\\newcommand{\\lan}{\\vct{k}}\n\\newcommand{\\cmp}{P_{\\text{nn}}}\n\n\\newcommand{\\dcp}{\\vct{d}^{\\text{cnn}}}\n\\newcommand{\\fcp}{\\vct{g}^{\\text{CNN}}}\n\\newcommand{\\beq}{\\begin{equation}}\n\\newcommand{\\ba}{\\begin{align}}\n\\newcommand{\\ea}{\\end{align}}\n\\newcommand{\\bea}[1]{\\begin{align}#1\\end{align}}\n\\newcommand{\\nrest}{\\bar{n}}\n\\newcommand{\\eeq}{\\end{equation}}\n\\newcommand{\\prox}{{{\\text{\\bf{prox}}}}}\n\\newcommand{\\cov}{{{\\text{\\bf{cov}}}}}\n\\newcommand{\\ex}{{{\\text{\\bf{ex}}}}}\n\\newcommand{\\modu}{{{\\text{{mod}}}}}\n\\newcommand{\\map}{{{\\text{\\bf{map}}}}}\n\\newcommand{\\var}[1]{{{\\text{\\bf{var}}}}[#1]}\n\\newcommand{\\vrn}{\\sigma}\n\\newcommand{\\Rcm}{\\Rcc_m}\n\\newcommand{\\lowo}{\\texttt{low}_\\oo^{\\alpha}}\n\\newcommand{\\sgt}[1]{\\tilde{\\sigma}^{(#1)}}\n\\newcommand{\\tsig}{\\tilde{\\sigma}}\n\\newcommand{\\func}[1]{{f_{\\text{CNN}}}(#1)}\n\\newcommand{\\funh}[1]{{{\\hat{f}}_{\\text{CNN}}}(#1)}\n\\newcommand{\\funcp}[1]{{f'_{\\text{cnn}}}(#1)}\n\\newcommand{\\funcw}[1]{{{{f}}'_{\\text{cnn}}}(#1)}\n\\newcommand{\\dev}{{{\\text{\\bf{dev}}}}}\n\\newcommand{\\meas}{{{\\text{\\bf{mean}}(\\sigma)}}}\n\\newcommand{\\nn}{\\nonumber}\n\\newcommand{\\la}{\\lambda}\n\\newcommand{\\laz}{\\la_0}\n\\newcommand{\\blaz}{\\bar{\\la}_0}\n\\newcommand{\\caz}{\\la_C}\n\\newcommand{\\saz}{L_\\sigma}\n\\newcommand{\\smax}{\\bar{\\sigma}_{\\max}}\n\\newcommand{\\Lat}{\\tilde{\\Lambda}}\n\\newcommand{\\K}{\\mtx{K}}\n\\newcommand{\\A}{{\\mtx{A}}}\n\\newcommand{\\Aa}{{\\mtx{A}}}\n\\newcommand{\\Bb}{{\\mtx{B}}}\n\\newcommand{\\N}{{\\mtx{N}}}\n\\newcommand{\\amp}{\\alpha_{\\text{cnn}}}\n\\newcommand{\\btm}{\\bigotimes}\n\\newcommand{\\robt}[1]{\\bigotimes_{\\ell=1}^D{#1}_{\\ell}}\n\\newcommand{\\robtu}[1]{\\bigotimes_{\\ell=1}^D{#1}^{(\\ell)}}\n\\newcommand{\\bd}{\\bigodot}\n\\newcommand{\\kapp}{s_{\\max}^{\\nu}}\n\\newcommand{\\kal}{\\prod_{i=1}^{D-1}\\tn{\\lay{i}}}\n\n\\newcommand{\\kall}[1]{1_{\\lan #1}}\n\\newcommand{\\lall}[1]{{\\order{\\lip_{\\lan #1}}}}\n\\newcommand{\\lell}[1]{{\\lip}_{\\lan #1}}\n\\newcommand{\\bell}[1]{{{{\\beta}}}_{\\lan #1}}\n\\newcommand{\\Ub}{{\\mtx{U}}}\n\\newcommand{\\Ubb}{{\\mtx{U}}}\n\\newcommand{\\kron}{\\otimes}\n\\newcommand{\\M}{{\\mtx{M}}}\n\\newcommand{\\dimondf}{\\W^\\svm}\n\\newcommand{\\name}{Att-SVM}\n\\newcommand{\\ten}{(\\{\\vb_\\ell\\}_{\\ell=1}^D)}\n\\newcommand{\\uten}{(\\{\\ub_\\ell\\}_{\\ell=1}^D)}\n\\newcommand{\\ken}{(\\{\\lay{i}\\}_{i=1}^D)}\n\\newcommand{\\B}{{{\\mtx{B}}}}\n\\newcommand{\\alx}{{{\\alpha_\\X}}}\n\\newcommand{\\aly}[1]{{{\\alpha_{\\X_{#1}}}}}\n\\newcommand{\\Ib}{{{\\mtx{I}}}}\n\\newcommand{\\fp}{F}\n\\newcommand{\\Xbin}{{{\\mtx{B}}}}\n\\newcommand{\\Gbin}{{{\\mtx{S}}}}\n\\newcommand{\\xbin}{{{\\mtx{b}}}}\n\\newcommand{\\Sb}{{{\\mtx{S}}}}\n\\newcommand{\\Sbt}{{\\mathbb{S}}^t}\n\\newcommand{\\Sbtt}{{\\mathbb{S}}'^{t}}\n\\newcommand{\\Gb}{{\\mtx{G}}}\n\\newcommand{\\ymean}{{\\bar{\\y}(\\w;\\text{avg})}}\n\\newcommand{\\yavg}{{{y}_{\\text{avg}}}}\n\\newcommand{\\ravg}[1]{{{r}_{{\\text{avg}}}(#1)}}\n\\newcommand{\\fmean}{{\\funcw{\\w;\\text{avg}}}}\n\\newcommand{\\diag}[1]{\\text{diag}(#1)}\n\\newcommand{\\out}{\\text{out}}\n\\newcommand{\\linest}{\\text{lin}}\n\\newcommand{\\inp}{\\text{in}}\n\\newcommand{\\inh}{\\hat{\\text{in}}}\n\\newcommand{\\sexp}{subexponential }\n\\newcommand{\\Lc}{{\\cal{L}}}\n\\newcommand{\\Hc}{{\\cal{H}}}\n\\newcommand{\\Lcz}{{\\cal{L}}^{0-1}}\n\\newcommand{\\Lczh}{{\\hat{\\cal{L}}}^{0-1}}\n\\newcommand{\\Lch}{{\\widehat{\\cal{L}}}}\n\\newcommand{\\Lcv}[1]{{\\cal{L}}^{#1}_{\\text{up}}}\n\\newcommand{\\Lct}{{\\tilde{\\cal{L}}}}\n\\newcommand{\\Lcb}{{\\bar{\\cal{L}}}}\n\\newcommand{\\Nc}{{\\cal{N}}}\n\\newcommand{\\xm}{\\x^{\\tsc{mix}}}\n\\newcommand{\\Jc}{{\\cal{J}}}\n\\newcommand{\\Dc}{{\\cal{D}}}\n\\newcommand{\\Dci}{{\\cal{D}}_{\\text{init}}}\n\n\\newcommand{\\Ro}{{\\cal{RO}}}\n\\newcommand{\\PC}{{\\cal{PC}}}\n\\newcommand{\\Pb}{{\\mtx{P}}}\n\\newcommand{\\Tb}{{\\mtx{T}}}\n\\newcommand{\\Tn}{{T_0}}\n\\newcommand{\\Kba}{{\\vct{k}}_{\\text{all}}}\n\\newcommand{\\Qb}{{\\mtx{Q}}}\n\\newcommand{\\QK}{{\\mtx{Q} \\mtx{K}^\\top}}\n\\newcommand{\\rng}{\\gamma}\n\\newcommand{\\ta}{\\tau}\n\\newcommand{\\gmax}{\\gamma^\\star}\n\\newcommand{\\ggap}{\\bar{\\gamma}_{\\tsc{gap}}}\n\\newcommand{\\Cb}{{\\mtx{C}}}\n\\newcommand{\\BC}{{\\bar{C}}}\n\\newcommand{\\Eb}{{\\mtx{E}}}\n\\newcommand{\\Hb}{{\\mtx{H}}}\n\\newcommand{\\Gc}{{\\cal{G}}}\n\\newcommand{\\Zc}{{\\cal{Z}}}\n\\newcommand{\\F}{{\\mtx{F}}}\n\\newcommand{\\Fa}{{\\mtx{F}}^\\bal}\n\\newcommand{\\diff}{{\\text{diff}}}\n\\newcommand{\\La}{{\\boldsymbol{{\\Lambda}}}}\n\\newcommand{\\noresamp}[1]{{\\textcolor{red}{#1}}}\n\\newcommand{\\sigmap}{\\phi'}\n\\newcommand{\\relu}[1]{\\phi(#1)}\n\\newcommand{\\one}[1]{{\\bm{1}}(#1)}\n\\newcommand{\\sigmal}[1]{\\sigma^{(#1)}}\n\\newcommand{\\sigmalp}[1]{\\sigma'^{(#1)}}\n\\newcommand{\\sigmaa}{\\sigma'_{\\text{all}}}\n\\newcommand{\\sigmai}[1]{\\sigma'_{\\text{all},#1}}\n\\newcommand{\\bSi}{{\\boldsymbol{{\\Sigma}}}}\n\\newcommand{\\bSii}{{\\boldsymbol{{\\Sigma}}}_{i,i}}\n\\newcommand{\\bSib}{\\bar{{\\boldsymbol{{\\Sigma}}}}}\n\\newcommand{\\bSit}{{\\boldsymbol{{\\tilde{\\Sigma}}}}}\n\\newcommand{\\bSih}{{\\boldsymbol{{\\hat{\\Sigma}}}}}\n\\newcommand{\\bmu}{{\\boldsymbol{{\\mu}}}}\n\\newcommand{\\Db}{{\\mtx{D}}}\n\\newcommand{\\bB}{{\\bar{B}}}\n\\newcommand{\\tB}{{\\tilde{B}}}\n\\newcommand{\\db}{{\\vct{d}}}\n\\newcommand{\\oneb}{{\\mathbb{1}}}\n\\newcommand{\\onebb}{{\\mathbf{1}}}\n\\newcommand{\\Iden}{{\\mtx{I}}}\n\n\\newcommand{\\gm}{\\gamma_m}\n\\newcommand{\\order}[1]{{\\cal{O}}(#1)}\n\\newcommand{\\OR}{\\text{OR}}\n\\newcommand{\\ordet}[1]{{\\widetilde{\\cal{O}}}(#1)}\n\\newcommand{\\rmax}[1]{{\\bf{r}_{\\max}(#1)}}\n\\newcommand{\\rbmax}[1]{{\\bf{\\bar{r}}_{\\max}(#1)}}\n\\newcommand{\\rmin}[1]{{\\bf{r}_{\\min}(#1)}}\n\\newcommand{\\gmmin}[1]{{\\gamma_{\\min}(#1)}}\n\\newcommand{\\gmmax}[1]{{\\gamma_{\\max}(#1)}}\n\n\\newcommand{\\smn}[1]{{s_{\\min}(#1)}}\n\\newcommand{\\smx}[1]{{s_{\\max}(#1)}}\n\\newcommand{\\z}{{\\vct{z}}}\n\\newcommand{\\zt}{{\\tilde{\\vct{z}}}}\n\\newcommand{\\fab}{f^\\bal_\\bt}\n\\newcommand{\\zb}{{\\bar{\\z}}}\n\\newcommand{\\el}{{\\ell}}\n\\newcommand{\\isnr}[1]{\\texttt{ISNR}(#1)}\n\\newcommand{\\sft}[1]{\\mathbb{S}(#1)}\n\\newcommand{\\sftk}[1]{\\mathbb{S}_k(#1)}\n\\newcommand{\\sftx}{\\mathbb{S}}\n\\newcommand{\\sfp}[1]{\\mathbb{S}'(#1)}\n\\newcommand{\\distd}[1]{\\texttt{dist}_\\dm\\left(#1\\right)}\n\\newcommand{\\tn}[1]{\\|{#1}\\|}\n\\newcommand{\\td}[1]{\\|{#1}\\|_\\dm}\n\\newcommand{\\tl}[1]{\\|{#1}\\|_{L_2}}\n\\newcommand{\\ts}[1]{\\|{#1}\\|_{\\Sc}}\n\\newcommand{\\ti}[1]{\\|{#1}\\|_{\\infty}}\n\\newcommand{\\nrm}[1]{\\|{#1}\\|}\n\\newcommand{\\inr}[1]{\\left<#1\\right>}\n\\newcommand{\\tone}[1]{\\|{#1}\\|_{\\ell_1}}\n\\newcommand{\\lix}[1]{\\|{#1}\\|_{\\Xc}}\n\\newcommand{\\lif}[1]{\\text{dist}_{\\FB}({#1})}\n\\newcommand{\\lit}[1]{\\text{max}(#1)}\n\\newcommand{\\lia}[1]{\\text{avg}(#1)}\n\\newcommand{\\lin}[1]{\\|{#1}\\|_{L_\\infty}}\n\\newcommand{\\tff}[1]{\\|{#1}\\|_{\\ell_4}}\n\\newcommand{\\tin}[1]{\\|{#1}\\|_{\\ell_\\infty}}\n\\newcommand{\\trow}[1]{\\|{#1}\\|_{2,\\infty}}\n\\newcommand{\\bad}{{\\bar{d}}}\n\\newcommand{\\Lcg}{\\tilde{\\Lc}}\n\\newcommand{\\Dp}{{D^+}}\n\\newcommand{\\tf}[1]{\\|{#1}\\|_{F}}\n\\newcommand{\\tnuc}[1]{\\|{#1}\\|_{\\star}}\n\\newcommand{\\te}[1]{\\|{#1}\\|_{\\psi_1}}\n\\newcommand{\\tsub}[1]{\\|{#1}\\|_{\\psi_2}}\n\\newcommand{\\tsut}[1]{\\|{#1}\\|_{\\psi_{2/3}}}\n\\newcommand{\\tsup}[1]{\\|{#1}\\|_{\\psi_a}}\n\\newcommand{\\dist}[1]{\\texttt{dist}\\left(#1\\right)}\n\\newcommand{\\upp}{{\\cal{B}}_{\\alpha,\\Gamma}}\n\\newcommand{\\upz}{{\\cal{B}}_{\\alpha_0,\\Gamma}}\n\\newcommand{\\dpz}{{\\cal{D}}_{\\alpha_0,\\Gamma}}\n\\newcommand{\\dpp}{{\\cal{D}}_{\\alpha,\\Gamma}}\n\\newcommand{\\mpp}{M_{\\alpha,\\Gamma}}\n\n\\newcommand{\\paf}{\\partial f(\\x)}\n\\newcommand{\\Cc}{\\mathcal{C}}\n\\newcommand{\\Rcc}{\\mathcal{R}}\n\\newcommand{\\Qcc}{\\mathcal{Q}}\n\\newcommand{\\Kcc}{\\mathcal{K}}\n\\newcommand{\\Ac}{\\mathcal{A}}\n\\newcommand{\\Al}[1]{\\Ac(#1)}\n\\newcommand{\\Alg}[1]{\\Ac_g(#1)}\n\\newcommand{\\Acg}{\\Ac_g}\n\\newcommand{\\At}{\\mathcal{A}}\n\n\\newcommand{\\Acr}{\\mathcal{A_\\text{ridge}}}\n\\newcommand{\\Bal}{{\\boldsymbol{\\Delta}}}\n\\newcommand{\\pbb}{\\vct{\\bar{p}}}\n\\newcommand{\\pbs}{\\tilde{\\pb}^\\svm}\n\\newcommand{\\del}{\\delta}\n\\newcommand{\\GG}{\\texttt{GG}}\n\\newcommand{\\BB}{\\texttt{BB}}\n\\newcommand{\\BG}{\\texttt{BG}}\n\\newcommand{\\bGam}{\\bar{\\Gamma}_y}\n\\newcommand{\\GB}{\\texttt{GB}}\n\\newcommand{\\delw}{\\delta_w}\n\\newcommand{\\delq}{\\delta_q}\n\\newcommand{\\bdel}{\\boldsymbol{\\delta}}\n\\newcommand{\\Ccb}{\\bar{\\mathcal{C}}}\n\\newcommand{\\Rc}{\\mathcal{O}}\n\\newcommand{\\Rcp}{\\mathcal{\\bar{O}}'}\n\n\\newcommand{\\btrue}{\\bbeta_{true}}\n\\newcommand{\\bt}{{\\boldsymbol{\\theta}}}\n\\newcommand{\\bet}{{\\boldsymbol{\\beta}}}\n\\newcommand{\\bT}{\\Theta}\n\\newcommand{\\bts}{{\\boldsymbol{\\beta}_\\st}}\n\\newcommand{\\btb}{\\bar{\\boldsymbol{\\theta}}}\n\\newcommand{\\btid}{\\boldsymbol{\\theta}^\\text{ideal}}\n\\newcommand{\\btt}{\\tilde{\\boldsymbol{\\theta}}}\n\\newcommand{\\bal}{{\\boldsymbol{\\alpha}}}\n\\newcommand{\\bgam}{\\boldsymbol{\\gamma}}\n\\newcommand{\\bga}{\\boldsymbol{\\gamma}^\\bal}\n\\newcommand{\\gamb}{\\bar{\\gamma}}\n\\newcommand{\\bab}{{\\boldsymbol{\\bar{\\alpha}}}}\n\\newcommand{\\sbl}[1]{\\sigma_{\\boldsymbol{\\alpha^{(#1)}}}}\n\\newcommand{\\bl}[1]{{\\boldsymbol{\\alpha}}^{(#1)}}\n\\newcommand{\\blb}[1]{\\bar{\\boldsymbol{\\alpha}}^{(#1)}}\n\\newcommand{\\bah}{{\\widehat{\\bal}}}\n\\newcommand{\\berm}{\\bal^{\\texttt{ERM}}}\n\\newcommand{\\bas}{{\\boldsymbol{\\alpha}_\\st}}\n\\newcommand{\\bth}{{\\boldsymbol{\\hat{\\beta}}}}\n\\newcommand{\\bPhi}{{\\boldsymbol{\\Phi}}}\n\\newcommand{\\bbteta}{\\widetilde{\\boldsymbol{\\theta}}}\n\\newcommand{\\bbeta}{{\\boldsymbol{\\beta}}}\n\\newcommand{\\ddelta}{{\\boldsymbol{\\delta}}}\n\\newcommand{\\DD}{{D}}\n\\newcommand{\\babeta}{{\\bar{\\beta}}}\n\\newcommand{\\balpha}{{\\bar{\\alpha}}}\n\\newcommand{\\bgamma}{\\gamma^\\star}\n\\newcommand{\\agam}{{\\bar{\\gamma}}_t}\n\\newcommand{\\No}{N}\n\\newcommand{\\Bc}{\\mathcal{B}}\n\\newcommand{\\Sc}{\\mathcal{S}}\n\\newcommand{\\Scc}{\\bar{\\mathcal{S}}}\n\\newcommand{\\Sca}{\\mathcal{S}_{\\text{all}}}\n\\newcommand{\\Dca}{\\mathcal{D}_{\\text{all}}}\n\\newcommand{\\Scn}{\\mathcal{S}_{\\new}}\n\\newcommand{\\Dcn}{\\mathcal{D}_{new}}\n\\newcommand{\\Scb}{\\bar{\\mathcal{S}}}\n\\newcommand{\\Sci}{\\mathcal{S}_{\\text{in}}}\n\\newcommand{\\Sco}{\\mathcal{S}_{\\text{out}}}\n\\newcommand{\\Ect}{\\mathcal{S}_{\\text{top}}}\n\\newcommand{\\Mc}{\\mathcal{M}}\n\\newcommand{\\pa}{{\\partial}}\n\\newcommand{\\Nn}{\\mathcal{N}}\n\\newcommand{\\pol}{^\\circ}\n\\newcommand{\\vb}{\\vct{v}}\n\\newcommand{\\Jb}{\\mtx{J}}\n\\newcommand{\\Jt}{\\mtx{\\tilde{J}}}\n\\newcommand{\\vbl}{\\vct{v}^{\\text{lin}}}\n\\newcommand{\\fb}{\\vct{f}}\n\\newcommand{\\Fb}{\\vct{F}}\n\\newcommand{\\FB}{\\mathbb{F}}\n\\newcommand{\\Ft}{\\tilde{\\vct{F}}}\n\\newcommand{\\fa}{\\tilde{\\vct{f}}}\n\\newcommand{\\ib}{{\\bf{i}}}\n\\newcommand{\\hib}{{\\bf{\\hat{i}}}}\n\\newcommand{\\Ic}{{\\mathcal{I}}}\n\\newcommand{\\all}{{\\text{all}}}\n\\newcommand{\\vh}{\\vct{\\hat{v}}}\n\\newcommand{\\vbb}{\\vct{\\bar{v}}}\n\\newcommand{\\Xb}{\\mtx{\\bar{X}}}\n\\newcommand{\\xb}{\\vct{\\bar{x}}}\n\\newcommand{\\abb}{\\mtx{\\bar{a}}}\n\\newcommand{\\ap}{\\mtx{a}'}\n\\newcommand{\\cb}{\\mtx{c}}\n\\newcommand{\\cbb}{\\mtx{\\bar{c}}}\n\\newcommand{\\kbb}{\\mtx{\\bar{k}}}\n\\newcommand{\\bbb}{\\mtx{\\bar{b}}}\n\\newcommand{\\nei}{\\text{support index}\\xspace}\n\\newcommand{\\neis}{\\text{support indices}\\xspace}\n\\newcommand{\\Nei}{\\text{Support index}\\xspace}\n\\newcommand{\\Neis}{\\text{Support indices}\\xspace}\n\\newcommand{\\NEIS}{\\text{Support Indices}\\xspace}\n\\newcommand{\\w}{\\vct{w}}\n\\newcommand{\\ww}{\\vct{V}}\n\\newcommand{\\lgt}{\\texttt{lgt}'}\n\\newcommand{\\ist}{i_\\st}\n\\newcommand{\\cdm}{c_\\dm}\n\\newcommand{\\cop}{c_\\texttt{up}}\n\\newcommand{\\cdn}{c_\\texttt{dn}}\n\\newcommand{\\Wp}{\\mtx{W}^\\dagger}\n\\newcommand{\\tilW}{\\widetilde{\\mtx{W}}}\n\\newcommand{\\tilw}{\\widetilde{\\vct{w}}}\n\\newcommand{\\ob}{\\mtx{o}}\n\\newcommand{\\obo}{\\mtx{o}_1(t)}\n\\newcommand{\\obt}{\\mtx{o}_2(t)}\n\\newcommand{\\obh}{\\mtx{\\hat{o}}}\n\\newcommand{\\wh}{{\\hat{\\mtx{w}}}}\n\\newcommand{\\li}{\\left<}\n\\newcommand{\\xdm}{\\Xi_\\dm}\n\\newcommand{\\ri}{\\right>}\n\\newcommand{\\s}{\\vct{s}}\n\\newcommand{\\sik}{\\s^{(\\ik)}}\n\\newcommand{\\sir}{\\s^R}\n\\newcommand{\\abik}{\\ab^{(\\ik)}}\n\\newcommand{\\abr}{\\ab^R}\n\\newcommand{\\ab}{{\\vct{a}}}\n\\newcommand{\\abm}{\\vct{\\bar{a}}}\n\\newcommand{\\abg}{\\vct{a}_{\\tsc{gap}}}\n\\newcommand{\\bgag}{{\\gamma}_{\\tsc{gap}}}\n\\newcommand{\\bgg}{\\gamma^{\\tsc{gap}}}\n\\newcommand{\\bggm}{\\gamma^{\\tsc{gap}}_{\\min}}\n\\newcommand{\\bgm}{\\bar{\\gamma}^{\\tsc{gap}}}\n\\newcommand{\\abp}{\\vct{a}^\\pb}\n\\newcommand{\\bb}{\\vct{b}}\n\\newcommand{\\ub}{{\\vct{u}}}\n\\newcommand{\\ubb}{\\bar{\\vct{u}}}\n\n\\newcommand{\\hh}{{\\vct{h}}}\n\n\\newcommand{\\ii}{{\\vct{i}}}\n\\newcommand{\\zm}[1]{{\\texttt{{{zm}}}[#1]}}\n\\newcommand{\\dd}{{\\vct{d}}}\n\\newcommand{\\ddt}{{\\vct{d}}_\\tau}\n\\newcommand{\\Zb}{\\mathbb{Z}}\n\n\\newcommand{\\hf}{\\hat{f}}\n\\newcommand{\\corrCA}{\\rho_\\Cc(\\M)}\n\\newcommand{\\corr}[1]{{\\texttt{corr\\_coef}}(#1)}\n\\newcommand{\\Tc}{\\mathcal{T}}\n\\newcommand{\\Tcb}{\\bar{\\mathcal{T}}}\n\\newcommand{\\TVc}{\\Tc\\cup\\Vc}\n\\newcommand{\\Ttc}{\\mathcal{T}_{\\text{test}}}\n\\newcommand{\\Fc}{\\mathcal{F}}\n\\newcommand{\\Fcl}{\\mathcal{F}^{\\text{lin}}}\n\\newcommand{\\Xc}{\\mathcal{X}}\n\\newcommand{\\Yc}{\\mathcal{Y}}\n\\newcommand{\\rel}{optimal\\xspace}\n\\newcommand{\\iopt}{non-optimal\\xspace}\n\\newcommand{\\irel}{non-optimal\\xspace}\n\n\n\n\\newcommand{\\qqq}[1]{{\\textcolor{red}{?{#1}?}}}\n\\newcommand{\\ihere}{{\\textcolor{red}{I AM HERE! }}}\n\\newcommand{\\mat}[1]{{\\text{mat}{#1}}}\n\\newcommand{\\gb}{\\bar{\\g}}\n\\newcommand{\\bs}{\\bar{s}}\n\\newcommand{\\cgain}{\\alpha_{\\text{CNN}}}\n\\newcommand{\\cgainp}{\\tn{\\E_{\\x\\sim\\Nn(0,\\Iden)}[\\gcnn{}]}}\n\\newcommand{\\gcnn}[1]{{\\vct{g}}_{\\text{cnn}#1}}\n\\newcommand{\\ccorr}{\\rho_{\\text{cnn}}}\n\\newcommand{\\kb}{\\vct{k}}\n\\newcommand{\\kbo}{\\vct{k}^\\op}\n\\newcommand{\\xbo}{\\vct{x}^\\op}\n\\newcommand{\\xh}{\\hat{\\x}}\n\n\\newcommand{\\cone}{\\Sc}\n\\newcommand{\\conb}{\\bar{\\Cc}}\n\\newcommand{\\con}[1]{\\texttt{cone}_{\\eps}(#1)}\n\n\n\\newcommand{\\xbr}{\\bar{\\h}}\n\n\n\\newcommand{\\low}{\\texttt{low}^{\\alpha}(\\X)}\n\\newcommand{\\high}{\\texttt{high}^{\\alpha}(\\X)}\n\\newcommand{\\lowi}{\\texttt{low}_\\ik^{\\alpha}}\n\\newcommand{\\higi}{\\texttt{high}_\\ik^{\\alpha}}\n\\newcommand{\\xdr}{\\tilde{\\x}}\n\\newcommand{\\xp}[1]{\\x^{(#1)}_\\prm}\n\\newcommand{\\Sn}[1]{\\Sc^{(#1)}}\n\\newcommand{\\Dn}[1]{\\Dc^{(#1)}}\n\n\\newcommand{\\xt}[1]{\\x^{(#1)}}\n\\newcommand{\\yt}[1]{y^{(#1)}}\n\\newcommand{\\ybt}{\\tilde{\\y}}\n\\newcommand{\\Xt}{\\tilde{\\X}}\n\\newcommand{\\gh}{\\hat{\\g}}\n\\newcommand{\\gt}{\\tilde{\\g}}\n\\newcommand{\\ir}{q}\n\\newcommand{\\sbs}{\\vct{s}^{\\texttt{ref}}}\n\\newcommand{\\bbg}{\\bgam^{\\tsc{gap}}}\n\\newcommand{\\bbs}{\\bar{\\s}}\n\\newcommand{\\gmb}{\\bar{\\gamma}}\n\\newcommand{\\irm}{q^\\pb_{\\max}}\n\\newcommand{\\ira}{q^{\\pb'}_{\\max}}\n\\newcommand{\\vbs}{\\tilde{\\vb}^\\svm}\n\\newcommand{\\vs}{\\vb^\\svm}\n\\newcommand{\\ps}{\\W^\\svm}\n\\newcommand{\\Ws}{\\W^\\svm}\n\\newcommand{\\Wma}{\\W^\\svm_\\bal}\n\\newcommand{\\Wsf}{\\W^\\svm}\n\\newcommand{\\Wsb}{\\bar{\\W}^\\svm}\n\\newcommand{\\Wcs}{\\Wc^\\svm}\n\\newcommand{\\ik}{{ik}}\n\\newcommand{\\itt}{{it}}\n\\newcommand{\\ittt}{{i\\tau}}\n\\newcommand{\\ikt}{{ikt}}\n\\newcommand{\\iktt}{{ik\\tau}}\n\\newcommand{\\ikix}{_{ik=(1,1)}^{(n,k)}}\n\\newcommand{\\inn}[1]{\\left<#1\\right>}\n\\newcommand{\\Ccd}{\\Cc_{\\eps,R_0}^\\dm}\n\\newcommand{\\aik}{\\alpha_{ik}}\n\\newcommand{\\prr}{\\pb^\\tsc{relax}}\n\\newcommand{\\pre}{\\pb^\\tsc{$\\eps$-rlx}}\n\\newcommand{\\pbr}{\\tilde{\\pb}^\\tsc{$\\eps$-rlx}}\n\\newcommand{\\qbr}{\\tilde{\\qb}^\\tsc{relax}}\n\n\\newcommand{\\pseb}{\\pbb^\\svm_\\eps}\n\\newcommand{\\pset}{\\tilde{\\pb}^\\svm_\\eps}\n\\newcommand{\\psdb}{\\pbb^\\svm_\\delta}\n\\newcommand{\\pse}{\\pb^\\svm_\\eps}\n\\newcommand{\\psd}{\\pb^\\svm_\\delta}\n\\newcommand{\\pst}{\\pb^\\star}\n\\newcommand{\\wst}{\\W^\\star}\n\\newcommand{\\gamp}{\\Gamma_\\eps}\n\\newcommand{\\gamt}{\\Gamma^{\\geq 2}_\\eps}\n\\newcommand{\\damp}{\\Gamma_\\delta}\n\n\\newcommand{\\pt}{\\tilde{\\pb}^\\svm}\n\\newcommand{\\RR}{\\bar{R}}\n\\newcommand{\\MM}{\\bar{M}}\n\\newcommand{\\psb}{\\bar{\\pb}^\\svm}\n\\newcommand{\\psp}{\\pb^\\beta}\n\\newcommand{\\pso}{\\pb^{\\svm\\star}}\n\\newcommand{\\Wso}{\\W^{\\svm\\star}}\n\n\\newcommand{\\mipp}{L_{\\max}}\n\\newcommand{\\mupp}{\\bar{\\mu}_{\\max}}\n\\newcommand{\\mapp}{\\mu}\n\\newcommand{\\tcnn}{{\\mtx{T}}_{\\text{cnn}}}\n\\newcommand{\\lcnn}{{\\mtx{L}}_{\\text{CNN}}}\n\\newcommand{\\smo}{S}\n\\newcommand{\\SM}{{{\\bf{S}}}_{L,\\kb}}\n\\newcommand{\\SMB}{{{\\bf{\\bar{S}}}}_{L,\\kb}}\n\\newcommand{\\ws}{{\\W^\\star}}\n\\newcommand{\\wss}{{\\w^\\star}}\n\\newcommand{\\yp}[1]{\\textcolor{red}{ #1}}\n\\newcommand{\\zeronorm}[1]{\\left\\|#1 \\right\\|_0}\n\\newcommand{\\unorm}[1]{\\left\\|#1 \\right\\|_u}\n\\newcommand{\\ynorm}[1]{\\left\\|#1 \\right\\|_{\\bar{y}}}\n\\newcommand{\\onetwonorm}[1]{\\left\\|#1\\right\\|_{1,2}}\n\\newcommand{\\opnorm}[1]{\\left\\|#1\\right\\|}\n\\newcommand{\\fronorm}[1]{\\left\\|#1\\right\\|_{F}}\n\\newcommand{\\onenorm}[1]{\\left\\|#1\\right\\|_{\\ell_1}}\n\\newcommand{\\twonorm}[1]{\\left\\|#1\\right\\|_{\\ell_2}}\n\\newcommand{\\wnorm}[2]{\\left\\|#1\\right\\|_{#2}}\n\\newcommand{\\Dnorm}[1]{\\left\\|#1\\right\\|_{D}}\n\\newcommand{\\oneinfnorm}[1]{\\left\\|#1\\right\\|_{1,\\infty}}\n\\newcommand{\\infnorm}[1]{\\left\\|#1\\right\\|_{\\ell_\\infty}}\n\\newcommand{\\nucnorm}[1]{\\left\\|#1\\right\\|_*}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\lab}{\\bar{\\la}}\n\\newcommand{\\avg}[1]{\\left< #1 \\right>}\n\n\n\\newcommand{\\mult}{B^D\\bar{M}N}\n\\newcommand{\\liptwo}{20R^3B\\nt^2\\laz^{-2}(B\\nt+1)}\n\\newcommand{\\lipp}{5R^2\\sqrt{B^3\\nt^3\\h}\\laz^{-2}\\tn{\\yT}}\n\\newcommand{\\lips}{6R^3B^2\\sqrt{\\nt^3\\h}\\laz^{-2}\\tn{\\yT}}\n\\newcommand{\\lipl}{5(RB\\nt\\sqrt{\\h}+1)RB\\nt\\laz^{-2}}\n\\newcommand{\\lip}{\\frac{5B^2\\tn{\\yT}}{\\laz^2}}\n\\newcommand{\\lipt}{6R^3B^2\\Gamma\\sqrt{\\nt^3\\h}\\laz^{-2} \\tn{\\yT}}\n\\newcommand{\\lipf}{{20R^4B^2\\laz^{-2}\\Gamma\\nt^2\\tn{\\yT}}}\n\n\\newcommand{\\lipsum}{30R^4B^2\\laz^{-2}\\Gamma(\\nt^2+\\nv^2) \\tn{\\yT}}\n\n\\newcommand{\\lipnn}{120B^4\\blaz^{-2}\\Gamma(\\nt^2+\\nv^2) \\tn{\\yT}}\n\\newcommand{\\bL}{\\bar{L}}\n\\newcommand{\\scl}{M}\n\\renewcommand{\\d}{\\mathrm{d}}\n\n\\newcommand{\\cA}{\\mathcal{A}}\n\\newcommand{\\x}{\\vct{x}}\n\\newcommand{\\xx}[1]{\\vct{x}^{(#1)}}\n\\newcommand{\\rb}{\\vct{r}}\n\\newcommand{\\rbb}{\\vct{\\widetilde{r}}}\n\\newcommand{\\y}{\\vct{y}}\n\\newcommand{\\yT}{\\vct{y}}\n\\newcommand{\\yh}{\\hat{y}}\n\\newcommand{\\ybh}{\\vct{\\hat{y}}}\n\\newcommand{\\W}{\\mtx{W}}\n\\newcommand{\\Ww}[1]{\\mtx{W}^{(#1)}}\n\\newcommand{\\Wt}{\\tilde{\\mtx{W}}}\n\\newcommand{\\Wc}{{\\cal{W}}}\n\\newcommand{\\Wcb}{{\\cal{W}}}\n\\newcommand{\\Vc}{{\\cal{V}}}\n\\newcommand{\\bgl}{{~\\big |~}}\n\n\n\n\n\n\\definecolor{emmanuel}{RGB}{255,127,0}\n\\newcommand{\\ejc}[1]{\\textcolor{emmanuel}{EJC: #1}}\n\n\\newcommand{\\p}{{\\vct{p}}}\n\\newcommand{\\Kb}{{\\mtx{K}}}\n\\newcommand{\\Kbb}{{\\mtx{\\bar{K}}}}\n\\newcommand{\\Qbb}{{\\mtx{\\bar{Q}}}}\n\\newcommand{\\Wb}{{\\mtx{\\bar{W}}}}\n\\newcommand{\\Wpro}{\\mtx{\\W}_{\\textnormal{prod}}}\n\\newcommand{\\Kbh}{{\\widehat{\\mtx{K}}}}\n\\newcommand{\\somelog}{16\\Kb\\log (\\Kb)}\n\\newcommand{\\somelg}{\\Kb\\log (\\Kb)}\n\\newcommand{\\pb}{{\\vct{p}}}\n\\newcommand{\\pp}[1]{{\\vct{p}}(#1)}\n\\newcommand{\\pr}[1]{{\\vct{\\bar{p}}}(#1)}\n\\newcommand{\\prl}[1]{{\\vct{\\tilde{p}}}(#1)}\n\\newcommand{\\prb}[1]{{\\vct{\\bar{p}}}(#1)}\n\\newcommand{\\wrb}[1]{{\\vct{\\bar{W}}}(#1)}\n\\newcommand{\\wrt}[1]{{\\vct{\\bar{W}}}_0(#1)}\n\\newcommand{\\dpb}{{\\vct{\\dot p}}}\n\\newcommand{\\drb}{{\\vct{\\dot r}}}\n\\newcommand{\\qb}{{\\vct{q}}}\n\\newcommand{\\qstar}{\\vct{q}_\\star}\n\\newcommand{\\qtt}{{\\vct{\\tilde{q}}}}\n\\newcommand{\\wstar}{\\vct{v}_\\star}\n\\newcommand{\\vstar}{\\vct{v}_\\star}\n\\newcommand{\\wstab}{\\bar{\\vb}_\\star}\n\\newcommand{\\vstab}{\\bar{\\vb}_\\star}\n\\newcommand{\\qstab}{\\bar{\\qb}_\\star}\n\\newcommand{\\qbb}{\\bar{\\qb}}\n\\newcommand{\\wbb}{\\bar{\\w}}\n\\newcommand{\\wtt}{{\\vct{\\tilde{w}}}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\Pro}{\\mathbb{P}}\n\\newcommand{\\C}{\\mathbb{C}}\n\\newcommand{\\Z}{\\mtx{Z}}\n\\newcommand{\\V}{\\mtx{V}}\n\\newcommand{\\Za}{\\mtx{Z}^\\bal}\n\\newcommand{\\<}{\\langle}\n\\renewcommand{\\>}{\\rangle}\n\\newcommand{\\Var}{\\textrm{Var}}\n\\newcommand{\\sgn}[1]{\\textrm{sgn}(#1)}\n\\renewcommand{\\P}{\\operatorname{\\mathbb{P}}}\n\\newcommand{\\E}{\\operatorname{\\mathbb{E}}}\n\\newcommand{\\Eh}{\\operatorname{\\mathbb{\\hat{E}}}}\n\n\n\n\\newcommand{\\grad}[1]{{\\nabla\\Lc(#1)}}\n\\newcommand{\\gradf}[1]{{\\nabla f(#1)}}\n\\newcommand{\\hessf}[1]{{\\nabla^2f(#1)}}\n\\newcommand{\\gradw}[1]{{\\nabla{\\Lc}(#1)}}\n\\newcommand{\\grd}[2]{{\\nabla\\Lc_{#1}(#2)}}\n\\newcommand{\\e}{\\mathrm{e}}\n\\newcommand{\\eb}{\\vct{e}}\n\\renewcommand{\\i}{\\imath}\n\n\\newcommand{\\vct}[1]{\\bm{#1}}\n\\newcommand{\\mtx}[1]{\\bm{#1}}\n\\newcommand{\\vba}{{\\bf{\\emph{v}}}}\n\\newcommand{\\pba}{{\\bf{\\emph{p}}}}\n\n\n\n\\newcommand{\\rank}[1]{\\texttt{rank}(#1)}\n\\newcommand{\\supp}{\\Sc}\n\\newcommand{\\restrict}[1]{\\big\\vert_{#1}}\n\\newcommand{\\Id}{\\text{\\em I}}\n\\newcommand{\\OpId}{\\mathcal{I}}\n\n\\newcommand{\\Real}{\\operatorname{Re}}\n\\newcommand{\\Imag}{\\operatorname{Im}}\n\n\\newcommand{\\piyp}{\\pi'_1(\\vct{y})}\n\\newcommand{\\piy}{\\pi_1(\\vct{y})}\n\\newcommand{\\piar}{\\pi_1(\\vct{a}_r)}\n\\newcommand{\\piarp}{\\pi'_1(\\vct{a}_r)}\n\\newcommand{\\set}{{\\cal{F}}}\n\\newcommand{\\des}{{\\x_0}}\n\n\\newcommand{\\Pc}{{\\cal{P}}}\n\\newcommand{\\X}{{\\mtx{X}}}\n\\newcommand{\\Y}{{\\mtx{Y}}}\n\\newcommand{\\Vb}{{\\mtx{V}}}\n\\newcommand{\\Vh}{\\hat{{\\mtx{V}}}}\n\\newcommand{\\Vbd}{{\\mtx{V}^\\dagger}}\n\\newcommand{\\Rb}{{\\mtx{R}}}\n\\newcommand{\\bR}{{\\bar{R}}}\n\n\\newcommand{\\calF}{\\mathcal{I}}\n\\newcommand{\\calS}{\\mathcal{N}}\n\n\n\\newcommand{\\note}[1]{{\\bf [{\\em Note:} #1]}}\n\n\\newcommand{\\iprod}[2]{\\left\\langle #1 , #2 \\right\\rangle}\n\\newcommand{\\gi}{\\ab_{\\gamma_i}}\n\\newcommand{\\ang}{\\text{ang}}\n\\newcommand{\\ham}[2]{{\\|#1,#2\\|_H}}\n\n\n\\newcommand{\\mc}{\\mathcal}\n\\newcommand{\\m}[1]{{\\bf{#1}}}\n\\renewcommand{\\mc}[1]{\\ensuremath{\\mathcal{#1}}} \n\\newcommand{\\g}{\\vct{g}}\n\\newcommand{\\mb}[1]{{\\mathbb{#1}}}\n\n\\renewcommand{\\qed}{\\hfill\\blacksquare}\n\n\n\n\n==== END OF /2308.16898/notation.tex ====\n==== BEGINNING OF /2308.16898/self_attn.tex ====\n\\documentclass{article} \n\\usepackage[margin=1in]{geometry}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\input{notation}\n\\usepackage{xspace}\n\\usepackage{pifont}\n\\newcommand{\\tsc}{\\textsl}\n\\newcommand{\\Prml}{\\text{Primal}\\xspace}\n\\newcommand{\\cls}{\\texttt{[CLS]}\\xspace}\n\\newcommand{\\prml}{\\text{primal}\\xspace}\n\\newcommand{\\fat}{f_{\\tsc{sa}}}\n\\newcommand{\\fapt}[1]{f_{\\tsc{cls}}(#1\\bT)}\n\\newcommand{\\fatt}{f_{\\tsc{cls}}}\n\\newcommand{\\faptt}{f^\\top_{\\tsc{cls}}}\n\\newcommand{\\fapn}[1]{f_{\\lnk}(#1\\bT)}\n\\newcommand{\\thetab}{\\boldsymbol{\\theta}}\n\\newcommand{\\acc}{\\text{acc}}\n\\newcommand{\\svm}{\\tsc{mm}}\n\\newcommand{\\Wm}{\\W^\\svm}\n\\newcommand{\\Wmt}{\\tilde\\W^\\svm}\n\\newcommand{\\Wu}{\\W^{\\texttt{uni}}}\n\\newcommand{\\Wbi}{\\W^{\\texttt{bi}}}\n\\newcommand{\\Wub}{\\bar{\\W}^{\\texttt{uni}}}\n\\newcommand{\\Wbb}{\\bar{\\W}^{\\texttt{bi}}}\n\\newcommand{\\op}{\\texttt{opt}}\n\\newcommand{\\opt}{\\texttt{opt}}\n\\newcommand{\\xop}{\\x^\\texttt{opt}}\n\\newcommand{\\reg}{\\tsc{reg}}\n\\newcommand{\\vstap}{\\vstar'}\n\\newcommand{\\Cbp}{\\Cb'}\n\\newcommand{\\Rcb}{\\bar{\\Rc}}\n\\newcommand{\\err}{\\texttt{err}}\n\\newcommand{\\ones}{\\onebb}\n\n\\newcommand{\\onet}{\\bar{\\onebb}}\n\\newcommand{\\taub}{{\\bar{\\tau}}}\n\\newcommand{\\taut}{\\tau}\n\\newcommand{\\Qc}{{\\cal{Q}}}\n\\newcommand{\\zX}[1]{\\z\\{#1\\}}\n\\newcommand{\\aX}[1]{\\ab\\{#1\\}}\n\\usepackage[utf8]{inputenc} \n\\usepackage[T1]{fontenc}    \n\\usepackage{hyperref}       \n\\usepackage{url}            \n\\usepackage{booktabs}       \n\\usepackage{amsfonts}       \n\\usepackage{nicefrac}       \n\\usepackage{microtype}      \n\\usepackage{xcolor}         \n\\usepackage{enumitem}\n\\usepackage{tikz}\n\n\\renewcommand \\thepart{}\n\\renewcommand \\partname{}\n\\usepackage[toc,page,header]{appendix}\n\\usepackage{minitoc}\n\n\n\n\\title{\nTransformers as Support Vector Machines}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\date{}\n\\begin{document}\n\n\\author{\\\\Davoud Ataee Tarzanagh$^{1\\star}$\\quad Yingcong Li$^{2\\star}$\\qquad Christos Thrampoulidis$^{3}$\\qquad Samet Oymak$^{4\\dagger}$}\n\n\n\n\\addtocontents{toc}{\\protect\\setcounter{tocdepth}{0}}\n\n\\maketitle\n\n\n{\\let\\thefootnote\\relax\\footnotetext{$^1$ University of Pennsylvania, \\texttt{tarzanaq@upenn.edu}. $^2$ University of California, Riverside, \\texttt{yli692@ucr.edu}. $^3$ University of British Columbia, \\texttt{cthrampo@ece.ubc.ca}. $^4$ University of Michigan, \\texttt{oymak@umich.edu}. $^\\star$ Equal contribution. $^\\dagger$ Corresponding author.}}\n\n\n\n\\begin{abstract} \nSince its inception in ``Attention Is All You Need'', the transformer architecture has led to revolutionary advancements in natural language processing. The attention layer within the transformer admits a sequence of input tokens $\\X$ and makes them interact through pairwise similarities computed as $\\texttt{softmax}(\\X\\Qb\\Kb^\\top\\X^\\top)$, where $(\\Kb,\\Qb)$ are the trainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism allows us to characterize the implicit bias of 1-layer transformers optimized with gradient descent, as follows. \\textbf{(1)} Optimizing the attention layer, parameterized by $(\\Kb,\\Qb)$, with vanishing regularization, converges in direction to an SVM solution minimizing the nuclear norm of the combined parameter $\\W:=\\Kb\\Qb^\\top$. Instead, directly parameterizing by $\\W$ minimizes a Frobenius norm SVM objective. \nWe  characterize this convergence, highlighting that it can occur in locally-optimal directions rather than global ones.\n\\textbf{(2)} Complementing this, for $\\W$-parameterization, we prove the local/global directional convergence of gradient descent under suitable geometric conditions. Importantly, we show that over-parameterization catalyzes global convergence by ensuring the feasibility of the SVM problem and by guaranteeing a benign optimization landscape devoid of stationary points. \\textbf{(3)} While our theory  applies primarily to linear prediction heads, we propose a more general SVM equivalence that  predicts the implicit bias of 1-layer transformers with nonlinear heads/MLPs. \nOur findings apply to general datasets, trivially extend to cross-attention layer, and their practical validity is verified via thorough numerical experiments. We also introduce open problems and future research directions. We believe these findings inspire a new perspective, interpreting multilayer transformers as a hierarchy of SVMs that separates and selects optimal tokens.\n\n\n\\end{abstract}\n\n\\input{sections/fig_main}\n\\input{sections/introduction}\n\\input{sections/prelim}\n\\input{sections/fig_rank}\n\\input{sections/inductive_bias}\n\\input{sections/sa-gd-converge}\n\\input{sections/sa-local-gd}\n\\input{sections/multitoken}\n\\input{sections/extensions}\n\n\n\\input{sections/related}\n\\input{sections/conclusion}\n\\input{sections/acknowledgements}\n\\bibliographystyle{alpha}\n\\bibliography{refs}\n\n\\newpage\n\\appendix\n\\input{supp/supp-roadmap.tex}\n\\addtocontents{toc}{\\protect\\setcounter{tocdepth}{3}}\n\\tableofcontents\n\n\n\\input{supp/separation}\n\\input{supp/app-sa-basics.tex}\n\\input{supp/proof_convergence}\n\n\\input{supp/reg_path}\n\\input{supp/reg_path_analysis}\n\n\n\\input{supp/app_exp.tex}\n\n\n\n\\end{document}\n==== END OF /2308.16898/self_attn.tex ====\n==== BEGINNING OF /2308.16898/sections/fig_rank.tex ====\n\\begin{figure}\n    \\centering\n    \\hspace{-10pt}\n    \\subfigure[Rank of attention SVM solutions with fixed $T=5$]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.25\\columnwidth, trim={1.3cm 1.4cm 0 0}, clip]{figs/rank_svm_diff_n.pdf}};\n        \\node at (-0.95,1.5) {\\small{$\\Ws$}};\n        \\node at (-0.95,1.) {\\small{$\\Ws_\\star$}};\n        \\node at (0,-2.2) {\\small{Varying $d$}};\n        \\node[rotate=90] at (-3,0) {\\small{Rank of SVM solution}};\n        \\end{tikzpicture}\n        \\label{fig rank svm n}\n    }\n    \\hspace{30pt}\n    \\subfigure[Rank of attention SVM solutions with fixed $n=5$]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.25\\columnwidth, trim={1.3cm 1.4cm 0 0}, clip]{figs/rank_svm_diff_T.pdf}};\n        \\node at (1.85,-0.8) {\\small{$\\Ws$}};\n        \\node at (1.85,-1.28) {\\small{$\\Ws_\\star$}};\n        \\node at (0,-2.2) {\\small{Varying $d$}};\n        \\node[rotate=90] at (-3.1,0) {\\small{Rank of SVM solution}};\n        \\end{tikzpicture}\n        \\label{fig rank svm T}\n    }\n    \\caption{Rank range of solutions for  \\eqref{eqn:sattnsvm} and \\eqref{eqn:sattnsvmst}, denoted as $\\Ws$ and $\\Ws_{\\star}$, solved using optimal tokens $(\\opt_i)_{i=1}^n$ and setting  $m=d$ (the rank constraint is eliminated). Both figures confirm ranks of $\\Ws$ and $\\Ws_\\star$ are bounded by $\\max(n,d)$, validating Lemma~\\ref{lem:rank}.}\n        \\label{fig rank}\n\\end{figure}\n==== END OF /2308.16898/sections/fig_rank.tex ====\n==== BEGINNING OF /2308.16898/sections/fig_main.tex ====\n\\begin{figure}\n    \\centering\n    \\begin{minipage}{.58\\textwidth}\n    \\centering\n    \n    \\hspace{-15pt}\n    \\subfigure[$\\W$-parameterization]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.37\\columnwidth]{figs/GD_converge_path_W.pdf}};\n        \\node[right] at (1.13,0.23) {\\scriptsize{$\\Wm\\z_1$}};\n        \\node[right] at (1.13,-0.07) {\\scriptsize{$\\Wm\\z_2$}};\n        \\end{tikzpicture}\n        \\label{fig path W}\n    }\n    \\hspace{-12pt}\n    \n    \\subfigure[$(\\Kb,\\Qb)$-parameterization]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.37\\columnwidth]{figs/GD_converge_path_KQ.pdf}};\n        \\node[right] at (1.13,0.23) {\\scriptsize{$\\Wm_\\st\\z_1$}};\n        \\node[right] at (1.13,-0.07) {\\scriptsize{$\\Wm_\\st\\z_2$}};\n        \n        \\end{tikzpicture}\n        \\label{fig path KQ}\n    }\n    \\vspace{0pt}\n    \\caption{GD convergence during training of cross-attention weight $\\W$ or $(\\Kb,\\Qb)$ with data. Teal and yellow markers represent tokens from $\\X_1$ and $\\X_2$, while stars mark optimal tokens. Solid lines in Figures {\\color{blue}(a)} and {\\color{blue}(b)} depict \\ref{eqn:sattnsvm} and  \\ref{eqn:sattnsvmst}  directions mapped to $\\z_1$ (red) and $\\z_2$ (blue), respectively.  Arrows illustrating GD trajectories converging towards these SVM directions. Red and blue dotted lines represent the corresponding separating hyperplanes.} \n    \\label{fig path}\n    \\end{minipage}\n    \\hspace{4pt}\n    \\begin{minipage}{0.4\\textwidth}\n    \\centering\n    \\vspace{7pt}\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.47\\columnwidth, trim={1.3cm 1.3cm 3.2cm 0}, clip]{figs/overparam_W_bar_intro.pdf}};\n        \\node[rotate=90] at (-3.3,0) {\\footnotesize{Percentage}};\n        \\node at (-0.7,-1.8){\\footnotesize{Varying $d$}};\n        \\node[right] at (1.8,1.2){\\scriptsize{Not Local}};\n        \\node[right] at (1.8,0.92){\\scriptsize{Global}};\n        \\node[right] at (1.8,0.64){\\scriptsize{Local}};\n        \\end{tikzpicture}\n    \\vspace{-17pt}\n    \\caption{\nPercentage of different convergence types when training cross-attention weights ($\\W$) using GD and varying dimension ($d$).  Red and blue bars represent the percentages of convergence to globally-optimal and locally-optimal (including global) SVM solutions, respectively. Teal bars are complements of the blue bars. \n\nLarger overparameterization ($d$) increases the likelihood of global convergence.\n    } \n    \\label{fig overparam W bar}\n\n    \\end{minipage}\n\\end{figure}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   \n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n==== END OF /2308.16898/sections/fig_main.tex ====\n==== BEGINNING OF /2308.16898/sections/svm_obj.tex ====\n\\input{sections/fig_svm_margin}\n\\subsection{Investigation on SVM objectives and GD convergence}\nUntil now, we have discussed the global and local convergence performances of gradient descent (GD). Theorem~\\ref{thm:local:gd} suggests that, without specific restrictions on tokens, when training with GD, the attention weight $\\W$ converges towards $\\Wma$. Here, the selected token indices $\\bal = (\\alpha_{i})_{i=1}^n$ may not necessarily be identical to $\\opt = (\\op_i)_{i=1}^n$. Experiments presented in Figures~\\ref{fig overparam W bar}, \\ref{fig overparam bar}, and \\ref{fig overparam} also support this observation. In this section, we focus on scenarios where $\\bal \\neq \\opt$ (e.g., when $\\Wm$ is not feasible) and investigate the question: Towards which local direction is GD most likely to converge?\n\nTo this goal, in Figure~\\ref{fig svm margin}, we consider SVM margin, which is defined by $1/\\tf{\\Wma}$, and investigate its connection to the convergence performance of GD.  On the left, we set $T=d=5$ and vary $n$ among $1, 5, 10, 15$; on the right, we fix $T=n=5$ and change $d$ to $2, 5, 10, 15$. All tokens are randomly generated from the unit sphere. The SVM margins corresponding to the selected tokens $\\bal$ are depicted as blue curves in the upper subfigures, while the SVM margins corresponding to the globally--optimal token indices ($\\bal=\\opt$) are shown as red curves. The red and blue bars in the lower subfigures represent the percentages of global and local convergence, respectively. Combining all these findings empirically demonstrates that when the global SVM objective yields a solution with a small margin (i.e., $1/\\tf{\\Wm}$ is small, and 0 when global SVM is not feasible), GD tends to converge towards a local direction with a comparatively larger margin.\n\n==== END OF /2308.16898/sections/svm_obj.tex ====\n==== BEGINNING OF /2308.16898/sections/fig_general.tex ====\n\\begin{figure}[t]\n    \\centering\n    \\hspace{-10pt}\n    \\subfigure[Evolution of softmax probabilities]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.22\\columnwidth, trim={1.3cm 1.3cm 0 0}, clip]{figs/sfx_prob_general.pdf}};\n        \\node at (1.1,-0.55) {\\small{$\\W$}};\n        \\node at (1.3,-1.05) {\\small{$(\\Kb,\\Qb)$}};\n        \\node at (0.2,-2.) {\\small{Iterations}};\n        \\node[rotate=90] at (-2.65,0) {\\small{Softmax probability}};\n        \\end{tikzpicture}\n        \\label{fig general sfx prob}\n    }\n    \\hspace{-10pt}\n    \\subfigure[ Corr. coeff. of GD and $\\Ws_\\bal$]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.22\\columnwidth, trim={1.3cm 1.3cm 0 0}, clip]{figs/corr_fro_general.pdf}};\n        \\node at (1.1,-0.55) {\\small{$\\W$}};\n        \\node at (1.3,-1.05) {\\small{$(\\Kb,\\Qb)$}};\n        \\node at (0.3,-2.) {\\small{Iterations}};\n        \\node[rotate=90] at (-2.65,0) {\\small{Correlation coefficient}};\n        \\end{tikzpicture}\n        \\label{fig general fro corr}\n    }\n    \\hspace{-10pt}\n    \\subfigure[ Corr. coeff. of GD and $\\Ws_{\\star,\\bal}$]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.22\\columnwidth, trim={1.3cm 1.3cm 0 0}, clip]{figs/corr_nuc_general.pdf}};\n        \\node at (1.1,-0.55) {\\small{$\\W$}};\n        \\node at (1.3,-1.05) {\\small{$(\\Kb,\\Qb)$}};\n        \\node at (0.3,-2.) {\\small{Iterations}};\n        \\node[rotate=90] at (-2.65,0) {\\small{Correlation coefficient}};\n        \\end{tikzpicture}\n        \\label{fig general nuc corr}\n    }\n   \\caption{Local convergence behaviour of GD when training cross-attention weights $\\W$ (blue) or $(\\Kb,\\Qb)$ (red) with random data: \\textbf{  (a)} displays the largest entry of the softmax outputs averaged over the dataset;  \\textbf{(b\\&c)} display the Pearson correlation coefficients of GD trajectories and the SVM solutions \\textbf{(b)} with the Frobenius norm objective $\\Ws_\\bal$ (solution of \\eqref{eqn:sattnsvm}) and \\textbf{(c)} with the nuclear norm objective $\\Ws_{\\st,\\bal}$ \n (solution of \\eqref{eqn:sattnsvmst}). These demonstrate the Frobenius norm bias of $\\W(k)$ and the nuclear norm bias of $\\Kb(k)\\Qb(k)^\\top$.}\n     \\label{fig general}\n\\end{figure}\n==== END OF /2308.16898/sections/fig_general.tex ====\n==== BEGINNING OF /2308.16898/sections/related.tex ====\n\\section{Related work}\\label{sec:related}\n\n\n\\subsection{Implicit regularization, matrix factorization, and sparsity}\n\nExtensive research has delved into gradient descent's implicit bias in separable classification tasks, often using logistic or exponentially-tailed losses for margin maximization \\cite{soudry2018implicit,gunasekar2018characterizing,nacson2019convergence,ji2021characterizing,kini2021label,moroshko2020implicit,ji2020directional}. The findings have also been extended to non-separable data using gradient-based techniques \\cite{ji2018risk,ji2019implicit,ji2020gradient}. Implicit bias in regression problems and losses has been investigated, utilizing methods like mirror descent \\cite{woodworth2020kernel, gunasekar2018characterizing,\nyun2020unifying, vaskevicius2019implicit, amid2020winnowing, amid2020reparameterizing,azizan2021stochastic,sun2022mirror}. Stochastic gradient descent has also been a subject of interest regarding its implicit bias \\cite{li2019towards,blanc2020implicit,liang2020just, haochen2020shape, li2022what, damian2021label, zou2021benefits}. This extends to the implicit bias of adaptive and momentum-based methods \\cite{qian2019implicit, wang2021momentum, wang2021implicit, ji2021fast}.\n\nIn linear classification, GD iterations on logistic loss and separable datasets converge to the hard margin SVM solution \\cite{soudry2018implicit,rosset2003margin,zhang2005boosting}. The attention layer's softmax nonlinearity behaves similarly, potentially favoring margin-maximizing solutions. Yet, the layer operates on tokens in input sequences, not for direct classification. Its bias leans toward an \\eqref{eqn:sattnsvm}, selecting relevant tokens while suppressing others. However, formalizing this intuition presents significant challenges: Firstly, our problem is nonconvex (even in terms of the $\\W$-parameterization), introducing new challenges and complexities. Secondly, it requires the introduction of novel concepts such as locally-optimal tokens, demanding a tailored analysis focused on the cones surrounding them. Our findings on the implicit bias of $(\\Kb,\\Qb)$-parameterization share conceptual similarities with \\cite{srebro2004maximum}, which proposes and analyzes a max-margin matrix factorization problem. Similar problems have also been studied more recently   in the context of neural-collapse phenomena \\cite{papyan2020prevalence} through an analysis of the implicit bias and regularization path of the unconstrained features model with cross-entropy  loss \\cite{thrampoulidis2022imbalance}. However, a fundamental distinction from these works lies in the fact that attention solves a different max-margin problem that separate tokens. Moreover, our results on $(\\Kb,\\Qb)$-parameterization are inherently connected to the rich literature on low-rank factorization \\cite{gunasekar2017implicit,arora2019implicit,timor2023implicit,tu2016low,stoger2021small}, stimulating further research. \\cite{tarzanagh2023margin} is the first work to establish the connection between attention and SVM, which is closest to our work. Here, we augment their framework, initially developed for a simpler attention model, to transformers by providing the first guarantees for self/cross-attention layers, nonlinear prediction heads, and realistic global convergence guarantees. While our Assumption \\ref{assum:opt:token} and local-convergence analysis align with \\cite{tarzanagh2023margin}, our contributions in global convergence analysis, benefits of overparameterization, and the generalized SVM-equivalence in Section \\ref{sec:multi} are unique to this work.\n\n\nIt is well-known that attention map (i.e.~softmax outputs) act as a feature selection mechanism and reveal the tokens that are relevant to classification. On the other hand, sparsity and lasso regression (i.e.~$\\ell_1$ penalization) \\cite{donoho2006compressed,tibshirani1996regression,tropp2007signal,chen2001atomic,candes2006robust} have been pivotal tools in the statistics literature for feature selection. Softmax and lasso regression exhibit interesting parallels: The Softmax output $\\s=\\sft{\\X\\W\\z}$ obeys $\\|\\s\\|_{\\ell_1}=1$ by design. Softmax is also highly receptive to being sparse because decreasing the temperature (i.e.~scaling up the weights $\\W$) eventually leads to a one-hot vector unless all logits are equal. We (also, \\cite{tarzanagh2023margin}) have used these intuitions to formalize attention as a \\emph{token selection mechanism}. This aspect is clearly visible in our primary SVM formulation \\eqref{eqn:sattnsvm} which selects precisely one token from each input sequence (i.e.~hard attention). Section \\ref{sec:multi} has also demonstrated how  \\eqref{eqn:mattnsvm} can explain more general sparsity patterns by precisely selecting desired tokens and suppressing others. We hope that this SVM-based token-selection viewpoint will motivate future work and deeper connections to the broader feature-selection and compressed sensing literature.\n\n\n\\subsection{Attention mechanism and transformers}\n\nTransformers, as highlighted by \\cite{vaswani2017attention}, revolutionized the domains of NLP and machine translation. Prior work on self-attention \\cite{cheng2016long,parikh2016decomposable,paulus2017deep,lin2017structured} laid the foundation for this transformative paradigm. In contrast to conventional models like MLPs and CNNs, self-attention models employ global interactions to capture feature representations, resulting in exceptional empirical performance.\n\nDespite their achievements, the mechanisms and learning processes of attention layers remain enigmatic. Recent investigations \\cite{edelman2022inductive,sahiner2022unraveling,ergen2022convexifying,baldi2022quarks,dong2021attention} have concentrated on specific aspects such as sparse function representation, convex relaxations, and expressive power. Expressivity discussions concerning hard-attention \\cite{hahn2020theoretical} or attention-only architectures \\cite{dong2021attention} are connected to our findings when $h(\\cdot)$ is linear. In fact, our work reveals how linear $h$ results in attention's optimization dynamics to collapse on a single token whereas nonlinear $h$ provably requires attention to select and compose multiple tokens. This supports the benefits of the MLP layer for expressivity of transformers. There is also a growing body of research aimed at a theoretical comprehension of in-context learning and the role played by the attention mechanism \\cite{akyurek2022learning,li2023transformers,ahn2023transformers,zhang2023trained,bai2023transformers,giannou2023looped}. \\cite{sahiner2022unraveling} investigate self-attention with linear activation instead of softmax, while \\cite{ergen2022convexifying} approximate softmax using a linear operation with unit simplex constraints. Their primary goal is to derive convex reformulations for training problems grounded in empirical risk minimization (ERM). In contrast, our methodologies, detailed in equations \\eqref{eqn:erm:w} and \\eqref{eqn:erm:kq}, delve into the nonconvex domain.\n\n\\cite{merrill2020effects,boix2023transformers} offer insights into the implicit bias of optimizing transformers. Specifically, \\cite{merrill2020effects} provide empirical evidence that an increase in attention weights results in a sparser softmax, which aligns with our theoretical framework. \\cite{boix2023transformers} study incremental learning and furnish both theory and numerical evidence that increments of the softmax attention weights ($\\Kb\\Qb^\\top$) are low-rank. Our theory aligns with this concept, as the SVM formulation \\eqref{eqn:qk:svm} of $(\\Kb,\\Qb)$ parameterization inherently exhibits low-rank properties through the nuclear norm objective, rank-$m$ constraint, and implicit constraint induced by Lemma \\ref{lem:rank}.\n\nSeveral recent works \\cite{jelassi2022vision,li2023theoretical,tian2023scan,noci2023shaped,oymak2023role,nguyen2023primal,fu2023can} aim to delineate the optimization and generalization dynamics of transformers. However, their findings usually apply under strict statistical assumptions about the data, while our study offers a comprehensive optimization-theoretic analysis of the attention model, establishing a formal linkage to max-margin problems and SVM geometry. This allows our findings to encompass the problem geometry and apply to diverse datasets. Overall, the max-margin equivalence  provides a fundamental comprehension of the optimization geometry of transformers, offering a framework for prospective research endeavors, as outlined in the subsequent section.\n==== END OF /2308.16898/sections/related.tex ====\n==== BEGINNING OF /2308.16898/sections/fig_nn_diff_d.tex ====\n\\begin{figure}\n    \\centering\n    \\hspace{-10pt}\n    \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[width=0.985\\textwidth, trim={1.3cm 1.5cm 0 0}, clip]{figs/nn_corr_diff_d.pdf}};\n        \\node at (-5.75,-1.7) {\\small{Iterations}};\n        \\node at (-5.75,1.7) {\\small{$d=4$}};\n        \\node at (-1.8,-1.7) {\\small{Iterations}};\n        \\node at (-1.8,1.7) {\\small{$d=6$}};\n        \\node at (2.15,-1.7) {\\small{Iterations}};\n        \\node at (2.15,1.7) {\\small{$d=8$}};\n        \\node at (6.15,-1.7) {\\small{Iterations}};\n        \\node at (6.15,1.7) {\\small{$d=10$}};\n        \\node[rotate=90] at (-8.35, 0) {\\small{$1-$correlation coefficient}};\n\n        \\node at (-6.63,-0.4) {\\scriptsize{$\\W^\\rfn$}};\n        \\node at (-6.74,-0.7) {\\scriptsize{$\\Ws$}};\n        \\node at (-6.6,-.95) {\\scriptsize{$\\W^\\ont$}};\n\n        \\node at (-2.67,-0.4) {\\scriptsize{$\\W^\\rfn$}};\n        \\node at (-2.78,-0.7) {\\scriptsize{$\\Ws$}};\n        \\node at (-2.64,-.95) {\\scriptsize{$\\W^\\ont$}};\n\n        \n        \n        \n\n        \\node at (3.39,1.15) {\\scriptsize{$\\W^\\rfn$}};\n        \\node at (3.28,0.85) {\\scriptsize{$\\Ws$}};\n        \\node at (3.42,0.6) {\\scriptsize{$\\W^\\ont$}};\n\n        \\node at (7.33,1.15) {\\scriptsize{$\\W^\\rfn$}};\n        \\node at (7.22,0.85) {\\scriptsize{$\\Ws$}};\n        \\node at (7.36,0.6) {\\scriptsize{$\\W^\\ont$}};\n    \\end{tikzpicture}\n    \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[width=0.985\\textwidth, trim={1.3cm 1.5cm 0 0}, clip]{figs/nn_thred_diff_d.pdf}};\n        \\node at (-5.75,-1.7) {\\small{$\\max_{i,\\tau}s_{i\\tau},~\\tau\\in\\Rcb_i$}};\n        \\node at (-1.8,-1.7) {\\small{$\\max_{i,\\tau}s_{i\\tau},~\\tau\\in\\Rcb_i$}};\n        \\node at (2.15,-1.7) {\\small{$\\max_{i,\\tau}s_{i\\tau},~\\tau\\in\\Rcb_i$}};\n        \\node at (6.15,-1.7) {\\small{$\\max_{i,\\tau}s_{i\\tau},~\\tau\\in\\Rcb_i$}};\n        \\node[rotate=90] at (-8.35, 0) {\\small{$1-$correlation coefficient}};\n    \\end{tikzpicture}\n    \\caption{ Behavior of GD  with nonlinear nonconvex prediction head and multi-token compositions. \\textbf{Upper:} The correlation between GD solution and three distinct baselines: ({\\color{black}{\\textbf{$\\cdots$}}}) $\\Ws$ obtained from \\eqref{eqn:mattnsvm};  ({\\color{black}{\\textbf{---}}}) $\\W^\\rfn$ obtained by calculating $\\Wf$ and determining the best linear combination $\\Wf+\\gamma \\Wsb$ that maximizes correlation with the GD solution; and  ({\\color{black}{\\textbf{-~-}}}) $\\W^\\ont$ obtained by solving \\eqref{eqn:sattnsvm} and selecting the highest probability token from the GD solution.  \\textbf{Lower:} Scatterplot of the largest softmax probability over masked tokens (per our $s_{i\\tau}\\leq 10^{-6}$ criteria) vs correlation coefficient.}\n    \\label{fig nn diff d}\n\\end{figure}\n==== END OF /2308.16898/sections/fig_nn_diff_d.tex ====\n==== BEGINNING OF /2308.16898/sections/fig_nn_main.tex ====\n\\begin{figure}[t]\n    \\centering\n    \\begin{minipage}{.37\\textwidth}\n    \n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.6\\columnwidth, trim={1.2cm 1.4cm 0 -1cm}, clip]{figs/rank_diff_m.pdf}};\n        \\node at (0,-2.) {\\small{{Varying $m$}}};\n        \\node[rotate=90] at (-2.7,0) {\\small{$1-$correlation coefficient}};\n        \\end{tikzpicture}\n        \\vspace{10pt}\n        \\caption{Convergence behavior of GD when training attention weights $(\\Kb,\\Qb)\\in\\R^{d\\times m}$ with random data and varying $m$. The misalignment between attention SVM and GD, $1-\\corr{\\Ws_{\\star,\\bal},\\Kb\\Qb^\\top}$, is studied. $\\Ws_{\\star,\\bal}$ is from \\eqref{eqn:sattnsvmst} with GD tokens $\\bal$ and $m=d$. Subfigures with fixed $n=5$ and $T=5$ show that as $m$ approaches or exceeds $n$, $\\Kb\\Qb^\\top$ aligns more with $\\Ws_{\\star,\\bal}$.  }\n        \\label{fig rank m}\n    \n    \\end{minipage}\n    \\hspace{5pt}\n    \\begin{minipage}{.6\\textwidth}\n    \n    \\subfigure[Evolution of correlation under varying $d$]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.33\\columnwidth, trim={1.3cm 1.3cm 0 0}, clip]{figs/nn_corr_itr_svm.pdf}};\n        \\node at (0.2,-1.9) {\\small{Iterations}};\n        \\node[rotate=90] at (-2.55,0) {\\small{$1-$correlation coefficient}};\n        \\end{tikzpicture}\n        \\label{fig nn itr}\n    }\n    \\hspace{-10pt}\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \\subfigure[$\\Gamma$ vs correlation coefficient]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.33\\columnwidth, trim={1.3cm 1.3cm 0 0}, clip]{figs/nn_corr_sfx_zero.pdf}};\n        \n        \n        \\node at (0.2,-1.9){\\small{Masked token threshold ($\\Gamma$)}};\n        \\end{tikzpicture}\n        \\label{fig nn sfx zero}\n    }\n    \\vspace{-10pt}\n    \\caption{Behavior of GD  with nonlinear nonconvex prediction head and multi-token compositions. \\textbf{(a)}: Blue, green, red and teal curves represent the evolution of $1-\\corr{\\W,\\W^{\\rfn}}$ for $d=4,6,8$ and $10$ respectively, which have been displayed in Figure~\\ref{fig nn diff d}(upper). \\textbf{(b)}: Over the $500$ random instances as discussed in Figure~\\ref{fig nn diff d}, we filter different instances by constructing masked set with tokens whose softmax output $<\\Gamma$ and vary $\\Gamma$ from $10^{-16}$ to $10^{-6}$. The corresponding results of $1-\\corr{\\W,\\W^\\rfn}$ are displayed in blue, green, red and teal curves.} \n    \\label{fig nn main}\n    \\end{minipage}\n\\end{figure}\n==== END OF /2308.16898/sections/fig_nn_main.tex ====\n==== BEGINNING OF /2308.16898/sections/inductive_bias.tex ====\n\\section{Understanding the Implicit Bias of Self-Attention}\\label{sec:bias}\n\nWe start by motivating the optimal token definition and establishing the global convergence of RPs which shed light on the implicit bias of attention parameterizations. Throughout, we maintain the following assumption regarding the loss function.\n\\begin{assumption}\\label{assum:loss:prope}\nOver any bounded interval $[a,b]$: (i) $\\ell:\\R\\rightarrow\\R$ is strictly decreasing; (ii) The derivative $\\ell^{\\prime}$ is bounded as $|\\ell^{\\prime}(u)|\\leq M_1$; (iii) $\\ell'$ is $M_0$-Lipschitz continuous.\n\n\n\n\\end{assumption}\nAssumption~\\ref{assum:loss:prope} encompasses many common loss functions, e.g., logistic  $\\ell\\left(u\\right)=\\log\\left(1+e^{-u}\\right)$, exponential  $\\ell\\left(u\\right)=e^{-u}$, and correlation $\\ell(u)=-u$ losses.  \n\n\\begin{lemma}[Optimal Tokens Minimize Training Loss]\\label{lem min risk} Suppose Assumption \\ref{assum:loss:prope} (i)-(ii) hold, and not all tokens are optimal per Definition~\\ref{score def}. Then, training risk obeys $\\Lc(\\W)>\\Lc_\\st:=\\frac{1}{n}\\sum_{i=1}^n \\ell(\\bgam_{i\\op_i})$. Additionally, suppose there are optimal indices $(\\op_i)_{i=1}^n$ for which \\eqref{eqn:sattnsvm} is feasible, i.e.~there exists a $\\W$ separating optimal tokens. This $\\W$ choice obeys $\\lim_{R\\rightarrow\\infty}\\Lc(R\\cdot\\W)=\\Lc_\\st$.\n\\end{lemma}\n\nThe result presented in Lemma~\\ref{lem min risk} originates from the observation that the output tokens of the attention layer constitute a convex combination of the input tokens. Consequently, when subjected to a strictly decreasing loss function, attention optimization inherently leans towards the selection of a singular token, specifically, the optimal token $(\\op_i)_{i=1}^n$. Our following theorem unveils the implicit bias ingrained within both attention parameterizations through RP analysis.\n\n\\begin{theorem}\\label{thm global reg path}\nSuppose Assumptions \\ref{assum:loss:prope} holds, optimal indices $(\\op_i)_{i=1}^n$ are unique, and \\eqref{eqn:sattnsvm} is feasible. Let $\\Wm$ be the unique solution of \\eqref{eqn:sattnsvm}, and let $\\Wc^\\svm_\\star$ be the solution set of \\eqref{eqn:sattnsvmst} with nuclear norm achieving objective $C_\\st$. Then, Algorithms~\\ref{RP-W} and \\ref{RP-QK}, respectively, satisfy:\n\\begin{itemize}\n\\item $\\W$-parameterization has Frobenius norm bias: $\\underset{R\\rightarrow\\infty}{\\lim} \\frac{\\Wb_R}{R}=\\frac{\\Wm}{\\tf{\\Wm}}$.\n\\item $(\\Kb,\\Qb)$-parameterization has nuclear norm bias: $\\underset{R\\rightarrow\\infty}{\\lim} \\dist{\\frac{\\Kbb_R\\Qbb_R^\\top}{R},\\frac{\\Wc^\\svm_\\star}{C_\\st}}=0$.\n\\begin{itemize}\n\\item Setting $m=d$: \\eqref{eqn:sattnsvmst} is a convex problem without rank constraints.\n\\end{itemize} \n\\end{itemize}\n\\end{theorem}\n\nTheorem~\\ref{thm global reg path} demonstrates that the RP of the $(\\Kb,\\Qb)$-parameterization converges to a max-margin solution of \\eqref{eqn:sattnsvmst} with nuclear norm objective on $\\W = \\Kb \\Qb^\\top$. When self-attention is directly parameterized by $\\W$, the RP converges to the solution of  \\eqref{eqn:sattnsvm} with a Frobenius norm objective. This result is the first to distinguish the optimization dynamics of $\\W$ and $(\\Kb,\\Qb)$ parameterizations, revealing the low-rank bias of the latter. These findings also provide a clear characterization of token optimality (Definition \\ref{score def}) and extend naturally to the setting with  multiple optimal tokens per sequence (Theorem \\ref{local RP thm} in appendix). By definition, the RP captures the global geometry and cannot be used for the implicit bias of GD towards locally-optimal directions. Sections \\ref{provable global} and \\ref{sec local} accomplish this goal through gradient-descent and localized RP analysis to obtain locally-applicable SVM equivalences. Note that, this theorem requires each input sequence has a unique optimal token per Definition~\\ref{score def}. Fortunately, this is a very mild condition as it holds for almost all datasets, namely, as soon as input features are slightly perturbed. \n\n\nTheorem \\ref{thm global reg path} establishes the implicit bias of attention from the perspective of RP analysis. This leads to the question: To what extent is this RP theory predictive of the implicit bias exhibited by GD?  To delve into this, we examine the gradient paths of $\\W(k)$ or $(\\Kb(k),\\Qb(k))$ and present the findings in Figure~\\ref{fig path}. We consider a scenario where $n=d=m=2$ and $T=5$, and employ cross-attention, where tokens $(\\z_1,\\z_2)$ are generated independently of the inputs $(\\X_1,\\X_2)$. The teal and yellow markers correspond to tokens from $\\X_1$ and $\\X_2$, respectively. The stars indicate the optimal token for each input. To provide a clearer view of the gradient convergence path, we illustrate the outcomes of training the attention weight $\\W$ or $(\\Kb,\\Qb)$ in the form of $\\W\\z_i$ or $\\Kb\\Qb^\\top\\z_i$, where $i={1,2}$. With reference to Equations (\\ref{eqn:sattnsvm}) and (\\ref{eqn:sattnsvmst}), the red and blue solid lines in Fig.~\\ref{fig path W} delineate the directions of $\\Wsf\\z_1$ and $\\Wsf\\z_2$, correspondingly. Conversely, the red and blue solid lines in Fig.~\\ref{fig path KQ} show the directions of $\\Ws_\\star\\z_1$ and $\\Ws_\\star\\z_2$. The red/blue arrows denote the corresponding directions of gradient evolution with the dotted lines representing the corresponding separating hyperplanes. Figure~\\ref{fig path} provides a clear depiction of the incremental alignment of $\\W(k)$ and $\\Kb(k)\\Qb(k)^\\top$ with their respective attention SVM solutions as $k$ increases. This strongly supports the assertions of Theorem~\\ref{thm global reg path}.\n\n\n\n\nIt is worth noting that \\eqref{eqn:sattnsvmst} imposes a nonconvex rank constraint, i.e., $\\texttt{rank}(\\W)\\leq m$. Nevertheless, this constraint becomes inconsequential if the unconstrained problem, with $m$ set to be greater than or equal to $d$, admits a low-rank solution, as demonstrated in Lemma \\ref{lem:rank}. Consequently, in our experimental endeavors, we have the flexibility to employ the unconstrained attention SVM for predicting the implicit bias. This concept is succinctly summarized by the following lemma.\n\n\n\\begin{lemma} \nLet $\\Wc^\\svm_\\star$ be the solution set of \\eqref{eqn:sattnsvmst} with nuclear norm achieving objective $C_\\st$. Further let $\\Wcs_{\\texttt{cvx}}$ be the solution set of \\eqref{eqn:sattnsvmst} with $m=d$ achieving objective $C_{\\texttt{cvx}}$. If $\\Wcs_\\st\\cap \\Wcs_{\\texttt{cvx}}\\neq\\emptyset$, then $C_\\st=C_{\\texttt{cvx}}$ and $\\Wcs_\\st\\subseteq \\Wcs_{\\texttt{cvx}}$. Also, if the elements of $\\Wcs_{\\texttt{cvx}}$ have rank at most $m$, then,  $\\Wcs_\\st=\\Wcs_{\\texttt{cvx}}$.  \n\\end{lemma}\n\n\n==== END OF /2308.16898/sections/inductive_bias.tex ====\n==== BEGINNING OF /2308.16898/sections/fig_multi_corrs.tex ====\n\\begin{figure}[t]\n    \\centering\n    \\hspace{-10pt}\n    \\subfigure[$\\lambda$ vs \\# selected tokens]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.22\\columnwidth, trim={1.3cm 1.3cm 0 0}, clip]{figs/multi_n_selected_lambda.pdf}};\n        \\node at (0,-2.) {\\small{$\\lambda$}};\n        \\node[rotate=90] at (-2.65,0) {\\small{\\# of selected tokens}};\n        \\end{tikzpicture}\n        \\label{fig multi ns diff lambda}\n    }\n    \\hspace{-10pt}\n    \\subfigure[$\\lambda$ vs correlation coefficient]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.22\\columnwidth, trim={1.3cm 1.3cm 0 0}, clip]{figs/multi_corr_itr.pdf}};\n        \\node[rotate=90] at (-2.7,0) {\\small{$1-$correlation coefficient}};\n        \\node at (0,-2.){\\small{Iterations}};\n        \\end{tikzpicture}\n        \\label{fig multi corr diff lambda}\n    }\n    \\hspace{-10pt}\n    \\subfigure[\\# selected tokens vs correlation coefficient]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.22\\columnwidth, trim={1.3cm 1.3cm 0 0}, clip]{figs/multi_corr_itr_n_token.pdf}};\n        \\node at (0,-2.) {\\small{Iterations}};\n        \\node[rotate=90] at (-2.65,0) {\\small{$1-$correlation coefficient}};\n        \\node at (1.25,1.4) {\\footnotesize{$\\text{\\# selected}=1$}};\n        \\node at (1.25,1.02) {\\footnotesize{$\\text{\\# selected}=2$}};\n        \\node at (1.25,0.64) {\\footnotesize{$\\text{\\# selected}=5$}};\n        \\node at (1.25,0.26) {\\footnotesize{$\\text{\\# selected}=9$}};\n        \\end{tikzpicture}\n        \\label{fig multi corr diff ns}\n    }\n\\caption{ Behavior of GD when selecting multiple tokens. \\textbf{(a)} The number of selected tokens increases with $\\lambda$. \\textbf{(b)} Predictivity of attention SVM solutions for varying $\\lambda$; Dotted curves depict the correlation corresponding to $\\Ws$ calculated via \\eqref{eqn:mattnsvm} and solid curves represent the correlation to $\\W^\\rfn$, which incorporates the $\\Wf$ correction. \\textbf{(c)} Similar to (b), but evaluating correlations over different numbers of selected tokens.}\n    \\label{fig multi corrs}\n\\end{figure}\n==== END OF /2308.16898/sections/fig_multi_corrs.tex ====\n==== BEGINNING OF /2308.16898/sections/fig_overparam_bar.tex ====\n\\begin{figure}[t]\n    \\centering\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \\subfigure[Convergence behaviour of  GD for $\\W$-parameterization]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.23\\columnwidth, trim={1.3cm 1.2cm 3.2cm 0}, clip]{figs/overparam_W_bar_v2.pdf}};\n        \\node[rotate=90] at (-4.,0) {\\small{Percentage}};\n        \\node at (-1,-2.2){\\small{Varying $d$}};\n        \\node at (2.88,1.5){\\footnotesize{Not Local}};\n        \\node at (2.71,1.15){\\footnotesize{Global}};\n        \\node at (2.66,0.8){\\footnotesize{Local}};\n        \\node at (2.95,0.45){\\footnotesize{Assum B.1}};\n        \\end{tikzpicture}\n        \\label{fig overparam W bar all}\n    }\n    \\hspace{-16pt}\n    \\subfigure[Convergence  behaviour of GD for $(\\Kb,\\Qb)$-parameterization]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.23\\columnwidth, trim={1.3cm 1.2cm 3.2cm 0}, clip]{figs/overparam_KQ_bar_v2.pdf}};\n        \\node[rotate=90] at (-4.,0) {\\small{Percentage}};\n        \\node at (-1,-2.2){\\small{Varying $d$}};\n        \\node at (2.88,1.5){\\footnotesize{Not Local}};\n        \\node at (2.71,1.15){\\footnotesize{Global}};\n        \\node at (2.66,0.8){\\footnotesize{Local}};\n        \\node at (2.95,0.45){\\footnotesize{Assum B.1}};\n        \\end{tikzpicture}\n        \\label{fig overparam KQ bar all}\n    }\n    \\caption{Percentage of different convergence types of GD when training cross-attention weights \\textbf{(a)}: $\\W$ or \\textbf{(b)}: $(\\Kb,\\Qb)$ with varying $d$. In both figures, red, blue, and teal bars represent the percentages of Global, Local (including Global), and Not Local convergence, respectively. The green bar corresponds to Assumption \\ref{assum:token:supp} where all tokens act as support vectors. Larger overparameterization ($d$) relates to a higher percentage of globally-optimal SVM convergence.}\n    \\label{fig overparam bar}\n\\end{figure}\n==== END OF /2308.16898/sections/fig_overparam_bar.tex ====\n==== BEGINNING OF /2308.16898/sections/fig_svm_margin.tex ====\n\\begin{figure}[t]\n    \\centering\n    \\hspace{-10pt}\n    \\begin{tikzpicture}\n        \\node at (-2.8,0) {\\includegraphics[height=0.45\\textwidth, trim={0 0 745 0}, clip]{figs/svm_margin.pdf}};\n        \\node at (3.3,0) {\\includegraphics[height=0.45\\textwidth, trim={548 0 0 0}, clip]{figs/svm_margin.pdf}};\n        \\node at (-2.5,0.) {\\small{Varying $n$}};\n        \\node at (2.2,0.) {\\small{Varying $d$}};\n        \\node[rotate=90] at (-5.25, 2) {\\small{SVM margin}};\n        \\node[rotate=90] at (-5.25, -1.8) {\\small{Percentage}};\n\n\n        \\node[right] at (5.2,3.08) {\\small{Global}};\n        \\node[right] at (5.2,2.6) {\\small{Local}};\n\n        \\node at (-2.5,-3.8) {\\small{Varying $n$}};\n        \\node at (2.2,-3.8) {\\small{Varying $d$}};\n        \\node[right] at (5.2,-.77) {\\small{Global}};\n        \\node[right] at (5.2,-1.25) {\\small{Local}};\n        \n    \\end{tikzpicture}\n    \\caption{Performance of GD convergence and corresponding SVM margin. \\textbf{Upper:} The SVM margins correspond to globally-optimal (red) and locally-optimal (blue) token indices, denoted as $1/\\tf{\\Wm}$ and $1/\\tf{\\Wma}$, respectively. \\textbf{Lower:} Percentages of global convergence (when $\\bal=\\opt$, red) and local convergence (when $\\bal\\neq\\opt$, blue).\n    }\n    \\label{fig svm margin}\n\\end{figure}\n==== END OF /2308.16898/sections/fig_svm_margin.tex ====\n==== BEGINNING OF /2308.16898/sections/conclusion.tex ====\n\\section{Discussion, Future Directions, and Open Problems}\\label{sec:conc}\n\nOur optimization-theoretic characterization of the self-attention model provides a comprehensive understanding of its underlying principles. The developed framework, along with the research presented in \\cite{tarzanagh2023margin}, introduces new avenues for studying transformers and language models. The key findings include:\n\\begin{enumerate}[label=$\\checkmark$, wide, labelwidth=!,itemindent=!, labelindent=1pt]\n\\item The optimization geometry of self-attention exhibits a fascinating connection to  hard-margin SVM problems. By leveraging linear constraints formed through outer products of token pairs, optimal input tokens can be effectively separated from non-optimal ones.\n\\item When gradient descent is employed without early-stopping, implicit regularization and convergence of self-attention naturally occur. This convergence leads to the maximum margin solution when minimizing specific requirements using logistic loss, exp-loss, or other smooth decreasing loss functions. Moreover, this implicit bias is unaffected by the step size, as long as it is sufficiently small for convergence, and  remains independent of the initialization process.\n\\end{enumerate}\nThe fact that gradient descent leads to a maximum margin solution may not be surprising to those who are familiar with the relationship between regularization path and gradient descent in linear and nonlinear neural networks  \\cite{soudry2018implicit,\ngunasekar2018characterizing, nacson2019convergence, ji2021characterizing,moroshko2020implicit,ji2020directional}. However, there is a lack of prior research or discussion regarding this connection to the  attention mechanism. Moreover, there has been no rigorous analysis or investigation into the exactness and independence of this bias with respect to the initialization and step size. Thus, we believe our findings and insights deepen our understanding of transformers and language models, paving the way for further research in this domain. Below, we discuss some notable directions and highlight open problems that are not resolved by the existing theory.\n\\begin{itemize}\n\\item \\textbf{Convergence Rates}: The current paper establishes asymptotic convergence of gradient descent; nonetheless, there is room for further exploration to characterize non-asymptotic convergence rates. Indeed, such an exploration can also provide valuable insights into the choice of learning rate, initialization, and the optimization method. \n\n\\item \\textbf{Gradient descent on $(\\Kb,\\Qb)$ parameterization:} We find it remarkable that regularization path analysis was able to predict the implicit bias of gradient descent. Complete analysis of gradient descent is inherently connected to the fundamental question of low-rank factorization \\cite{gunasekar2017implicit,li2018algorithmic}. We believe formalizing the implicit bias of gradient descent under margin constraints presents an exciting open research direction for further research.\n\n\\item \\textbf{Generalization analysis:} An important direction is the generalization guarantees for gradient-based algorithms. The established connection to hard-margin SVM can facilitate this because the SVM problem is amenable to statistical analysis. This would be akin to how kernel/NTK analysis for deep nets enabled a rich literature on generalization analysis for traditional deep learning.\n\\item \\textbf{Global convergence of gradient descent:} We lack a complete characterization of the directional convergence of gradient descent. We ask: \\emph{Where does gradient descent directionally-converge from arbitrary initialization for 1-layer self-attention?} The role of over-parameterization as conjectured in Section \\ref{sec overparam} and the notion of locally-optimal directions discussed in Section \\ref{sec local} constitute important pieces of this puzzle (also see the discussion in \\cite{tarzanagh2023margin}).\n\n\\item \\textbf{Realistic architectures:} Naturally, we wish to explore whether max-margin equivalence can be extended to more realistic settings: Can the theory be expanded to handle multi-head attention, multi-layer architectures, and MLP nonlinearities? We believe the results in Section \\ref{sec:multi} take an important step towards this direction by including analytical formulae for the implicit bias of the attention layer under nonlinear prediction heads. \n\n\n\n\\item \\textbf{Jointly optimizing attention and prediction head:} It would be interesting to study the joint optimization dynamics of attention weights and prediction head $h(\\cdot)$. This problem can be viewed as a novel low-rank factorization type problem where $h(\\cdot)$ and $\\W$ are factors of the optimization problem, only, here, $\\W$ passes through the softmax nonlinearity. To this aim, \\cite{tarzanagh2023margin} provides a preliminary geometric characterization of the implicit bias for a simpler attention model using regularization path analysis. Such findings can potentially be generalized to the analysis of gradient methods and full transformer block.\n\\end{itemize}\n==== END OF /2308.16898/sections/conclusion.tex ====\n==== BEGINNING OF /2308.16898/sections/acknowledgements.tex ====\n\\section*{Acknowledgements}\n\nThis work was supported by the NSF grants CCF-2046816 and CCF-2212426, Google Research Scholar award, and Army Research Office grant W911NF2110312. The authors thank Xuechen Zhang, Ankit Singh Rawat, Mahdi Soltanolkotabi, Jason Lee, Arkadas Ozakin, Ramya Korlakai Vinayak, and Babak Hassibi for helpful suggestions and discussion.\n==== END OF /2308.16898/sections/acknowledgements.tex ====\n==== BEGINNING OF /2308.16898/sections/sa-gd-converge.tex ====\n\\section{Global Convergence of Gradient Descent}\\label{provable global}\n\n\nIn this section, we will establish conditions that guarantee the global convergence of GD. Concretely, we will investigate when GD solution selects the \\emph{optimal token within each input sequence} through the softmax nonlinearity and coincides with the solution of the RP. Section~\\ref{sec local} will complement this with showing that self-attention can more generally converge to locally-optimal max-margin directions. We identify the following conditions as provable catalysts for global convergence: (i) Optimal tokens have relatively large scores; (ii) Initial gradient direction is favorable; (iii) Overparameterization, i.e.~$d$ is appropriately large. \n\n\\subsection{Properties of optimization landscape}\nWe start by establishing some fundamental properties of Objectives \\eqref{eqn:erm:w} and \\eqref{eqn:erm:kq}.\n\n\\begin{lemma}\\label{lem:lip}\nUnder Assumption~\\ref{assum:loss:prope}, $ \\nabla\\Lc(\\W)$,   $ \\nabla_{\\Kb} \\Lc(\\Kb,\\Qb)$, and  $\\nabla_{\\Qb} \\Lc(\\Kb,\\Qb)$ are $L_{\\W}$,  $L_{\\Kb}$, $L_{\\Qb}$--Lipschitz continuous, respectively, where $a_i=\\|\\vb\\|~\\|\\z_i\\|^2 \\|\\X_i \\|^3$,  $b_i= M_0\\|\\vb\\|~\\|\\X_i\\|+ 3  M_1 $ for all $i\\in[n]$,\n\\begin{align}\\label{eqn:lip:cons:erm}\nL_{\\W}:=\\frac{1}{n}\\sum_{i=1}^{n} a_i b_i, \\quad L_{\\Kb}:= \\|\\Qb\\|L_{\\W}, \\quad \\textnormal{and} \\quad L_{\\Qb}:= \\|\\Kb\\|L_{\\W}.\n\\end{align}\n\\end{lemma}\n\nThe next assumption will play an important role ensuring the attention layer has a benign optimization landscape.\n\n\\begin{assumption}\\label{assum:token}\nOptimal tokens' indices $(\\op_i)_{i=1}^n$ are unique and one of the following conditions on the tokens holds:\n\\begin{enumerate}[label={\\textnormal{{\\textbf{B.\\arabic*}}}}, wide, labelwidth=!,itemindent=!, labelindent=1pt]\n\\item \\label{assum:token:supp} All tokens are support vectors, i.e., $ (\\x_{i\\op_i}-\\x_{it})^\\top\\Ws\\z_i= 1$ for all $t\\neq \\op_i$ and $i\\in[n]$.\n\n\\item \\label{assum:opt:token} The tokens' scores, as defined in Definition~\\ref{score def}, satisfy $\\bgam_{it}=\\bgam_{i\\tau}<\\bgam_{i\\op_i}$,\nfor all $t,\\tau\\neq \\op_i$ and $i\\in[n]$.\n\\end{enumerate}\n\\end{assumption}\n\n\n\nAssumption \\ref{assum:token:supp} is directly linked to overparameterization and holds practical significance. In scenarios such as classical SVM classification, where the goal is to separate labels, overparameterization leads to the situation where \\emph{all training points become support vectors}. Consequently, the SVM solution aligns with the least-squares minimum-norm interpolation, a concept established in \\cite{muthukumar2021classification, hsu2021proliferation} under broad statistical contexts. Assumption \\ref{assum:token:supp} represents an analogous manifestation of this condition. Therefore, in cases involving realistic data distributions with sufficiently large $d$, we expect the same phenomena to persist, causing the SVM solution $\\Wm$ to coincide with \\eqref{eqn:sattnsvm}. \n\nDrawing on insights from \\cite[Theorem 1]{hsu2021proliferation} and our  Theorem \\ref{thm:separation}, we expect that the necessary degree of overparameterization remains moderate. Specifically, in instances where input sequences follow an independent and identically distributed (IID) pattern and tokens exhibit IID isotropic distributions, we posit that $d\\gtrsim (T+n)\\log(T+n)$ will suffice. More generally, the extent of required overparameterization will be contingent on the covariance of tokens \\cite{bartlett2020benign, muthukumar2021classification} and the distribution characteristics of input sequences \\cite{wang2022binary}. \n\nAssumption \\ref{assum:opt:token} stipulates that non-optimal tokens possess identical scores which constitutes a relatively stringent assumption that we will subsequently relax. Under Assumption~\\ref{assum:token}, we establish that when optimization problem \\eqref{eqn:erm:w} is trained using GD, the norm of parameters will diverge. \n\n\n\n\n\\begin{theorem}\n\\label{diverg:norm:w}\nSuppose Assumption~\\ref{assum:loss:prope} on the loss function $\\ell$ and Assumption \\ref{assum:token} on the tokens hold.  \n\n\\begin{itemize}\n\\item \nThere is no $\\W\\in\\R^{d\\times d}$ satisfying $\\nabla \\Lc(\\W)=0$.\n    \\item  Algorithm~\\ref{GD-W} with the step size $\\eta \\leq 1 /L_{\\W}$ and any starting point $\\W(0)$ satisfies \n\n$\\lim_{k\\rightarrow\\infty} \\tf{\\W(k)}=\\infty$.\n\n\n\n\n\\end{itemize}\n\\end{theorem}\nThe feasibility of SVM (per Theorem \\ref{thm:separation}) is a necessary condition for the convergence of GD to the $\\Wm$ direction. However, it does not inform us about the optimization landscape. Two additional criteria are essential for convergence: the absence of stationary points $\\nabla\\Lc(\\W)=0$ and divergence of parameter norm to infinity. Theorem \\ref{diverg:norm:w} above precisely guarantees both of these criteria under Assumption \\ref{assum:token}. \n\n\\input{sections/fig_overparam_bar}\n\n\\subsection{Provable global convergence of 1-layer transformer}\nIn the quest for understanding the global convergence of a 1-layer transformer,  \\cite[Theorem 2]{tarzanagh2023margin} provided the first global convergence analysis of the attention in a restrictive scenario where $n=1$ and under the assumption \\ref{assum:opt:token}. Here, we present two new conditions for achieving global convergence towards the max-margin direction $\\Wm$ based on: \\textbf{(I)} the initial gradient direction, and \\textbf{(II)} over-parameterization. For the first case, we provide precise theoretical guarantees. For the second, we offer strong empirical evidence, supported by Theorem \\ref{diverg:norm:w}, and a formal conjecture described in Section \\ref{sec overparam}. We remind the reader that we optimize the attention weights $\\W$ while fixing the linear prediction head $h(\\x)=\\vb^\\top\\x$. This approach avoids trivial convergence guarantees where an over-parameterized $h(\\cdot)$whether it is a linear model or an MLPcan be used to achieve zero training loss without providing any meaningful insights into the functionality of the attention mechanism.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\paragraph{(I) Global convergence under good initial gradient.} To ensure global convergence, we identify an assumption that prevents GD from getting trapped at suboptimal tokens that offer no scoring advantage compared to other choices. To establish a foundation for providing the convergence of GD to the globally optimal solution $\\Ws$, we need the following definitions.  For parameters $\\mu >0$ and $R>0$, define\n\n\\begin{align}\\label{eqn:con:nabla0:main}\n\\conb_{\\mu,R}:=\\Bigg\\{ \\tf{\\W}\\geq R~\\Big|~  \\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\frac{\\W}{\\tf{\\W}}\\ri\\geq \\mu \\quad \\textnormal{for all}\\quad t\\neq \\op_i,~  i\\in[n]\\Bigg\\}.\n\\end{align}\nThis is the set of all $\\W$s that separate the optimal tokens from the rest with margin $\\mu$. We will show that, for any $\\mu>0$, the optimization landscape of this set is favorable and, if the updates remain in the set, the gradient descent will maximize the margin and find $\\Wm$.\n\n\\begin{assumption}[First GD Step is Separating]\\label{assum:nabla0} For some $\\iota>0$ and all $t\\neq \\op_i,~i\\in[n]$: $ (\\x_{it}-\\x_{i\\op_i})^\\top\\nabla\\Lc(0)\\z_i\\geq \\iota$.\n\\end{assumption}\n\n\n\\begin{theorem}\\label{conv:gd:w:global:nabla0}\nSuppose Assumption~\\ref{assum:loss:prope} on the loss function $\\ell$ and Assumption \\ref{assum:nabla0} on the initial gradient hold. \n\n\\begin{itemize}\n\n\n\\item  For any $\\mu>0$, there exists  $R>0$ such  that   $\\conb_{\\mu,R}$ does not contain any  stationary points. \n\\item Fix any $\\mu \\in  (0, \\iota/\\tf{\\nabla \\Lc(0)})$. Consider GD iterations with $\\W(0)=0$, $\\W(1)=-R\\nabla\\,\\Lc(0)/\\tf{\\nabla\\Lc(0)}$, and $\\W(k+1)=\\W(k)-\\eta\\nabla\\Lc(\\W(k))$ for $k\\ge 1$, where $\\eta\\le 1/L_{\\W}$ and $R$ sufficiently large. If all iterates remain within $\\conb_{\\mu,R}$, then $\\lim_{k\\rightarrow\\infty} \\tf{\\W(k)}=\\infty$ and $\\lim_{k\\rightarrow\\infty}\\frac{\\W(k)}{\\tf{\\W(k)}}=\\frac{\\Wm}{\\tf{\\Wm}}$.\n\\end{itemize}\n\\end{theorem}\nNote that the second result of Theorem~\\ref{conv:gd:w:global:nabla0}, i.e.,\nthe divergence of parameter norm to infinity and directional convergence requires that all GD iterations remain within $\\conb_{\\mu,R}$ defined in \\eqref{eqn:con:nabla0:main}. In Appendix~\\ref{app B4}, we show that if for all $\\W \\in \\conb_{\\mu,R}(\\Ws)$, $\\min_{i \\in [n]}\\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\W-\\eta\\nabla\\Lc(\\W) \\ri-\\min_{i \\in [n]}\\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\W \\ri$ is lower bounded by $(2\\eta\\mu/\\tf{\\Wm})\\iprod{-\\nabla\\mc{L}(\\W)}{\\Wm}$, then all GD iterations $\\W(k)$ remain within $\\conb_{\\mu,R}$. While this condition may appear complicated, it is essentially a tight requirement for updates to remain within $\\conb_{\\mu,R}$. Finally, it is worth mentioning that, if a stronger correlation condition between initial gradient $\\nabla\\Lc(0)$ and $\\Wm$ holds, one can also prove that updates remain within a tighter cone around $\\Wm$ through ideas developed in Theorem \\ref{thm:local:gd} by landing $\\W(1)$ around $\\Wm$ direction. However, we opt to state here the result for the milder condition  $\\conb_{\\mu,R}$. \n\n\n\n\n\n\n\n\n\n\n\n\n \\input{sections/fig_overparam}\n\n\n\n\\paragraph{(II) Global convergence via overparameterization.} \nIn the context of standard neural networks, overparameterization has been recognized as a pivotal factor for the global convergence of GD \\cite{du2018gradient,allen2019convergence,li2018learning,oymak2019overparameterized}. However, conventional approaches like the neural tangent kernel \\cite{jacot2018neural} do not seamlessly apply to our scenario, given our assumption on fixed $h$ and the avoidance of achieving zero loss by trivially fitting $h$. Furthermore, even when we train $h$ to achieve zero loss, it doesn't provide substantial insights into the implicit bias of the attention weights $\\W$. Conversely, Theorem \\ref{diverg:norm:w} illustrates the benefits of over-parameterization in terms of convergence. Considering that Assumption \\ref{assum:token:supp} is anticipated to hold as the dimension $d$ increases, the norm of the GD solution is bound to diverge to infinity. This satisfies a prerequisite for converging towards the globally-optimal SVM direction $\\Ws$. \n\n\n\nThe trend depicted in Figure \\ref{fig overparam bar}, where the percentage of global convergence (red bars) approaches $100\\\n\n\n\n\n\n\n\nFurthermore, the observations in Figure \\ref{fig overparam} regarding the percentages of achieving global convergence reaching 100 with larger $d$ reaffirm that overparameterization leads the attention weights to converge directionally towards the optimal max-margin direction outlined by \\eqref{eqn:sattnsvm} and \\eqref{eqn:sattnsvmst}. \n\nIn the upcoming section, we will introduce locally-optimal directions, to which GD can be proven to converge when appropriately initialized. We will then establish a condition that ensures the \\emph{globally-optimal direction is the sole viable locally-optimal direction}. This culmination will result in a formal conjecture detailed in Section \\ref{sec overparam}.\n\n\n\n\n\n==== END OF /2308.16898/sections/sa-gd-converge.tex ====\n==== BEGINNING OF /2308.16898/sections/exp_report.tex ====\n\\section{Experiments}\n\n\\begin{figure}[t]\n\\centering\n\\hspace{-10pt}\n\\subfigure[Gradient path]{\n    \\includegraphics[width=.33\\columnwidth]{figs/GD_path.pdf}\n    \\label{fig:path}\n    \n}\n\\hspace{-10pt}\n\\subfigure[Correlations when training $\\W$]{\n    \\includegraphics[width=.33\\columnwidth]{figs/corr_W.pdf}\n    \\label{fig:corr W}\n    \n}\n\\hspace{-10pt}\n\\subfigure[Correlations when training $\\Kb,\\Qb$]{\n    \\includegraphics[width=.33\\columnwidth]{figs/corr_KQ.pdf}\n    \\label{fig:corr KQ}\n}\n\\vspace{-2mm}\n\\caption{The convergence behavior of the gradient descent on the attention weights $\\W$ or $(\\Kb,\\Qb)$ using the logistic loss. \n\n}\n\\label{fig:main_fig}\\vspace{-5pt}\n\\end{figure}\n\\begin{figure}[t]\n\\centering\n\\hspace{-10pt}\n\\subfigure[Gradient path]{\n    \\includegraphics[width=.33\\columnwidth]{figs/GD_path_1.pdf}\n    \\label{fig:path 1}\n    \n}\n\\hspace{-10pt}\n\\subfigure[Correlations when training $\\W$]{\n    \\includegraphics[width=.33\\columnwidth]{figs/corr_W_1.pdf}\n    \\label{fig:corr W 1}\n    \n}\n\\hspace{-10pt}\n\\subfigure[Correlations when training $\\Kb,\\Qb$]{\n    \\includegraphics[width=.33\\columnwidth]{figs/corr_KQ_1.pdf}\n    \\label{fig:corr KQ 1}\n}\n\\vspace{-2mm}\n\n\\hspace{-10pt}\n\\subfigure[Gradient path]{\n    \\includegraphics[width=.33\\columnwidth]{figs/GD_path_0.pdf}\n    \\label{fig:path 0}\n    \n}\n\\hspace{-10pt}\n\\subfigure[Correlations when training $\\W$]{\n    \\includegraphics[width=.33\\columnwidth]{figs/corr_W_0.pdf}\n    \\label{fig:corr W 0}\n    \n}\n\\hspace{-10pt}\n\\subfigure[Correlations when training $\\Kb,\\Qb$]{\n    \\includegraphics[width=.33\\columnwidth]{figs/corr_KQ_0.pdf}\n    \\label{fig:corr KQ 0}\n}\n\\vspace{-2mm}\n\\caption{Other results when using different $\\z_i$s.\n}\n\\label{fig:main_fig 01}\\vspace{-5pt}\n\\end{figure}\n\\begin{itemize}\n    \\item Set $n=d=2$, $T=3$. $\\x_{it}$ are randomly sampled from unit sphere, and $\\vb=[0~1]^\\top$ and labels $Y_i$ are all ones. \n    \\item Figure~\\ref{fig:main_fig} shows the convergence results when training with attention weights $\\W$ or $(\\Kb,\\Qb)$. For simplicity, let $\\tilde{\\W}=\\Qb\\Kb^\\top$. To visualize the gradient evolution, we focus on $\\W\\z_i$ and $\\tilde\\W\\z_i$ for $i\\in[n]$, so that it will clearly show how $\\W\\z_i$ ($\\tilde\\W\\z_i$) converges in direction to the max-margin direction within each input. Fig.~\\ref{fig:path}: Cyan and green markers represent different inputs and the stars are the optimal token within each input. Solid lines (red and blue) represent $\\Ws_F\\z_1$ and $\\Ws_F\\z_2$ and dashed lines represent $\\Ws_\\star\\z_1$ and $\\Ws_\\star\\z_2$ (we are overlaping). We also plot the trajectories $\\W(t)\\z_i$ and $\\tilde\\W(t)\\z_i$, $i\\in\\{1,2\\}$ in solid and dashed arrows. \n    \\item Fig.~\\ref{fig:corr W} and \\ref{fig:corr KQ}: Define the correlation of two matrices $\\W_1,\\W_2\\in\\R^{d\\times d}$ as follows: \n    \\[\n    \\text{corr($\\W_1,\\W_2$)}=1-\\left\\|\\frac{\\W_1}{\\|\\W_1\\|_F}-\\frac{\\W_2}{\\|\\W_2\\|_F}\\right\\|_F^2.\n    \\]\n    Then in Fig.~\\ref{fig:corr W}, blue and orange curves show evolutions of corr($\\W(t),\\Ws_F$) and corr($\\W(t),\\Ws_\\star$), respectively, and in Fig.~\\ref{fig:corr KQ}, blue and orange curves represent corr($\\tilde\\W(t),\\Ws_F$) and corr($\\tilde\\W(t),\\Ws_\\star$).\n    \\item \\textbf{Random Data Experiments}: Set $n=T=3$ and $d=4$. $\\x_{it}$ and $\\vb$ are randomly sampled from unit sphere, and $Y_i$ are uniformly $\\pm1$. Conduct $100$ random and successful trials ensuring that 1) attention succeeds in selecting one token per input, $\\max_{t\\in[T]}\\sft{\\X_i\\W\\z_i}_t=1$; 2) problems are separable and \\eqref{eqn:sattnsvm} are feasible. Then over the $100$ trials, the averaged correlations are\n    \\[\n    [\\text{corr($\\W,\\Ws_F$)}~~\\text{corr($\\W,\\Ws_\\star$)}]=[0.997~~ 0.726],\n    \\]\n    \\[\n    [\\text{corr($\\tilde\\W,\\Ws_F$)}~~\\text{corr($\\tilde\\W,\\Ws_\\star$)}]=[0.749~~0.898].\n    \\]\n    If setting $\\tilde\\W(0)=\\Ws_\\star$, then\n    \\[\n    [\\text{corr($\\tilde\\W,\\Ws_F$)}~~\\text{corr($\\tilde\\W,\\Ws_\\star$)}]=[0.769~~0.982].\n    \\]\n    \\item \\textbf{Random Data Experiments v2}: Set $n=4,T=4,d=20$ and train with cross attention where $\\z_{i}$'s are also randomly sampled from unit sphere. Results averaged form $100$ trials are\n    \\[\n    \\text{Frob:}~[\\text{corr($\\tilde\\W,\\Ws_F$)}~~\\text{corr($\\tilde\\W,\\Ws_\\star$)}]=[0.996~~0.607].\n    \\]\n    \\[\n    \\text{Nuc:}~~[\\text{corr($\\tilde\\W,\\Ws_F$)}~~\\text{corr($\\tilde\\W,\\Ws_\\star$)}]=[0.692~~0.840].\n    \\]\n    \\begin{enumerate}\n        \\item After setting $\\Kb,\\Qb\\in\\R^{d\\times m}$ to be low rank with  $m=rank(\\Wso)$, we obtain\n        \\[\n    \\text{Nuc:}~~[\\text{corr($\\tilde\\W,\\Ws_F$)}~~\\text{corr($\\tilde\\W,\\Ws_\\star$)}]=[0.695~~0.837].\n    \\]\n    \\item After setting $\\Kb(0)=\\Ub\\bSi^{1/2}+0.1\\cdot\\Nc(0,\\Iden)$ and $\\Qb(0)=\\Vb\\bSi^{1/2}+0.1\\cdot\\Nc(0,\\Iden)$ where $\\Ub\\bSi\\Vb=\\Wso/\\|\\Wso\\|$, we obtain\n    \\[\n    \\text{Nuc:}~~[\\text{corr($\\tilde\\W,\\Ws_F$)}~~\\text{corr($\\tilde\\W,\\Ws_\\star$)}]=[0.634~~0.968].\n    \\]\n    \\item Combining low rank and initialization together, we obtain\n    \\[\n    \\text{Nuc:}~~[\\text{corr($\\tilde\\W,\\Ws_F$)}~~\\text{corr($\\tilde\\W,\\Ws_\\star$)}]=[0.598~~0.979].\n    \\]\n    \\end{enumerate}\n\n\\end{itemize}\n\n\\begin{figure}[t]\n\\centering\n\\hspace{-10pt}\n    \\includegraphics[width=.5\\columnwidth]{figs/diff_nT.pdf}\n    \\label{fig:diff nT}\n    \n\n\\caption{The probabilities of global convergence. We train attention weight $\\W$ with different $(n,T,d)$ choices, and examine the probabilities of global convergence under each setting. Here, $\\W$ globally converges in direction to $\\Wso$ only if $\\arg\\max_{t\\in[T]}\\sft{\\X_i\\W\\z_{i}}_t=\\op_i$ for all $i\\in[n]$.\n}\n\\label{fig:main_fig}\\vspace{-5pt}\n\\end{figure}\n==== END OF /2308.16898/sections/exp_report.tex ====\n==== BEGINNING OF /2308.16898/sections/sa-local-gd.tex ====\n\\section{Understanding Local Convergence of 1-Layer Transformer}\\label{sec local}\n\nSo far, we have primarily focused on the convergence to the global direction dictated by \\eqref{eqn:sattnsvm}. In this section, we investigate and establish the local directional convergence of GD as well as RP.\n\n\\input{sections/fig_general}\n\n\\subsection{Local convergence of gradient descent}\\label{local GD sec}\n\nTo proceed, we introduce locally-optimal directions by adapting Definition 2 of \\cite{tarzanagh2023margin}. \n\n\\begin{definition}[\\NEIS and Locally-Optimal Direction]\\label{def loc opt} \nFix token indices $\\bal=(\\alpha_i)_{i=1}^n$. Solve \\eqref{eqn:sattnsvm} with $ (\\opt_i)_{i=1}^n$ replaced with $\\boldsymbol{\\alpha} = (\\alpha_i)_{i=1}^n$ to obtain $\\Wma$. Consider the set $\\Tc_i\\subset[T]$ such that $(\\x_{i\\alpha_i}-\\x_{it})^\\top \\Wma \\z_i=1$ for all $t\\in\\Tc_i$. We refer to $(\\Tc_i)_{i=1}^n$ as the \\neis of $\\bal$. Additionally, if for all $i\\in[n]$ and $t\\in\\Tc_i$ scores per Definition~\\ref{score def} obey $\\bgam_{i\\alpha_i}>\\bgam_{it}$, indices $\\bal=(\\alpha_i)_{i=1}^n$ are called \\emph{locally-optimal} and $\\Wma$ is called a \\emph{locally-optimal direction}.\n\\end{definition}\n\nIn words,  the concept of local optimality requires that the selected tokens denoted as $\\bal$ should have scores that are higher than the scores of their neighboring tokens referred to as \\neis. It is important to observe that the tokens defined as $\\op=(\\op_i)_{i=1}^n$, which we term as the optimal tokens, inherently satisfy the condition of local optimality. Moving forward, we will provide Theorem $\\ref{thm:local:gd}$ which establishes that when the process of GD is initiated along a direction that is locally optimal, it gradually converges in that particular direction, eventually aligning itself with $\\Wma$. This theorem immediately underscores the fact that if there exists a direction of local optimality (apart from the globally optimal direction $\\Wm$), then when GD commences from any arbitrary starting point, it does not achieve global convergence towards $\\Wm$. \n\n\n\n\n\nTo provide a basis for discussing local convergence of GD, we establish a cone centered around $\\Wma$ \nusing the following construction. For parameters $\\mu \\in (0,1)$ and $R>0$, we define $\\Cc_{\\mu,R}(\\Wma)$ as the set of matrices $\\W \\in\\R^{d\\times d}$ such that $\\tf{\\W}\\geq R$ and  the correlation coefficient between $\\W$ and $\\Wma$ is at least $1-\\mu$:\n\n\n\n\\begin{align}\\label{eqn:coneofw:r:main}\n\n\n\\Cc_{\\mu,R}({\\Wma})=\\left\\{ \\tf{\\W}\\geq R~\\Big|~  \\left\\langle \\frac{\\W}{\\tf{\\W}},\\frac{\\Wma}{\\tf{{\\Wma}}} \\right\\rangle \\geq 1-\\mu \\right\\}.\n\\end{align}\n\n\n\n\n\n\n\n\\begin{theorem}\n\\label{thm:local:gd} \nSuppose Assumption~\\ref{assum:loss:prope} on the loss $\\ell$ holds, and let $\\bal=(\\alpha_i)_{i=1}^n$ be locally optimal tokens according to Definition \\ref{def loc opt}. Let $ \\Wma$ denote the SVM solution obtained via \\eqref{eqn:sattnsvm} by  replacing $(\\opt_i)_{i=1}^n$ with $\\boldsymbol{\\alpha} = (\\alpha_i)_{i=1}^n$. \n\n\\begin{itemize}\n    \\item \\label{lem:cond:t1}  There exist parameters $\\mu=\\mu(\\bal) \\in (0,1)$ and  $R>0$ such  that   $ \\Cc_{\\mu,R} (\\Wma)$ does not contain any  stationary points.\n    \\item  Algorithm~\\ref{GD-W} with $\\eta \\leq 1 /L_{\\W}$ and any $\\W(0) \\in \\Cc_{\\mu,R}(\\Wma)$ satisfies $\\lim_{k\\rightarrow\\infty} \\tf{\\W(k)} = \\infty$  and $\\lim_{k\\rightarrow\\infty} \\frac{\\W(k)}{\\tf{\\W(k)}} = \\frac{\\Wma}{\\tf{\\Wma}}$.\n\\end{itemize}\n\\end{theorem}    \nThis theorem establishes the existence of positive parameters $\\mu=\\mu(\\bal)>0$ and $R>0$ such that there are no stationary points within  $\\Cc_{\\mu,R}(\\Wma)$. Furthermore, if GD is initiated within $\\Cc_{\\mu,R}(\\Wma)$, it will converge in the direction of $\\Wma/\\tf{\\Wma}$. It is worth mentioning that stronger Theorem \\ref{diverg:norm:w} (e.g. global absence of stationary points) is applicable whenever all tokens are support i.e.~$\\Tcb_i=\\emptyset$ for all $i\\in[n]$. \n\n\n\n\n\nIn Figure~\\ref{fig general}, we consider setting where $n=6$, $T=8$, and $d=10$. The displayed results are averaged from $100$ random trials. We train cross-attention models with $\\x_{it},\\z_{i},\\vb\\in\\R^d$ randomly sampled from unit sphere, and apply the normalized GD approach with fixed step size $\\eta=0.1$.  In Figure~\\ref{fig general sfx prob} we calculate the softmax probability via $\\frac{1}{n}\\sum_{i=1}^n\\max_{t\\in[T]}\\sft{\\X_i\\tilde\\W(k)\\z_i}_t$ for either $\\tilde\\W=\\W$ or $\\Kb\\Qb^\\top$ at each iteration. Both scenarios result in probability $1$, which indicates that attention weights succeed in selecting one token per input. Then following Definition~\\ref{def loc opt} let $\\bal=(\\alpha_i)_{i=1}^n$ be the token indices selected by GD and denote $\\Ws_{\\star,\\bal}$ as the corresponding SVM solution of \\eqref{eqn:sattnsvmst}. Define the correlation coefficient of two matrices as \n$\\texttt{corr\\_coef}(\\W_1,\\W_2):=\\li\\W_1,\\W_2\\ri/\\|\\W_1\\|_F\\|\\W_2\\|_F$. \nFigures~\\ref{fig general fro corr}\n and \\ref{fig general nuc corr} illustrate the correlation coefficients of attention weights ($\\W(k)$ and $\\Kb(k)\\Qb(k)^\\top$) with respect to $\\Ws_\\bal$ and $\\Ws_{\\star,\\bal}$. The results demonstrate that $\\W$ ($\\Kb\\Qb^\\top$) ultimately reaches a $1$ correlation with $\\Wsf_\\bal$ ($\\Ws_{\\star,\\bal}$), which suggests that $\\W$ ($\\Kb\\Qb^\\top$) converges in the direction of $\\Wsf_\\bal$ ($\\Ws_{\\star,\\bal}$). This further validates Theorem~\\ref{thm:local:gd}. \n\n\n\\input{sections/overparam}\n\\input{sections/svm_obj}\n\\input{sections/local-reg-path}\n==== END OF /2308.16898/sections/sa-local-gd.tex ====\n==== BEGINNING OF /2308.16898/sections/fig_overparam_app.tex ====\n\\begin{figure}[t]\n    \\centering\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \\subfigure[Train attention weights $\\Kb,\\Qb\\in\\R^{d\\times m}$]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.25\\columnwidth, trim={1.2cm 1.4cm 0 0}, clip]{figs/rank_diff_m.pdf}};\n        \\node at (0,-2.2) {\\small{{Varying $m$}}};\n        \\node[rotate=90] at (-3.1,0) {\\small{$1-$correlation coefficient}};\n        \\end{tikzpicture}\n        \\label{fig rank m}\n    }\n    \\hspace{10pt}\n    \\subfigure[Convergence types of $(\\Kb,\\Qb)$-parameterization]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.25\\columnwidth, trim={1.3cm 1.2cm 3.2cm 0}, clip]{figs/overparam_KQ_bar.pdf}};\n        \\node[rotate=90] at (-4.3,0) {\\small{Percentage}};\n        \\node at (-1,-2.2){\\small{Varying $d$}};\n        \\node at (3.2,1.6){\\small{Not Local}};\n        \\node at (3.2,1.24){\\small{Global}};\n        \\node at (3.2,0.87){\\small{Local}};\n        \\node at (3.2,0.5){\\small{Assum B.1}};\n        \\end{tikzpicture}\n        \\label{fig overparam KQ bar}\n    }\n    \\caption{ Convergence behavior of GD when training cross-attention weights $\\W$ (solid) or $(\\Kb,\\Qb)$ (dashed) with random data and varying $m$ and $d$. \\textbf{(a)}: Fixing dimensionality at $d=20$, attention weights $\\Kb,\\Qb\\in\\R^{d\\times m}$ are trained for $m\\leq10$. The misalignment between attention SVM and GD, $1-\\corr{\\Ws_{\\star,\\bal},\\Kb\\Qb^\\top}$, is studied. $\\Ws_{\\star,\\bal}$ is from \\eqref{eqn:sattnsvmst} with GD tokens $\\bal$ and $m=d$. Subfigures with fixed $n=5$ and $T=5$ show that as $m$ approaches or exceeds $n$, $\\Kb\\Qb^\\top$ aligns more with $\\Ws_{\\star,\\bal}$.  \\textbf{(b)}: Red, blue, and teal bars represent the percentages of Global, Local (including Global), and Not Local convergence, respectively. The green bar corresponds to Assumption \\ref{assum:token:supp} where all tokens act as support vectors. Larger overparameterization ($d$) relates to a higher percentage of optimal SVM convergence.} \n    \\label{fig overparam bar}\n\\end{figure}\n==== END OF /2308.16898/sections/fig_overparam_app.tex ====\n==== BEGINNING OF /2308.16898/sections/overparam.tex ====\n\n\n\\subsection{Overparameterization conjecture: When local-optimal directions disappear}\\label{sec overparam}\n\nIn Section \\ref{provable global} we demonstrated that larger $d$ serves as a catalyst for global convergence to select the optimal indices $\\op=(\\op_i)_{i=1}^n$. However, Section \\ref{local GD sec} shows that the convergence can be towards locally-optimal directions rather than global ones. How do we reconcile these? Under what precise conditions, can we expect global convergence?\n\nThe aim of this section is gathering these intuitions and stating a concrete conjecture on the global convergence of the attention layer under geometric assumptions related to overparameterization. To recap, Theorem \\ref{thm:separation} characterizes when \\eqref{eqn:sattnsvm} is feasible and Theorem \\ref{diverg:norm:w} characterizes when the parameter norm provably diverges to infinity, i.e. whenever all tokens are support vectors of \\eqref{eqn:sattnsvm} (Assumption \\ref{assum:token:supp} holds). On the other hand, this is not sufficient for global convergence, as GD can converge in direction to locally-optimal directions per Section \\ref{local GD sec}. Thus, to guarantee global convergence, we need to ensure that \\textbf{globally-optimal direction is the only viable one}. Our next assumption is a fully-geometric condition that precisely accomplishes this.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{assumption}[There is always an optimal \\nei]\\label{assume:all_opt_supp} For any choice of $\\bal=(\\alpha_i)_{i=1}^n$ with $\\bal\\neq \\op$ when solving \\eqref{eqn:sattnsvm} with $\\op\\gets\\bal$, there exists $i\\in[n]$ such that $\\alpha_i\\neq\\op_i$ and $\\op_i\\in \\Tc_i$.\n\n\\end{assumption}\n\n\n\nThis guarantees that no $\\bal\\neq \\opt$ can be locally-optimal because it has a \\nei with higher score at the input $i$ with $\\alpha_i\\neq \\op_i$. Thus, this ensures that global direction $\\Wm$ is the unique locally-optimal direction obeying Def.~\\ref{def loc opt}. Finally, note  that local-optimality in Def.~\\ref{def loc opt} is one-sided: GD can provably converge to locally-optimal directions, while we do not provably exclude the existence of other directions. Yet, Theorem 4 of \\cite{tarzanagh2023margin} shows that local RPs (see Section \\ref{sec:local reg path} for details) can only converge to locally-optimal directions for almost all datasets\\footnote{To be precise, they prove this for their version of Def.~\\ref{def loc opt}, which is stated for an attention model $f(\\pb)=\\vb^\\top\\X^\\top\\sft{\\X\\W\\pb}$ admitting an analogous SVM formulation. }. This and Figure \\ref{fig overparam bar} provide strong evidence that Def.~\\ref{def loc opt} captures all possible convergence directions of GD, and as a consequence, that Assumption \\ref{assume:all_opt_supp} guarantees that $\\Wm$ is the only viable direction to converge.\\smallskip\n\n\n\\noindent\\ding{225} \\textbf{Integrating the results and global convergence conjecture.} Combining Assumptions \\ref{assum:token:supp} and \\ref{assume:all_opt_supp}, we have concluded that gradient norm diverges and $\\Wm$ is the only viable direction to converge. Thus, we conclude this section with the following \\textbf{conjecture}: Suppose $\\op=(\\op_i)_{i=1}^n$ are the unique optimal indices with strictly highest score per sequence and Assumptions \\ref{assum:token:supp} and \\ref{assume:all_opt_supp} hold. Then, for almost all datasets (e.g.~add small IID gaussian noise to input features), GD with a proper constant learning rate converges to $\\Wm$ of \\eqref{eqn:sattnsvm} in direction.\n\n\n\nTo gain some intuition, consider Figure \\ref{fig overparam bar}: Here, red bars denote the frequency of global convergence whereas green bars denote the frequency of Assumption \\ref{assum:token:supp} holding over random problem instances. In short, this suggests that Assumption \\ref{assum:token:supp} occurs less frequently than global convergence, which is consistent with our conjecture. \n\nOn the other hand, verifying Assumption \\ref{assume:all_opt_supp} is more challenging due to its combinatorial nature. A stronger condition that implies Assumption \\ref{assume:all_opt_supp} is when \\emph{all optimal indices $(\\op_i)_{i=1}^n$ are support vectors of the SVM. That is, either $\\op_i=\\alpha_i$ or $\\op_i\\in\\Tc_i$, $\\forall~i\\in[n]$.} When the data follows a statistical model, this stronger condition could be verified through probabilistic tools building on our earlier ``all training points are support vectors'' discussion \\cite{muthukumar2021classification}. More generally, we believe a thorough statistical investigation of \\eqref{eqn:sattnsvm} is a promising direction for future work.\n\n\n==== END OF /2308.16898/sections/overparam.tex ====\n==== BEGINNING OF /2308.16898/sections/prelim.tex ====\n\\section{Preliminaries}\\label{sec:prelim}\n\n\\begin{wrapfigure}{r}{0.31\\linewidth}\\vspace{-0.5cm}\n\t\\centering\n\t\\includegraphics[scale=0.57]{figs/attn_bias_this.pdf}\n \\label{fig:main_bias}\\vspace{-0.8cm}\\caption{Implicit biases of the attention layer and logistic regression.}\\vspace{-0.5cm}\n\\end{wrapfigure}\n\n\\textbf{Unveiling the relationship between attention and linear SVMs.}  For linear classification, it is well-established that GD iterations on logistic loss and separable datasets converge towards the hard margin SVM solution, which effectively separates the two classes within the data \\cite{soudry2018implicit,rosset2003margin,zhang2005boosting}. \nThe softmax nonlinearity employed by the attention layer exhibits an exponentially-tailed behavior similar to the logistic loss; thus, attention may also be biased towards margin-maximizing solutions. However, the attention layer operates on tokens within an input sequence, rather than performing classification directly. Therefore, its bias is towards an SVM, specifically \\eqref{eqn:sattnsvm}, which aims to separate the tokens of input sequences by selecting the relevant ones and suppressing the rest.  Nonetheless, formalizing this intuition is considerably more challenging: The presence of the highly nonlinear and nonconvex softmax operation renders the analysis of standard GD algorithms intricate. Additionally, the 1-layer transformer in \\eqref{eq:erm:init} does not inherently exhibit a singular bias towards a single \\eqref{eqn:sattnsvm} problem, even when using a linear head $h$. Instead, it can result in multiple locally optimal directions induced by their associated SVMs. We emphasize that \\cite{tarzanagh2023margin} is the first work to make this attention$\\leftrightarrow$SVM connection. Here, we augment their framework to transformers by developing the first guarantees for self/cross-attention layer, nonlinear prediction heads, and global convergence.\n\n\n\\smallskip\n\\noindent \\textbf{Notation.} \nFor any integer $N \\geq 1$, let $[N]:=\\{1, \\dots, N\\}$. We use lowercase and uppercase bold letters (e.g., $\\ab$ and $\\A$) to represent vectors and matrices, respectively. The entries of a vector $\\ab$ are denoted as $\\ab_i$. For a matrix $\\A$, $\\tn{\\A}$ denotes the spectral norm, i.e.~maximum singular value, $\\tnuc{\\A}$ denotes the nuclear norm, i.e.~summation of all singular values, and $\\tf{\\A} := \\sqrt{ \\tr(\\A^\\top \\A)}$ denotes the Frobenius norm. $\\dist{\\cdot,\\cdot}$ denotes the Euclidean distance between two sets.\n\nThe minimum / maximum of two numbers $a$, $b$ is denoted as $a\\wedge b$ / $a\\vee b$. The big-O notation $\\mc{O}(\\cdot)$ hides the universal constants. \n\n\n\n\n\n\n\\smallskip\n\\noindent\\textbf{Optimization problem definition.} \nWe use a linear head $h(\\x)=\\vb^\\top\\x$ for most of our theoretical exposition. Given dataset $(Y_i,\\X_i,\\z_i)_{i=1}^n$, we minimize the empirical risk of an 1-layer transformer using combined weights $\\W\\in\\R^{d\\times d}$ or individual weights $\\Kb,\\Qb\\in\\R^{d\\times m}$ for a fixed head and decreasing loss function:\\vspace{-3pt}\n\\begin{align}\\label{eqn:erm:w}\n\\Lc(\\W)&=\\frac{1}{n}\\sum_{i=1}^n \\ell\\left(Y_i\\cdot \\vb^\\top\\X_i^\\top \\sft{\\X_i\\W\\z_{i}}\\right), \\tag{W-ERM}\\\\\n\\Lc(\\Kb,\\Qb)&=\\frac{1}{n}\\sum_{i=1}^n \\ell\\left(Y_i\\cdot \\vb^\\top\\X_i^\\top \\sft{\\X_i\\Kb\\Qb^\\top\\z_{i}}\\right). \\label{eqn:erm:kq} \\tag{KQ-ERM}\n\\end{align}\n\n\n\nWe can recover the self-attention model by setting $\\z_i$ to be the first token of $\\X_i$, i.e.,~$\\z_i\\gets \\x_{i1}$. While the above formulation regresses a single label $Y_i$ per $(\\X_i,\\z_i)$, in Sections \\ref{sec:multi} and  \\ref{sec multioutput}, we show that our findings gracefully extend to the sequence-to-sequence classification setting where we output and classify $T$ tokens per inputs $\\X_i,\\Z_i\\in\\R^{T\\times d}$. See Sections \\ref{sec:multi} and \\ref{sec multioutput} for results on nonlinear prediction heads.\n\n\n\\smallskip\n\\noindent\\textbf{Optimization algorithms.} \nGiven a parameter $R>0$, we consider an $\\ell_2$-norm bound $R$, and define the regularized path solution associated with  Objectives \\eqref{eqn:erm:w} and \\eqref{eqn:erm:kq}, respectively as \\eqref{RP-W} and \\eqref{RP-QK}. These update rules allow us to find the solution within a constrained region defined by the norm bound. The RP illustrates the evolution of $\\Wb_R$ as $R$ increases, capturing the essence of GD where the ridge constraint serves as an approximation for the number of iterations. Previous studies, including \\cite{rosset2003margin,suggala2018connecting,ji2020gradient,tarzanagh2023margin}, have examined the implicit bias of logistic regression and established a connection between the directional convergence of the RP (i.e., $\\lim_{R\\rightarrow \\infty} \\Wb_R/R$) and GD. In \\cite{tarzanagh2023margin}, the concept of a local RP was also employed to investigate implicit bias along local directions. For GD, with appropriate initialization and step size $\\eta>0$, we describe the optimization process associated with \\eqref{eqn:erm:w} and \\eqref{eqn:erm:kq} as \\eqref{GD-W} and \\eqref{GD-QK}, respectively.\n\n\\smallskip\n\\tcbset{colback=white!5!white,colframe=black!5!black,colback=green!1!white}\n\\begin{tcolorbox}[height=2cm, sidebyside,righthand width=7cm]\n\\begin{small}\n\\vspace{-.25cm}\n\\hspace{-.2cm} Given $\\W(0) \\in \\R^{d\\times d}$, $\\eta>0$, for $k\\geq 0$ do:\n\\begin{equation}\\tag{\\small{W-GD}}\n\n\\W(k+1) = \\W(k) -\\eta \\nabla \\Lc(\\W(k)).\n\\label{GD-W}    \n\\end{equation}\n\\end{small}\n\\tcblower\n\\begin{small}\n\\hspace{-.2cm} Given $\\Qb(0), \\Kb(0)  \\in \\R^{d\\times m}$, $\\eta>0$, for $k\\geq 0$ do:\n\\begin{equation}\\label{GD-QK}\n\\hspace{-.1cm}\n\\begin{bmatrix}\n\\Kb(k+1)   \\\\\n \\Qb(k+1) \\\\\n\\end{bmatrix}\n= \\begin{bmatrix}\n\\Kb(k)   \\\\\n \\Qb(k) \\\\\n\\end{bmatrix} \n-\\eta \n\\begin{bmatrix}\n    \\nabla_{\\Kb} \\Lc\\left(\\Kb(k), \\Qb(k)\\right)\\\\\n    \\nabla_{\\Qb} \\Lc\\left(\\Kb(k), \\Qb(k)\\right)\n\\end{bmatrix}.\n\n\n\n\n\\tag{\\small{KQ-GD}}\n\\end{equation}\n\\end{small}\n\\end{tcolorbox}\n\n\n\\begin{tcolorbox}[height=1.8cm, sidebyside,righthand width=7cm]\n\\begin{small}\n\\hspace{-.2cm} Given $R>0$, find $d\\times d$ matrix:\n\\begin{align}\\tag{W-RP}\n\\Wb_R=\\underset{\\tf{\\W}\\leq R}{\\arg\\min}~\\Lc(\\W). \n\\label{RP-W}\n\\end{align}\n\\end{small}\n\\tcblower\n\\begin{small}\n\\hspace{-.2cm} Given $R>0$, find $d\\times m$ matrices:\n\\begin{align}\\tag{KQ-RP}\n(\\Kbb_R,\\Qbb_R)=\\underset{\\tf{\\Kb}^2+\\tf{\\Qb}^2\\leq 2R}{\\arg\\min}\\Lc(\\Kb,\\Qb).\n\\label{RP-QK}\n\\end{align}\n\\end{small}\n\\end{tcolorbox}\n\n\n\n\n\\subsection{Optimal tokens and hard-margin SVM problem for cross attention} \nGiven $\\X_i\\in\\R^{T\\times d},\\z_i\\in\\R^d$,  we present a convex hard-margin SVM problem, denoted as \\eqref{eqn:sattnsvm}, that aims to separate a specific token from the remaining tokens in the input sequence $\\X_i$. This problem is jointly solved for all inputs, allowing us to examine the optimization properties of cross-attention. To delve deeper, we introduce the concept of optimal tokens, which are tokens that minimize the training objective under the decreasing loss function $\\ell(\\cdot)$ as shown in Lemma~\\ref{lem min risk}. This exploration will introduce the notions of token scores and optimality, providing insights into the underlying principles of self-attention mechanisms \\cite{tarzanagh2023margin}.\n\n\\begin{definition}[Token Score and Optimality]\\label{score def}\nGiven a prediction head $\\vb\\in\\R^d$, the score of a token $\\x_{it}$ of input $\\X_i$ is defined as $\\bgam_{it} = Y_i \\cdot \\vb^\\top\\x_{it}$. The optimal token for each input $\\X_i$ is given by the index $\\op_i \\in \\arg\\max_{t \\in [T]} \\bgam_{it}$ for all $i \\in [n]$.\n\\end{definition}\nBy introducing token scores and identifying optimal tokens, we can better understand the importance of individual tokens and their impact on the overall objective. The token score quantifies the contribution of a token to the prediction or classification task, while the optimal token represents the token that exhibits the highest relevance within the corresponding input sequence. \n\n\\smallskip\n\\noindent$\\bullet$ \\textbf{Hard-margin SVM for $\\W$-parameterization.} Equipped with the set of optimal indices  $\\opt:=(\\opt_i)_{i=1}^n$ as per Definition~\\ref{score def}, we introduce the following SVM formulation associated to \\eqref{eqn:erm:w}:\n\n\\begin{tcolorbox}[colback=white!5!white,colframe=black!5!black,colback=green!1!white]\n\\vspace{5pt}\n\\begin{equation}\\tag{\\name}\n\\Wm=\\arg\\min_{\\W}\\tf{\\W}\n\\quad \\text{subj. to} \\quad (\\x_{i\\op_i}-\\x_\\itt)^\\top\\W\\z_i\\geq 1\\quad  \\text{for all} \\quad t \\neq \\op_i, \\quad  i\\in[n]\n\\label{eqn:sattnsvm}.\n\\end{equation}\n\\end{tcolorbox}\n\nThe existence of matrix $\\Wm$ implies the separability of tokens $(\\opt_i)_{i=1}^n$ from the others. The terms $a_{\\itt}:=\\x_\\itt^\\top\\W\\z_i$ represent the dot product between the key-query features before applying the softmax nonlinearity. This dot product is a crucial characteristic of self-attention and we can express the SVM constraints in \\eqref{eqn:sattnsvm} as $a_{i\\op_i}\\geq a_\\itt+1$. Thus,  \\eqref{eqn:sattnsvm} finds the most efficient direction that ensures the optimal token $\\x_{i\\op_i}$ achieves the highest similarity with query $\\z_i$ among all key embeddings. Our first result shows that \\eqref{eqn:sattnsvm} is feasible under mild over-parameterization. \n\n\\begin{theorem}\\label{thm:separation} Suppose $d\\geq \\max(T-1,n)$. Then, almost all datasets $(Y_i,\\X_i,\\z_i)_{i=1}^n$ -- including the self-attention setting with $\\z_i\\gets\\x_{i1}$ -- obey the following: \n\\eqref{eqn:sattnsvm} is feasible i.e.,~$\\Wm$ separates the desired tokens $\\opt=(\\opt_i)_{i=1}^n$.\n\\end{theorem}\n\n\nWe note that the convex formulation \\eqref{eqn:sattnsvm} does not  fully capture the GD geometry on \\eqref{eqn:erm:w}. In a more general sense, GD can provably converge to an SVM solution over locally-optimal tokens, as detailed in Section~\\ref{local GD sec}. For deeper insight into \\eqref{eqn:sattnsvm}, consider that the attention layer's output is a convex mixture of input tokens. Thus, if minimizing the training loss involves choosing the optimal token $\\x_{i\\op_i}$, the softmax similarities should eventually converge to a one-hot vector, precisely including $\\x_{i\\op_i}$ (assigned 1), while ignoring all other tokens (assigned 0s). This convergence requires the attention weights $\\W$ to diverge in norm to saturate the softmax probabilities.  Due to the exponential-tailed nature of the softmax function, the weights converge directionally to the max-margin solution. This phenomenon resembles the implicit bias of logistic regression on separable data \\cite{soudry2018implicit,ji2018risk}. Lemma \\ref{lem min risk} formalizes this intuition and rigorously motivates optimal tokens.\n\n\\smallskip\n\\noindent$\\bullet$ \\textbf{Non-convex SVM for $(\\Kb,\\Qb)$-parameterization.} The objective function \\eqref{eqn:erm:kq} has an extra layer of nonconvexity compared to \\eqref{eqn:erm:w} as $(\\Kb,\\Qb)$ corresponds to a matrix factorization of $\\W$. To study this, we introduce the following nonconvex SVM problem over $(\\Kb,\\Qb)$ akin to \\eqref{eqn:sattnsvm}:\n\n\\smallskip\n\\begin{tcolorbox}\n\\vspace{-7pt}\n\\begin{equation}\\tag{KQ-SVM}\n\\min_{\\Kb,\\Qb}~\\frac{1}{2} \\left(\\|\\Kb\\|_F^2+\\|\\Qb\\|_F^2\\right)\n\\quad \\text{subj. to} \\quad (\\x_{i\\op_i}-\\x_\\itt)^\\top\\Kb\\Qb^\\top\\z_i\\geq 1\\quad \\text{for all} \\quad t \\neq \\op_i, \\quad  i\\in[n]\n\\label{eqn:qk:svm}.\n\\end{equation}\n\\end{tcolorbox}\nEven if the direction of GD is biased towards the SVM solution, it does not have to converge to the global minima of \\eqref{eqn:qk:svm}. Instead, it can  converge towards a KarushKuhnTucker (KKT) point of the max-margin SVM. Such KKT convergence has been studied by \\cite{lyu2019gradient,ji2020directional} in the context of other nonconvex margin maximization problems. Fortunately, \\eqref{eqn:erm:kq} may not be as daunting as it may initially seem: Our experiments in Figures~\\ref{fig path} and \\ref{fig general} reveal that GD is indeed biased towards the global minima of \\eqref{eqn:qk:svm}. This global minima is achieved by setting $\\W:=\\Kb\\Qb^\\top$ and finding the factorization of $\\W$ that minimizes the quadratic objective, yielding the following $\\W$-parameterized SVM with nuclear norm objective:\n\n\\smallskip\n\\begin{tcolorbox}[colback=white!5!white,colframe=black!5!black,colback=green!1!white]\n\\vspace{5pt}\n\\begin{equation}\\tag{Att-SVM$_\\star$}\n\\Wm_\\st\\in\\underset{\\texttt{rank}(\\W)\\leq m}{\\arg\\min}\\tnuc{\\W}\n\\quad \\text{subj. to} \\quad (\\x_{i\\op_i}-\\x_\\itt)^\\top\\W\\z_i\\geq 1\\quad   \\text{for all} \\quad t \\neq \\op_i, \\quad  i\\in[n]\n\\label{eqn:sattnsvmst}.\n\\end{equation}\n\\vspace{-10pt}\n\\end{tcolorbox}\n\n\nAbove, the nonconvex rank constraint arises from the fact that the rank of $\\W = \\Kb\\Qb^\\top$ is at most $m$. However, under the condition of the full parameterization where $m \\geq d$, the rank constraint disappears, leading to a convex nuclear norm minimization problem. Besides, the nuclear norm objective inherently encourages a low-rank solution \\cite{recht2010guaranteed,fazel2002matrix,srebro2004maximum}. \nLemma \\ref{lem:rank}, presented below, demonstrates that this guarantee holds whenever $n \\leq m$. This observation is further supported by our experiments (see Fig.~\\ref{fig rank}). Thus, it offers a straightforward rationale for why setting $m < d$ is a reasonable practice, particularly in scenarios involving limited data.\n\n\n\\begin{lemma}\\label{lem:rank} Any optimal solution of \\eqref{eqn:sattnsvm} or \\eqref{eqn:sattnsvmst} is at most rank $n$. More precisely, the  row space of $\\Ws$ or $\\Ws_\\st$ lies within $\\texttt{span}(\\{\\z_i\\}_{i=1}^n)$.\n\\end{lemma}\n\n\nFigure~\\ref{fig rank} illustrates rank range of solutions for  \\eqref{eqn:sattnsvm} and \\eqref{eqn:sattnsvmst}, denoted as $\\Ws$ and $\\Ws_{\\star}$, solved using optimal tokens $(\\opt_i)_{i=1}^n$ and setting  $m=d$ (the rank constraint is eliminated). Each result is averaged over 100 trials, and for each trial, ${\\x}_{it}$, ${\\z}_i$, and linear head ${\\vb}$ are randomly sampled from the unit sphere. In Fig.~\\ref{fig rank svm n}, we fix $T=5$ and vary $n$ across $\\{5,10,15\\}$. Conversely, in Fig.~\\ref{fig rank svm T}, we keep $n=5$ constant and alter $T$ across $\\{5,10,15\\}$.  Both figures confirm rank of $\\Ws$ and $\\Ws_\\star$ are bounded by $\\max(n,d)$, validating Lemma~\\ref{lem:rank}.\n\n\n==== END OF /2308.16898/sections/prelim.tex ====\n==== BEGINNING OF /2308.16898/sections/local-reg-path.tex ====\n\\subsection{Guarantees on local regularization path}\\label{sec:local reg path}\n\nIn this section, we provide a \\emph{localized} regularization path analysis for general objective functions. As we shall see, this will also allow us to predict the local solutions of gradient descent described in Section \\ref{local GD sec}. Let $\\dm$ denote a general norm objective. Given indices $\\bal=(\\alpha_i)_{i=1}^{n}$, consider the formulation\n\n\\begin{equation}\\tag{$\\dm$-SVM}\n \\Wm_{\\dm, \\alpha}=\\underset{\\texttt{rank}(W)\\leq m}{\\arg\\min}\\|\\W\\|_{\\diamond}\n\\quad \\text{subj. to} \\quad (\\x_{i\\alpha_{i}}-\\x_\\itt)^\\top\\W\\z_i\\geq 1\\quad   \\text{for all} \\quad t \\neq \\alpha_{i}, \\quad \n i\\in[n]\\label{dmattnsvm}.\n\\end{equation}\n\n\nIn this section, since $\\dm$ is clear from the context, we will use the shorthand $\\Wm_{\\alpha} := \\Wm_{\\dm, \\alpha}$ and denote the optimal solution set of \\eqref{dmattnsvm} as $\\Wcs := \\Wcs_{\\dm, \\alpha}$. It is important to note that if the $\\dm$-norm is not strongly convex, $\\Wcs$ may not be a singleton. Additionally, when $m=d$, the rank constraint becomes vacuous, and the problem becomes convex. The following result is a slight generalization of Theorem \\ref{thm:separation} and demonstrates that choosing a large $d$ ensures the feasibility of \\eqref{dmattnsvm} uniformly over all choices of $\\bal$. The proof is similar to that of Theorem~\\ref{thm:separation}, as provided in Appendix~\\ref{app sep}.\n\n\\begin{theorem}\\label{separation thm} Suppose $d\\geq \\max(T-1,n)$ and $m=d$. Then, almost all datasets\\footnote{Here, \\emph{``almost all datasets''} means that adding i.i.d.~gaussian noise, with arbitrary nonzero variance, to the input features will almost surely result in SVM's feasibility.} $(Y_i,\\X_i,\\z_i)_{i=1}^n$ -- including the self-attention setting with $\\z_i\\gets\\x_{i1}$ -- obey the following: For any choice of indices $\\bal=(\\alpha_i)_{i=1}^n\\subset[T]$, \\eqref{dmattnsvm} is feasible,  i.e.~the attention layer can separate and select indices $\\bal$.\n\\end{theorem}\n\n\n\n\nTo proceed, we define the \\emph{local regularization path}, which is obtained by solving the $\\dm$-norm-constrained problem over a $\\bal$-dependent cone denoted as $\\con{\\bal}$. This cone has a simple interpretation: it prioritizes tokens with a lower score than $\\bal$ over tokens with a higher score than $\\bal$. This interpretation sheds light on the convergence towards locally optimal directions: lower-score tokens create a barrier for $\\bal$ and prevent optimization from moving towards higher-score tokens.\n\n\\begin{definition}[Low\\&High Score Tokens and Separating Cone]\\label{HL cone def main} Given $\\al\\in[T]$, input sequence $\\X$ with label $Y$, $h(\\cdot):\\R^d\\rightarrow\\R$, and score $\\bgam_t=Y\\cdot h(\\x_t)$ for all $t\\in[T]$, define the low and high score tokens as\n\\[\n\\low:=\\left\\{t\\in[T]\\bgl \\bgam_t<\\bgam_\\al\\right\\},\\quad \\high:=\\left\\{t\\in[T]-\\{\\alpha\\}\\bgl \\bgam_t\\geq \\bgam_\\al\\right\\}.\n\\]\nFor input $\\X_i$ and index $\\alpha_i$, we use the shorthand notations $\\texttt{low}^\\alpha_i$ and $\\texttt{high}^\\alpha_i$. Finally define $\\con{\\bal}$ as\n\\begin{align}\n\\con{\\bal}:=\\left\\{\\texttt{rank}(W)\\leq m\\bgl \\min_{i\\in[n]}\\max_{t\\in\\texttt{low}^\\alpha_i}\\min_{\\tau\\in\\texttt{high}^\\alpha_i} (\\x_\\itt-\\x_\\ittt)^\\top\\W\\z_i\\geq \\eps\\tf{\\W}\\right\\}.\\label{cone alpha eq1}\n\\end{align}\n\\end{definition}\n\n\nOur next lemma relates this cone definition to locally-optimal directions of Definition \\ref{def loc opt}.\n\\begin{lemma} \\label{lemma cone main}Suppose \\eqref{dmattnsvm} is feasible. If indices $\\bal$ are locally-optimal, $\\Wma\\in \\con{\\bal}$ for all sufficiently small $\\eps>0$. Otherwise, $\\Wma\\not\\in \\con{\\bal}$ for all $\\eps>0$. Additionally, suppose optimal indices $\\op_i\\in\\arg\\max_{t\\in[T]}\\bgam_\\itt$ are unique and set $\\bal\\gets\\op$. Then, $\\con{\\opt}$ is the set of all rank-$\\leq$$m$ matrices (i.e.~global set).\n\\end{lemma}\n\nLemma~\\ref{lemma cone main} can be understood as follows: Among the SVM solutions $\\Wma$, only those that are locally optimal demonstrate a barrier of low-score tokens, effectively acting as a protective shield against higher-score tokens. Moreover, in the case of globally optimal tokens (with the highest scores), the global set $\\con{\\opt}$ can be chosen, as they inherently do not require protective measures. The subsequent result introduces our principal theorem, which pertains to the regularization path converging towards the locally-optimal direction over $\\con{\\bal}$ whenever $\\bal$ is locally optimal.\n\n\\begin{theorem} [Convergence of Local Regularization Path]\\label{local RP thm1} Suppose Assumption \\ref{assum:loss:prope} holds. Fix locally-optimal token indices $\\bal=(\\al_i)_{i=1}^n$ and $R_0,\\eps>0$. Consider the norm-constrained variation of \\eqref{cone alpha eq1} defined as \n\\[\n\\Ccd:=\\con{\\bal}\\bigcap \\left\\{\\W\\bgl \\td{\\W}\\geq R_0\\right\\}.\n\\]\nDefine local RP as $\\Wb_R=\\min_{\\Ccd,\\td{\\W}\\leq R}\\Lc(\\W)$ where $\\Lc(\\W)$ is given by \\eqref{eqn:erm:w}. Let $\\Wcs$ be the set of minima for \\eqref{dmattnsvm} and $\\xdm>0$ be the associated margin i.e.~$\\xdm=1/\\td{\\Wma}$. For any sufficiently small $\\eps>0$ and sufficiently large $R_0= \\order{1/\\eps}>0$, $\\lim_{R\\rightarrow\\infty} \\dist{\\frac{\\Wb_R}{R\\xdm},\\Wcs}=0$. Additionally, suppose optimal indices $\\op=(\\op_i)_{i=1}^n$ are unique and set $\\bal\\gets\\op$. Then, the same convergence guarantee on regularization path holds by setting $\\Ccd$ as the set of rank-$\\leq$$m$ matrices.\n\\end{theorem}\n\nNote that when setting $m=d$, the rank constraint is eliminated. Consequently, specializing this theorem to the Frobenius norm aligns it with Theorem \\ref{thm:local:gd}. On the other hand, by assigning $\\dm$ as the nuclear norm and $\\bal\\gets\\op$, the global inductive bias of the nuclear norm is recovered, as stated in Theorem \\ref{thm global reg path}.\n\nWe would like to emphasize that both this theorem and Theorem \\ref{thm global reg path} are specific instances of Theorem \\ref{local RP thm} found in Appendix \\ref{sec multioutput}. It is worth noting that, within this appendix, we establish all regularization path results for sequence-to-sequence classification, along with a general class of \\emph{monotonicity-preserving} prediction heads outlined in Assumption \\ref{ass cvx seq}. The latter significantly generalizes linear heads, highlighting the versatility of our theory. The following section presents our discoveries concerning general nonlinear heads.\n==== END OF /2308.16898/sections/local-reg-path.tex ====\n==== BEGINNING OF /2308.16898/sections/multitoken.tex ====\n\n\n\\section{Toward A More General SVM Equivalence for Nonlinear Prediction Heads}\\label{sec:multi}\n\n\nSo far, our theory has focused on the setting where the attention layer selects a single optimal token within each sequence. As we have discussed, this is theoretically well-justified under linear head assumption and certain nonlinear generalizations. On the other hand, for arbitrary nonconvex $h(\\cdot)$ or multilayer transformer architectures, it is expected that attention will select multiple tokens per sequence. This motivates us to ask:\n\\begin{quote}\n    \\textbf{Q:}~What is the implicit bias and the form of $\\W(k)$ when the GD solution is composed by multiple tokens?\n\\end{quote}\n\nIn this section, our goal is to derive and verify the generalized behavior of GD. Let $\\xat_i=\\X_i^\\top \\s^{\\W}_i$ denote the composed token generated by the attention layer where $\\s^{\\W}_i=\\sft{\\X_i\\W\\z_i}$ are the softmax probabilities corresponding to $\\W$. Suppose GD trajectory converges to achieve the risk $\\Lc_\\star=\\min_{\\W}\\Lc(\\W)$, and the eventual token composition achieving $\\Lc_\\star$ is given by \n\\[\n\\xast_i=\\X_i^\\top \\s^\\st_i,\n\\]\nwhere $\\s^\\st_i$ are the eventual softmax probability vectors that dictate the token composition. Since attention maps are sparse in practice, we are interested in the scenario where $\\s^\\st_i$ is sparse i.e.~it contains some zero entries. This can only be accomplished by letting $\\tf{\\W}\\rightarrow\\infty$. However, unlike the earlier sections, we wish to allow for arbitrary $\\s^\\st_i$ rather than a one-hot vector which selects a single token. \n\nTo proceed, we aim to understand the form of GD solution $\\W(k)$ responsible for composing $\\xast_i$ via the softmax map $\\s^\\st_i$ as $\\tf{\\W}\\rightarrow\\infty$. Intuitively, $\\W(k)$ should be decomposed into two components via\n\\begin{align}\n\\W(k)\\approx \\Wf+\\tf{\\W(k)}\\cdot \\Wsb,\\label{multi-token soln}\n\\end{align}\nwhere $\\Wf$ is the {finite component} and $\\Wsb$ is the {directional component} with $\\tf{\\Wsb}=1$. Define the {selected set} $\\Rc_i\\subseteq[T]$ to be the indices $\\s^\\st_\\itt\\neq 0$ and the {masked (i.e.~suppressed) set} as $\\Rcb_i=[T]-\\Rc_i$ where softmax entries are zero. In the context of earlier sections, we could also call these the \\emph{optimal set} and the \\emph{non-optimal set}, respectively.\n\\begin{itemize}[label=$\\bullet$, wide, labelwidth=!,itemindent=!, labelindent=5pt]\n\\item \\textbf{Finite component:} The job of $\\Wf$ is to assign nonzero softmax probabilities within each $\\s^\\st_i$. This is accomplished by ensuring that, $\\Wf$ induces the probabilities of $\\s^\\st_i$ over $\\Rc_i$ by satisfying the softmax equations\n\\[\n\\frac{e^{\\x_\\itt^\\top \\Wf\\z_i}}{e^{\\x_\\ittt^\\top \\Wf\\z_i}}=e^{(\\x_\\itt-\\x_\\ittt)^\\top \\Wf\\z_i}=\\s^\\st_\\itt/\\s^\\st_\\ittt,\n\\]\nfor $t,\\tau\\in\\Rc_i$. Consequently, this $\\Wf$ should satisfy the following linear constraints\n\\begin{equation}\n(\\x_\\itt-\\x_\\ittt)^\\top \\Wf\\z_i=\\log(\\s^\\st_\\itt/\\s^\\st_\\ittt)\\quad\\text{for all}\\quad t,\\tau\\in\\Rc_i,~i\\in[n].\\label{smax eqn}\n\\end{equation}\n\n\n\n\n\n\n\n\n\\item \\textbf{Directional component:} While $\\Wf$ creates the composition by allocating the nonzero softmax probabilities, it does not explain sparsity of attention map. This is the role of $\\Wsb$, which is responsible for selecting the selected tokens $\\Rc_i$ and suppressing the masked ones $\\Rcb_i$ by assigning zero softmax probability to them. To predict direction component, we build on the theory developed in earlier sections. Concretely, there are two constraints $\\Wsb$ should satisfy\n\\begin{enumerate}\n\\item \\textbf{Equal similarity over selected tokens:} For all $t,\\tau\\in\\Rc_i$, we have that $(\\x_\\itt-\\x_\\ittt)^\\top \\W\\z_i=0$. This way, softmax scores assigned by $\\Wf$ are not disturbed by the directional component and $\\Wf+R\\cdot\\Wsb$ will still satisfy the softmax equations \\eqref{smax eqn}.\n\\item \\textbf{Max-margin against masked tokens:} For all $t\\in\\Rc_i,\\tau\\in\\Rcb_i$, enforce the margin constraint $(\\x_\\itt-\\x_\\ittt)^\\top \\W\\z_i\\geq 1$ subject to minimum norm $\\tf{\\W}$. \n\\end{enumerate}\nCombining these yields the following convex generalized SVM formulation\n \\begin{tcolorbox}[colback=white!5!white,colframe=black!5!black,colback=green!1!white]\n \\vspace{-7pt}\n\\begin{align}\\tag{Gen-SVM}\n\\Wm=\\arg\\min_{\\W}\\tf{\\W}\n\\quad \\text{subj. to} \\quad\\begin{cases} \\forall~t\\in\\Rc_i,\\tau\\in\\Rcb_i:~(\\x_\\itt-\\x_\\ittt)^\\top\\W\\z_i\\geq 1,\\\\\n\\forall~t,\\tau\\in\\Rc_i:~\\quad\\quad(\\x_\\itt-\\x_\\ittt)^\\top\\W\\z_i=0,\\end{cases}\\quad  \\forall  1\\leq i\\leq n.\n\\label{eqn:mattnsvm}\n\\end{align}\n\\end{tcolorbox}\n\\noindent and set the normalized direction in \\eqref{multi-token soln} to $\\Wsb=\\Ws/\\tf{\\Ws}$.\n\\end{itemize}\nIt is important to note that  \\eqref{eqn:mattnsvm} offers a substantial generalization beyond the scope of the previous sections, where the focus was on selecting a single token from each sequence, as described in the main formulation \\eqref{eqn:sattnsvm}. This broader solution class introduces a more flexible approach to the problem.\n\n\n\\input{sections/fig_nn_diff_d}\n\n\nWe present experiments showcasing the predictive power of the \\eqref{eqn:mattnsvm} equivalence in nonlinear scenarios. We conducted these experiments on random instances using an MLP denoted as $h(\\cdot)$, which takes the form of $\\onebb^\\top\\texttt{ReLU}(\\x)$. We begin by detailing the preprocessing step and our setup. For the attention SVM equivalence analytical prediction, clear definitions of the selected and masked sets are crucial. These sets include token indices with nonzero and zero softmax outputs, respectively. However, practically, reaching a precisely zero output is not feasible. Hence, we define the selected set as tokens with softmax outputs exceeding $10^{-3}$, and the masked set as tokens with softmax outputs below $10^{-6}$. We also excluded instances with softmax outputs falling between $10^{-6}$ and $10^{-3}$ to distinctly separate the concepts of \\emph{selected} and \\emph{masked} sets, thereby enhancing the predictive accuracy of the attention SVM equivalence. In addition to the filtering process, we focus on scenarios where the label $Y=-1$ exists to enforce \\emph{non-convexity} of prediction head $Y_i\\cdot h(\\cdot)$. It is worth mentioning that when all labels are $1$, due to the convexity of $Y_i\\cdot h(\\cdot)$, GD tends to select one token per input, and Equations \\eqref{eqn:mattnsvm} and \\eqref{eqn:sattnsvm} yield the same solutions.  The results are displayed in Figure~\\ref{fig nn diff d}, where $n=3$, $T=4$, and $d$ varies within ${4, 6, 8, 10}$. We conduct 500 random trials for different choices of $d$, each involving ${\\x}_{it}$, ${\\z}_i$, and ${\\vb}$ randomly sampled from the unit sphere. We apply normalized GD with a step size $\\eta=0.1$ and run $2000$ iterations for each trial.\n\n\\begin{enumerate}[label=$\\bullet$, wide, labelwidth=!,itemindent=!, labelindent=5pt]\n\\item Figure \\ref{fig nn diff d} (upper) illustrates the correlation evolution between the GD solution and three distinctive baselines: ({\\color{black}{\\textbf{$\\cdots$}}}) $\\Ws$ obtained from \\eqref{eqn:mattnsvm};  ({\\color{black}{\\textbf{---}}}) $\\W^\\rfn$ obtained by calculating $\\Wf$ and determining the best linear combination $\\Wf+\\gamma \\Wsb$ that maximizes correlation with the GD solution; and  ({\\color{black}{\\textbf{-~-}}}) $\\W^\\ont$ obtained by solving \\eqref{eqn:sattnsvm} and selecting the highest probability token from the GD solution.  For clearer visualization, the logarithmic scale of correlation misalignment is presented in Figure~\\ref{fig nn diff d}. In essence, our findings show that $\\W^\\ont$ yields unsatisfactory outcomes, whereas $\\Ws$ attains a significant correlation coefficient in alignment with our expectations. Ultimately, our comprehensive SVM-equivalence $\\W^\\rfn$ further enhances correlation, lending support to our analytical formulas. It's noteworthy that SVM-equivalence displays higher predictability in a larger $d$ regime (with an average correlation exceeding $0.99$). This phenomenon might be attributed to more frequent directional convergence in higher dimensions, with overparameterization contributing to a smoother loss landscape, thereby expediting optimization. \n\\item Figure \\ref{fig nn diff d} (lower) offers a scatterplot overview of the $500$ random problem instances that were solved. The $x$-axis represents the largest softmax probability over the masked set, denoted as $\\max_{i,\\tau}s_{i\\tau}$ where $\\tau\\in\\Rcb_i$. Meanwhile, the $y$-axis indicates the predictivity of the SVM-equivalence, quantified as $1-\\texttt{corr\\_coef}(\\W,\\W^\\rfn)$. From this analysis, two significant observations arise. Primarily, there exists an inverse correlation between softmax probability and SVM-predictivity. This correlation is intuitive, as higher softmax probabilities signify a stronger divergence from our desired \\emph{masked set} state (ideally set to $0$). Secondly, as dimensionality ($d$) increases, softmax probabilities over the masked set tend to converge towards the range of $10^{-15}$ (effectively zero). Simultaneously, attention SVM-predictivity improves, creating a noteworthy correlation.\n\\end{enumerate}\n\n\n\n\n\\subsection{When does attention select multiple tokens?}\\label{sec when}\nIn this section, we provide a concrete example where the optimal solution indeed requires combining multiple tokens in a nontrivial fashion. Here, by nontrivial we mean that, we select more than 1 tokens from an input sequence but we don't select all of its tokens. Recall that, for linear prediction head, attention will ideally select the single token with largest score for almost all datasets. Perhaps not surprisingly, this behavior will not persist for nonlinear prediction heads. For instance in Figure~\\ref{fig nn diff d}, the GD output $\\W$ aligned better in direction with $\\Ws$ than $\\W^\\ont$. Specifically, here we prove that if we make the function $h_Y(\\x):=Y\\cdot h(\\x)$ concave, then optimal softmax map can select multiple tokens in a controllable fashion. $h_Y(\\x)$ can be viewed as generalization of the linear score function $Y\\cdot \\vb^\\top\\x$. In the example below, we induce concavity by incorporating a small $-\\la\\tn{\\x}^2$ term within a linear prediction head and setting $h(\\x)=\\vb^\\top\\x-\\la\\tn{\\x}^2$ with $Y=1$.\n\n\\begin{lemma}\\label{example dataset} Given $\\vb\\in\\R^d$, recall the score vector $\\bgam=\\X\\vb$. Without losing generality, assume $\\bgam$ is non-increasing. Define the vector of score gaps $\\bbg\\in\\R^{T-1}$ with entries $\\bbg_t=\\bgam_{t}-\\bgam_{t+1}$. Suppose all tokens within the input sequence are orthonormal and for some $\\tau\\geq 2$, we have that \n\\begin{align}\n\\tau\\bbg_\\tau/2>\\bbg_1.\\label{tau description}\n\\end{align}\nSet $h(\\x)=\\vb^\\top\\x-\\la\\tn{\\x}^2$ where $\\tau\\bbg_\\tau/2>\\la>\\bbg_1$, $\\ell(x)=-x$, and $Y=1$. Let $\\Bal_T$ denote the $T$-dimensional simplex. Define the unconstrained softmax optimization associated to the objective $h$ where we make $\\s:=\\sft{\\X\\W\\z}$ a free variable, namely,\n\\begin{align} \n\\min_{\\s\\in\\Bal_T}\\ell(h(\\X\\s))=\\min_{\\s\\in\\Bal_T}\\la \\tn{\\X^\\top \\s}^2-\\vb^\\top\\X^\\top \\s.\\label{direct opt}\n\\end{align}\nThen, the optimal solution $\\s^\\st$ contains at least $2$ and at most $\\tau$ nonzero entries.\n\\end{lemma}\n\n\\input{sections/fig_multi_corrs}\n\nFigure \\ref{fig multi corrs} presents experimental findings concerning Lemma \\ref{example dataset} across random problem instances. For this experiment, we set $n=1$, $T=10$, and $d=10$. The results are averaged over $100$ random trials, with each trial involving the generation of randomly orthonormal vectors $\\x_{1t}$ and the random sampling of vector $\\vb$ from the unit sphere. Similar to the processing step in Figure~\\ref{fig nn diff d}, and following Figure~\\ref{fig nn diff d} (lower) which illustrates that smaller softmax outputs over masked sets correspond to higher correlation coefficients, we define the selected and masked token sets. Specifically, tokens with softmax outputs $>10^{-3}$ are considered selected, while tokens with softmax outputs $<10^{-8}$ are masked. Instances with softmax outputs between $10^{-8}$ and $10^{-3}$ are filtered out.\n\nFigure \\ref{fig multi ns diff lambda} shows that the number of selected tokens grows alongside $\\lambda$, a prediction consistent with Lemma \\ref{example dataset}. When $\\lambda=0$, the head $h(\\x)=\\vb^\\top\\x$ is linear, resulting in the selection of only one token per input. Conversely, as $\\lambda$ exceeds a certain threshold (e.g., $\\lambda>2.0$ based on our criteria), the optimization consistently selects all tokens. Figure \\ref{fig multi corr diff lambda} and \\ref{fig multi corr diff ns} delve into the predictivity of attention SVM solutions for varying $\\lambda$ and different numbers of selected tokens. The dotted curves in both figures represent $1-\\corr{\\W,\\Ws}$, while solid curves indicate $1-\\corr{\\W,\\W^{\\rfn}}$, where $\\W$ denotes the GD solution. Overall, the SVM-equivalence demonstrates a strong correlation with the GD solution (consistently above $0.95$). However, selecting more tokens (aligned with larger $\\lambda$ values) leads to reduced predictivity.\n\nTo sum up, we have showcased the predictive capacity of the generalized SVM equivalence regarding the inductive bias of 1-layer transformers with nonlinear heads. Nevertheless, it's important to acknowledge that this section represents an initial approach to a complex problem, with certain caveats requiring further investigation (e.g., the use of filtering in Figures \\ref{fig nn diff d} and \\ref{fig multi corrs}, and the presence of imperfect correlations). We aspire to conduct a more comprehensive investigation, both theoretically and empirically, in forthcoming work.\n\n\n==== END OF /2308.16898/sections/multitoken.tex ====\n==== BEGINNING OF /2308.16898/sections/fig_nn_corrs.tex ====\n\\begin{figure}\n    \\centering\n    \\hspace{-10pt}\n    \\subfigure[$\\W^{\\text{adapt}}$]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.22\\columnwidth, trim={1.3cm 1.3cm 0 0}, clip]{figs/nn_corr_itr_adapt.pdf}};\n        \\node at (0,-2.) {\\small{Iterations}};\n        \\node[rotate=90] at (-2.65,0) {\\small{$1-$correlation coefficient}};\n        \\end{tikzpicture}\n        \\label{fig nn corr adapt}\n    }\n    \\hspace{-10pt}\n    \\subfigure[$\\Ws$]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.22\\columnwidth, trim={1.3cm 1.3cm 0 0}, clip]{figs/nn_corr_itr_svm.pdf}};\n        \n        \\node at (0,-2.){\\small{Iterations}};\n        \\end{tikzpicture}\n        \\label{fig nn corr svm}\n    }\n    \\hspace{-10pt}\n    \\subfigure[$\\W^\\dagger$]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.22\\columnwidth, trim={1.3cm 1.3cm 0 0}, clip]{figs/nn_corr_itr_single.pdf}};\n        \\node at (0,-2.) {\\small{Iterations}};\n        \n        \\end{tikzpicture}\n        \\label{fig nn corr single}\n    }\n    \\caption{} \n    \\label{fig nn corrs}\n\\end{figure}\n==== END OF /2308.16898/sections/fig_nn_corrs.tex ====\n==== BEGINNING OF /2308.16898/sections/fig_multi_diff_tau.tex ====\n\\begin{figure}[t]\n    \\centering\n    \\hspace{-10pt}\n    \\subfigure[$\\tau$ and $\\lambda$ parameters relationship]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.22\\columnwidth, trim={1cm 1.3cm 0 0}, clip]{figs/multi_lambda_tau.pdf}};\n        \\node at (0,-2.) {\\small{$\\tau$}};\n        \\node[rotate=90] at (-2.65,0) {\\small{$\\lambda$}};\n        \\end{tikzpicture}\n        \\label{fig tau lambda}\n    }\n    \\hspace{-10pt}\n    \\subfigure[ $\\tau$ and  \\# of selected tokens relationship]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.22\\columnwidth, trim={1cm 1.3cm 0 0}, clip]{figs/multi_n_selected_tau.pdf}};\n        \\node at (0,-2.) {\\small{$\\tau$}};\n        \\node[rotate=90] at (-2.7,0) {\\small{\\# of selected tokens}};\n        \\end{tikzpicture}\n        \\label{fig tau ns}\n    }\n    \\hspace{-10pt}\n    \\subfigure[Distribution of \\# selected tokens over varying $\\tau$]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.22\\columnwidth, trim={1.3cm 1.3cm 0 0}, clip]{figs/multi_prob_tau.pdf}};\n        \\node[rotate=90] at (-2.7,0) {\\small{Probabilities}};\n        \\node at (0,-2.){\\small{\\# of selected tokens}};\n        \\end{tikzpicture}\n        \\label{fig tau prob}\n    }\n    \\caption{ Behavior of GD when selecting multiple tokens.} \n    \\label{fig multi tau}\n\\end{figure}\n==== END OF /2308.16898/sections/fig_multi_diff_tau.tex ====\n==== BEGINNING OF /2308.16898/sections/introduction.tex ====\n\\section{Introduction}\nSelf-attention, the central component of the transformer architecture, has revolutionized natural language processing (NLP) by empowering the model to identify complex dependencies within input sequences \\cite{vaswani2017attention}. By assessing the relevance of each token to every other token, self-attention assigns varying degrees of importance to different parts of the input sequence. This mechanism has proven highly effective in capturing long-range dependencies, which is essential for applications arising in NLP ~\\cite{kenton2019bert,brown2020language,raffel2020exploring}, computer vision~\\cite{fan2021multiscale,liu2021swin,touvron2021training,chen2023jigsaw}, and reinforcement learning~\\cite{janner2021offline,chen2021decision,wu2022flowformer}.  \nRemarkable success of the self-attention mechanism and transformers has paved the way for the development of sophisticated language models such as GPT4 \\cite{gpt4},\nBard \\cite{bard}, LLaMA \\cite{touvron2023llama}, and  ChatGPT \\cite{openai_chatgpt}. \n\\begin{quote}\n\\textbf{Q:}~Can we characterize the optimization landscape and implicit bias of transformers? \nHow does the attention layer select and compose tokens when trained with gradient descent?\n\\end{quote}\n\nWe address these questions by rigorously connecting the optimization geometry of the attention layer and a hard max-margin SVM problem, namely \\eqref{eqn:sattnsvm}, that separates and selects the optimal tokens from each input sequence. This formalism, which builds on the recent work \\cite{tarzanagh2023margin}, is practically meaningful as demonstrated through experiments, and sheds light on the intricacies of self-attention. \n\nThroughout, given  input sequences $\\X,\\Z\\in\\R^{T\\times d}$ with length $T$ and embedding dimension $d$, we study the core cross-attention and self-attention models:\n\\begin{subequations}\\label{eqn:sa:obj}\n\\begin{align}\nf_{\\texttt{cross}}(\\X,\\Z)&:= \\sftx(\\Z \\Qb\\K^\\top\\X^\\top)\\X\\V, \n\\label{xatt eq}\\\\\nf_{\\texttt{self}}(\\X)&:= \\sftx(\\X \\Qb\\K^\\top\\X^\\top)\\X\\V.\n\\label{satt eq}\n\\end{align}\n\\end{subequations}\nHere, $\\Kb, \\Qb  \\in \\R^{d\\times m}$, $\\V \\in \\R^{d\\times v} $ are the trainable key, query, value matrices respectively; $\\sft{\\cdot}$ denotes the softmax nonlinearity, which is applied row-wise on  $\\X \\Qb\\K^\\top\\X^\\top$.  Note that self-attention  \\eqref{satt eq} is a special instance of the cross-attention \\eqref{xatt eq} by setting $\\Z\\gets \\X$. To expose our main results, suppose the first token of $\\Z$ -- denoted by $\\z$ -- is used for prediction. Concretely, given a training dataset $(Y_i,\\X_i, \\z_i)_{i=1}^n$ with labels $Y_i\\in \\{-1,1\\}$ and inputs $\\X_i\\in\\R^{T\\times d},\\z_i\\in\\R^d$, we consider the empirical risk minimization with a decreasing loss function $\\ell(\\cdot):\\R\\rightarrow\\R$, represented as follows:\n\\begin{align}\\label{eq:erm:init} \n\\Lc(\\K,\\Qb)=\\frac{1}{n}\\sum_{i=1}^n \\ell \\left(Y_i\\cdot f(\\X_i,\\z_i)\\right),\\quad\\text{where}~~f(\\X_i,\\z_i)=h\\left(\\X^\\top_i \\sftx\\left(\\X_i \\K\\Qb^\\top\\z_i\\right)\\right).\n\\end{align}\nHere, $h(\\cdot):\\R^{d}\\rightarrow\\R$ is the prediction head that subsumes the value weights $\\Vb$. In this formulation, the model $f(\\cdot)$ precisely represents a one-layer transformer where an MLP follows the attention layer. Note that, we recover the  self-attention in \\eqref{eq:erm:init} by setting $\\z_i\\gets \\x_{i1}$, where $\\x_{i1}$ denotes the first token of the sequence $\\X_i$\\footnote{Note that for simplicity, we set $\\z_i = \\x_{i1}$, but it can be any other row of $\\X_i$.}. The softmax operation, due to its nonlinear nature, poses a significant challenge when optimizing \\eqref{eq:erm:init}. The problem is nonconvex and nonlinear even when the prediction head is fixed and linear. In this study, we focus on optimizing the attention weights ($\\Kb,\\Qb$ or $\\W$) and overcome such challenges to establish a fundamental SVM equivalence.\\footnote{We fix $h(\\cdot)$ and only optimize the attention weights. This is partly to avoid the degenerate case where $h(\\cdot)$ can be used to achieve zero training loss (e.g.~via standard arguments like NTK \\cite{jacot2018neural}) without providing any meaningful insight into the functionality of the attention mechanism.} \n\n\n\n\n\n\n\n\n\n\n\n\nThe paper's main contributions are as follows:\n\n\n\n\\begin{enumerate}[label=$\\bullet$, wide, labelwidth=!,itemindent=!, labelindent=5pt]\n\\item \\textbf{Implicit bias of the attention layer (Secs. \\ref{sec:prelim}-\\ref{sec:bias}).}  Optimizing the attention parameters $(\\Kb,\\Qb)$ with vanishing regularization converges in direction towards a max-margin solution of \\eqref{eqn:sattnsvmst} with the nuclear norm objective of the combined parameter $\\W:=\\K\\Qb^\\top$ (Thm \\ref{thm global reg path}). \nIn the case of directly parameterizing cross-attention by the combined parameter $\\W$, the regularization path (RP) directionally converges to \\eqref{eqn:sattnsvm} solution with the Frobenius norm objective. To our knowledge, this is the first result to formally distinguish the optimization dynamics of $\\W$ vs $(\\Kb,\\Qb)$ parameterizations, revealing the low-rank bias of the latter.  Our theory clearly characterizes the \\emph{optimality} of selected tokens (Definition~\\ref{score def}) and naturally extends to sequence-to-sequence or causal classification settings (see \\ref{seqattnsvm} and Theorem \\ref{local RP thm} in appendix). \n\n\n\\item \\textbf{Convergence of gradient descent (Secs. \\ref{provable global}-\\ref{sec local}).} Gradient descent (GD) iterates for the combined key-query variable $\\W$ converge in direction to a \\emph{locally-optimal} solution of \\eqref{eqn:sattnsvm} with appropriate initialization and a linear head $h(\\cdot)$ (Sec. \\ref{sec local}). For local optimality, selected tokens must have higher scores than their neighboring tokens. Locally-optimal directions are not necessarily unique and are characterized in terms of the problem geometry.  As a key contribution, we identify geometric conditions that guarantee convergence to the globally-optimal direction (Sec. \\ref{provable global}). Besides these, we show that over-parameterization (i.e.~dimension $d$ being large, and equivalent conditions) catalyzes global convergence by ensuring \\textbf{(1)} feasibility of \\eqref{eqn:sattnsvm}, and, \\textbf{(2)} benign optimization landscape, in the sense that there are no stationary points and no spurious locally-optimal directions (see Sec.~\\ref{sec overparam}). These are illustrated in Figures \\ref{fig path} and \\ref{fig overparam W bar}. \n\n\n\n\n\\item \\textbf{Generality of SVM equivalence (Sec. \\ref{sec:multi}).} When optimizing with linear $h(\\cdot)$, the attention layer is inherently biased towards selecting a single token from each sequence (a.k.a.~hard attention). This is reflected in \\eqref{eqn:sattnsvm} and arises from output tokens being convex combinations of the input tokens. In contrast, we show that nonlinear heads necessitate composing multiple tokens, highlighting their importance in the transformer's dynamics (Sec. \\ref{sec when}). Using insights gathered from our theory, we propose a more general SVM equivalence. Remarkably, we demonstrate that our proposal accurately predicts the implicit bias of attention trained by gradient descent under general scenarios not covered by theory (e.g.~$h(\\cdot)$ being an MLP). Specifically, our general formulae decouple attention weights into two components: A \\textbf{directional component} governed by SVM which selects the tokens by applying a 0-1 mask, and a \\textbf{finite component} which dictates the precise composition of the selected tokens by adjusting the softmax probabilities.\n\\end{enumerate}\n\nAn important feature of these findings is that they apply to arbitrary datasets (whenever SVM is feasible) and are numerically verifiable. We extensively validate the max-margin equivalence and implicit bias of transformers through enlightening experiments. We hold the view that these findings aid in understanding transformers as hierarchical max-margin token-selection mechanisms, and we hope that our outcomes will serve as a foundation for upcoming studies concerning their optimization and generalization dynamics.\n\n\\smallskip\n\\noindent\\textbf{Overview.}~The paper is structured as follows: Section \\ref{sec:prelim} introduces preliminaries on self-attention and optimization. Section~\\ref{sec:bias} analyzes self-attention's optimization geometry, showing the RP of attention parameters converges to a max-margin solution. Sections \\ref{provable global} and \\ref{sec local} present global and local gradient descent analyses, respectively, demonstrating convergence of $\\W$, the key-query variable, towards the solution of \\eqref{eqn:sattnsvm}. Section \\ref{sec:multi} provides our results on nonlinear prediction heads and generalized SVM equivalence. \n\nSection~\\ref{sec:related} discusses relevant literature. Finally, Section~\\ref{sec:conc} concludes the paper with open problems and future research directions inspired by our findings. All proofs are deferred to the appendix.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n==== END OF /2308.16898/sections/introduction.tex ====\n==== BEGINNING OF /2308.16898/sections/zzz_prelim.tex ====\n\\begin{comment}\nLet us now shift our focus towards solving the SVM problem using optimal indices denoted by $(\\opt_i)_{i=1}^n$. It is important to note that the problem can be formulated for arbitrary sets of indices $\\boldsymbol{\\alpha} = (\\alpha_i)_{i=1}^n$, which will reveal locally optimal directions, as defined in Section~\\ref{sec local}. We introduce the following formulation:\n\\begin{tcolorbox}[colback=white!5!white,colframe=black!5!black,colback=green!1!white]\n\\begin{equation}\\tag{\\name}\n\\Wm_\\dm=\\arg\\min_{\\W}\\td{\\W}\n\\quad \\text{subject to} \\quad \\min_{t\\neq \\op_i}(\\x_{i\\op_i}-\\x_\\itt)^\\top\\W\\z_i\\geq 1\\quad \\text{for all} \\quad 1\\leq i\\leq n\\label{eqn:sattnsvm}.\n\\end{equation}\n\\end{tcolorbox}\nIn \\eqref{eqn:sattnsvm}, we use $\\diamond$ to denote a generic objective function that captures the inductive bias of the optimization problem. Throughout our discussion, we will use $\\Wm$ to represent the unique solution of \\eqref{eqn:sattnsvm} with the Frobenius norm objective, and $\\Wm_\\star$ to denote the solution of \\eqref{eqn:sattnsvm} with the nuclear norm. In the following, we provide insights and discussions on the implicit bias of gradient descent applied to \\eqref{eqn:erm:w} towards $\\Wm$. In the next section, we delve into discussions on $\\Wm_\\star$ and its connection to \\eqref{eqn:erm:kq} and the SVM problem parameterized by $(\\Kb,\\Qb)$.\n\n\n\n\nIt is worth noting that the existence of matrix $\\Wm$ implies the separability of tokens $\\op_i$ from the others. Additionally, the term $a_{\\itt}=\\x_\\itt^\\top\\W\\z_i$ represents the dot product between the key-query features before applying the softmax nonlinearity. This dot product is a crucial characteristic of self-attention. Therefore, we can express the SVM constraints in \\eqref{eqn:sattnsvm} as $a_{i\\op_i}\\geq a_{\\itt}+1$. This formulation aims to find the most efficient direction that ensures the optimal token $\\x_{i\\op_i}$ achieves the highest similarity with query $\\z_i$ among all key embeddings.\n\nTo gain further insight into equation \\eqref{eqn:sattnsvm}, let's consider that the output token of the attention layer is a convex mixture of the input tokens. If the training loss is minimized by selecting the optimal token $\\x_{i\\op_i}$, then the softmax similarities should converge to a one-hot vector that precisely includes $\\x_{i\\op_i}$ (assigned a value of 1) and disregards all other tokens (assigned 0s). For this convergence to occur, the attention weights $\\W$ should diverge in norm to saturate the softmax probabilities. However, the weights converge in direction to the max-margin SVM due to the exponential-tailed nature of the softmax function. This phenomenon resembles the inductive bias of logistic regression on separable data \\cite{soudry2018implicit,ji2018risk}. Lemma \\ref{lem min risk} formalizes this intuition and provides a rigorous motivation for our definition of optimal tokens.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Non-convex hard-margin SVM problem for cross attention with parameters $(\\mathbf{Q},\\mathbf{K})$ } \n\nThe objective function in \\eqref{eqn:erm:kq} generally lacks convexity in terms of $(\\Qb, \\Kb)$. However, when training without regularization \\footnote{ maybe give $\\lambda=1e-16$...} the token separation with a maximum margin occurs. In this case, gradient descent update \\ref{GD-QK}  produces iterations that diverge in norm but converge in direction (refer to Figure~\\ref{fig path}). This behavior resembles the margin maximization observed in non-convex problems such as homogeneous neural networks \\cite{lyu2019gradient,ji2020directional}, which is an implicit regularization effect of gradient descent. Notably, \\cite{lyu2019gradient,ji2020directional} has demonstrated that solutions obtained through gradient descent and gradient flow converge in direction towards a KarushKuhnTucker (KKT) point of the max-margin SVM. Motivated by the aforementioned works and supported by preliminary experiments illustrated in Figure~\\ref{fig path}, along with a comprehensive numerical study detailed in Section~\\ref{sec:exprim}, we introduce a nonconvex SVM problem. This problem is parameterized by $(\\Qb, \\Kb)$ similar to its twin problem \\eqref{eqn:erm:kq},  and its objective is to achieve a separation between the optimal tokens indexed by $(\\opt_i)_{i=1}^n$ and\nfrom the remaining tokens in the input sequence $\\{\\X_i\\}_{i=1}^n$.\n\n\n\n\n\\tcbset{colback=white!5!white,colframe=black!5!black,colback=green!1!white}\n\\begin{tcolorbox}[height=2cm]\n\\begin{equation}\\tag{KQ-SVM}\n\\Kb,\\Qb=\\arg\\min_{\\Kb,\\Qb}~\\frac{1}{2} \\left(\\|\\Kb\\|_F^2+\\|\\Qb\\|_F^2\\right)\n \\quad \\text{subj. to}   \\quad \\min_{t\\neq \\op_i}(\\x_{i\\op_i}-\\x_\\itt)^\\top\\Kb\\Qb^\\top\\z_i\\geq 1\\quad \\text{for all} \\quad 1\\leq i\\leq n\\label{eqn:qk:svm}.\n\\end{equation}\n\\end{tcolorbox}\n\n\n\n\n\n\n\nA KKT point does not always represent an optimal solution for the maximum-margin problem. This is analogous to showing that an unconstrained optimization problem reaches a point where the gradient is zero, without proving whether it is a global minimum, local minimum, or saddle point. While convergence to a KKT point in the \\eqref{eqn:qk:svm} formulation indicates a tendency towards maximizing the margin in the parameter space, it does not provide a guarantee for reaching a maximum-margin solution.\n\nOur objective is to demonstrate that the non-convex optimization problem \\eqref{eqn:erm:kq} at hand is not as daunting as it may initially seem. Specifically, we aim to establish that, apart from the global optimum, all other KKT points  act as saddle points that can be circumvented through gradient descent updates \\ref{GD-QK}. In fact, we identify the implicit regularizer as the \\textit{nuclear norm} and illustrate that, even in the absence of constraints on the factored matrix $\\W=\\Kb\\Qb^\\top$ and employing a full-dimensional factorization, optimization through gradient descent on the $(\\Qb, \\Kb)$ factorization directs us towards the solution of \\eqref{eqn:sattnsvm} with the minimum nuclear norm, i.e. $\\Wm_\\star$.   Our empirical investigation allows us to conjecture that, with appropriate step size,  \\ref{GD-QK} converges to the solution of \\eqref{eqn:sattnsvm} with the minimum nuclear norm, and we provide empirical evidence to support this conjecture, substantiating it within certain restricted settings.\n\\end{comment}\n\n\n\n\n\n\n\n==== END OF /2308.16898/sections/zzz_prelim.tex ====\n==== BEGINNING OF /2308.16898/sections/extensions.tex ====\n\\section{Extending the Theory to Sequential and Causal Predictions}\\label{sec seq extend}\n\n\n\n\n\nWhile our formulations (\\ref{eqn:erm:w} \\& \\ref{eqn:erm:kq}) regress a single label $Y_i$ per $(\\X_i,\\z_i)$, we extend in Appendix \\ref{sec multioutput}  our findings to the sequence-to-sequence classification setting, where we output and classify all $T$ tokens per input $\\X_i,\\Z_i\\in\\R^{T\\times d}$. In this scenario, we prove that all of our RP guarantees remain intact after introducing a slight generalization of \\eqref{eqn:sattnsvm}. Concretely, consider the following ERM problems for sequential and causal settings:\n\\begin{align}\n\\Lc^{\\texttt{seq}}(\\W)=\\frac{1}{n}\\sum_{i=1}^n\\sum_{k=1}^T \\ell(Y_\\ik\\cdot h( \\X_i^\\top \\sft{\\X_i\\W\\z_\\ik})),\\quad\\text{and}\\quad\n\\Lc^{\\texttt{csl}}(\\W)=\\frac{1}{n}\\sum_{i=1}^n\\sum_{k=1}^T \\ell(Y_\\ik\\cdot h( \\X_i^\\top \\sftk{\\X_i\\W\\z_\\ik})).\n\\end{align}\nBoth equations train $T$ tokens per input $(\\X_i,\\Z_i)$ and, as usual, we recover self-attention via $\\Z_i\\gets \\X_i$. For the causal setting, we use the masked attention $\\sftk{\\cdot}$ which calculates the softmax probabilities over the first $k$ entries of its input and sets the remaining $T-k$ entries to zero. This way, the $k$'th prediction of the transformer only utilizes tokens from $1$ to $k$ and not the future tokens.\n\n\nLet $\\bal=(\\alpha_\\ik)\\ikix$ be tokens to be selected by attention (e.g.~locally-optimal indices, see Def.~\\ref{seq loc opt}). Then, the sequential generalization of \\eqref{eqn:sattnsvm} corresponding to $\\Lc^{\\texttt{seq}}(\\W)$ is given by\n\\begin{equation}\n \\min_{\\W}\\tf{\\W}\n\\quad \\text{subj. to} \\quad  (\\x_{i\\aik}-\\x_\\itt)^\\top\\W\\z_\\ik\\geq 1\\quad\\textnormal{for all}\\quad {t\\neq \\aik},~~k\\in[T],~~ i\\in[n]~~\\label{seqattnsvm1}.\n\\end{equation}\nWe refer the reader to Appendix \\ref{sec multioutput} which rigorously establishes all RP results for this sequential classification setting. On the other hand, for the causal inference setting SVM should reflect the fact that the model is not allowed to make use of future tokens. Note that the SVM constraints directly arise from softmax calculations. Thus, since attention is masked over the indices $t\\leq k$ and $k\\in[T]$, the SVM constraints should apply over the same mask. Thus, we can consider the straightforward generalization of global-optimality where $\\op_\\ik$ is the token index with highest score over indices $t\\leq k$ and introduce an analogous definition for local-optimality. This leads to the following variation of \\eqref{seqattnsvm1}, which aims to select indices $\\alpha_\\ik\\in [k]$ from the first $k$ tokens\n\\begin{equation*}\n \\min_{\\W}\\tf{\\W}\n\\quad \\text{subj. to} \\quad   (\\x_{i\\aik}-\\x_\\itt)^\\top\\W\\z_\\ik\\geq 1\\quad \\textnormal{for all} \\quad  t\\neq \\aik,~~t\\leq k, ~~k \\in[T],~~i\\in[n]\\label{cslattnsvm}.\n\\end{equation*}\n\nCausal attention is a special case of a general attention mask which can restrict softmax to arbitrary subset of the tokens. Such general masking can be handled similar to \\eqref{cslattnsvm} by enforcing SVM constraints over the nonzero support of the mask. Finally, the discussion so far extends our main theoretical results and focuses on selecting single token per sequence. It can further be enriched by the generalized SVM equivalence developed in Section \\ref{sec:multi} to select and compose multiple tokens by generalizing \\eqref{eqn:mattnsvm}.\n==== END OF /2308.16898/sections/extensions.tex ====\n==== BEGINNING OF /2308.16898/sections/experiments.tex ====\n\\section{Experiments}\\label{sec:exprim}\n\n\n\\red{\nTODO:\n\\begin{itemize}\n    \\item \\textbf{Page 2}: visualization using $\\W\\z$ + overparam + ?\n    \\item Low rank experiment: vary $m$ and train $(\\Kb,\\Qb)$\n    \\item Nonlinear head e.g. ReLU function\n    \\item Self attention using $\\V$\n\\end{itemize}\n}\n\n\\red{[Move this part to Experiment section?]}\\ct{agree}\n\n \n\n\n\n\n==== END OF /2308.16898/sections/experiments.tex ====\n==== BEGINNING OF /2308.16898/sections/fig_overparam.tex ====\n\\begin{figure}[t]\n    \\centering\n    \n    \\subfigure[Global convergence for varying $n,d$]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.25\\columnwidth, trim={1.3cm 1.3cm 0 0}, clip]{figs/overparam_diff_n.pdf}};\n        \\node at (0,-2.3) {\\small{Varying $d$}};\n        \\node[rotate=90] at (-3.1,0) {\\small{Prob. of global convergence}};\n        \\node at (1.7,0.3){\\small{{$\\W$}}};\n        \\node at (1.9,-.1){\\small{{$(\\Kb,\\Qb)$}}};\n        \\end{tikzpicture}\n        \\label{fig overparam diff n}\n    }\n    \\hspace{30pt}\n    \\subfigure[Global convergence for varying $T,d$]{\n        \\begin{tikzpicture}\n        \\node at (0,0) {\\includegraphics[height=.25\\columnwidth, trim={1.3cm 1.3cm 0 0}, clip]{figs/overparam_diff_T.pdf}};\n        \\node at (0,-2.3) {\\small{Varying $d$}};\n        \\node[rotate=90] at (-3.1,0) {\\small{Prob. of global convergence}};\n        \\node at (1.7,0.3){\\small{{$\\W$}}};\n        \\node at (1.9,-.1){\\small{{$(\\Kb,\\Qb)$}}};\n        \\end{tikzpicture}\n        \\label{fig overparam diff T}\n    }\n    \\caption{Global convergence behavior of GD when training cross-attention weights $\\W$ (solid) or $(\\Kb,\\Qb)$ (dashed) with random data. The blue, green, and red curves represent the probabilities of global convergence for \\textbf{(a)}: fixing $T=5$ and varying $n \\in \\{5,10,20\\}$ and \\textbf{(b)}: fixing $n=5$ and varying $T \\in\\{5,10,20\\}$. Results demonstrate that for both attention models, as $d$ increases (due to over-parameterization), attention weights tend to select optimal tokens $(\\opt_i)_{i=1}^n$.\n}\n   \\label{fig overparam}\n\\end{figure}\n==== END OF /2308.16898/sections/fig_overparam.tex ====\n==== BEGINNING OF /2308.16898/supp/toy_dataset.tex ====\n\\section{Toy Dataset for Globally-Convergent Self-Attention}\n\nIn this section, we introduce a toy dataset model for which Assumption \\ref{assum:nabla0} holds with $\\z_i\\gets\\x_{i1}$. Importantly, this dataset captures the spirit of self-attention by allowing the model to retrieve different output tokens depending on the value of the first token. More concretely, in our model, whenever $\\x_1$ admits a value $\\ab_j$ from a dictionary $\\Ac=\\{\\ab_j\\}_{j=1}^r$, self-attention should retrieve a relevant token $\\x_t=\\bb_j$ from a dictionary $\\Bc=\\{\\bb_j\\}_{j=1}^r$.\n\n\n\\begin{definition} [Toy Distribution for Self-Attn]\\label{def data model} Data $(\\X,Y)$ is generated according to $\\Dc_{\\texttt{data}}$ as follows: Let $\\rho >1$ be the index of $\\x_1$'s \\emph{relevant token} (that is allowed to be random).\n\\begin{itemize}\n\\item \\textbf{Relevant token:} $(\\x_1,\\x_\\rho)$ has a uniform distribution over $r$ values $(\\ab_j,\\bb_j)_{j=1}^r$ with associated labels $(y_j)_{j=1}^r$. That is, whenever $(\\x_1,\\x_\\rho)=(\\ab_j,\\bb_j)$, the output label is $Y=y_j$. \n\\item \\textbf{Fixed score:} For some $\\gamma_1,\\gamma_\\rho$ and all $j\\in[r]$: $y_j \\vb^\\top\\ab_j=\\gamma_1$, $y_j \\vb^\\top\\bb_j=\\gamma_\\rho>0$.\n\\item \\textbf{Other tokens} $t\\not\\in\\{\\rho,1\\}$ are bounded, independent of the rest, and $\\E[\\x_t]=\\E[\\x_t\\x_t^\\top]\\vb=0$.\n\\end{itemize}\n\\end{definition}\n\n\\redp{Orthogonality condition for concrete example: Suppose vectors $(\\ab_j,\\bb_j)_{j=1}^r$ are all pairwise orthonormal and other tokens have small projections to the subspace $\\text{span}(\\ab_j,\\bb_j)_{j=1}^r$. First, this would provide a simple explicit example to instantiate Theorem \\ref{toy data thm} and Assumption \\ref{assume sep}. Second, based on Def.~\\ref{def data model}, this would also allow us to discuss the setting where \n\\begin{itemize}\n\\item Self-attention selects $\\x_\\rho$ if $\\gamma_\\rho>\\gamma_1$.\n\\item Self-attention selects $\\x_1$ if $\\gamma_1>\\gamma_\\rho$.\n\\end{itemize}\n$\\gamma_\\rho$ vs $\\gamma_1$ is related to the following concern: How can our model highlight ``self'' of self-attention?}\n\n\\begin{assumption}[Separation] \\label{assume sep}Consider the dataset $\\Dc_{\\texttt{data}}$ of Def.~\\ref{def data model}. Let $\\W_\\rho=\\frac{1}{r}\\sum_{j=1}^r\\bb_j\\ab_j^\\top$. Assume $|\\gamma_1|$ is sufficiently small, $T$ is sufficiently large and for an arbitrarily small $\\eps>0$: \n\n\\begin{align}\n\\gamma_\\rho-y_j\\vb^\\top \\x_t\\geq\\eps\\quad\\text{and}\\quad (\\bb_j-\\x_t)^\\top\\W_\\rho\\ab_j\\geq \\eps\\quad\\text{for all}~j\\in[r],t\\neq\\rho\\quad\\text{almost surely}.\\label{sep condition}\n\\end{align}\n\\end{assumption}\n\n\n\n\\begin{theorem}\\label{toy data thm} Consider the dataset model $\\Dc_{\\texttt{data}}$ of Def.~\\ref{def data model}. Denote the initial population gradient $\\nabla\\Lc(0):=\\E_{\\Dc_{\\texttt{data}}}[\\nabla\\Lc(0)]$. Let $\\W_1=\\frac{1}{r}\\sum_{j=1}^r\\ab_j\\ab_j^\\top$ and $\\W_\\rho=\\frac{1}{r}\\sum_{j=1}^r\\bb_j\\ab_j^\\top$. We have that\n\\begin{align}\n&\\nabla\\Lc(0)=\\frac{\\ell'(0)}{T}(\\gamma_1\\W_1+\\gamma_\\rho\\W_\\rho)-\\frac{\\ell'(0)}{T^2}(\\gamma_1+\\gamma_\\rho)(\\W_1+\\W_\\rho)\\label{eq nablaL0}\n\\end{align}\nAdditionally, suppose Assumption \\ref{assume sep} holds. Then, $\\x_\\rho$ is the optimal token and Assumption \\ref{assum:nabla0} holds almost surely i.e.\n\n\\[\n\\underset{t\\in[T]}{\\min}\\li(\\x_t-\\x_\\rho)^\\top\\nabla\\Lc(0)\\x_1\\ri>0.\n\\]\n\\end{theorem}\n\\begin{proof} We will make use of the simple structure of softmax at $\\W=0$. Let $\\bgam=Y\\cdot\\X\\vb$ and $\\Fb_t=\\x_t\\x_1^\\top$. Using the fact that softmax derivative at $0$ is simply all $1/T$ vector, we have that\n\\begin{align}\n\\nabla\\Lc(0)=\\frac{\\ell'(0)}{T}\\E[\\sum_{t=1}^T \\bgam_t\\Fb_t]-\\frac{\\ell'(0)}{T^2}\\E[\\sum_{t=1}^T \\bgam_t\\sum_{t=1}^T \\Fb_t].\n\\end{align}\n\nLet $\\bSi=\\E[\\x_t\\x_t^\\top]$ for $t\\notin\\{1,\\rho\\}$. To proceed, observe that, for $t\\not\\in\\{1,\\rho\\}$\n\\[\n\\E[\\bgam_t\\Fb_t]=\\E[Y\\vb^\\top\\x_t\\x_t\\x_1^\\top]=\\E[\\vb^\\top\\x_t\\x_t]\\E[Y\\x_1]^\\top=\\bSi\\vb\\E[Y\\x_1]^\\top=0.\n\\]\nSimilarly for $t\\not\\in\\{1,\\rho\\}$, we have $\\E[\\sum_{i=1}^T \\bgam_i\\Fb_t]=0$ by additionally using $\\E[\\x_t]=0$.\n\nWhat remains is $t=1$ and $t=\\rho$. Using fixed score assumption, we find\n\\[\n\\E[\\bgam_1\\Fb_1]=\\E[Y\\vb^\\top\\x_1\\x_1\\x_1^\\top]=\\gamma_1\\W_1,\\quad\\E[\\bgam_\\rho\\Fb_\\rho]=\\E[Y\\vb^\\top\\x_\\rho\\x_\\rho\\x_1^\\top]=\\gamma_\\rho\\W_\\rho.\n\\]\nSimilarly, we obtain\n\\[\n\\E[\\sum_{t=1}^T \\bgam_t\\sum_{t=1}^T \\Fb_t]=\\frac{1}{T^2}(\\gamma_1+\\gamma_\\rho)(\\W_1+\\W_\\rho),\n\\]\nto conclude with \\eqref{eq nablaL0}. To conclude with Assumption \\ref{assum:nabla0}, we use Assumption \\ref{assume sep} and observe that $T\\nabla\\Lc(0)$ is arbitrarily close to $T\\hat{\\nabla}\\Lc(0)$ where $\\hat{\\nabla}\\Lc(0)=\\frac{\\ell'(0)\\gamma_\\rho}{T}\\W_\\rho$. Now, recalling $\\ell'(0)<0$, applying the right hand-side of \\eqref{sep condition} with $\\hat{\\nabla}\\Lc(0)\\propto-\\W_\\rho$, and using the boundedness of tokens and $\\eps>0$ as perturbation buffer, we obtain the desired statement.\n\\end{proof}\n==== END OF /2308.16898/supp/toy_dataset.tex ====\n==== BEGINNING OF /2308.16898/supp/app_exp.tex ====\n\\section{Supporting Experiments}\\label{app supp exp}\n\n\\input{sections/fig_nn_main}\n\nIn this section, we introduce implementation details and additional experiments. Code is available at\n\\begin{center}\n\\url{https://github.com/umich-sota/TF-as-SVM}\n\\end{center}\nWe create a 1-layer self-attention using \\texttt{PyTorch}, training it with the SGD optimizer and a learning rate of $\\eta=0.1$. We apply normalized gradient descent to ensure divergence of attention weights. The attention weight $\\W$ is then updated through\n\\[\n\\W(k+1)=\\W(k)-\\eta\\frac{\\nabla\\Lc(\\W(k))}{\\|\\nabla\\Lc(\\W(k))\\|_F}.\n\\]\nIn the setting of $(\\Kb,\\Qb)$-parameterization, we noted that with extended training iterations, the norm of the combined parameter $\\Kb\\Qb^\\top$ consistently rises, despite the gradient being treated as zero due to computational limitations. To tackle this issue, we introduce a minor regularization penalty to the loss function, ensuring that the norms of $\\Kb$ and $\\Qb$ remain within reasonable bounds. This adjustment involves\n\\[\n\\widetilde\\Lc(\\Kb,\\Qb)=\\Lc(\\Kb,\\Qb)+\\lambda(\\|\\Kb\\|^2_F+\\|\\Qb\\|^2_F).\n\\]\nHere, we set $\\lambda$ to be the the smallest representable number, e.g. computed as $1+\\lambda\\neq1$ in \\texttt{Python}, which is around $2.22\\times10^{-16}$. Therefore, $\\Kb,\\Qb$ parameters are updated as follows.\n\\[\n\\Kb(k+1)=\\Kb(k)-\\eta\\frac{\\nabla\\widetilde\\Lc_\\Kb(\\Kb(k),\\Qb(k))}{\\|\\nabla\\widetilde\\Lc_\\Kb(\\Kb(k),\\Qb(k))\\|_F}, \\qquad \\Qb(k+1)=\\Qb(k)-\\eta\\frac{\\nabla\\widetilde\\Lc_\\Qb(\\Kb(k),\\Qb(k))}{\\|\\nabla\\widetilde\\Lc_\\Qb(\\Kb(k),\\Qb(k))\\|_F}.\n\\]\n$\\bullet$ As observed in previous work \\cite{tarzanagh2023margin}, and due to the exponential expression of softmax nonlinearity and computation limitation, \\texttt{PyTorch} has no guarantee to select optimal tokens when the score gap is too small. Therefore in Figures~\\ref{fig overparam W bar}, \\ref{fig overparam bar} and \\ref{fig overparam}, we generate random tokens making sure that $\\min_{i\\in[n],t\\neq\\op_i}\\bgam_{i\\op_i}-\\bgam_{it}\\geq\\underline\\gamma$ and we choose $\\underline\\gamma=0.1$ in our experiments.\n\n\n\\paragraph{Rank sensitivity of $(\\Kb,\\Qb)$-parameterization (Figure~\\ref{fig rank m}).} In Figure \\ref{fig rank} and Lemma~\\ref{lem:rank}, we have both theoretically and empirically established that the rank of the SVM solution, denoted as $\\Ws$ in \\eqref{eqn:sattnsvm} or $\\Ws_\\st$ in \\eqref{eqn:sattnsvmst}, is at most rank $\\max(n,d)$. Now, moving to Figure~\\ref{fig rank m}, we delve into GD performance across various dimensions of $\\Kb,\\Qb\\in\\R^{d\\times m}$ while keeping $d=20$ fixed and varying $m$ from $1$ to $10$. In the upper subfigure, we maintain a constant $n=5$ and vary $T$ within $\\{5,10,15\\}$, while in the lower subfigure, $T$ is fixed at $5$ and $n$ changes within $\\{5,10,15\\}$. Results are depicted using blue, green, and red dashed curves, with both $y$-axes representing $1-\\corr{\\W,\\Ws_{\\st,\\bal}}$, where $\\W$ represents the GD solution and $\\Ws_{\\st,\\bal}$ is obtained from \\eqref{eqn:sattnsvmst} by employing token indices $\\bal$ selected via GD and setting the rank limit to $m=d$. Observing both subfigures, we note that a larger $n$ necessitates a larger $m$ for attention weights $\\Kb\\Qb^\\top$ to accurately converge to the SVM solution (Figure~\\ref{fig rank m}(lower)). Meanwhile, performances remain consistent across varying $T$ values (Figure \\ref{fig rank m}(upper)). This observation further validates Lemma \\ref{lem:rank}. Furthermore, the results demonstrate that $\\W$ converges directionally towards $\\Ws_{\\st,\\bal}$ as long as $m\\gtrsim n$, thereby confirming the assertion in our Theorem~\\ref{thm:local:gd}.  \n\n\n\n\\paragraph{Behavior of GD  with nonlinear nonconvex prediction head and multi-token compositions (Figure~\\ref{fig nn main}).} To better investigate how correlation changes with data dimension $d$, we collect the solid curves in Figure~\\ref{fig nn diff d}(upper) and construct as Figure~\\ref{fig nn itr}. Moreover, Figure \\ref{fig nn sfx zero} displays the average correlation of instances (refer to scatters in Figure~\\ref{fig nn diff d} (lower)), considering masked tokens with softmax probability $<\\Gamma$. Both findings highlight that higher $d$ enhances alignment. For $d\\geq8$ or $\\Gamma\\leq10^{-9}$, the GD solution $\\W$ achieves a correlation of $>0.99$ with the SVM-equivalence $\\W^{\\rfn}$, defined in Section~\\ref{sec:multi}.\n\n\\input{sections/fig_multi_diff_tau}\n\n\\paragraph{Investigation of Lemma~\\ref{example dataset} over different $\\tau$ selections (Figure~\\ref{fig multi tau}).}   Consider the setting of Section~\\ref{sec when} and Lemma~\\ref{example dataset}.  Figure~\\ref{fig multi corrs} explores the influence of $\\lambda$ on the count of tokens selected by GD-derived attention weights. As $\\lambda$ increases, the likelihood of selecting more tokens also increases. Shifting focus to Figure~\\ref{fig multi tau}, we examine the effect of $\\tau$. For each outcome, we generate random $\\lambda$ values, retaining pairs $(\\lambda,\\X)$ satisfying $\\tau$ constraints, with averages derived from $100$ successful trials. The results indicate a positive correlation among $\\tau$, $\\lambda$, and the number of selected tokens. Moreover, Figure~\\ref{fig tau prob} provides a precise distribution of selected token counts across various $\\tau$ values (specifically $\\tau\\in\\{3,5,7,9\\}$). The findings confirm that the number of selected tokens remains within the limit of $\\tau$, thus validating the assertion made in Lemma~\\ref{example dataset}.\n\n==== END OF /2308.16898/supp/app_exp.tex ====\n==== BEGINNING OF /2308.16898/supp/supp-roadmap.tex ====\n\n\n\n\n\n\n\\paragraph{Roadmap.} The appendix is organized as follows:\n\n\n\\begin{itemize}\n\\item Appendix \\ref{app sep} provides the proof of Theorem \\ref{thm:separation}. \n\\item Appendix~\\ref{app:sec:aux} provides auxiliary lemmas about the training risk. \n\\item Appendix~\\ref{app:sec:gd:global} presents the proofs for the global convergence of gradient descent (Section \\ref{provable global}). \n\\item Appendix \\ref{app local proofs} presents the proofs for the local convergence of gradient descent (Section \\ref{sec local}). \n\\item Appendix~\\ref{sec multioutput} provides a general regularization path analysis. This analysis addresses the inductive bias of the attention layer for general norm objectives and beyond-linear prediction heads under a sequence-to-sequence classification model. The seq2seq aspect also goes beyond our results in the main body where we predict using single output token (Sections \\ref{sec:bias} and \\ref{sec:local reg path}).\n\\item Appendix \\ref{app supp exp} provides additional experiments and their discussion.\n\n\\end{itemize}\n\n==== END OF /2308.16898/supp/supp-roadmap.tex ====\n==== BEGINNING OF /2308.16898/supp/app-sa-basics.tex ====\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Auxiliary Lemmas}\\label{app:sec:aux}\n\n\n\\subsection{Proof of Lemma \\ref{lem:rank}}\\label{app low rank proof}\nLet $\\W^\\svm_\\dm$ denote either solution of \\eqref{eqn:sattnsvm} or \\eqref{eqn:sattnsvmst}. We claim that $\\W^\\svm_\\dm$ is at most rank $n$.\n\nSuppose the claim is wrong and row space of $\\W^\\svm_\\dm$ does not lie within $\\Sc=\\texttt{span}(\\{\\z_i\\}_{i=1}^n)$. Let $\\W=\\Pi_{\\Sc}(\\W^\\svm_\\dm)$ denote the matrix obtained by projecting the rows of $\\W^\\svm_\\dm$ on $\\Sc$. Observe that $\\W$ satisfies all SVM constraints since $\\W\\z_i=\\W^\\svm_\\dm\\z_i$ for all $i\\in[n]$. For Frobenius norm, using $\\W^\\svm_\\dm\\neq \\W$, we obtain a contradiction via $\\tf{\\W^\\svm_\\dm}^2=\\tf{\\W}^2+\\tf{\\W^\\svm_\\dm-\\W}^2>\\tf{\\W}^2$. For nuclear norm, we can write $\\W=\\Ub\\bSi\\Vb^\\top$ with $\\bSi\\in\\R^{r\\times r}$ where $r$ is dimension of $\\Sc$ and $\\texttt{column\\_span}(\\Vb)= \\Sc$. \n\\\\\nTo proceed, we split the problem into two scenarios.\n\n\\\\\n\\noindent\\textbf{Scenario 1:} Let $\\Ub_\\perp,\\Vb_\\perp$ be orthogonal complements of $\\Ub,\\Vb$ -- viewing matrices with orthonormal columns as subspaces. Suppose $\\Ub_\\perp^\\top \\W^\\svm_\\dm\\Vb_\\perp\\neq 0$. Then, singular value inequalities (which were also used in earlier works on nuclear norm analysis \\cite{recht2011null,oymak2010new,oymak2011simplified}) guarantee that $\\tnuc{\\W^\\svm_\\dm}\\geq \\tnuc{\\Ub^\\top \\W^\\svm_\\dm\\Vb}+\\tnuc{\\Ub_\\perp^\\top \\W^\\svm_\\dm\\Vb_\\perp}>\\tnuc{\\W}$.\n\\\\\n\\noindent\\textbf{Scenario 2:} Now suppose $\\Ub_\\perp^\\top \\W^\\svm_\\dm\\Vb_\\perp= 0$. Since $\\W^\\svm_\\dm\\Vb_\\perp\\neq 0$, this implies $\\Ub^\\top \\W^\\svm_\\dm\\Vb_\\perp\\neq 0$. Let $\\W'=\\Ub\\Ub^\\top\\W^\\svm_\\dm$ which is a rank-$r$ matrix. Since $\\W'$ is a subspace projection, we have $\\tnuc{\\W'}\\leq \\tnuc{\\W^\\svm_\\dm}$. Next, observe that $\\tnuc{\\W}=\\texttt{trace}(\\Ub^\\top \\W\\Vb)=\\texttt{trace}(\\Ub^\\top \\W'\\Vb)$. On the other hand, $\\texttt{trace}(\\Ub^\\top \\W'\\Vb)<\\tnuc{\\W'}$ because the equality in \\emph{von Neumann's trace inequality} happens if and only if the two matrices we are inner-producting, namely $(\\W',\\Ub\\Vb^\\top)$, share a joint set of singular vectors \\cite{carlsson2021neumann}. However, this is not true as the row space of $\\W^\\svm_\\dm$ does not lie within $\\Sc$. Thus, we obtain $\\tnuc{\\W}<\\tnuc{\\W'}\\leq \\tnuc{\\W^\\svm_\\dm}$ concluding the proof via contradiction. $\\qed$ \n\n\n\\subsection{Proof of Lemma \\ref{lem min risk}}\nWe first show that  $\\Lc(\\W)>\\Lc_\\st=\\frac{1}{n}\\sum_{i=1}^n \\ell(\\bgam_{i\\op_i})$. The token at the output of the attention layer is given by $\\ab_i=\\X_i^\\top \\s_i$, where $\\sft{\\X_i\\W\\z_i}=\\s_i$. Here, $\\ab_i$ can be written as $\\ab_i=\\sum_{t\\in[T]}\\s_\\itt\\x_\\itt$, where $\\s_\\itt\\geq 0$ and $\\sum_{t\\in[T]}\\s_\\itt=1$. \nTo proceed, using the linearity of $h(\\x)=\\vb^\\top\\x$, we find that\n\n\n\n\n\\begin{align}\\label{eqn:w:low}\n\\nonumber\n  \\Lc(\\W)= \\frac{1}{n} \\sum_{i=1}^n  \\ell(Y_i\\cdot  h(\\ab_i)) &= \\frac{1}{n} \\sum_{i=1}^n  \\ell( Y_i\\cdot  \\sum_{t\\in[T]}\\s_\\itt h(\\x_\\itt))\\\\\n  &\\geq  \\frac{1}{n} \\sum_{i=1}^n  \\ell( Y_i\\cdot  h(\\x_{i\\op_i})) =\\frac{1}{n} \\sum_{i=1}^n  \\ell(\\bgam_{i\\op_i})=\\Lc_\\st. \n\\end{align}\nHere, the inequality follows since  $\\bgam_{it} = Y_i \\cdot h(\\x_\\itt)=  Y_i \n \\cdot \\vb^\\top\\x_{it} \\leq \\bgam_{i\\op_i}$ by Definition \\ref{score def} and strictly-decreasing nature of the loss $\\ell$ due to Assumption~\\ref{assum:loss:prope}.  \n\nOn the other hand, since not all tokens are optimal, there exists a token index $(i,t)$ for which $Y_i\\cdot h(\\x_\\itt)<Y_i\\cdot h( \\x_{i\\op_i})$. Since all softmax entries obey $\\s_\\itt>0$ for finite $\\W$, this implies the strict inequality $\\ell(Y_i\\cdot h(\\ab_i)) >\\ell(Y_i\\cdot h(\\x_{i\\op_i}))$. This leads to the desired conclusion $\\Lc(\\W)>\\Lc_\\st$.\n\nNext, we show that if \\eqref{eqn:sattnsvm} is feasible i.e.~there exists a $\\W$ separating some optimal indices $(\\op_i)_{i=1}^n$ from the other tokens, then  $\\lim_{R\\rightarrow\\infty}\\Lc(R\\cdot\\W)=\\Lc_\\st$. Note that, this assumption does not exclude the existence of other optimal indices. This implies that, letting $\\lim_{R\\rightarrow\\infty} \\sft{\\X_i(R\\cdot\\W)\\z_i}$ saturates the softmax and will be equal to the indicator function at $\\op_i$ for all inputs $i\\in[n]$. Thus, $\\s_\\itt\\rightarrow 0$ for $t\\neq\\op_i$ and $\\s_\\itt\\rightarrow 1$ for $t=\\op_i$. Using $M_1$-Lipschitzness of $\\ell$, we can write \n\\[\n\\left|\\ell(Y_i\\cdot h(\\x_{i\\op_i}))-\\ell( Y_i\\cdot h(\\ab_i))\\right|\\leq M_1 \\left|h(\\ab_i)-h(\\x_{i\\op_i})\\right|.\n\\]\nSince $h$ is linear, it is $\\tn{\\vb}$-Lipschitz implying \n\n\\begin{align*}\n \\left|\\ell(Y_i\\cdot h(\\x_{i\\op_i}))-\\ell(Y_i\\cdot h(\\ab_i))\\right|\\leq M_1\\tn{\\vb}\\cdot\\tn{\\ab_i-\\x_{i\\op_i}}.   \n\\end{align*}\nSince $\\ab_i\\rightarrow\\x_{i\\op_i}$ as $R\\rightarrow\\infty$, \\eqref{eqn:w:low} gives $\\lim_{R\\rightarrow\\infty}\\Lc(R\\cdot\\W)=\\Lc_\\st$.\n\n$\\qed$\n\n==== END OF /2308.16898/supp/app-sa-basics.tex ====\n==== BEGINNING OF /2308.16898/supp/reg_path_analysis.tex ====\n\n\nTo proceed with our analysis, let us define the set of optimal solutions $\\Wm_{\\bal}:=\\Ws_{\\dm,\\bal}$ to \\eqref{seqattnsvm}. Let us denote this set by $\\Wcs_{\\bal}:=\\Wcs_\\dm(\\bal)$. Note that, if the $\\dm$-norm is not strongly-convex, $\\Wcs_{\\bal}$ may not be singleton. \n\n\n\n\n\n\\subsection{Local regularization path and proof of Theorem \\ref{local RP thm1}}\\label{app:local:RP:thm1}\n\nWe first recall \\emph{local regularization path} which solves the $\\dm$-norm-constrained problem over a conic slice $\\Cc$, namely $\\Wb(R)=\\min_{\\td{\\W}\\leq R,\\W\\in\\Cc}\\Lc(\\W)$. We will show that proper local RP directionally converge to locally-optimal directions. Setting the cone to be the rank-$m$ manifold $\\Rcm$ (and more specifically the set of all matrices $\\R^{d\\times d}$), this will also establish the convergence of global RP to the globally-optimal direction.\n\nOur cone definition $\\con{\\bal}$ is induced by a token selection $\\bal=(\\al_\\ik)\\ikix$ and has a simple interpretation: It prioritizes tokens with lower score than $\\bal$ over tokens with high-score than $\\bal$. This way lower score tokens create a barrier for $\\bal$ and prevents optimization to move towards higher score tokens.\n\\begin{definition}[Low\\&High Score Tokens and Separating Cone]\\label{HL cone def} Given $\\al\\in[T]$, input sequence $\\X$ with label $Y$, $h(\\cdot):\\R^d\\rightarrow\\R$, and score $\\bgam_t=Y\\cdot h(\\x_t)$ for all $t\\in[T]$, define the low and high score tokens as\n\\[\n\\low=\\left\\{t\\in[T]\\bgl \\bgam_t<\\bgam_\\al\\},\\quad \\high=\\{t\\in[T]-\\{\\alpha\\}\\bgl \\bgam_t\\geq \\bgam_\\al \\right\\}.\n\\]\nFor input $\\X_\\ik$ and index $\\alpha_\\ik$, we use the shorthand notations $\\lowi,\\higi$. Finally define $\\con{\\bal}$ as\n\\begin{align}\n\\con{\\bal}=\\left\\{\\W\\in\\Rcm\\bgl \\min_{i\\in[n]}\\max_{t\\in\\lowi}\\min_{\\tau\\in\\higi} \\inn{\\F_\\ikt-\\F_\\iktt,\\W}\\geq \\eps\\tf{\\W} \\right\\}.\\label{cone alpha eq}\n\\end{align}\n\\end{definition}\n\n\\begin{lemma}\\label{lemma cone} Consider the cone definition of \\eqref{cone alpha eq} and suppose an SVM solution $\\Wma$ exists. If indices $\\bal$ are locally-optimal, $\\Wma\\in \\con{\\bal}$ for all sufficiently small $\\eps>0$. Otherwise, $\\Wma\\not\\in \\con{\\bal}$ for all $\\eps>0$. Additionally, suppose optimal indices $\\op_\\ik\\in\\arg\\max_{t\\in[T]}\\bgam_\\ikt$ are unique. Then, $\\con{\\opt}=\\Rcm$.\n\\end{lemma}\n\\begin{proof} Suppose $\\bal$ is locally optimal. Observe that, thanks to local optimality, $\\Wma$ obeys\n\\[\n\\min_{t\\in\\Tc_\\ik}\\inn{\\F_\\ikt,\\Wma}>\\max_{\\tau\\not\\in\\Tc_\\ik\\cup\\{\\al_\\ik\\}}\\inn{\\F_\\iktt,\\Wma},\n\\]\nfor all $i\\in[n]$. Next, observe that $\\Tc_\\ik\\subseteq \\lowi$ and $\\higi\\subseteq\\Tcb_\\ik=[T]-\\Tc_\\ik-\\{\\al_\\ik\\}$. Thus, the inequality \\eqref{cone alpha eq} holds for small enough $\\eps>0$.\n\nConversely, suppose $\\bal$ is not locally-optimal. Fix \\nei $t\\in\\Tc_\\ik$ with $t\\in \\higi$. Since $t\\in\\Tc_\\ik$, observe that\n\\[\n\\inn{\\F_\\ikt,\\Wma}\\geq \\max_{\\tau\\neq \\al_\\ik}\\inn{\\F_\\iktt,\\Wma}.\n\\]\nIn other words, for this $i\\in[n]$, we found\n\\[\n\\max_{\\tau\\in\\lowi} \\inn{\\F_\\iktt-\\F_\\ikt,\\Wma}\\leq 0,\n\\]\nviolating \\eqref{cone alpha eq} definition for any $\\eps>0$. To show the final claim, observe that, setting $\\bal:=\\op$, we have that $\\higi=\\emptyset$ for all $i\\in[n]$ as $\\op_\\ik$ are unique optimal indices. Thus, there is no constraint enforced on the cone definition in \\eqref{cone alpha eq} making it equal to the rank-$m$ manifold $\\Rcm$.\n\\end{proof}\n\nOur main assumption regarding prediction head is a monotonicity condition which is a strict generalization of linearity: We ask for $h$ to preserve the order of token scores under convex combinations.\n\\begin{assumption}[$h$ preserves the top score]\\label{ass cvx seq} The functions $(h_k)_{k=1}^K$ are $L_h$-Lipschitz in Euclidean distance. Given $\\bal=(\\alpha_\\ik)\\ikix$, there exists a scalar $c:=c_\\bal>0$ such that for all $i\\in[n],k\\in[K]$ the following holds: Consider any convex combination \n\\[ \\x(\\s)=\\sum_{t\\in \\lowi\\cup\\{\\alpha_\\ik\\}}s_t\\cdot\\x_\\itt\\quad\\text{where}\\quad \\sum_{t\\in \\lowi\\cup\\{\\alpha_\\ik\\}}s_t=1,~s_t\\geq 0.\n\\] \nWe have that $Y_\\ik\\cdot h_k(\\x(\\s))\\leq \\bga_\\ik-c(1-s_{\\alpha_\\ik})$ where $\\bga_\\ik=Y_\\ik\\cdot h_k(\\xa_{\\ik})$ is the score of $\\alpha_\\ik$.\n\\end{assumption}\nThis condition states that \\textbf{convex combinations of tokens with scores lower than $\\alpha_\\ik$ cannot achieve a score higher than $\\alpha_\\ik$}. Here, $1-s_{\\alpha_\\ik}$ term denotes the total share of non-optimal tokens. We require this condition to hold over the training dataset rather than the full domain $\\R^d$. Crucially, it is a strict generalization of the linearity assumption: Any linear $h_k$ satisfies Assumption \\ref{ass cvx seq} by setting $c_\\bal>0$ to be the difference between the score of $\\alpha_\\ik$ and the largest score within $\\lowi$ i.e.\n\\begin{align}\nc_\\bal:=\\min_{i\\in[n],k\\in[K]}\\{\\bga_\\ik-\\max_{t\\in\\lowi} \\bgam_\\ikt\\}>0.\\label{c choice}\n\\end{align}\nThis can be seen by writing $h_k(\\x(\\s))=\\sum_{t\\in \\lowi\\cup\\{\\alpha_\\ik\\}}s_t\\cdot \\bgam_\\ikt=\\bga_\\ik+\\sum_{t\\in \\lowi}s_t\\cdot (\\bgam_\\ikt-\\bga_\\ik)\\leq \\bga_\\ik-c_\\bal(1-s_{\\alpha_\\ik})$.\n\nTo provide a nonlinear example, consider the setting all labels are $Y_\\ik=1$ and $h$ is an arbitrary convex function. Thanks to convexity, we can write $h(\\x(\\s))\\leq \\sum_{t=1}^Ts_t\\cdot h(\\x_t)=\\sum_{t\\in \\lowi\\cup\\{\\alpha_\\ik\\}}s_t\\cdot \\bgam_\\ikt$. Thus, we can use the same choice of $c_\\bal$ in \\eqref{c choice}. \n\nWe remark that, in Section \\ref{sec:multi}, we derive formulae describing general inductive bias of attention without enforcing any assumption on $h$. These formulae allow for arbitrary output tokens generated by the transformer model trained by gradient descent. This includes the setting where gradient descent selects and composes multiple tokens from each sequence rather than a single token $\\alpha_\\ik$.\n\nThe following result is our main theorem regarding the convergence of regularization path to locally-optimal directions when restricted over the cone $\\con{\\bal}$.\n\\begin{theorem} [Convergence of Local Regularization Path]\\label{local RP thm} Suppose \\eqref{seqattnsvm} is feasible and $\\bal=(\\al_\\ik)\\ikix$ are locally-optimal token indices. Suppose Assumptions \\ref{assum:loss:prope}\\&\\ref{ass cvx seq} hold. Recall $\\con{\\bal}$ of \\eqref{cone alpha eq} and consider the norm-constrained cone \\[\n\\Ccd:=\\con{\\bal}\\bigcap\\{\\W\\bgl \\td{\\W}\\geq R_0\\}.\n\\]\nDefine the conic regularization path $\\wrb{R}=\\min_{\\Ccd,\\td{\\W}\\leq R}\\Lc(\\W)$. Let $\\Wcs_{\\bal}$ be its set of minima and $\\xdm>0$ be the associated margin i.e.~$\\xdm=1/\\td{\\Wcs_{\\bal}}$. For any sufficiently small $\\eps>0$ and sufficiently large $R_0= \\order{1/\\eps}>0$, $\\lim_{R\\rightarrow\\infty} \\dist{\\frac{\\wrb{R}}{R\\xdm},\\Wcs_{\\bal}}=0$. Additionally, suppose optimal indices $\\op=(\\op_\\ik)\\ikix$ are unique and set $\\bal\\gets\\op$. Then, the same RP convergence guarantee holds with $\\Ccd=\\Rcm$.\n\\end{theorem}\n\n\n\\begin{proof} We will prove that $\\wrb{R}$ is the optimal direction and also $\\td{\\wrb{R}}\\rightarrow \\infty$. Define the absolute constant \n\\[\n\\cdm=\\min_{\\td{\\W}=1}\\tf{\\W}.\n\\]\nThis guarantees that for any $\\W$ we have $\\tf{\\W}\\geq \\cdm\\td{\\W}$. Also denote $\\epsd=\\cdm\\eps$. Let us first determine the $\\eps$ parameter: Fix $\\Wma\\in\\Wcs_{\\bal}$. For general $\\bal$, we can choose any $\\eps>0$ that is sufficiently small to guarantee $\\Wma\\in\\con{\\bal}$ based on Lemma \\ref{lemma cone}. For $\\bal=\\op$, our analysis will entirely avoid using $\\eps$, specifically, observe that $\\con{\\bal}=\\Rcm$ based on Lemma \\ref{lemma cone}.\n    \n\\noindent\\textbf{Step 1:} Let us first prove that $\\wrb{R}$ achieves the optimal risk as $R\\rightarrow\\infty$ -- rather than problem having finite optima. Define norm-normalized $\\Wsb=\\xdm\\Wma$. Note that $\\Wma$ separates tokens $\\bal$ from rest of the tokens for each $i,k\\in[n]\\times [K]$. Thus, we have that\n\\begin{align}\n\\lim_{R\\rightarrow\\infty}\\Lc(\\wrb{R})\\leq\\lim_{R\\rightarrow\\infty}\\Lc(R\\cdot\\Wsb):=\\Lc_\\star= \\frac{1}{n}\\sum_{i=1}^n\\sum_{k=1}^K\\ell(\\bga_\\ik).\\label{asymp loss}\n\\end{align}\nOn the other hand, for any choice of $\\W\\in \\con{\\bal}$, set $\\x^{\\W}_\\ik=\\sum_{t=1}^T \\sft{\\X_i\\W\\z_\\ik}_t\\x_t$. Set softmax probabilities $\\sik=\\sft{\\X_i\\W\\z_\\ik}$. Recalling $\\lowi,\\higi$ definitions, we can decompose the attention features as\n\\begin{align}\n\\x^{\\W}_\\ik=\\sik_{\\al_\\ik}\\xa_\\ik+\\sum_{t\\in\\lowi}\\sik_t\\x_\\itt+\\sum_{\\tau\\in\\higi}\\sik_\\tau\\x_{\\ittt}.\n\\end{align}\nWhen $\\bal=\\op$, note that we simply have $\\higi=\\emptyset$. This will be important for setting $R_0=0$ and $\\Ccd=\\Rcm$ in the proof for $\\op$ indices.\n\nSet $\\bgg_\\ikt=\\bgam_\\ikt-\\bga_\\ik=Y_\\ik\\cdot (h_k(\\x_\\itt)-h_k(\\xa_\\ik))$. Building on $L_h$-Lipschitzness of the prediction head $h_k(\\cdot)$, we define\n\\begin{align}\n&B:=\\max_{i\\in[n],k\\in[K]}\\max_{t,\\tau\\in[T]}L_h\\cdot \\tn{\\x_\\itt-\\x_\\ittt}\\geq |\\bgg_\\ikt|.\\label{BB eq}\n\\end{align}\n\n\n\n\n\n\n\nDefine $P^\\ik:=\\sum_{t\\in\\lowi}\\sik_t$, $Q^\\ik:=\\sum_{t\\in\\higi}\\sik_t$, and $\\bgam^{\\W}_\\ik=Y_\\ik\\cdot h_k(\\x^{\\W}_\\ik)$. Also set temporary variables $\\x'=(\\sik_{\\al_\\ik}+Q^\\ik)\\xa_\\ik+\\sum_{t\\in\\lowi}\\sik_t\\x_\\itt$ and $\\bgam'=Y_\\ik\\cdot h_k(\\x^{\\W}_\\ik)$. \nUsing Assumption \\ref{ass cvx seq} on $\\x'$ and noticing $P^\\ik=1-\\sik_{\\al_\\ik}-Q^\\ik$, observe that\n\\[ \n|\\bgam^{\\W}_\\ik-\\bgam'|\\leq BQ^\\ik\\quad\\text{and}\\quad \\bgam'\\leq \\bga_\\ik-c_\\bal P^\\ik. \n\\] \nRecall from \\eqref{c choice} that, when $h_k$ are linear functions, $c_\\bal$ can be chosen as \n\n\\begin{align*}\nc_\\bal:=\\min_{i\\in[n],k\\in[K]}\\min_{t\\in\\lowi}-\\bgg_\\ikt>0.\n\\end{align*}\n\n\nTo summarize, applying Assumption \\ref{ass cvx seq}, we obtain the following score inequalities\n\\begin{align}\\label{score decomp}\n&\\bgam^{\\W}_\\ik\\leq \\bga_\\ik-c_\\bal P^\\ik+BQ^\\ik,\\\\\n\n&|\\bgam^{\\W}_\\ik-\\bga_\\ik|\\leq L_h\\tn{\\x^{\\W}_\\ik-\\xa_\\ik}\\leq L_h \\sum_{t\\neq \\al_\\ik}\\sik_t\\tn{\\x_\\ikt-\\xa_\\ik}\\leq B(1-\\sik_{\\al_\\ik}).\\label{lip score gap}\n\n\n\\end{align}\nWe will use the $\\bgam^{\\W}_\\ik-\\bga_\\ik$ term in \\eqref{score decomp} to evaluate $\\W$ against the reference loss \\eqref{asymp loss}. Let $\\abik=\\X_i\\W\\z_\\ik$. Now since $\\W\\in \\con{\\bal}$, there exists $t\\in \\lowi$ obeying $\\abik_t-\\max_{\\tau\\in\\higi} \\abik_\\tau\\geq \\eps \\tf{\\W}\\geq \\epsd\\td{\\W}$. Denote $D^\\ik:=(\\sum_{t\\in [T]}e^{\\abik_t})^{-1}$ to be the softmax denominator i.e.~sum of exponentials. We find that,\n\\begin{align}\nQ^\\ik=\\sum_{\\tau\\in\\higi}\\sik_\\tau=D^\\ik\\sum_{\\tau\\in\\higi}e^{\\abik_\\tau}\\leq D^\\ik Te^{\\abik_t-\\eps\\tf{\\W}}\\leq Te^{-\\epsd\\td{\\W}}P^\\ik.\\label{qikeq}\n\\end{align}\nConsequently, the score difference obeys\n\n\\[\n\\bgam^{\\W}_\\ik-\\bga_\\ik\\leq BQ^\\ik-c_\\bal P^\\ik\\leq (BTe^{-\\epsd\\td{\\W}}-c_\\bal)P^\\ik.\n\\]\nAbove, the right hand side is strictly negative as soon as $\\td{\\W}\\geq R_0:=\\frac{1}{\\epsd}\\log\\frac{BT}{c_\\bal}$. Note that, this condition applies to all $(i,k)\\in[n]\\times [K]$ pairs uniformly for the same $R_0$. Consequently, for any $\\td{\\W}\\geq R_0$, for all $i,k$ and $\\W\\in \\con{\\bal}$, we have that $\\bgam^{\\W}_\\ik<\\bga_\\ik$. Additionally, when $\\bal=\\op$, note that $Q^\\ik=0$ since $\\higi=\\emptyset$. Thus, $R_0=0$ suffices to ensure $\\bgam^{\\W}_\\ik<\\bga_\\ik$. Using the strictly-decreasing nature of $\\ell$, we conclude with the fact that for all (finite) $\\W\\in \\con{\\bal}$, \n\\[\n\\Lc(\\W)=\\frac{1}{n}\\sum_{i=1}^n\\sum_{k=1}^K\\ell(\\bgam^{\\W}_\\ik)> \\Lc_\\st=\\frac{1}{n}\\sum_{i=1}^n\\sum_{k=1}^K\\ell(\\bga_\\ik),\n\\]\nwhich implies $\\td{\\wrb{R}}\\rightarrow\\infty$.\n\n\\noindent\\textbf{Step 2:} To proceed, we show that $\\wrb{R}$ converges in direction to $\\Wcs_{\\bal}$. Suppose this is not the case i.e.~convergence fails. We will obtain a contradiction by showing that $\\Wsb_R=R\\cdot\\Wsb$ achieves a strictly superior loss compared to $\\wrb{R}$. Also define the normalized parameter $\\wrt{R}=\\frac{\\wrb{R}}{R\\xdm}$ and $\\W'=\\frac{\\wrb{R}}{\\td{\\wrb{R}}\\xdm}$. Note that $\\wrt{R}$ is obtained by scaling down $\\W'$ since $\\td{\\wrb{R}}\\leq R$ and $\\W'$ obeys $\\td{\\W'}=\\td{\\Wma}$.\n\nSince $\\wrt{R}$ fails to converge to $\\Wcs_{\\bal}$, for some $\\delta>0$, there exists arbitrarily large $R>0$ such that $\\dist{\\wrt{R},\\Wcs_{\\bal}}\\geq \\delta$. This translates to the suboptimality in terms of margin constraints as follows: First, distance with respect to the $\\dm$-norm obeys $\\distd{\\wrt{R},\\Wcs_{\\bal}}\\geq \\delta$ for some updated $\\delta\\gets \\cdm\\delta$. Secondly, using triangle inequality,\n\\[ \n\\text{This implies that either~~~}\\td{\\wrt{R}}\\leq \\td{\\Wma}-\\delta/2\\text{~~~or~~~}\\distd{\\W',\\Wcs_{\\bal}}\\geq \\delta/2.\n\\]\nIn either scenario, $\\wrt{R}$ strictly violates one of the margin constraints of \\eqref{seqattnsvm}: If $\\td{\\wrt{R}}\\leq \\td{\\Wma}-\\delta/2$, then, since the optimal SVM objective is $\\td{\\Wma}$, there exists a constraint $(i,k)$ for which $\\inn{\\Fa_{\\ik}-\\F_{\\ikt},\\wrt{R}}\\leq 1-\\frac{\\delta}{2\\td{\\Wma}}$. If $\\distd{\\W',\\Wcs_{\\bal}}\\geq \\delta/2$, then, $\\W'$ has same SVM objective but it is strictly bounded away from the solution set. Thus, for some $\\eps:=\\eps(\\delta)>0$, $\\W'$ and its scaled down version $\\wrt{R}$ strictly violate an SVM constraint achieving margin $\\leq 1-\\eps$. Without losing generality, suppose $\\wrt{R}$ violates the first constraint. Thus, for a properly updated $\\delta>0$ (that is function of the initial $\\delta>0$) and for $(i,k)=(1,1)$ and some \\nei $\\tau\\in \\Tc_\\oo$,\n\\begin{align}\n\\inn{\\Fa_{\\oo}-\\F_{\\oo t},\\wrt{R}}\\leq 1-\\delta.\\label{margin violate}\n\n\\end{align}\nNow, we will argue that this will lead to a contradiction by proving $\\Lc(\\Wsb_R)<\\Lc(\\wrb{R})$ for sufficiently large $R$.\n\nTo obtain the result, we establish a refined softmax probability control as in Step 1 by studying distance to $\\Lc_\\star$. Following \\eqref{score decomp}, denote the score function at $\\wrb{R}$ via $\\bgam^R_\\ik=\\bgam_\\ik^{\\wrb{R}}$ as shorthand notation. Similarly, let $\\sir_\\ik=\\sft{\\abr_\\ik}$ with $\\abr_\\ik=\\X_i\\wrb{R}\\z_\\ik$. Set the corresponding notation for the reference parameter $\\Wsb_R$ as $\\bgam^\\st_\\ik,\\s^\\st_\\ik,\\ab^\\st_\\ik$. \n\nCritically, recall the above inequalities \\eqref{qikeq} that applies to both $\\W\\in\\{\\wrb{R},\\Wsb_R\\}\\subset\\con{\\bal}$ for an index $(i,k)$ and \\nei $t\\in\\Tc_\\ik$\n\\begin{align}\n\\nonumber \nQ^\\ik&=\\sum_{\\tau\\in\\higi}\\s_\\iktt=D^\\ik\\sum_{\\tau\\in\\higi}e^{\\ab_{\\iktt}} \\\\\n&\\leq D^\\ik Te^{\\ab_\\ikt-\\epsd\\td{\\W}}\\leq Te^{-\\epsd\\td{\\W}}P^\\ik\\leq Te^{-\\epsd\\td{\\W}}(1-\\s_{\\ik\\al_\\ik}), \\label{qik bound}\n\\end{align}\nwhere $P^\\ik=\\sum_{\\tau\\in\\lowi}\\s_{\\iktt}$ and $P^\\ik+Q^\\ik= 1-\\s_{\\ik\\al_\\ik}$. \n\n\nNote that, setting $R_0\\geq \\order{1/\\epsd}=\\order{1/\\eps}$, we guarantee that, for any $(i,k)\\in[n]\\times [K]$\n\\begin{align}\nP^\\ik\\geq Q^\\ik\\implies P^\\ik \\geq 0.5(1-\\s_{\\ik\\al_\\ik}). \\label{pik bound}\n\\end{align}\nAdditionally, when $\\bal=\\op$, note that $Q^\\ik=0$ since $\\higi=\\emptyset$. Thus, $R_0=0$ suffices to ensure \\eqref{pik bound}.\n\nTo proceed, recall that $R\\geq \\td{\\wrb{R}}\\geq R_0$ by definition since $\\wrb{R}\\in \\Ccd$ and recall $\\xdm:=1/\\td{\\Wma}$. Equipped with these, we note the following softmax inequalities on the selected tokens $\\al_\\ik$\n\\begin{align}\n&\\s^\\st_{\\ik\\al_\\ik}\\geq \\frac{1}{1+Te^{-R\\xdm}}\\geq 1-Te^{-R\\xdm}\\quad \\text{for all}\\quad (i,k)\\in[n]\\times [K], \\label{salpha bounds}\\\\\n&s^R_{\\ik\\al_\\ik}\\leq \\frac{1}{1+e^{-(1-\\delta)\\td{\\wrb{R}}\\xdm}}\\leq \\frac{1}{1+e^{-(1-\\delta)R\\xdm}}\\quad\\text{for}\\quad (i,k)=(1,1).\\nn\n\\end{align}\nThe former inequality is thanks to $\\Wma$ achieving $\\geq 1$ margins on all tokens $[T]-\\al_\\ik$ and the latter arises from the $\\delta$-margin violation of $\\wrb{R}$ at $(i,k)=(1,1)$ i.e.~Eq.~\\eqref{margin violate}. Since $\\ell$ is strictly decreasing with Lipschitz gradient and the scores are upper/lower bounded by an absolute constant (as tokens are bounded, $(h_k)_{k=1}^K$ are Lipschitz, and both are fixed), we know that $\\cop\\geq -\\ell'(\\bgam_\\ik^{\\W})\\geq \\cdn$ for some constants $\\cop>\\cdn>0$. Thus, following Eq.~\\eqref{BB eq} and the score decomposition \\eqref{score decomp}, and using \\eqref{qik bound},\\eqref{pik bound},\\eqref{salpha bounds} we can write\n\\begin{align}\n\\nonumber\n\\Lc(\\wrb{R})-\\Lc_\\star&\\geq \\frac{1}{n}[\\ell(\\bgam_\\oo^{\\wrb{R}})-\\ell(\\bga_\\oo)]\\geq \\frac{\\cdn}{n}(\\bga_{\\oo}-\\bgam_\\oo^{\\wrb{R}})\\\\\n&\\geq \\frac{\\cdn}{n}(c_\\bal P^\\oo_{\\wrb{R}}-BQ^\\oo_{\\wrb{R}})\\label{q11 eq}\\\\\n&\\geq \\frac{\\cdn}{n}(1-\\s_{\\oo\\al_\\oo}^R)(0.5c_\\bal -BTe^{-\\epsd \\td{\\wrb{R}}}) \\nonumber\\\\\n\\nonumber\n&\\geq \\frac{\\cdn}{n}\\frac{1}{1+e^{(1-\\delta)R\\xdm}}(0.5c_\\bal-BTe^{-\\epsd R_0}).\n\\end{align}\nAbove, recalling the choice $R_0\\geq \\order{1/\\epsd}=\\order{1/\\eps}$, $R\\geq R_0$ implies $BTe^{-\\epsd R_0}\\leq c_\\bal/4$ to obtain\n\\begin{align}\n\\Lc(\\wrb{R})-\\Lc_\\star\\geq \\frac{\\cdn\\cdot c_\\bal}{4n}\\frac{1}{1+e^{(1-\\delta)R\\xdm}}.\\label{ineq prl}\n\\end{align}\nAdditionally when $\\bal=\\op$, since $Q^\\oo_{\\wrb{R}}=0$ in \\eqref{q11 eq}, the bound above holds with $R_0=0$ by directly using \\eqref{q11 eq}.\n\nConversely, we upper bound the difference between $\\Lc(\\Wsb_R)$ and $\\Lc_\\star$ as follows. Define the worst-case loss difference for $\\wrb{R}$ as $(i',k')=\\arg\\max_{i\\in[n],k\\in[K]}[\\ell(\\bgam_\\ik^\\st)-\\ell(\\bga_\\ik)]$. Using \\eqref{lip score gap}\\&\\eqref{salpha bounds}, we write\n\n\\begin{equation}\n\\begin{aligned}\n\\Lc(\\Wsb_R)-\\Lc_\\star&\\leq \\max_{i\\in[n],k\\in[K]}[\\ell(\\bgam_\\ik^\\st)-\\ell(\\bga_\\ik)]\\leq \\cop\\cdot(\\bga_{i'k'}-\\bgam^\\st_{i'k'})\\\\\n&\\leq \\cop\\cdot(1-\\s_{i'k'\\al_{i'k'}}^\\st)B\\\\\n&\\leq \\cop\\cdot Te^{-R\\xdm}B.\\label{desired Wmm bound}\n\\end{aligned}\n\\end{equation}\nCombining the last inequality and \\eqref{ineq prl}, we conclude that $\\Lc(\\Wsb_R)<\\Lc(\\wrb{R})$ whenever\n\\[\n\\cop T\\cdot e^{-R\\xdm}B<\\frac{\\cdn\\cdot c_\\bal }{4n}\\frac{1}{1+e^{(1-\\delta)R\\xdm}}\\iff \\frac{e^{R\\xdm}}{1+e^{(1-\\delta)R\\xdm}}> \\frac{4\\cop Tn B}{\\cdn c_\\bal }.\n\\]\nThe left hand-side inequality holds for all sufficiently large $R$: Specifically, as soon as $R$ obeys $R>\\frac{1}{\\delta\\xdm}\\log(\\frac{8\\cop Tn B}{\\cdn c_\\bal})$. This completes the proof of the theorem via contradiction as we obtained $\\Lc(\\wrb{R})>\\Lc(\\Wsb_R)$.\n\\end{proof}\n\n\n\\subsection{Global regularization path}\\label{app:global:RP:thm}\n\n\n\n\n\nThe following result is a direct corollary of Theorem \\ref{local RP thm}. Namely, we simply restate the final line of this theorem that applies to optimal tokens.\n\\begin{corollary}[Global Convergence of Regularization Path]\\label{cor gm} Suppose Assumptions \\ref{assum:loss:prope}\\&\\ref{ass cvx seq} hold and the optimal indices $\\op_\\ik=\\arg\\max_{t\\in[T]}\\bgam_\\ikt$ are unique. Consider the global regularization path $\\Wb_{\\dm,R}=\\min_{\\W\\in\\Rcm,\\td{\\W}\\leq R}\\Lc(\\W)$. Let $\\Wcb^\\svm_\\dm$ be the non-empty solution set of \\eqref{seqattnsvm} with $\\bal\\gets\\op$ normalized to have unit $\\dm$-norm. Then\n\\[\n\\lim_{R\\rightarrow\\infty}\\dist{\\frac{\\Wb_{\\dm,R}}{R},\\Wcb^\\svm_\\dm}\n\\]\n\\end{corollary}\n\nThe next corollary directly targets application to \\eqref{serm-w} and \\eqref{serm-kq}. This corollary is also a strict generalization of Theorem \\ref{thm global reg path}. Specifically, we immediately recover Theorem \\ref{thm global reg path} by specializing this to the single-output setting $K\\gets 1$ and full-dimensional parameterization $m\\gets d$.\n\\begin{corollary}\\label{cor global reg path} Suppose Assumptions \\ref{assum:loss:prope}\\&\\ref{ass cvx seq} hold and the optimal indices $\\op_\\ik=\\arg\\max_{t\\in[T]}\\bgam_\\ikt$ are unique. Consider the regularization paths associated to \\eqref{serm-w} and \\eqref{serm-kq}:\n\\begin{align}\n&\\Wb_R=\\underset{\\W\\in\\Rcm,\\tf{\\W}\\leq R}{\\arg\\min}\\Lc(\\W)\\quad\\text{and}\\quad \\Kbb_R,\\Qbb_R=\\underset{\\tf{\\Kb}^2+\\tf{\\Qb}^2\\leq 2R}{\\arg\\min}\\Lc(\\Kb,\\Qb)\n\\end{align}\nSuppose \\eqref{seqattnsvm} is feasible for $\\bal\\gets\\op$. Let $\\Ws$ be the unique solution of \\eqref{seqattnsvm} with Frobenius norm and $\\Wc^{\\svm}_\\star$ be the solution set of \\eqref{seqattnsvm} with nuclear norm and cost function $\\tnuc{\\Wc^{\\svm}_\\star}$. We have that\n\n\\[\n\\lim_{R\\rightarrow\\infty} \\frac{\\Wb_R}{R}=\\frac{\\Ws}{\\tf{\\Ws}},\\quad\\lim_{R\\rightarrow\\infty} \\dist{\\frac{\\Qbb_R\\Kbb_R^\\top}{R},\\frac{\\Wcs_\\star}{\\tnuc{\\Wc^{\\svm}_\\star}}}=0.\n\\]\n\\end{corollary}\n\\begin{proof} We directly apply Corollary \\ref{cor gm} with $\\dm=F$ and $\\dm=\\star$ respectively. To obtain the result on $\\Wb_R$, we note that $\\Ws$ is unique because Frobenius norm-squared is strongly convex. To obtain the result on $(\\Qbb_R,\\Kbb_R)$, we use Lemma \\ref{kqw mapping} and observe that\n\\[\n\\Wb_{\\st,R}:=\\Qbb_R\\Kbb_R^\\top\\in\\underset{\\W\\in\\Rcm,\\tnuc{\\W}\\leq R}{\\arg\\min}\\Lc(\\W).\n\\]\nWe then apply Corollary \\ref{cor gm} with $\\dm=\\star$ to conclude with the convergence of the path $ \\Wb_{\\st,R}$.\n\\end{proof}\n\n\n\\subsubsection{Proof of Theorem \\ref{thm global reg path}}\n\\input{supp/reg_path_glob}\n\n==== END OF /2308.16898/supp/reg_path_analysis.tex ====\n==== BEGINNING OF /2308.16898/supp/reg_path.tex ====\n\n\n\\section{Convergence of Regularization Path for Sequence-to-Sequence Setting}\\label{sec multioutput}\n\n\n\n\n\n\nIn this section, we provide proofs for the regularization path analysis. We first provide a more general formulation of the optimization problem that allows for regressing multiple token outputs. To distinguish from \\eqref{eqn:erm:w}\\&\\eqref{eqn:erm:kq}, let us call this more general version Sequence Empirical Risk Minimization (SERM).\n\n\n\n\\noindent\\textbf{Problem definition:} \n\nRather than a single input sequence $\\X$, let us allow for two input sequences $\\X\\in\\R^{T\\times d}$ and $\\Z\\in\\R^{K\\times d}$ with $(\\x_t)_{t=1}^T$ and $(\\z_k)_{k=1}^K$. The cross-attention admits $\\X,\\Z$ and outputs $K$ tokens. We will also allow for $K$ separate prediction heads $(h_k)_{k=1}^K$ for individual cross attention outputs which strictly generalizes the setting where we used single prediction head $h(\\cdot)$. Denote the training labels associated to each token as $\\Y=(Y_\\ik)_{i=1}^K$. Given $n$ samples $(\\Y_i,\\X_i,\\Z_i)_{i=1}^n$, for a decreasing loss function $\\ell(\\cdot)$, minimize the empirical risk by the prediction of first attention output either univariate ($\\W\\in\\R^{d\\times d}$) or bivariate ($\\Kb,\\Qb\\in\\R^{d\\times m}$) fashion:\n\\begin{align}\n\\Lc(\\W)&=\\frac{1}{n}\\sum_{i=1}^n\\sum_{k=1}^K \\ell(Y_\\ik\\cdot h_k( \\X_i^\\top \\sft{\\X_i\\W\\z_\\ik})),\\label{serm-w}\\tag{SERM-W}\\\\\n\\Lc(\\Kb,\\Qb)&=\\frac{1}{n}\\sum_{i=1}^n \\ell(Y_\\ik\\cdot h_k( \\X_i^\\top \\sft{\\X_i\\Kb\\Qb^\\top\\z_\\ik})).\\label{serm-kq}\\tag{SERM-KQ}\n\\end{align}\nIn order to recover the single-output self-attention model, we can simply set $K=1$ and $\\z_{i1}\\gets\\x_{i1}$ and $h_k\\gets h$. To proceed, we introduce the more general version of the \\eqref{eqn:sattnsvm} problem, which we refer to as \\emph{Sequential Cross-Attention SVM} to preserve consistent phrasing. Suppose $\\Kb,\\Qb\\in\\R^{d\\times m}$ with $m\\leq d$ and let $\\Rcm$ denote the set of rank-$m$ matrices in $\\R^{d\\times d}$. Given indices $\\bal=(\\aik)_{\\ik=(1,1)}^{(n,K)}$, consider the SVM with $\\dm$-norm constraint\n\n\\begin{equation}\\tag{S\\name}\n \\Wm_{\\dm,\\bal}\\in\\arg\\min_{\\W\\in\\Rcm}\\|\\W\\|_{\\diamond}\n \\quad \\text{s.t.}   \\quad \\min_{t\\neq \\aik}(\\x_{i\\aik}-\\x_\\itt)^\\top\\W\\z_\\ik\\geq 1\\quad\\forall\\quad i\\in[n],k\\in[K]\\label{seqattnsvm}.\n\\end{equation}\n\nWhen solution is non-unique, we denote the solution set by $\\Wcs(\\bal)$. In what follows, we denote $\\F_\\ikt:=\\x_\\itt\\z_{\\ik}^\\top$ and given $\\bal$, we denote $\\xa_\\ik=\\x_{i\\alpha_\\ik}$ and $\\Fa_\\ik:=\\x_{i\\alpha_\\ik}\\z_{\\ik}^\\top$. With this notation, we can equivalently write\n\n\\begin{equation}\\tag{S\\name'}\n \\Wm_{\\dm,\\bal}\\in\\arg\\min_{\\W\\in\\Rcm}\\|\\W\\|_{\\diamond}\n \\quad \\text{s.t.} \\quad \\min_{t\\neq \\aik}\\inn{\\Fa_\\ik-\\F_\\ikt,\\W}\\geq 1\\quad\\forall~ i\\in[n],k\\in[K]\\label{seqattnsvm2}.\n\\end{equation}\n\n\n\\begin{definition}[\\Neis and Locally-Optimal Indices]\\label{seq loc opt} Fix token indices $\\bal=(\\alpha_\\ik)\\ikix$ for which \\eqref{seqattnsvm} is feasible to obtain $\\Wma:= \\Wm_{\\dm,\\bal}$. Define token scores as\n\\[\n\\bgam_\\ikt=Y_\\ik\\cdot h_k(\\x_\\itt),\\quad \\bga_\\ik:=\\bgam_{ik\\alpha_\\ik}=Y_\\ik\\cdot h_k(\\xa_\\ik).\n\\]\nConsider tokens $\\Tc_\\ik\\subset[T]$ such that $\\inn{\\Fa_\\ik-\\F_\\ikt,\\Wma}=1$ for all $t\\in\\Tc_\\ik$. $\\Tc_\\ik$ is allowed to be an empty set. We refer to $\\Tc_\\ik$ as \\neis of $\\Fa_\\ik=\\xa_\\ik\\z_\\ik^\\top$ and define its complement $\\Tcb_\\ik=[T]-\\Tc_\\ik-\\{\\al_\\ik\\}$.  Additionally, token indices $\\bal=(\\alpha_\\ik)\\ikix$ are called locally-optimal if for all $i\\in[n],k\\in[K]$ and  $t\\in\\Tc_\\ik$, token scores obey $\\bga_\\ik>\\bgam_\\ikt$. Associated $\\Wma$ is called a locally-optimal direction. Finally, let $\\op_\\ik\\in\\arg\\max_{t\\in[T]}\\bgam_\\ikt$ be the optimal indices and define the associated $\\Ws(\\op)$ to be a globally-optimal direction.\n\\end{definition}\n\n\\begin{lemma}[Mapping regularization path of $(\\Kb,\\Qb)$ to $\\W$]\\label{kqw mapping} Let $\\Kb,\\Qb\\in\\R^{d\\times m}$ and consider regularization path solutions of \\eqref{serm-w} and \\eqref{serm-kq}\n\\begin{align}\n&\\Wb_R\\in\\underset{\\W\\in\\Rcm:\\tnuc{\\W}\\leq R}{\\arg\\min}\\Lc(\\W)\\label{Wpath}\\\\\n&\\Kbb_R,\\Qbb_R\\in\\underset{\\tf{\\Kb}^2+\\tf{\\Qb}^2\\leq 2R}{\\arg\\min}\\Lc(\\Kb,\\Qb).\\label{KQpath}\n\\end{align}\nFor all $R\\geq 0$, there is a one-to-one map between the set of solutions $\\Wb_R$ of \\eqref{Wpath} and $\\Kbb_R\\Qbb_R^\\top$ of \\eqref{KQpath}.\n\n\\end{lemma}\n\\begin{proof} To prove the mapping, first fix a $\\Wb_R$ solution with rank $m$, set $\\Lc_F=\\Lc(\\Wb_R)$ and show the existence of $\\Kb,\\Qb$ with $\\Kb\\Qb^\\top=\\Wb_R$ feasible for \\eqref{KQpath} and $\\Lc(\\Kb,\\Qb)\\leq \\Lc_F$. Use the singular value decomposition $\\Wb_R=\\Ub\\bSi\\Vb^\\top$ with $\\bSi\\in\\R^{m\\times m}$ being diagonal matrix of singular values. Set $\\Kb=\\Ub\\sqrt{\\bSi}$ and $\\Qb=\\Vb\\sqrt{\\bSi}$. Observe that $\\Kb\\Qb^\\top=\\W$ and\n\\[\n\\tf{\\Kb}^2=\\tf{\\Qb}^2=\\sum_{i=1}^m\\sqrt{\\bSi_{ii}}^2=\\tnuc{\\Wb_R}\\leq R.\n\\]\nThus, $\\Kb,\\Qb$ achieves $\\Lc(\\Kb,\\Qb)=\\Lc_F$. Conversely, given $\\Kbb_R,\\Qbb_R$ with $\\Lc_\\st=\\Lc(\\Kbb_R,\\Qbb_R)$, $\\W=\\Kbb_R\\Qbb_R^\\top$ obeys $\\Lc(\\W)=\\Lc_\\st$ and, using the standard nuclear norm inequality, we have\n\\[\n\\tnuc{\\W}=\\tnuc{\\Kbb_R\\Qbb_R^\\top}\\leq \\frac{1}{2}(\\tf{\\Kbb_R}^2+\\tf{\\Qbb_R}^2)=R.\n\\]\nThis shows $\\W$ is feasible for \\eqref{Wpath}. Combining the two findings above, we find that optimal costs are equal ($\\Lc_\\st=\\Lc_F$) and for any $(\\Kbb_R,\\Qbb_R)$ solution there exists a $\\Wb_R$ solution and vice versa.\n\\end{proof}\n\n\n==== END OF /2308.16898/supp/reg_path.tex ====\n==== BEGINNING OF /2308.16898/supp/proof_convergence.tex ====\n\n\\subsection{Proof of Lemma~\\ref{lem:lip}}\n\nLet\n\\begin{equation*}\n\\bgam_i=Y_i\\cdot \\X_i\\vb, \\quad \n \\hb_i=\\X_i\\W \\z_{i}.\n\\end{equation*}\nFrom Assumption~\\ref{assum:loss:prope}, $\\ell:\\R\\rightarrow\\R$ is differentiable. Hence,  the gradient evaluated at $\\W$ is given by\n\\begin{equation}\\label{grad def}\n\\nabla\\Lc(\\W)=\\frac{1}{n}\\sum_{i=1}^n \\ell' \\left(\\bgam_i^\\top \\sft{\\hb_i}\\right)\\cdot \\X_i^\\top  \\sfp{\\hb_i}  \\bgam_i  \\z_{i}^\\top,\n\\end{equation}\nwhere\n\\begin{equation}\\label{eqn:der:soft}\n\\sfp{\\hb} = \\text{diag}\\left(\\sft{\\hb}\\right) - \\sft{\\hb} \\sft{\\hb}^\\top \\in \\R^{T\\times T}.    \n\\end{equation}\nNote that \n\\begin{equation}\\label{eqn:sprime:bnorm}\n \\| \\sfp{\\hb} \\| \\leq \\| \\sfp{\\hb} \\|_F \\leq  1.  \n\\end{equation}\n\nHence,  for any $\\W,\\dot{\\W}\\in \\R^{d\\times d}$, $i\\in[n]$,  we have\n\\begin{subequations}\n\\begin{align}\\label{eqn:soft:lipcons1}\n\\left\\|\\sft{\\hb_i}-\\sft{\\dot{\\hb}_i}\\right\\| \\leq \\left\\|\\hb_i-\\dot{\\hb}_i\\right\\| \\leq \\|\\X_i\\|~\\|\\z_i\\|~\\left\\|\\W-\\dot{\\W}\\right\\|_F,\n\\end{align}\nwhere $\\dot{\\hb}_i=\\X_i\\dot{\\W} \\z_{i}$.\n\nSimilarly, \n\\begin{align}\\label{eqn:soft:lipcons2}\n\\nonumber\n\\left\\|\\sfp{\\hb_i}-\\sfp{\\dot{\\hb}_i}\\right\\|_F & \\leq \\left\\|\\sft{\\hb_i} - \\sft{\\dot{\\hb_i}}\\right\\| +   \\left\\|\\sft{\\hb_i} \\sft{\\hb_i}^\\top- \\sft{\\dot{\\hb_i}} \\sft{ \\dot{\\hb_i}}^\\top\\right\\|_F\n\\\\\n & \\leq 3 \\|\\X_i\\|~\\|\\z_i\\|~\\left\\|\\W-\\dot{\\W}\\right\\|_F.\n\\end{align}\n\\end{subequations}\nNext, for any $\\W,\\dot{\\W}\\in\\R^{d\\times d}$, we get\n\\begin{align}\\label{eqn:obj:lipcons}\n  \\nonumber\n \\left\\|\\nabla \\mc{L}(\\W)-\\nabla \\mc{L}(\\dot{\\W})\\right\\|_F\n  &\\leq \\frac{1}{n}  \\sum_{i=1}^n\\left\\| \\ell' \\left(\\bgam_i^\\top \\sft{\\hb_i}\\right) \\cdot \\z_{i}  \\bgam_i^\\top \\sfp{\\hb_i} \\X_i - \\ell' \\left(\\bgam_i^\\top \\sft{\\dot{\\hb}_i}\\right) \\cdot \\z_{i}  \\bgam_i^\\top \\sfp{\\dot{\\hb}_i} \\X_i \\right\\|_F\\\\ \n    \\nonumber\n       & \\le \\frac{1}{n}\\sum_{i=1}^{n}  \\left\\|\\z_{i}  \\bgam_i^\\top \\sfp{\\dot{\\hb}_i} \\X_i \\right\\|_F~\\left| \\ell' \\left(\\bgam_i^\\top \\sft{\\hb_i}\\right) - \\ell' \\left(\\bgam_i^\\top \\sft{\\dot{\\hb}_i}\\right)  \\right| \\\\\n         \\nonumber\n       &+ \\frac{1}{n}\\sum_{i=1}^{n} \\left|  \\ell' \\left(\\bgam_i^\\top \\sft{\\hb_i}\\right)\\right|~\\left\\| \\z_{i}  \\bgam_i^\\top \\sfp{\\hb_i} \\X_i - \\z_{i}  \\bgam_i^\\top \\sfp{\\dot{\\hb}_i} \\X_i \\right\\|_F \\\\\n         \\nonumber\n       & \\le \\frac{1}{n}\\sum_{i=1}^{n}  M_0 ~\\|\\bgam_i\\|^2~\\|\\z_i\\| ~\\|\\X_i\\|~\\left\\|\\sft{\\hb_i}-\\sft{\\dot{\\hb}_i}\\right\\| \\\\\n       & +   \\frac{1}{n}\\sum_{i=1}^{n}  M_1  ~\\|\\bgam_i\\|~\\|\\z_i\\|~\\|\\X_i \\|~\\left\\|\\sfp{\\hb_i}-\\sfp{\\dot{\\hb}_i}\\right\\|_F,\n\\end{align}\nwhere the second inequality follows from the fact that $|ab - cd| \\leq |d||a-c|+ |a||b-d|$ and the third inequality uses Assumption~\\ref{assum:loss:prope} and \\eqref{eqn:sprime:bnorm}.\n\nSubstituting \\eqref{eqn:soft:lipcons1}  and \\eqref{eqn:soft:lipcons2} into \\eqref{eqn:obj:lipcons}, we get\n\\begin{align*}\n\\left\\|\\nabla \\mc{L}(\\W)-\\nabla \\mc{L}(\\dot{\\W})\\right\\|_F &\\leq  \\frac{1}{n}\\sum_{i=1}^{n} \\left( M_0  ~\\|\\bgam_i\\|^2\\|\\z_i\\|^2\\|\\X_i\\|^2+ 3  M_1 \\|\\bgam_i\\|~\\|\\z_i\\|^2~\\|\\X_i \\|^2\\right)  \\|\\W-\\dot{\\W}\\|_F\\\\\n &\\leq  \\frac{1}{n}\\sum_{i=1}^{n} \\left( M_0  ~\\|\\vb\\|^2\\|\\z_i\\|^2\\|\\X_i\\|^4+ 3  M_1 \\|\\vb\\|~\\|\\z_i\\|^2~\\|\\X_i \\|^3\\right)  \\|\\W-\\dot{\\W}\\|_F\\\\\n&\\leq  L_{\\W}~\\|\\W-\\dot{\\W}\\|_F,\n\\end{align*}\nwhere $L_{\\W}$ is defined in \\eqref{eqn:lip:cons:erm}.\n\n\nLet $\\g_{i}=\\X_i\\Kb \\Qb^\\top\\z_{i}$. We have\n\\begin{subequations}\\label{grad def KQ}\n\\begin{align}\n\\nabla_{\\Kb} \\Lc(\\Kb,\\Qb)=\\frac{1}{n}\\sum_{i=1}^n \\ell' \\left(\\bgam_i^\\top \\sft{\\g_i}\\right) \\cdot  \\z_{i}  \\bgam_i^\\top \\sfp{\\g_i} \\X_i \\Qb,\\\\\n\\nabla_{\\Qb} \\Lc(\\Kb,\\Qb)=\\frac{1}{n}\\sum_{i=1}^n \\ell' \\left(\\bgam_i^\\top \\sft{\\g_i}\\right) \\cdot \\X_i^\\top  \\sfp{\\g_i}  \\bgam_i  \\z_{i}^\\top \\Kb.\n\\end{align}\n\\end{subequations}\nBy the similar argument as in \\eqref{eqn:obj:lipcons}, for any $\\Qb$ and $\\dot\\Qb\\in\\R^{d\\times m}$, we have\n\\begin{align}\\label{eqn:objqk:lipcons}\n  \\nonumber\n \\left\\|\\nabla_{\\Qb} \\mc{L}(\\Kb,\\Qb)-\\nabla_{\\Qb} \\mc{L}(\\Kb,\\dot{\\Qb})\\right\\|_F\n  &\\leq \\frac{\\|\\Kb\\|}{n}  \\sum_{i=1}^n\\left\\| \\ell' \\left(\\bgam_i^\\top \\sft{\\hb_i}\\right) \\cdot \\z_{i}  \\bgam_i^\\top \\sfp{\\hb_i} \\X_i - \\ell' \\left(\\bgam_i^\\top \\sft{\\dot{\\hb}_i}\\right) \\cdot \\z_{i}  \\bgam_i^\\top \\sfp{\\dot{\\hb}_i} \\X_i \\right\\|_F\\\\ \n       & \\leq L_{\\W} \\|\\Kb\\| ~\\|\\Qb-\\dot{\\Qb}\\|_F.\n\\end{align}\nSimilarly, for any $\\Kb,\\dot\\Kb\\in\\R^{d\\times m}$, we get   \n$$\n\\left\\|\\nabla_{\\Kb} \\mc{L}(\\Kb,\\Qb)-\\nabla_{\\Kb} \\mc{L}(\\dot{\\Kb},\\Qb)\\right\\|_F \\leq  L_{\\W} \\|\\Qb\\| ~\\|\\Kb-\\dot{\\Kb}\\|_F.\n$$\n$\\qed$\n\n\n\\subsection{A useful lemma for gradient descent analysis}\n\\begin{lemma}\\label{lem:q_reduce} For any $\\X \\in\\R^{T\\times d}$, $\\W,\\V \\in \\R^{d\\times d}$ and $\\z, \\vb \\in \\R^{d}$, let $\\ab= \\X\\V \\z$, $\\s=\\sft{\\X\\W\\z}$, and $\\bgam=\\X\\vb$. Set\n\\begin{equation*}\n\\Gamma=\\sup_{t,\\tau\\in[T]}|\\bgam_t-\\bgam_\\tau|~~~\\textnormal{and}~~~A=\\sup_{t\\in[T]}\\tn{\\ab_t}.\n\\end{equation*}\nWe have that\n  \\[\n    \\left|\\ab^\\top\\textnormal{diag}(\\s) \\bgam-\\ab^\\top\\s\\s^\\top\\bgam-\\sum_{t\\geq 2}^T (\\ab_1-\\ab_t)\\s_t(\\bgam_1-\\bgam_t)\\right|\\leq 2\\Gamma A(1-\\s_1)^2.\n  \\]\n\\end{lemma}\n\n\n \\begin{proof}\nThe proof is similar to \\cite[Lemma~4]{tarzanagh2023margin}, but for the sake of completeness, we provide it here.  Set $\\gamb=\\sum_{t=1}^T \\bgam_t\\s_t$.  We have \n\\begin{align*}\n\\bgam_1-\\gamb=\\sum_{t\\geq 2}^T (\\bgam_1-\\bgam_t)\\s_t,~~\\textnormal{and}~~|\\bgam_1-\\gamb|\\leq \\Gamma (1-\\s_1).\n\\end{align*}    \nThen,\n  \\begin{align} \n  \\nonumber \n    \\ab^\\top\\diag{\\s}\\bgam-\\ab^\\top\\s\\s^\\top\\bgam&=\\sum_{t=1}^T \\ab_t\\bgam_t\\s_t-\\sum_{t=1}^T \\ab_t\\s_t\\sum_{t=1}^T \\bgam_t\\s_t\\\\\n    &=\\ab_1\\s_1(\\bgam_1-\\gamb)-\\sum_{t\\geq 2}^T\\ab_t\\s_t(\\gamb-\\bgam_t). \\label{grad def step3}\n  \\end{align}\nSince \n$$\n\\left|\\sum_{t\\geq 2}^T\\ab_t\\s_t(\\gamb-\\bgam_t)-\\sum_{t\\geq 2}^T\\ab_t\\s_t(\\bgam_1-\\bgam_t)\\right|\\leq A\\Gamma (1-\\s_1)^2,\n$$\nwe obtain\\footnote{For simplicity, we use $\\pm$ on the right hand side to denote the upper and lower bounds.}\n  \\begin{align*}  \n    \\ab^\\top\\diag{\\s}\\bgam-\\ab^\\top\\s\\s^\\top\\bgam&=\\ab_1\\s_1(\\bgam_1-\\gamb)-\\sum_{t\\geq 2}^T\\ab_t\\s_t(\\bgam_1-\\bgam_t)\\pm A\\Gamma (1-\\s_1)^2\\\\\n    &=\\ab_1\\s_1\\sum_{t\\geq 2}^T (\\bgam_1-\\bgam_t)\\s_t-\\sum_{t\\geq 2}^T\\ab_t\\s_t(\\bgam_1-\\bgam_t)\\pm A\\Gamma (1-\\s_1)^2\\\\\n    &=\\sum_{t\\geq 2}^T (\\ab_1\\s_1-\\ab_t)\\s_t(\\bgam_1-\\bgam_t)\\pm A\\Gamma (1-\\s_1)^2\\\\\n    &=\\sum_{t\\geq 2}^T (\\ab_1-\\ab_t)\\s_t(\\bgam_1-\\bgam_t)\\pm 2A\\Gamma (1-\\s_1)^2.\n    \n  \\end{align*}\nHere,  $\\pm$ on the right handside uses the fact that\n  \\[\n  \\left|\\sum_{t\\geq 2}^T (\\ab_1\\s_1-\\ab_1)\\s_t(\\bgam_1-\\bgam_t)\\right|\\leq (1-\\s_1)A\\Gamma\\sum_{t\\geq 2}^T\\s_t=(1-\\s_1)^2A\\Gamma.\n  \\]\n\\end{proof}\n\n\n\n\n\\section{Global Convergence of Gradient Descent}\\label{app:sec:gd:global}\n\n\\subsection{Divergence of norm of the iterates}\n\nThe next lemma establishes the descent property of gradient descent for $\\mathcal{L}(\\W)$ under Assumption \\ref{assum:loss:prope}. \n\n\\begin{lemma}[Descent Lemma]\\label{lem:grad:descent}\nUnder Assumption \\ref{assum:loss:prope}, if $\\eta \\leq 1/L_{\\W}$, then for any initialization $\\W(0)$, Algorithm~\\ref{GD-W} satisfies:\n\\begin{align}\\label{eq:descent:obj new}\n\\mathcal{L}(\\W(k+1))-\\mathcal{L}(\\W(k))\\leq-\\frac{\\eta}{2} \\tf{\\nabla \\mathcal{L}(\\W(k))}^2,\n\\end{align}\nfor all $k\\ge0$. Additionally, it holds that $\\sum_{k=0}^{\\infty} \\tf{\\nabla\\mathcal{L}\\left(\\W(k)\\right)}^{2}<\\infty$, and $\\lim_{k\\rightarrow \\infty}\n\\tf{\\nabla\\mathcal{L}\\left(\\W\\left(k\\right)\\right)}^{2}=0$.\n\\end{lemma}\n\\begin{proof}\nThe proof is similar to \\cite[Lemma~6]{tarzanagh2023margin}.\n\n\\end{proof}\n\nThe lemma below reveals that the correlation between the training loss's gradient at any arbitrary matrix $\\W$ and the attention SVM solution  $ \\Wm$ is negative. Consequently, for any finite $\\W$, $\\li\\nabla\\Lc(\\W), \\Wm\\ri$ cannot be equal to zero.\n\n\\begin{lemma}\\label{global des lem} \nLet $ \\Wm$ be the SVM solution of \\eqref{eqn:sattnsvm}. Suppose Assumptions \\ref{assum:loss:prope} and \\ref{assum:token} hold.  Then,  for all $\\W\\in\\R^{d\\times d}$, the training loss \\eqref{eqn:erm:w} obeys $\\li\\nabla\\Lc(\\W),\\Wm\\ri<0$. \n\\end{lemma}\n\n\\begin{proof}\nLet\n\\begin{equation}\n\\hbm_i=  \\X_{i} \\Wm \\z_i, ~~~\\bgam_i=Y_i\\cdot \\X_i\\vb,~~~\\textnormal{and}~~~\n \\hb_i=\\X_i\\W \\z_{i}.    \n\\end{equation}\nLet us recall the gradient evaluated at $\\W$ which is given by \n\\begin{align}\\label{grad def new}\n\\nabla\\Lc(\\W)=\\frac{1}{n}\\sum_{i=1}^n\\ell' \\left(\\bgam_i^\\top \\sft{\\hb_i}\\right) \\cdot \\X_i^\\top  \\sfp{\\hb_i}  \\bgam_i  \\z_{i}^\\top,\n\\end{align}\n which implies that \n\\begin{equation}\\label{eqn:grad:prod:p}\n    \\begin{split}\n\\li\\nabla\\Lc(\\W),\\Wm\\ri&= \\frac{1}{n}\\sum_{i=1}^n \\ell' \\left(\\bgam_i^\\top \\sft{\\hb_i}\\right)  \\cdot \\iprod{ \\X_i^\\top  \\sfp{\\hb_i}  \\bgam_i  \\z_{i}^\\top}{\\Wm}\\\\\n&= \\frac{1}{n}\\sum_{i=1}^n\n\\ell'_i  \\cdot \\tr\\left(  (\\Wm)^\\top  \\X_{i}^\\top \\sfp{\\hb_i} \\bgam_i  \\z_{i}^\\top\\right)\\\\\n\n\n\n\n\n\n&= \\frac{1}{n}\\sum_{i=1}^n\n\\ell'_i \\cdot \\hbm_{i}^\\top \\sfp{\\hb_i} \\bgam_i \\\\\n&=  \\frac{1}{n}\\sum_{i=1}^n\n\\ell'_i \\cdot  \\left(\\hbm^\\top_i\\diag{\\s_i}\\bgam_i-\\hbm^\\top_i\\s_i\\s^\\top_i\\bgam_i\\right).        \n    \\end{split}\n\\end{equation}\nHere, let $\\ell'_i:=\\ell'(\\bgam_i^\\top\\sft{\\hb_i})$, $\\s_i=\\sft{\\hb_i}$ and the third equality uses $\\tr\\left(\\bb\\ab^\\top\\right) = \\ab^\\top \\bb$.\n\nIn order to move forward, we will establish the following result, with a focus on the equal score condition (Assumption B.2 [in <a href=\"https://arxiv.org/pdf/2308.16898#Item.5\">original paper</a>]): Let $\\gamma=\\bgam_{t\\geq 2}$ be a constant, and let $\\bgam_1$ and $\\bar{\\hb}_1$ represent the largest indices of vectors $\\bgam$ and $\\hbm$ respectively. For any vector $\\s$ that satisfies $\\sum_{t\\in[T]}\\s_t=1$ and $\\s_t> 0$, we aim to prove that $\\hbm^\\top\\diag{\\s}\\bgam-\\hbm^\\top\\s\\s^\\top\\bgam>0$. To demonstrate this, we proceed by writing the following:\n\\begin{equation}\\label{grad def2}\n\\begin{split}\n\\hbm^\\top\\diag{\\s}\\bgam-\\hbm^\\top\\s\\s^\\top\\bgam&=\\sum_{t=1}^T \\bar{\\hb}_t\\bgam_t \\s_t-\\sum_{t=1}^T  \\bar{\\hb}_t \\s_t\\sum_{t=1}^T \\bgam_t \\s_t\\\\\n&=\\left(\\bar{\\hb}_1\\bgam_1\\s_1+\\gamma\\sum_{t\\geq 2}^T\\bar{\\hb}_t\\s_t\\right)-\\Big(\\bgam_1\\s_1+\\gamma(1-\\s_1)\\Big)\\left(\\bar{\\hb}_1\\s_1+\\sum_{t\\geq 2}^T \\bar{\\hb}_t\\s_t\\right)\\\\\n&=\\bar{\\hb}_1(\\bgam_1-\\gamma) \\s_1(1-\\s_1)-(\\bgam_1-\\gamma)\\s_1\\sum_{t\\geq 2}^T \\bar{\\hb}_t \\s_t\\\\\n&=(\\bgam_1-\\gamma)(1- \\s_1) \\s_1\\left[\\bar{\\hb}_1-\\frac{ \\sum_{t\\geq 2}^T \\bar{\\hb}_t \\s_t}{\\sum_{t\\geq 2}^T\\s_t}\\right]\\\\\n&\\geq(\\bgam_1-\\gamma)(1- \\s_1) \\s_1 (\\bar{\\hb}_1-\\max_{t\\geq 2}\\bar{\\hb}_t).\n\\end{split}\n\\end{equation}\nTo proceed, define\n\\begin{equation*}\n\\bgag^i=\\bgam_{i\\opt_i}-\\max_{t\\neq\\opt_i}\\bgam_{it}~~\\textnormal{and}~~\\bhbg^i=\\bar \\hb_{i\\opt_i}-\\max_{t\\neq\\opt_i}\\bar \\hb_{it}.\n\\end{equation*}\nWith these, we obtain \n\n\n\n\\begin{equation}\\label{eqn:al:lem}\n\n\\hbm^\\top_i\\diag{\\s_i}\\bgam_i-\\hbm^\\top_i\\s_i\\s^\\top_i\\bgam_i\\geq\\bgag^i\\bhbg^i(1-\\s_{i\\opt_i})\\s_{i\\opt_i}.\n\\end{equation}\nNote that \n\\begin{equation*}\n\\begin{split}\n& \\bhbg^i=\\min_{t\\neq \\opt_i}~(\\x_{i \\opt_i}-\\x_{it})^\\top\\Wm \\z_i \\geq1,  \\\\\n&\\bgag^i=\\min_{t\\neq \\opt_i}~\\bgam_{i\\opt_i}-\\bgam_{it} >0,\\\\\n&\\s_{i\\opt_i}(1-\\s_{i\\opt_i}) > 0.    \n\\end{split}\n\\end{equation*}\n\\begin{comment}\n    Hence,\n\\begin{equation}\\label{eqn:lower}\nc_0:=\\min_{i \\in [n]}\\left\\{ \\left(\\min_{t\\neq \\opt_i}~(\\x_{i \\opt_i}-\\x_{it})^\\top\\Wm \\z_i\\right) \\cdot \\left(\\min_{t\\neq \\opt_i}~\\bgam_{i\\opt_i}-\\bgam_{it}\\right) \\cdot \\s_{i\\opt_i}(1-\\s_{i\\opt_i}) \\right\\}>0.\n\\end{equation}\nIt follows from  \\eqref{eqn:al:lem} and \\eqref{eqn:lower} that \n\\begin{equation}\\label{eqn:al:lem:2}\n\n\\min_{i \\in [n]}\\left\\{\\hbm^\\top_i\\diag{\\s_i}\\bgam_i-\\hbm^\\top_i\\s_i\\s^\\top_i\\bgam_i \\right\\} \\geq c_0>0.\n\\end{equation}\nFurther, by our assumption $\\ell'_i<0$.  \nSince by Assumption \\ref{assum:loss:prope}, $\\ell'$ is continuous and the domain is bounded, the maximum is attained and negative, and thus  \n\\begin{equation}\\label{eqn:bound:lprim}\n-c_1= \\max_{x} \\ell'(x), \\qquad  \\textnormal{for some} \\quad c_1>0.     \n\\end{equation}\nHence, using \\eqref{eqn:al:lem:2} and \\eqref{eqn:bound:lprim} in  \\eqref{eqn:grad:prod:p}, we obtain \n\\begin{equation}\\label{eqn:grad:prod:p:fin}\n    \\begin{split}\n\\li\\nabla\\Lc(\\W),\\Wm\\ri \\leq  - c<0, \\quad \\textnormal{where} \\quad c=c_1 \\cdot c_0.\n    \\end{split}\n\\end{equation}\n\n\n\n\n\n\n\n\n\nIn the scenario that Assumption~\\ref{assum:token:supp} holds (all tokens are support), $\\hbm_t=\\x_{it}^\\top\\Wm \\z_i $ is constant for all $t\\geq 2$. Hence, following similar steps as in \\eqref{grad def2} completes the proof. \n\\end{comment}\nHence,\n\\begin{equation}\\label{eqn:lower}\n\\min_{i \\in [n]}\\left\\{ \\left(\\min_{t\\neq \\opt_i}~(\\x_{i \\opt_i}-\\x_{it})^\\top\\Wm \\z_i\\right) \\cdot \\left(\\min_{t\\neq \\opt_i}~\\bgam_{i\\opt_i}-\\bgam_{it}\\right) \\cdot \\s_{i\\opt_i}(1-\\s_{i\\opt_i}) \\right\\}>0.\n\\end{equation}\nIt follows from  \\eqref{eqn:al:lem} and \\eqref{eqn:lower} that \n\\begin{equation}\\label{eqn:al:lem:2}\n\n\\min_{i \\in [n]}\\left\\{\\hbm^\\top_i\\diag{\\s_i}\\bgam_i-\\hbm^\\top_i\\s_i\\s^\\top_i\\bgam_i \\right\\}>0.\n\\end{equation}\nFurther, by Assumption \\ref{assum:loss:prope}, $\\ell'_i<0$, $\\ell'$ is continuous and the domain is bounded, the maximum is attained and negative, and thus  \n\\begin{equation}\\label{eqn:bound:lprim}\n\\max_{x} \\ell'(x)<0.    \n\\end{equation}\nHence, using \\eqref{eqn:al:lem:2} and \\eqref{eqn:bound:lprim} in  \\eqref{eqn:grad:prod:p}, we obtain \n\\begin{equation}\\label{eqn:grad:prod:p:fin}\n    \\begin{split}\n\\li\\nabla\\Lc(\\W),\\Wm\\ri <0.\n    \\end{split}\n\\end{equation}\n\n\n\n\n\n\n\n\n\nIn the scenario that Assumption~\\ref{assum:token:supp} holds (all tokens are support), $\\hbm_t=\\x_{it}^\\top\\Wm \\z_i $ is constant for all $t\\geq 2$. Hence, following similar steps as in \\eqref{grad def2} completes the proof. \n\n\\end{proof}\n\n\\subsubsection{Proof of Theorem ~\\ref{diverg:norm:w}}\n\nIt follows from Lemma~\\ref{lem:grad:descent} that under Assumption \\ref{assum:loss:prope}, $\\eta \\leq 1/L_{\\W}$, and for any initialization $\\W(0)$, the gradient descent sequence $\\W(k+1)=\\W(k)-\\eta\\nabla \\mathcal{L}(\\W(k))$ satisfies $\\lim_{k\\rightarrow \\infty}\n\\tf{\\nabla\\mathcal{L}\\left(\\W\\left(k\\right)\\right)}^{2}=0$.  \n\nFurther,  it follows from Lemma~\\ref{global des lem} that $\\li\\nabla\\Lc(\\W), \\Wm\\ri <0$  for all $\\W\\in\\R^{d\\times d}$. Hence, for any finite $\\W$, $\\li\\nabla\\Lc(\\W), \\Wm\\ri$ cannot be equal to zero.  Therefore, there are no finite critical points $\\W$, for which $\\nabla \\mc{L} (\\W)=0$ which contradicts Lemma~\\ref{lem:grad:descent}. This\nimplies that $\\left\\Vert \\W\\left(k\\right)\\right\\Vert \\rightarrow\\infty$. \n\\vspace{.1cm}\n\n\n\n$\\qed$\n\n\n\n\n\n\n\n\n\n\n\\subsection{Global convergence under good initial gradient}\\label{app B4} To ensure global convergence, we identify an assumption that prevents GD from getting trapped at suboptimal tokens that offer no scoring advantage compared to other choices. To establish a foundation for providing the convergence of GD to the globally optimal solution $\\Ws$, we present the following definitions.  For parameters $\\mu \\in (0,1)$ and $R>0$, consider the following subset of the sphere and its associated cone:\n\\begin{subequations}\\label{eqn:con:nabla0}\n\\begin{align}\n&\\Scc_{\\mu} (\\Ws):=\\left\\{\\W \\in \\mathbb{R}^{d \\times d}~\\Big|~  \\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\frac{\\W}{\\tf{\\W}}\\ri\\geq \\frac{\\mu}{\\tf{\\Ws}}\\quad \\textnormal{for all}\\quad t\\neq \\op_i, \\quad  i\\in[n]\\right\\},\\\\\n&\\conb_{\\mu,R}(\\Ws):=\\left\\{  \\W\\in\\Scc_\\mu (\\Ws) ~\\Big|~   \\tf{\\W}\\geq R\\right\\}.\n\\end{align}\n\\end{subequations}\nNote that the $\\conb_{\\mu,R}(\\Ws)$ definition is equivalent to the $\\conb_{\\mu,R}$ definition in \\eqref{eqn:con:nabla0:main} with a change of variable $\\mu\\gets\\tf{\\Wm}\\cdot \\mu$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{lemma}\n\\label{glocal cond} \nSuppose Assumption~\\ref{assum:loss:prope} holds and let $\\op=(\\op_i)_{i=1}^n$ be the unique globally-optimal indices with $\\Wm$ denoting the \\ref{eqn:sattnsvm} solution. Define the margin $\\Theta=1/\\tf{\\Ws}$. Let $\\s_{i}=\\sft{\\X_i\\W\\z_i}$. For any $\\mu>0$, there exists a sufficiently large $\\RR_\\mu=\\order{1/\\mu}$ (see \\eqref{R bound2}) such that:\n\\begin{enumerate}[label={\\textnormal{\\textbf{L\\arabic*.}}}, wide, labelwidth=!,itemindent=!, labelindent=5pt]\n\\item \\label{lem:gcond:l1} There is no stationary point within  $ \\conb_{\\mu,\\RR_\\mu}(\\Ws)$, where $\\conb_{\\mu,\\RR_\\mu} (\\Ws)$ is defined in \\eqref{eqn:con:nabla0}. \n\\item\\label{lem:gcond:l2} For all $\\V\\in \\Scc_{\\mu} (\\Ws)$ with $\\tf{\\V}=\\tf{\\Wm}$  and $\\W\\in\\conb_{\\mu,\\RR_\\mu}(\\Wm)$, there exist dataset dependent constants $C,c>0$ such that \n\\begin{subequations}\\label{zero:g:lbound}\n\\begin{align}\n&C\\cdot \\frac{1}{n}\\sum_{i=1}^n \\left(1-\\s_{i\\op_i}\\right) \\geq -\\Big\\langle\\nabla\\Lc(\\W),\\V \\Big\\rangle\\geq c\\cdot \\mu\\cdot  \\frac{1}{n} \\sum_{i=1}^n  \\left(1-\\s_{i\\op_i}\\right)>0, \\label{zero1:g:bound} \\\\\n& -\\li\\frac{\\V}{\\tf{\\V}},\\frac{\\nabla\\Lc(\\W)}{\\tf{\\nabla\\Lc(\\W)}}\\ri \\geq  \\frac{c}{C} \\cdot \\frac{\\Theta}{\\bar{A}}>0, \\label{zero2:g:bound}\\\\\n&\\tf{\\nabla\\Lc(\\W)}\\leq \\bar{A}C \\cdot \\frac{1}{n} \\sum_{i=1}^n  \\left(1-\\s_{i\\op_i}\\right). \\label{zero3:g:bound}\n\\end{align}\n\\end{subequations}\nHere,  $\\s_{i\\opt_i}=(\\sft{\\X_i\\W \\z_{i}})_{\\opt_i}$, $\\bar{A}=\\max_{i\\in[n],t,\\tau\\in[T]}\\tn{\\x_{it}- \\x_{i\\tau}}~\\tn{\\z_i}$, and $\\Theta=1/\\tf{\\Ws}$.\n\n\n\n\n\n \n\n\n\n\n\n\\end{enumerate}\n\\end{lemma}\n\n\\begin{proof} For simplicity let $R=\\RR_\\mu$, $\\W\\in\\conb_{\\mu,R}(\\Ws)$ and \n\\begin{equation}\\label{mu choice2}\n\\begin{split}\n\n&A=\\max_{i\\in[n],t,\\tau\\in[T]} \\frac{(\\tn{\\x_{it}}\\vee\\tn{\\x_{it}-\\x_{i\\tau}})\\cdot\\tn{\\z_i}}{\\Theta}.\n\\end{split}\n\\end{equation}\n\nThe following inequalities hold for all $\\V\\in \\Scc_{\\mu},~\\tf{\\V}=\\tf{\\Wm}$ and all $i\\in[n], t\\neq \\op_i$:\n\\begin{equation}\\label{cone-A-eq}\n\\begin{split}\n\n\nA\\geq(\\x_{i\\op_i}-\\x_{it})^\\top \\V \\z_i &\\geq \\mu.\n\\end{split}\n\\end{equation}\n\n \n\nTo proceed, we write the gradient correlation following \\eqref{grad def} and \\eqref{eqn:grad:prod:p}\n\\begin{align}\\label{grad def32}\n\\li\\nabla\\Lc(\\W),\\V\\ri&=\\frac{1}{n}\\sum_{i=1}^n\\ell'_i\\cdot\\li\\hb_i,\\sfp{\\hp_i}\\bgam_i\\ri,\n\\end{align}\nwhere we denoted $\\ell'_i=\\ell'(Y_i\\cdot \\vb^\\top \\X_i^\\top\\sft{\\hp_i})$, $\\hb_i=\\X_i\\V \\z_{i}$, $\\hp_i= \\X_i\\W \\z_{i}$, $\\s_i=\\sft{\\hp_i}$. \n\nIt follows from \\eqref{mu choice2} that $A\\geq \\max_{i\\in[n],t\\in[T]}\\tn{\\hb_{it}}$. Using \\eqref{cone-A-eq}, we can bound the softmax probabilities $\\s_i=\\sft{\\hp_i}$ as follows, for all $i\\in[n]$:\n\\begin{align}\\label{soft prob bound2}\n&S_i:= \\sum_{\\tau\\neq \\op_i}\\s_{i\\tau}\\leq T e^{-R\\mu\\Theta}\\s_{i\\op_i}\\leq T e^{-R\\mu\\Theta}.\n\\end{align}\n\nRecall scores $\\bgam_{it}=Y_i\\cdot\\vb^\\top \\x_{it}$. Define the score gaps:\n\\begin{equation*}\n \\bgg_i=\\bgam_{i\\op_i}-\\max_{t\\neq\\op_i}\\bgam_{it},~~~ \\bgm_i=\\bgam_{i\\op_i}-\\min_{t\\neq\\op_i}\\bgam_{it},~~~\\textnormal{and}~~~\\Gamma=\\sup_{i\\in[n],t,\\tau\\in[T]}|\\bgam_{it}-\\bgam_{i\\tau}|. \n\\end{equation*}\nLet us focus on a fixed datapoint $i\\in[n]$, assume (without losing generality) $\\op_i=1$, and drop subscripts $i$.\n\nDirectly applying Lemma \\ref{lem:q_reduce}, we obtain\n\\[\n  \\big|\\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam-\\sum_{t\\geq 2}^T (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2\\Gamma A(1-\\s_1)^2.\n\\]\n\\noindent To proceed, let us upper/lower bound the gradient correlation. Since $A\\geq \\hb_1-\\hb_t\\geq \\mu>0$ from \\eqref{cone-A-eq}, setting $S:=\\sum_{t\\neq\\op_i}\\s_t=1-\\s_1$, we find\n\\begin{equation}\n A\\cdot S\\cdot \\bgm  \\geq\\sum_{t\\neq\\op} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\geq \\mu\\cdot S\\cdot \\bgg.\\label{aggregate2}\n\\end{equation}\nNext we show that $S=1-\\s_1$ dominates $(1-\\s_1)^2=S^2$ for large $R$. Specifically, we wish for \n\\begin{align}\\label{wishfor2}\n\\mu S \\bgg/2\\geq 2\\Gamma A(1-\\s_1)^2\\iff S\\geq \\frac{4}{\\mu}\\frac{\\Gamma A}{\\bgg}S^2\\iff S\\leq \\frac{\\mu\\bgg}{4\\Gamma A}.\n\\end{align}\nUsing \\eqref{soft prob bound2}, what we wish is ensured for all $i\\in[n]$, by guaranteeing $Te^{-R\\mu\\Theta}\\leq \\frac{\\mu\\bgg}{4\\Gamma A}$. That is, by choosing\n\\begin{align}\\label{R bound2}\nR\\geq \\frac{1}{\\mu\\Theta}\\log\\left(\\frac{4T\\Gamma A}{\\mu\\bggm}\\right),\n\\end{align}\nwhere $\\bggm=\\min_{i\\in[n]}\\bgg_i$ is the global scalar corresponding to the worst case score gap over all inputs. \n\nWith the above choice of $R$, we guaranteed\n\\[\n  2 A(1-\\s_1)\\cdot \\bgm\\geq 2A\\cdot S\\cdot \\bgm \\geq \\sum_{t\\neq\\op} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\geq\\frac{\\mu\\cdot S\\cdot \\bgg}{2}\\geq\\frac{\\mu(1-\\s_1) \\bgg}{2},\n\\]\nvia \\eqref{wishfor2} and \\eqref{aggregate2}. \n\n\nSince this holds over all inputs, going back to the gradient correlation \\eqref{grad def32} and averaging above over all inputs $i\\in[n]$ and plugging back the indices $i$, we obtain the advertised bound \n\\begin{align}\\label{pbb corr2}\n  \n  \\frac{2A}{n}\\sum_{i\\in [n]} -\\ell'_i\\cdot S_i\\cdot \\bgm_i\\geq -\\li\\nabla\\Lc(\\W),\\V\\ri\\geq \\frac{\\mu}{2n}\\sum_{i\\in [n]} -\\ell'_i\\cdot S_i\\cdot \\bgg_i.\n\\end{align}\nLet $-\\ell'_{\\min/\\max}$ be the min/max values negative loss derivative admits over the ball $[-A,A]$ and note that $\\max_{i\\in[n]}\\bgm_i>0$ and $\\min_{i\\in[n]}\\bgg_i>0$ are dataset dependent constants. Then, we declare the constants $C=-2A\\ell'_{\\max}\\cdot \\max_{i\\in[n]}\\bgm_i>0,c=-(1/2)\\ell'_{\\min}\\cdot \\min_{i\\in[n]}\\bgg_i>0$ to obtain the bound \\eqref{zero1:g:bound}. \n\nThe proof of  \\eqref{zero2:g:bound} and \\eqref{zero3:g:bound} follows similarly as the proof of Lemma \\ref{local cond}. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\end{proof}\n\n\n\n\nThe following lemma shows that as $\\pi$ approaches zero, the negative gradient of the loss function at $ \\W\\in \\conb_{\\mu,R}(\\Wm)$  becomes more correlated with the max-margin solution ($\\Ws$) than with $\\W$ itself.\n\n\\begin{lemma}\n\\label{lem:glocal:corr} \nSuppose Assumption~\\ref{assum:loss:prope} holds and let $\\op=(\\op_i)_{i=1}^n$ be the unique optimal tokens with $ \\Wm$ denoting the SVM solution. Fix any $\\mu>0$ (per Lemma \\ref{glocal cond}). For any choice of $\\pi>0$, there exists $R:=R_{\\pi} \\geq \\bar{R}_\\mu$ such that, for any $ \\W\\in \\conb_{\\mu,R}(\\Wm)$, we have\n\\[\n \\li \\nabla\\Lc(\\W), \\frac{\\W}{\\tf{\\W}} \\ri\\geq (1+\\pi)\\li \\nabla\\Lc(\\W), \\frac{\\Wm}{\\tf{\\Wm}}\\ri.\n\\]\nHere, $\\conb_{\\mu,R}(\\Wm)$ is the cone defined at  \\eqref{eqn:con:nabla0}.\n\\end{lemma}\n\\begin{proof}\n \nLet  $\\Wb= \\tf{\\Wm} \\W/\\tf{\\W}$, $\\hb_i=\\X_i\\Wb \\z_{i}$, $\\hbm_i= \\X_i\\Ws \\z_{i}$, and $\\s_i=\\sft{\\X_i\\W \\z_{i}}$. To establish the result, we will prove that, for sufficiently large $R$ and for any $\\W\\in \\conb_{\\mu,R}(\\Wm)$:\n\\begin{align}\\label{main local cond2}\n\\nonumber \n\\li -\\nabla\\Lc(\\W),\\frac{\\W}{\\tf{\\W}}\\ri&= -\\frac{1}{n}\\sum_{i=1}^n\\ell'_i \\cdot  \\li \\hb_i, \\sfp{\\X_i\\W \\z_{i}}\\bgam_i\\ri\\\\\n&\\leq - \\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot  \\li\\hbm_i, \\sfp{\\X_i\\W \\z_{i}}\\bgam_i\\ri=(1+\\pi)\\li-\\nabla\\Lc(\\W), \\frac{\\ps}{\\tf{\\Ws}}\\ri.\n\\end{align}\nDirectly applying Lemma \\ref{lem:q_reduce}, for all $\\V\\in \\Scc_\\mu$ with $\\tf{\\V}=\\tf{\\Wm}$ and $\\hp_i=\\X_i\\V \\z_i$, we have found\n\\begin{align}\n  \\big|\\hp^\\top_i\\diag{\\s_i}\\bgam_i-\\hp^\\top_i\\s_i\\s^\\top_i\\bgam_i-\\sum_{t\\neq\\op_i} (\\hp_{i1}-\\hp_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\big|\\leq 2\\Gamma A(1-\\s_{i1})^2.\n\\end{align}\nRecalling $\\hbm_{i1}-\\hbm_{it}\\geq 1$, we note that $\\sum_{t\\neq\\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq \\sum_{t\\neq\\op_i} (\\hbm_{i1}-\\hbm_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})$. Now plugging in $\\hb,\\hbm$ in the bound above and assuming $\\pi\\leq 1$ (w.l.o.g.), \\eqref{main local cond2} is implied by the following stronger inequality\n\\begin{align*}\n-\\frac{1}{n}&\\sum_{i=1}^n\\ell'_i \\cdot \\left(6\\Gamma A(1-\\s_{i1})^2+ \\sum_{t\\neq \\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it}) \\right)\\\\\n&\\leq -\\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i  \\cdot \\sum_{t\\neq \\op_i} (\\hbm_{i1}-\\hbm_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n&\\leq-\\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\neq \\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align*}\nFirst, we claim that $0.5\\pi\\sum_{t\\in \\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\geq 6\\Gamma A(1-\\s_{i1})^2$ for all $i \\in [n]$.  \nThe proof of this claim directly follows the argument in Lemma~\\ref{glocal cond}, \n(namely following \\eqref{soft prob bound2}, \\eqref{wishfor2}, \\eqref{R bound2}) \nwe have that $1-\\s_{i1}\\leq Te^{-R\\mu\\Theta}$ and $\\bgam_{i1}-\\bgam_{it}\\geq \\bggm$ for all $i \\in [n]$. This leads to the choice (for $D_0\\geq 12$)\n\\begin{align}\n  R\\geq R_\\pi =\\frac{1}{\\mu\\Theta}\\log\\left(\\frac{D_{0}\\cdot T\\Gamma A}{\\pi\\bggm}\\right).\\label{Rpi choice2}\n\\end{align}\nWe shall choose $D_0$ sufficiently large such that $R_{\\pi}\\geq \\bar{R}_{\\mu}$, where $\\bar{R}_{\\mu}$ is defined in Lemma \\ref{glocal cond}.\n\nFollowing this control over the perturbation term $6\\Gamma A(1-\\s_{i1})^2$, to conclude with the result, what remains is proving the comparison\n\\begin{align}\\label{desired comp2}\n-\\frac{1}{n} \\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\neq\\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq - \\frac{1+0.5\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\neq\\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align}\n\\noindent\\textbf{Scenario 1:} $\\tf{\\Wb-\\Wm}\\leq \\eps=\\frac{\\pi}{4A\\Theta}$ for some $\\eps>0$.  In this scenario, for any $ t\\neq \\op_i$ and $i\\in [n ]$, we have\n\\[\n|\\hb_{it}-\\hbm_{it}|=|\\x_{it}^\\top (\\Wb-\\Wm)  \\z_{i}|\\leq A\\Theta\\eps=\\frac{\\pi}{4}.\n\\]\nConsequently, we obtain \n\\[\n\\hb_{i1}-\\hb_{it}\\leq \\hbm_{i1}-\\hbm_{it}+2A\\Theta\\eps= 1+0.5\\pi.\n\\] \nSimilarly, $\\hb_{i1}-\\hb_{it}\\geq 1-0.5\\pi\\geq 0.5$. Since all terms $\\hb_{i1}-\\hb_{it},\\s_{it},\\bgam_{i1}-\\bgam_{it}$ in \\eqref{desired comp2} are nonnegative, we  obtain \\eqref{desired comp2}. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\noindent\\textbf{Scenario 2:} $\\tf{\\Wb-\\Wm}\\geq \\eps=\\frac{\\pi}{4A\\Theta}$.  Since $\\Wb$ is not max-margin solution, in this scenario, for some $i \\in  [n]$, $\\nu=\\nu(\\eps)>0$, and $\\tau\\neq\\op_i$, we have that\n\\begin{align*}\n\\hb_{i1}-\\hb_{i\\tau}\\leq 1-2\\nu.\n\\end{align*}\nHere $\\tau=\\arg\\max_{\\tau\\neq\\op_i} \\x_{i\\tau}\\Wb \\z_i$ denotes the nearest point to $\\hb_{i1}$ (along the $\\Wb$ direction). Recall that $\\s=\\sft{  \\RR\\hb}$, where  $\\RR=R\\Theta=\\tf{\\W}/\\tf{\\Wm}$. To proceed, let $ \\underline{\\hb}_i:=\\min_{t \\neq\\op_i}\\hb_{i1}-\\hb_{it}$,\n\\begin{align*}\n\\mc{I}:=\\left\\{ i\\in[n]: \\underline{\\hb}_i \\leq 1-2\\nu \\right\\}, \\qquad [n]-\\mc{I}:=\\left\\{ i\\in[n]:  1-2\\nu  <  \\underline{\\hb}_i\\right\\}.\n\n\n\\end{align*}\nFor all $ i \\in [n]-\\mc{I}$,\n\\begin{equation}\\label{eqn:grad:difff0}\n\\begin{split}\n      \\sum_{t\\neq \\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it}) &- (1+0.5\\pi) \\sum_{t\\neq \\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n      & \\leq  \\left(2A - (1+0.5\\pi)\\right)\\Gamma\\sum_{t\\neq \\op_i,~\\hb_{i1}-\\hb_{it} \\geq 1+\\frac{\\pi}{2} } \\s_{it} \\\\\n      & \\leq  \\left(2A - (1+0.5\\pi)\\right)\\Gamma Te^{-\\RR(1+\\frac{\\pi}{2})} \\\\\n      &\\leq   2A\\Gamma  T e^{-\\RR(1+\\frac{\\pi}{2})}.\n      \n      \n      \n\\end{split}\n\\end{equation}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor all $ i \\in \\mc{I}$, split the tokens into two groups: Let $\\Nc_i$ be the group of tokens obeying $ \\hb_{i1}-\\hb_{it} \\leq 1-\\nu$ and $\\bar\\Nc_i:=[T]-\\{\\op_i\\}-\\Nc_i$ be the rest of the neighbors. Observe that\n\\[\n\\frac{\\sum_{t\\in\\bar{\\Nc}_i}\\s_{it}}{\\sum_{t\\neq\\op_i}\\s_{it}}\\leq  T\\frac{e^{\\nu \\RR}}{e^{2\\nu\\RR}}=Te^{-\\RR\\nu}.\n\\]\nUsing $|\\hb_{i1}-\\hb_{it}|\\leq 2A$ and  $\\bggm=\\min_{i\\in[n]}\\bgg_i =\\min_{i\\in[n]} (\\bgam_{i1}-\\max_{t\\neq\\op_i}\\bgam_{it})$, observe that \n\\[\n\\sum_{t\\in\\bar\\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq \\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm} \\sum_{t\\neq\\opt_i} \\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\]\nThus, \n\\begin{align*}\n  \\sum_{t\\neq \\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})&= \\sum_{t\\in \\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})+\\sum_{t\\in\\bar\\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\nonumber\\\\\n\n  &\\leq \\sum_{t\\in \\Nc_i} (1-\\nu)\\s_{it}(\\bgam_{i1}-\\bgam_{it})+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm} \\sum_{t\\neq \\op_i} \\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n  &\\leq \\left(1-\\nu+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}\\right)\\sum_{t\\neq \\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n &\\leq \\left(1+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}\\right)\\sum_{t\\neq \\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align*}\nHence, choosing \n\\begin{align}\nR\\geq\\frac{1}{\\nu\\Theta}\\log\\left(\\frac{8\\Gamma AT}{\\bggm\\pi}\\right)\\label{R bound pi}\n\\end{align}\nresults in that\n\\begin{equation}\\label{eqn:grad:difff1}\n    \\begin{split}\n     &\\sum_{t\\neq\\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})  - \\left(1+\\frac{\\pi}{2}\\right) \\sum_{t\\neq\\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}) \\\\\n   &\\leq\\left(\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}-\\frac{\\pi}{2}\\right)\\sum_{t\\neq\\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n   &\\leq -\\frac{\\pi}{4}\\sum_{t\\neq\\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n   &\\leq-\\frac{\\pi}{4T}\\bggm  e^{-\\bar{R} (1-2\\nu)}.      \n    \\end{split}\n\\end{equation}\nHere, the last inequality follows from the fact that $\\sum_{t\\neq\\op_i}\\s_{it}\\geq \\max_{t\\neq\\op_i}\\s_{it}\\geq\\frac{e^{-\\bar{R}(1-2\\nu)}}{\\sum_{t=1}^Te^{-\\bar{R}(\\hb_{i1}-\\hb_{it})}}\\geq e^{-\\bar{R}(1-2\\nu)}/T$.\n\n\nFrom Assumption~\\ref{assum:loss:prope}, we have $c_{\\min}\\leq-\\ell'\\leq c_{\\max}$ for some positive constants $c_{\\min}$ and $c_{\\max}$. It follows from  \\eqref{eqn:grad:difff0} and \\eqref{eqn:grad:difff1} that \n\\begin{align*}\n-\\frac{1}{n}\\sum_{i}^n \\ell_i' \\cdot&\\left(\n      \\sum_{t\\neq\\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})- \\sum_{t\\neq\\op_i} (1+0.5\\pi)\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\right)\\\\\n      & \\leq    c_{\\max}2A\\Gamma  T \\Gamma e^{-\\RR(1 +\\frac{\\pi}{2})}-\\frac{c_{\\min}}{nT}\\cdot\\frac{\\pi\\bggm}{4}e^{-\\bar{R} (1-2\\nu)}\\\\\n      \n      & \\leq 0.\n\\end{align*}\nCombing with \\eqref{R bound pi}, this is guaranteed by \nchoosing \n\n\n\n\\[\n  R\\geq \\max\\left\\{\\frac{1}{\\nu\\Theta}\\log\\left(\\frac{8\\Gamma AT}{\\bggm\\pi}\\right),\\frac{1}{(2\\nu+\\pi/2)\\Theta}\\log\\left(\\frac{8n\\Gamma AT^2 c_{\\max}}{c_{\\min}\\bggm\\pi}\\right)\\right\\},\n\\]\nwhere $\\nu=\\nu(\\frac{\\pi}{4A\\Theta})$ depends only on $\\pi$ and global problem variables. \n\nCombining this with the prior $R$ choice \\eqref{Rpi choice2} (by taking maximum), we conclude with the statement.\n\\end{proof}\n\n\n\n\n\n\n\n\\subsubsection{Proof of Theorem~\\ref{conv:gd:w:global:nabla0}}\\label{app:glob:nab0}\nThe following theorem is a restatement of Theorem \\ref{conv:gd:w:global:nabla0}. Two minor differences are: (1) We state with the change of variable $\\mu\\gets\\tf{\\Wm}\\cdot \\mu$; see discussions below \\eqref{eqn:con:nabla0}. (2) We also include \\eqref{lem:zglob:l3} in the statement of the theorem.\n\\begin{theorem}\\label{conv:gd:w:global:nabla0:app}\nSuppose Assumption~\\ref{assum:loss:prope} on the loss function $\\ell$ and Assumption \\ref{assum:nabla0} on the initial gradient hold.  \n\\begin{enumerate}[label={\\textnormal{\\textbf{L\\arabic*.}}}, wide, labelwidth=!,itemindent=!, labelindent=5pt]\n  \\item   \\label{lem:zglob:l1}For any $\\mu>0$, there exists  $R>0$ such  that   $\\conb_{\\mu,R}(\\Ws)$ defined in \\eqref{eqn:con:nabla0} does not contain any  stationary points. \n\\item \\label{lem:zglob:l2} Fix any  $\\mu \\in  (0,\\min (1,\\iota \\tf{\\Ws}/\\tf{\\nabla \\Lc(0)})$. Consider GD iterations with $\\W(0)=0$, $\\W(1)=-R\\nabla\\Lc(0)/\\tf{\\nabla\\Lc(0)}$, and $\\W(k+1)=\\W(k)-\\eta\\nabla\\Lc(\\W(k))$ for $\\eta\\le 1/L_{\\W}$, $k\\ge 1$, and $R$ sufficiently large. If all iterates remain within $\\conb_{\\mu,R}$, then $\\lim_{k\\rightarrow\\infty} \\tf{\\W(k)}=\\infty$ and $\\lim_{k\\rightarrow\\infty}\\frac{\\W(k)}{\\tf{\\W(k)}}=\\frac{\\Wm}{\\tf{\\Wm}}$.\n\\item  \\label{lem:zglob:l3} Assume $\\eta\\leq 1/L_{\\W}$ and for all $\\W \\in \\conb_{\\mu,R}(\\Ws)$ with sufficiently large $R$?\n\\begin{align}\\label{assum:extra}\n   \\min_{i \\in [n]}\\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\W-\\eta\\nabla\\Lc(\\W) \\ri \\geq     \\min_{i \\in [n]}\\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\W\\ri - \\frac{2\\eta\\mu}{\\tf{\\Wm}^2}\\iprod{\\nabla\\mc{L}(\\W)}{\\Wm},\n\\end{align}\nthen all GD iterations remain within $\\conb_{\\mu,R}(\\Wm)$.\n\\end{enumerate}\n\\end{theorem}\n\n\n\\begin{proof}\nNote that \\ref{lem:zglob:l1} is a direct corollary of Lemma~\\ref{glocal cond}. We proceed with the proof of \\ref{lem:zglob:l2} and \\ref{lem:zglob:l3}.  \n\nWe provide the proof in four steps:\\\\\n\\textbf{Step~1: $\\conb_{\\mu,R^0_\\mu}(\\Wm)$ construction}.  Let us denote the initialization lower bound as $R^0_\\mu:=R$, where $R$ is given in the Theorem~\\ref{conv:gd:w:global:nabla0:app}'s statement. Consider an arbitrary value of  $\\epsilon \\in (0, \\mu/2)$ and let $1/(1+\\pi)=1-\\epsilon$. We additionally denote $R_\\eps\\gets R_\\pi\\vee 1/2$ where $R_\\pi$ was defined in Lemma~\\ref{lem:glocal:corr}. At initialization $\\W(0)$, we set $\\eps=\\mu/2$ to obtain $R^0_\\mu= R_{\\mu/2}$. \n\nWe proceed to show  $\\mu \\in  (0,\\min (1,\\iota \\tf{\\Ws}/\\tf{\\nabla \\Lc(0)})$.  It follows from  Assumption~\\ref{assum:nabla0}  and under zero initialization for GD ($\\W(0)=0$) that\n$$\n\\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top,   - \\nabla \\mc{L}(\\W(0)) \\right\\rangle =\\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top,   - \\nabla \\mc{L}(0) \\right\\rangle\\ge \\iota  > 0, \n$$\nfor some positive constant $\\iota $. Hence, for any initial step size $\\eta (0)>0$ and $\\W(1)=-\\eta(0) \\nabla\\Lc(0)$, \n\\begin{equation}\\label{eqn:decpath:zinit}\n\\begin{split}\n    \\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\frac{\\W (1)}{\\tf{\\W(1)}}\\ri &=  \\frac{\\eta (0) }{\\tf{\\W(1)}}  \\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top,   - \\nabla \\mc{L}(0) \\right\\rangle \\\\\n     & \\geq  \\frac{\\iota  \\eta (0) }{\\tf{\\W(1)}}=  \\frac{\\iota }{ \\tf{\\nabla \\mc{L}(0)}}\\\\\n     &\\geq  \\frac{\\mu}{\\tf{\\Wm}}.\n\\end{split}\n\\end{equation}\nHere, the last inequality follows from our choice of $\\mu$ in the theorem statement, i.e.\n\\begin{align}\\label{eqn:mu:zero}\n \\mu \\in \\left(0, \\min\\left(1, \\frac{\\iota \\tf{\\Wm}}{\\tf{\\nabla \\mc{L}(0)}}\\right)\\right).\n\\end{align}\n\nThis $\\mu$ choice induces the conic set $\\conb_{\\mu,R^0_\\mu}(\\Wm)$ with  $R^0_\\mu= R_{\\mu/2}$, where $R_{\\mu/2}$ was defined in Lemma~\\ref{lem:glocal:corr}. \nNow, given the parameter  $ \\mu $ satisfying \\eqref{eqn:mu:zero}, we can choose $\\eta (0)$ such that $\\tf{\\W (1)} \\geq R^0_\\mu$ and $\\W(1)\\in\\conb_{\\mu, R^0_\\mu}(\\Wm)$. To achieve this, since $\\W(0)=0$, we obtain  \n\\begin{equation}\\label{eqn:stepeta0}\n    \\eta (0) =\\frac{R^0_\\mu}{\\tf{\\nabla \\mc{L}(0) }}. \n    \n\\end{equation}\nSince by our definition, $R^0_\\mu \\leftarrow R$,  \\eqref{eqn:stepeta0} gives $\\W(1)$ in the theorem's statement.\n\n\n\n\\noindent \\textbf{Step~2: There are no stationary points within $\\conb_{\\mu,R_\\mu^0}(\\Wm)$.} \nThis step follows from \\ref{lem:zglob:l1}. Specifically, \n\n\n\nwe can apply Lemma~\\ref{glocal cond} to find that: For all $\\V,\\W\\in  \\bar{\\Sc}_{\\mu}(\\Ws)$ with $\\tf{\\W} \\neq 0$ and $\\tf{\\W} \\geq R^0_\\mu$,  we have that $-\\li\\V, \\nabla \\Lc(\\W)\\ri$ is strictly positive.\n\n\\\\\n\\emph{Gradient correlation holds for large parameter norm.}  \n\n\n\nIt follows from  Lemma~\\ref{lem:glocal:corr} that, there exists $ R_\\epsilon\\geq \\bar{R}_\\mu\\vee 1/2$ such that all  $ \\W \\in \\conb_{\\mu,R_\\epsilon}(\\Wm)$ satisfy\n\\begin{align}\\label{eqn:neg:corr:0}\n\\iprod{-\\nabla\\mc{L}(\\W)}      {\\frac{\\Wm}{\\tf{\\Wm}}} \\geq (1-\\epsilon)    \\iprod{-\\nabla \\mc{L}(\\W)}{\\frac{\\W}{\\tf{\\W}}}.\n\\end{align}\nThe following argument applies to a general $\\eps\\in(0,\\mu/2)$. However, at initialization $\\W(0)=0$, we have set $\\eps=\\mu/2$ and defined the initialization radius as $R^0_\\mu= R_{\\mu/2}$. To proceed, we will prove the main statements \\eqref{lem:zglob:l2} and \\eqref{lem:zglob:l3} as follows.\n\\begin{itemize}\n\\item Proving \\ref{lem:zglob:l3}: In \\textbf{Step 3}, we will assume Condition \\eqref{assum:extra} to prove that gradient iterates remain within $\\conb_{\\mu,R_\\eps}(\\Ws)$. Concretely, for any $\\epsilon \\in (0, \\mu/2)$, we will show that after gradient descent enters the conic set $\\conb_{\\mu,R_\\eps}(\\Ws)$ for the first time, it will never leave the set under Condition \\eqref{assum:extra} of the theorem statement and \\eqref{eqn:neg:corr:0}. In what follows, let us denote $k_\\eps$ to be the first time gradient descent enters $\\conb_{\\mu,R_\\eps}(\\Ws)$. Note that for $\\eps\\gets\\mu/2$, $k_\\eps=0$ i.e.~the point of initialization.\n\n\\item Proving \\ref{lem:zglob:l2}: In \\textbf{Step 4}, assuming iterates within $\\conb_{\\mu,R_\\eps}(\\Ws)$, we will prove that the norm diverges (as a result such $k_\\eps$ is guaranteed to exist) and, additionally, the gradient updates asymptotically aligns with $\\Ws$. \n\\end{itemize}\n\n\n\n\n\n\n\n\\textbf{Step~3 (Proof of \\ref{lem:zglob:l3}): Updates remain inside the cone $\\conb_{\\mu,R_\\eps}(\\Ws)$.}   Note that if $\\W(k) \\in \\conb_{\\mu,R_\\eps}(\\Ws)$ for all $k \\geq 1$, the required condition in \\ref{lem:zglob:l2} holds, and we proceed to \\textbf{Step 4}. In this step, we show \\ref{lem:zglob:l3}. Specifically, we show that under Condition \\eqref{assum:extra} and using  \\eqref{eqn:neg:corr:0}, all iterates $\\W(k) \\in \\conb_{\\mu,R_\\eps}(\\Ws)$ remain within $\\conb_{\\mu,R_\\eps}(\\Ws)$.\n\nTo proceed, by leveraging the results from \\textbf{Step 1} and \\textbf{Step 2}, we demonstrate that the gradient iterates, with an appropriate constant step size, starting from $\\W(k_\\eps) \\in \\conb_{\\mu,R_\\eps}(\\Ws)$, remain within this set. We proceed by induction.  Suppose that the claim holds up to iteration $k \\geq k_\\eps$. This implies that $ \\W(k) \\in \\conb_{\\mu,R_\\eps}(\\Ws)$. Hence, recalling $\\conb_{\\mu,R_\\eps}(\\Ws)$ defined in \\eqref{eqn:con:nabla0}, there exists scalar $\\mu=\\mu(\\bal) \\in (0,1)$  and $R_\\eps$ such that  $\\tf{\\W(k)}\\geq R_\\eps$, and\n\n\n\n\n\n\\begin{equation*}\n\\begin{split}\n\\left\\langle (\\x_{i\\op_i}-\\x_{it})\\z_i^\\top,\\frac{\\W(k)}{\\tf{\\W(k)}} \\right\\rangle  \\geq \\mu\\Theta,\n\\end{split}\n\\end{equation*}\n\n\n\nwhere $\\Theta=1/\\tf{\\Wm}$. \n\nLet \n\\begin{subequations}\\label{eqn:rho:def:nabla0}\n\\begin{align}\n\\frac{1}{1-\\epsilon} \\iprod{ {\\frac{\\Wm}{\\tf{\\Wm}}}}{-\\nabla\\mc{L}(\\W(k))} =:\\rho(k)>0.\n\\end{align}\n\\end{subequations}\nUsing \\eqref{assum:extra}, we have \n\\begin{equation}\\label{eqn:localgd:1:nabla0}\n    \\begin{split}\n   \\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top,    \\frac{\\W(k+1)}{\\tf{\\W(k)}} \\right\\rangle &=   \\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top,    \\frac{\\W(k)}{\\tf{\\W(k)}} -\\frac{\\eta}{\\tf{\\W(k)}}\\nabla \\mc{L}(\\W(k)) \\right\\rangle\\\\   \n\n      & \\geq \\mu \\Theta +\\frac{ 2\\eta  (1-\\epsilon)\\mu \\Theta \\rho(k) }{\\tf{\\W(k)}}.\n    \\end{split}\n\\end{equation}\n\n\nFrom Lemma~\\ref{glocal cond},   we have $\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle<0$~ which implies that $\\tf{\\W(k+1)} \\geq \\tf{\\W(k)}$.  This together with  $R_\\eps$ definition and $\\tf{\\W(k)}\\geq 1/2$ implies that  \n\\begin{align*}\n\\tf{\\W(k+1)}&\\leq\\frac{1}{{2\\tf{\\W(k)}}} \\left(\\tf{\\W(k+1)}^2+\\tf{\\W(k)}^2\\right)\\\\\n& = \\frac{1}{2\\tf{\\W(k)}} \\left(2\\tf{\\W(k)}^2-2\\eta\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle+\\eta^2\\tf{\\nabla \\Lc(\\W(k))}^2\\right)\\\\\n       &\\leq  \\tf{\\W(k)}- \\frac{\\eta}{\\tf{\\W(k)}}\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle + \\eta^2 \\|\\nabla \\Lc(\\W(k))\\|_F^2.\n\\end{align*}\n\nThus,\n\\begin{equation}\\label{eqn:localgd:2:nabla0}\n\\begin{split}\n  \\frac{\\tf{\\W(k+1)}}{\\tf{\\W(k)}}& \\leq  1- \\frac{\\eta}{\\tf{\\W(k)}}\n       \\left\\langle \\nabla \\Lc(\\W(k)),\\frac{\\W(k)}{\\tf{\\W(k)}} \\right\\rangle + \\eta^2 \\frac{\\|\\nabla \\mc{L}(\\W(k))\\|_F^2}{\\tf{\\W(k)}}\\\\\n& \\leq 1- \\frac{\\eta}{(1-\\epsilon)\\tf{\\W(k)}}  \\iprod{\\nabla\\mc{L}(\\W(k))}\n     {\\frac{\\Wm}{\\tf{\\Wm}}}+ \\eta^2 \\frac{\\|\\nabla \\mc{L}(\\W(k))\\|_F^2}{\\tf{\\W(k)}}\\\\\n      & \\leq  1 + \\frac{\\eta \\rho(k)}{\\tf{\\W(k)}} + \\frac{\\eta^2\\|\\nabla \\mc{L}(\\W(k))\\|_F^2}{\\tf{\\W(k)}}=:C_1(\\rho(k),\\eta).\n\\end{split}\n\\end{equation}\nHere, the second inequality uses \\eqref{eqn:neg:corr:0}. \n\nNow, it follows from \\eqref{eqn:localgd:1:nabla0} and \\eqref{eqn:localgd:2:nabla0} that \n\\begin{equation}\\label{eqn:localgd:3:nabla0}\n\\begin{split}\n\\min_{t\\neq \\op_i,~i\\in[n]} ~~  \\left\\langle  (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top, \\frac{\\W(k+1)}{\\tf{\\W(k+1)}}\\right\\rangle   &\\geq \\frac{1}{C_1({\\rho}(k),\\eta)} \\left(\\mu \\Theta+\\frac{2\\eta (1-\\epsilon)\\mu \\Theta  {\\rho}(k)}{\\tf{\\W(k)}}\\right)\\\\\n& = \\mu \\Theta+\\frac{\\eta\\mu \\Theta}{C_1({\\rho}(k),\\eta)} \\left(\\frac{ \\big(2(1-\\epsilon) -1 \\big)  \\rho(k)}{\\tf{\\W(k)}}\n-   \\eta \\frac{\\tf{\\nabla \\mc{L}(\\W(k))}^2 }{\\tf{\\W(k)}}\\right)\\\\\n& = \\mu \\Theta+\\frac{\\eta\\mu \\Theta}{C_1({\\rho}(k),\\eta)} \\left(\\frac{ (1-2\\epsilon)   \\rho(k)}{\\tf{\\W(k)}}\n-   \\eta  \\frac{\\tf{\\nabla \\mc{L}(\\W(k))}^2 }{\\tf{\\W(k)}}\\right)\\\\\n\n\n& \\geq \\mu \\Theta,\n\\end{split}\n\\end{equation}\n where the last inequality uses our choice of stepsize $\\eta\\leq 1/L_W$ in Theorem~\\ref{conv:gd:w:global:nabla0}'s statement. Specifically, we need $\\eta$ to be small to ensure the last inequality. We will guarantee this by choosing a proper $R_\\eps$ in Lemma \\ref{lem:glocal:corr}. Specifically, Lemma \\ref{lem:glocal:corr} leaves the choice of $D_0$ in $R_\\eps$ lower bound of \\eqref{Rpi choice2} open (it can always be chosen larger). Here, by choosing $D_0\\gtrsim 1/L_{\\W}$ will ensure $\\eta\\leq 1/L_{\\W}$ works well.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{equation}\\label{eqn:zeta:mu:0}\n\\begin{split}\n    \\eta &\\leq    \\big( 1-\\mu\\big)\\mu\n \\frac{c}{C}  \\frac{\\Theta}{\\bar{A}}   \\frac{1}{\\bar{A}C T}    e^{R_\\mu^0\\Theta/2}\\\\\n \n \n &\\leq      \\frac{1-2\\epsilon  }{1-\\epsilon}   \\frac{c \\mu}{C}   \\frac{\\Theta}{\\bar{A}}    \\frac{1}{\\bar{A}C T}  e^{R_\\mu^0\\Theta/2} \\\\\n &\\leq   \\big(1-2\\epsilon \\big)   \\frac{\\rho(k) } { \\|\\nabla \\mc{L}(\\W(k))\\|^2_F}.\n\\end{split}    \n\\end{equation}\n\n\n\n\n\n\n\n\n\nHere, the first inequality follows since $\\epsilon \\in (0, \\mu/2)$ (as seen in \\textbf{Step 2}). Also,  $\\mu < 1$ implies that $1-\\mu > 0$, we obtain $\\eta > 0$. The last inequality is obtained from Lemma~\\ref{glocal cond}:\n\\begin{align*}\n    \\frac{\\rho(k) } { \\tf{\\nabla \\mc{L}(\\W(k))}} &= - \\frac{1}{1-\\epsilon} \\iprod{ \\frac{\\nabla\\mc{L}(\\W(k))}{\\tf{\\nabla \\mc{L}(\\W(k))}}}\n     {\\frac{\\Wm}{\\tf{\\Wm}}}  \\geq \\frac{1}{1-\\epsilon} \\cdot \\frac{c \\mu}{C} \\cdot \\frac{\\Theta}{\\bar{A}},\\\\\n         \\frac{1} { \\tf{\\nabla \\mc{L}(\\W(k))}} &{\\geq \\frac{1}{\\bar{A}C \\cdot \\frac{1}{n} \\sum_{i=1}^n  \\left(1-\\s_{i\\op_i}\\right)} \\geq     \\frac{1}{ \\bar{A} C T e^{-R_\\mu^{0}\\Theta/2}} }\n\\end{align*}\nfor some data dependent constrants $c$, $C$, $\\bar{A}=\\max_{i\\in[n],t,\\tau\\in[T]}\\tn{(\\x_{it}- \\x_{i\\tau})}~\\tn{\\z_i}$, and $\\Theta=1/\\tf{\\Ws}$.\n\nThe remainder of the proof of this step is identical to \\eqref{eqn:pitoC0}--\\eqref{eqn:pitoC02}, with the replacement of $C_0$ by $D_0$ and the tracking of changes. Specifically, Lemma \\ref{lem:glocal:corr} leaves the choice of $D_0$ in $R_\\eps$ lower bound of \\eqref{Rpi choice2} open (it can always be chosen larger). Hence,  for sufficiently large $D_0$, we have\n\\begin{align}\n\\eta \\leq \\frac{1}{L_{\\W}}\\leq   \\big( 1-\\mu\\big)\\mu\n \\frac{c}{C}  \\frac{\\Theta}{\\bar{A}}   \\frac{1}{\\bar{A}C T}    e^{R_\\mu^0\\Theta/2}.\n\\end{align}\n\n\nThis implies \\eqref{eqn:localgd:3:nabla0} and  $\\W(k+1) \\in\\conb_{\\mu,R_\\eps}(\\Ws)$. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\noindent\\textbf{Step 4 (Proof of \\ref{lem:zglob:l2}): $\\W(k)$ and $\\Wm$ perfectly align over time.} \nBy theorem statement (alternatively via \\textbf{Step 3}), we have that all iterates remain within the initial conic set i.e.~$\\W(k)\\in\\conb_{\\mu,R^0_\\mu}(\\Ws)$ for all $k\\geq 0$. Note that it follows from Lemma~\\ref{glocal cond}  that  $\\li\\nabla\\Lc(\\W), \\Ws/\\tf{\\Ws}\\ri<0$, for any finite $\\W \\in \\conb_{\\mu,R^0_\\mu}(\\Ws)$. Hence, there are no finite critical points $\\W \\in \\conb_{\\mu,R^0_\\mu}(\\Ws)$, for which $\\nabla \\mc{L} (\\W)=0$. Now, based on Lemma~\\ref{lem:grad:descent}, which guarantees that $\\nabla\\Lc(\\W(k))\\rightarrow 0$, this\nimplies that $\\left\\Vert \\W\\left(k\\right)\\right\\Vert \\rightarrow\\infty$. Consequently, for any choice of $\\eps\\in (0,\\mu/2)$ there is an iteration $k_\\eps$ such that, for all $k\\geq k_\\eps$, $\\W(k)\\in\\conb_{\\mu,R_\\eps}(\\Ws)$. Once within $\\conb_{\\mu,R_\\eps}(\\Ws)$,  multiplying both sides of \\eqref{eqn:neg:corr:0} by the stepsize $\\eta$ and using the gradient descent update, we get\n\\begin{equation*}\n\\begin{split}\n     \\left\\langle \\W(k+1)-\\W(k),\\frac{ \\Wm}{\\tf{\\Wm}} \\right\\rangle &\\geq  (1-\\epsilon) \\left\\langle \\W(k+1)-\\W(k), \\frac{\\W(k)}{\\tf{\\W(k)}}\\right\\rangle\\\\\n     &= \\frac{(1-\\epsilon)}{2\\tf{\\W(k)}}\\left(\\tf{\\W(k+1)}^2- \\tf{\\W(k)}^2-\\tf{\\W(k+1)-\\W(k)}^2\\right) \\\\\n     & \\geq (1-\\epsilon)\\left( \\frac{1}{2\\tf{\\W(k)}} \\left(\\tf{\\W(k+1)}^2- \\tf{\\W(k)}^2\\right)-\\tf{\\W(k+1)-\\W(k)}^2\\right) \\\\\n     & \\geq (1-\\epsilon)\\left(\\tf{\\W(k+1)}- \\tf{\\W(k)}-\\tf{\\W(k+1)-\\W(k)}^2\\right) \\\\\n          & \\geq (1-\\epsilon)\\Big(\\tf{\\W(k+1)}- \\tf{\\W(k)}- 2\\eta  \\left(\\mc{L}(\\W(k))-\\mc{L}(\\W(k+1))\\right) \\Big).\n\\end{split}\n\\end{equation*}\n\nHere, the second inequality is obtained from  $\\tf{\\W(k)}\\geq 1/2$; the third inequality follows since  for any $a, b >0$, we have $  (a^2-b^2)/(2b) -  (a-b) \\geq 0$; and the last inequality  uses Lemma~\\ref{lem:grad:descent}.\n\n\n\nSumming the above inequality over $k\\geq k_\\eps$ gives \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{align*}\n      \\left\\langle\\frac{\\W(k)}{\\tf{\\W(k)}}, \\frac{\\Wm}{\\tf{\\Wm}} \\right\\rangle \\ge1-\\epsilon+ \\frac{C(\\epsilon,\\eta)}{\\tf{\\W(k)}}, \\qquad \\W(k)\\in\\conb_{\\mu,R_\\eps}(\\Ws),   \n\\end{align*}\nwhere $\\mathcal{L}_{\\star}\\leq\\mathcal{L}\\left(\\W\\left(k\\right)\\right)$ for all $k\\geq k_\\eps$, and \n\\begin{equation*}\nC(\\epsilon,\\eta)= \\left\\langle \\W(k_\\eps), \\frac{ \\Wm}{\\tf{\\Wm}}\\right\\rangle-(1-\\epsilon)\\tf{\\W(k_\\eps)} -2\\eta (1-\\epsilon) (\\mc{L}(\\W(k_\\eps))-\\mathcal{L}_{\\star}).\n\\end{equation*}\nConsequently,\n    \\begin{align*}\n      \\liminf_{k\\to\\infty}\\iprod{\\frac{\\W(k)}{\\tf{\\W(k)}}}{\\frac{\\Wm}{\\tf{\\Wm}}}\\ge1-\\epsilon, \\qquad \\W(k)\\in\\conb_{\\mu,R_\\eps}(\\Ws).  \n    \\end{align*}\nSince  $\\epsilon \\in (0, \\mu/2)$  is arbitrary, this implies $\\W(k)/\\tf{\\W(k)}\\to  \\Wm/\\tf{\\Wm}$.\n\\end{proof}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Local Convergence of Gradient Descent}\\label{app local proofs}\nTo provide a basis for discussing local convergence of GD, we establish a cone centered around $\\Wma$  using the following construction. For parameters $\\mu \\in (0,1)$ and $R>0$, we define $\\Cc_{\\mu,R}(\\Wma)$ as the set of matrices $\\W \\in\\R^{d\\times d}$ such that $\\tf{\\W}\\geq R$ and  the correlation coefficient between $\\W$ and $\\Wma$ is at least $1-\\mu$:\n\n\n\\begin{subequations}\n\\begin{align}\\label{eqn:coneofw:r}\n\\Sc_{\\mu}(\\Wma)&:= \\left\\{\\W\\in\\R^{d\\times d}~:~\\left\\langle\\frac{\\W}{\\tf{\\W}},\\frac{{\\Wma}}{\\tf{{\\Wma}}} \\right\\rangle \\geq 1-\\mu\\right\\}, \\\\\n\\Cc_{\\mu,R}({\\Wma})&:= \\Sc_{\\mu}(\\Wma) \\cap  \\left\\{\\W\\in\\R^{d\\times d}~:~\\tf{\\W}\\geq R\\right\\}.\n\\end{align}\n\\end{subequations}\n\\begin{lemma}\n\\label{local cond} \nSuppose Assumption~\\ref{assum:loss:prope} on the loss function $\\ell$ holds, and let $\\bal=(\\alpha_i)_{i=1}^n$ be locally optimal tokens according to Definition \\ref{def loc opt}. Let $ \\Wm= \\Wm_\\bal$ denote the SVM solution obtained via \\eqref{eqn:sattnsvm} by applying the Frobenius norm and replacing $(\\opt_i)_{i=1}^n$ with $\\boldsymbol{\\alpha} = (\\alpha_i)_{i=1}^n$. \n\n\n\n\n\n\nThere exists a scalar $\\mu=\\mu(\\bal)>0$ such that for sufficiently large $\\RR_\\mu$:\n\\begin{enumerate}[label={\\textnormal{\\textbf{L\\arabic*.}}}, wide, labelwidth=!,itemindent=!, labelindent=5pt]\n\\item \\label{lem:cond:l1} There is no stationary point within  $ \\Cc_{\\mu,\\RR_\\mu} (\\Wm)$.\n\n\\item\\label{lem:cond:l2} For all $\\V\\in \\Sc_{\\mu}(\\Wm)$ with $\\tf{\\V}=\\tf{\\Wm}$  and $\\W\\in\\Cc_{\\mu,\\RR_\\mu}(\\Wm)$, there exist dataset dependent constants $C,c>0$ such that \n\\begin{subequations}\\label{local:g:lbound}\n\\begin{align}\n&C\\cdot \\frac{1}{n}\\sum_{i=1}^n \\left(1-\\s_{i\\alpha_i}\\right) \\geq -\\Big\\langle\\nabla\\Lc(\\W),\\V \\Big\\rangle\\geq c\\cdot  \\frac{1}{n} \\sum_{i=1}^n  \\left(1-\\s_{i\\alpha_i}\\right)>0, \\label{local1:g:bound} \\\\\n&\\tf{\\nabla\\Lc(\\W)}\\leq \\bar{A}C \\cdot \\frac{1}{n} \\sum_{i=1}^n  \\left(1-\\s_{i\\alpha_i}\\right), \\label{local2:g:bound}\\\\\n& -\\li\\frac{\\V}{\\tf{\\V}},\\frac{\\nabla\\Lc(\\W)}{\\tf{\\nabla\\Lc(\\W)}}\\ri \\geq  \\frac{c}{C} \\cdot \\frac{\\Theta}{\\bar{A}}>0. \\label{local3:g:bound}\n\\end{align}\n\\end{subequations}\nHere, $\\s_{i\\alpha_i}= (\\sft{\\X_i\\W \\z_{i}})_{\\alpha_i}$, $\\bar{A}=\\max_{i\\in[n],t,\\tau\\in[T]}\\tn{(\\x_{it}- \\x_{i\\tau})}~\\tn{\\z_i}$, and $\\Theta=1/\\tf{\\Ws}$.\n\n\n\n\n\n\\end{enumerate}\n\\end{lemma}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{proof}\nLet $R=\\RR_\\mu$, $(\\Tc_i)_{i=1}^n$ be the set of all \\neis per Definition \\ref{def loc opt}. Let $\\Tcb_i=[T]-\\Tc_i-\\{\\alpha_i\\}$ be the non-\\neis. Let\n\\begin{equation}\\label{mu choice}\n\\begin{split}\n&\\Theta=1/\\tf{\\Wm},\\\\\n&\\delta= \\frac{1}{2}\\min_{i\\in[n]}\\min_{t\\in\\Tc_i,\\tau\\in\\Tcb_i}(\\x_{it}-\\x_{i\\tau})^\\top \\Wm \\z_{i},\\\\\n&A=\\max_{i\\in[n],t\\in[T]} \\frac{\\tf{\\x_{it} \\z_i^\\top}}{\\Theta},\\\\\n& \\mu\\leq \\mu(\\delta)=\\frac{1}{8}\\left(\\frac{\\min(0.5,\\delta)}{A}\\right)^2.\n\\end{split}\n\\end{equation}\n\n\nSince $\\Wm$ is the max-margin model ensuring $(\\x_{i\\alpha_i}-\\x_{it})^\\top\\Wm \\z_i\\geq 1$, the following inequalities hold for all $\\W\\in \\cone_\\mu(\\Wm),~\\tf{\\W}=\\tf{\\Wm}$ and all $i\\in[n], t\\in\\Tc_i,\\tau\\in\\Tcb_i$:\n\\begin{equation}\\label{cone-non-nei}\n\\begin{split}\n(\\x_{it}-\\x_{i\\tau})^\\top \\W \\z_i&\\geq \\delta>0,\\\\\n(\\x_{i\\alpha_i}-\\x_{i\\tau})^\\top \\W \\z_i&\\geq 1+\\delta,\\\\\n\\frac{3}{2}\\geq(\\x_{i\\alpha_i}-\\x_{it})^\\top \\W \\z_i &\\geq \\frac{1}{2}.\n\\end{split}\n\\end{equation}\nHere, we used $\\tf{\\W-\\Wm}^2/\\tf{\\Wm}^2\\leq 2\\mu$ which implies $\\tf{\\W-\\Wm}\\leq \\sqrt{2\\mu}/\\Theta$.\n\n\n\n\n\nTo proceed, we write the gradient correlation following \\eqref{grad def} and \\eqref{grad def2}\n\\begin{align}\\label{grad def3}\n\\li\\nabla\\Lc(\\W),\\V\\ri&=\\frac{1}{n}\\sum_{i=1}^n\\ell'_i\\cdot\\hb_i^\\top\\sfp{\\hp_i}\\bgam_i,\n\\end{align}\nwhere we denoted $\\ell'_i=\\ell'(Y_i\\cdot \\vb^\\top \\X_i^\\top\\sft{\\hp_i})$, $\\hb_i=\\X_i\\V \\z_{i}$, $\\hp_i= \\X_i\\W \\z_{i}$, and $\\s_i=\\sft{\\hp_i}$.  \n\nUsing \\eqref{cone-non-nei}, for all $t\\in\\Tc_i,\\tau\\in \\Tcb_i$, for all $\\W\\in \\Cc_{\\mu,R}(\\Wm)$, we have that\n\\begin{align*}\n&\\hp_{it}-\\hp_{i\\tau}\\geq R\\Theta\\delta,\\\\\n&\\hp_{i\\alpha_i}-\\hp_{i\\tau}\\geq R\\Theta(1+\\delta),\\\\\n&\\hp_{i\\alpha_i}-\\hp_{it}\\geq R\\Theta/2.    \n\\end{align*}\nConsequently, we can bound the softmax probabilities $\\s_i=\\sft{\\hp_i}$ over non-\\neis as follows: For all $i\\in[n]$ and any $t_i\\in \\Tc_i$\n\\begin{subequations}\n\\begin{align}\\label{soft prob bound}\n&S_i:=\\sum_{\\tau\\in\\Tc_i}\\s_{i\\tau} \n\n\\leq T e^{-R\\Theta/2}\\s_{i\\alpha_i}\\leq T e^{-R\\Theta/2},\\\\\n&Q_i:=\\sum_{\\tau\\in\\Tcb_i}\\s_{i\\tau} \\leq T e^{-R\\Theta\\delta}\\s_{it_i}\\leq T e^{-R\\Theta\\delta}S_i.\n\\end{align}\n\\end{subequations}\n\nRecall scores $\\bgam_{it}=Y_i\\cdot\\vb^\\top \\x_{it}$. Define the score gaps over \\neis:\n\\begin{equation*}\n \\bgg_i=\\bgam_{i\\alpha_i}-\\max_{t\\in\\Tc_i}\\bgam_{it}~~~ \\textnormal{and}~~~ \\bgm_i=\\bgam_{i\\alpha_i}-\\min_{t\\in\\Tc_i}\\bgam_{it}. \n\\end{equation*}\nIt follows from \\eqref{mu choice} that \n\\begin{align*}\n&A=\\max_{i\\in[n],t\\in[T]} \\frac{\\tf{\\x_{it} \\z_i^\\top}}{\\Theta}\\geq \\max_{i\\in[n],t\\in[T]}\\tn{\\hb_{it}}.\n\\end{align*}\nDefine the $\\bal$-dependent global scalar $\\Gamma=\\sup_{i\\in[n],t,\\tau\\in[T]}|\\bgam_{it}-\\bgam_{i\\tau}|$.\n\n\nLet us focus on a fixed datapoint $i\\in[n]$, assume (without losing generality) $\\alpha_i=1$, and drop subscripts $i$.\n\nDirectly applying Lemma \\ref{lem:q_reduce}, we obtain\n\\[\n  \\big|\\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam-\\sum_{t\\geq 2}^T (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2\\Gamma A(1-\\s_1)^2.\n\\]\nTo proceed, let us decouple the non-\\neis within $\\sum_{t\\geq 2}^T (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)$ via\n\\[\n\\big|\\sum_{t\\in\\Tcb} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2Q\\Gamma A.\n\\]\nAggregating these, we found\n\\begin{align}\n  \\big|\\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam-\\sum_{t\\in \\Tc} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2\\Gamma A((1-\\s_1)^2+Q).\\label{aggregate}\n\\end{align}\n\nTo proceed, let us upper/lower bound the gradient correlation.  We use two bounds depending on $\\V\\in\\Sc_{\\mu}(\\Ws)$ (\\textbf{Case 1}) or general $\\V\\in\\R^{d\\times d}$ (\\textbf{Case 2}).\n\n\n\\noindent$\\bullet$ \\textbf{Case 1:  $\\V\\in\\Sc_{\\mu}(\\Ws)$.} Since $1.5\\geq \\hb_1-\\hb_t\\geq 0.5$ following \\eqref{cone-non-nei}, we find\n\\[\n 1.5\\cdot S\\cdot \\bgm  \\geq\\sum_{t\\in \\Tc} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\geq 0.5\\cdot S\\cdot \\bgg,\n\\]\nwhere recall the definition of $S$ (having dropped subscripts) in \\eqref{soft prob bound}. \n\n\\noindent$\\bullet$ \\textbf{Case 2: $\\Vb\\in\\R^{d\\times d}$ and $\\tf{\\V}=\\tf{\\Wm}$.}  Define $\\bar{A}=\\max_{i\\in[n],t,\\tau\\in[T]}\\tn{\\x_{it}-\\x_{i\\tau}}~\\tn{\\z_i}$. For any $\\tf{\\V}=\\tn{\\Ws}$, we use the fact that\n$$\\tn{\\hb_1-\\hb_t}\\leq \\tf{(\\x_{it}-\\x_{i\\tau}) \\z_i^\\top}\\cdot\\tf{\\V}\\leq \\frac{\\bar{A}}{\\Theta}.$$\nNote that by definition $ \\frac{\\bar{A}}{\\Theta} \\geq 1$. To proceed, we can upper bound\n\\begin{align}\n\\frac{\\bar{A}}{\\Theta}\\cdot S\\cdot \\bgm  \\geq\\sum_{t\\in \\Tc} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t).\\label{wishwish2}\n\\end{align}\n\nNext we claim that for both cases, $S$ dominates $((1-\\s_1)^2+Q)$ for large $R$. Specifically, we wish for \n\\begin{align}\\label{wishfor}\n\\frac{S\\cdot \\bgg}{4}\\geq 4\\Gamma A\\max((1-\\s_1)^2,Q)\\iff S\\geq 16\\frac{\\Gamma A}{\\bgg}\\max((1-\\s_1)^2,Q).\n\\end{align}\nNow choose $R\\geq \\delta^{-1}\\log(T)/\\Theta$  to ensure $Q\\leq S$ since $Q\\leq Te^{-R\\Theta\\delta}S$ from \\eqref{soft prob bound}. Consequently\n\\[\n(1-\\s_1)^2=(Q+S)^2\\leq 4S^2\\leq 4STe^{-R\\Theta/2}.\n\\]\nCombining these, what we wish is ensured by guaranteeing\n\\begin{align}\\label{s bound}\n  S\\geq 16\\frac{\\Gamma A}{\\bgg}\\max(4STe^{-R\\Theta/2},Te^{-R\\Theta\\delta}S).\n\\end{align}\nThis in turn is ensured for all inputs $i\\in[n]$ by choosing \n\\begin{align}\\label{R bound}\nR\\geq \\frac{\\max(2,\\delta^{-1})}{\\Theta}\\log\\left(\\frac{64T\\Gamma A}{\\bggm}\\right),\n\\end{align}\nwhere $\\bggm=\\min_{i\\in[n]}\\bgg_i$ is the global scalar which is the worst case score gap over all inputs. \n\\\\\n$\\bullet$ \\textbf{Case 1: $\\V\\in\\Sc_{\\mu}(\\Ws)$}. With the above choice of $R$, we guaranteed\n\\[\n  2 (1-\\s_1)\\cdot \\bgm\\geq 2\\cdot S\\cdot \\bgm \\geq \\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam\\geq\\frac{S\\cdot \\bgg}{4}\\geq\\frac{(1-\\s_1) \\bgg}{8}.\n\\]\nvia \\eqref{wishfor} and \\eqref{aggregate}. \n\nSince this holds over all inputs, going back to the gradient correlation \\eqref{grad def3} and averaging above over all inputs $i\\in[n]$ and plugging back the indices $i$, we obtain the advertised bound \n\\begin{align}\\label{pbb corr}\n\n\\frac{2}{n}\\sum_{i\\in [n]} -\\ell'_i\\cdot S_i\\cdot \\bgm_i\\geq -\\li\\nabla\\Lc(\\W),\\V\\ri\\geq \\frac{1}{8n}\\sum_{i\\in [n]} -\\ell'_i\\cdot S_i\\cdot \\bgg_i.\n\\end{align}\nLet $-\\ell'_{\\min/\\max}$ be the min/max values negative loss derivative admits over the ball $[-A,A]$ and note that $\\max_{i\\in[n]}\\bgm_i>0$ and $\\min_{i\\in[n]}\\bgg_i>0$ are dataset dependent constants. Then, we declare the constants $C=-2\\ell'_{\\max}\\cdot \\max_{i\\in[n]}\\bgm_i>0,c=-(1/8)\\ell'_{\\min}\\cdot \\min_{i\\in[n]}\\bgg_i>0$ to obtain the bound \\eqref{local1:g:bound}. \n\\vspace{.2cm}\n\\\\\n\\noindent$\\bullet$ \\textbf{Case 2: $\\Vb\\in\\R^{d\\times d}$ and $\\tf{\\V}=\\tf{\\Wm}$.} Next, we show \\eqref{local2:g:bound} and \\eqref{local3:g:bound}. For any $\\V \\in \\mathbb{R}^{d \\times d}$ satisfying $\\tf{\\V}=\\tf{\\Ws}$, using \\eqref{wishwish2} and the  choice of $R$ in \\eqref{R bound} similarly guarantees \n$$\n\\frac{2\\bar{A}}{\\Theta }(1-\\s_1) \\bgm\\geq \\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam,\n$$\nfor fixed input. Going back to the gradient correlation \\eqref{grad def3} and averaging above over all inputs $i\\in[n]$, with the same definition of $C>0$, we obtain\n\\begin{align}\n\\frac{ \\bar{A} C}{  \\Theta n}\\sum_{i\\in [n]} (1-\\s_{i\\alpha_i})\\geq -\\li\\nabla\\Lc(\\W),\\V\\ri.\\label{local lamma general upper}\n\\end{align}\nTo proceed, since \\eqref{local lamma general upper} holds for any $\\V\\in\\R^{d\\times d}$, we observe that when setting $\\V=\\frac{\\tf{\\Ws}}{\\tf{\\nabla\\Lc(\\W)}}\\cdot \\nabla\\Lc(\\W)$, this implies that\n\\[ \n\\li\\nabla\\Lc(\\W),\\V\\ri = \\tf{\\nabla\\Lc(\\W)}\\cdot \\tf{\\Ws}\\leq \\frac{\\bar{A} C}{\\Theta \n n}\\sum_{i\\in [n]} (1-\\s_{i\\alpha_i}).\n\\]\nSimplifying $\\Theta=1/\\tf{\\Ws}$ on both sides gives \\eqref{local2:g:bound}. \n\\\\\nCombining the above inequality with \\eqref{pbb corr}, we obtain that for all $\\V,\\W\\in\\Sc_{\\mu}(\\Ws)$\n\\[ \n-\\li\\frac{\\V}{\\tf{\\V}},\\frac{\\nabla\\Lc(\\W)}{\\tf{\\nabla\\Lc(\\W)}}\\ri\\geq \\frac{c \\Theta }{C\\bar{A}},\n\\]\nwhich gives \\eqref{local3:g:bound}.\n\n\n\n\n\\end{proof}\n\n\\begin{lemma}\n\\label{lem:local:corr} \nSuppose Assumption~\\ref{assum:loss:prope} on the loss function $\\ell$ holds, and let $\\bal=(\\alpha_i)_{i=1}^n$ be locally optimal tokens according to Definition \\ref{def loc opt}. Let $ \\Wm= \\Wm_\\bal$ denote the SVM solution obtained via \\eqref{eqn:sattnsvm} by replacing $(\\opt_i)_{i=1}^n$ with $\\boldsymbol{\\alpha} = (\\alpha_i)_{i=1}^n$. Let $\\mu=\\mu(\\bal)>0$ and $\\bar{R}_{\\mu}$ be defined as in Lemma~\\ref{local cond}. For any choice of $\\pi>0$, there exists $R_\\pi \\geq \\bar{R}_{\\mu}$ such that, for any $ \\W\\in \\Cc_{\\mu,R_\\pi}(\\Wm)$, we have\n\\[\n \\li \\nabla\\Lc(\\W), \\frac{\\W}{\\tf{\\W}} \\ri\\geq (1+\\pi)\\li \\nabla\\Lc(\\W), \\frac{\\Wm}{\\tf{\\Wm}}\\ri.\n\\]\n\\end{lemma}\n\\begin{proof}\nLet  $R=R_{\\pi}$, $\\Wb=\\tf{\\Wm} \\W/\\tf{\\W} $, $\\hb_i=\\X_i\\Wb \\z_{i}$, and $\\hbm_i= \\X_i \\Wm \\z_{i}$.   To establish the result, we will prove that, for sufficiently large $R$ and for any $\\W\\in \\Cc_{\\mu,R}(\\Wm)$:\n\\begin{align}\\label{main local cond}\n\\nonumber \n\\li -\\nabla\\Lc(\\W),\\frac{\\W}{\\tf{\\W}}\\ri&= -\\frac{1}{n}\\sum_{i=1}^n\\ell'_i \\cdot  \\li \\hb_i, \\sfp{\\X_i\\W \\z_{i}}\\bgam_i\\ri\\\\\n&\\leq - \\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot  \\li\\hbm_i, \\sfp{\\X_i\\W \\z_{i}}\\bgam_i\\ri=(1+\\pi)\\li-\\nabla\\Lc(\\W), \\frac{\\ps}{\\tf{\\Ws}}\\ri.\n\\end{align}\n\nFollowing \\eqref{aggregate}, for all $\\W\\in \\Sc_{\\mu}(\\Wm)$ with $\\tf{\\W}=\\tf{\\Wm}$, $\\hp=\\X\\W \\z$, and $\\s=\\sft{\\hp}$, we have found\n\\begin{align}\n  \\big|\\hp^\\top_i\\diag{\\s_i}\\bgam_i-\\hp^\\top_i\\s_i\\s^\\top_i\\bgam_i-\\sum_{t\\in \\Tc_i} (\\hp_{i1}-\\hp_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\big|\\leq 2\\Gamma A((1-\\s_{i1})^2+Q_i), \n\\end{align}\nwhere $\\Tc_i$ is the set of support indices. \n\n\nPlugging in $\\hb,\\hbm$ in the bound above and assuming $\\pi\\leq 1$ (w.l.o.g.), \\eqref{main local cond} is implied by the following stronger inequality\n\\begin{align*}\n-\\frac{1}{n}&\\sum_{i=1}^n\\ell'_i \\cdot \\left(6\\Gamma A((1-\\s_{i1})^2+Q_i)+ \\sum_{t\\in \\Tc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it}) \\right)\\\\\n&\\leq -\\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i  \\cdot \\sum_{t\\in \\Tc_i} (\\hbm_{i1}-\\hbm_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n&=-\\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align*}\nFirst, we claim that $0.5\\pi\\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\geq 6\\Gamma A((1-\\s_{i1})^2+Q_i)$ for all $i \\in [n]$.  The proof of this claim directly follows the earlier argument, namely, following \\eqref{wishfor}, \\eqref{s bound}, and \\eqref{R bound}  which leads to the choice \n\\begin{equation}\\label{R boundC0}\nR \\ge\\frac{\\max(2,\\delta^{-1})}{\\Theta}\\log\\left(\\frac{C_0\\cdot T\\Gamma A}{\\pi\\bggm}\\right),    \n\\end{equation}\nfor some constant $C_0>0$. Using \\eqref{R bound}, we choose $C_0 \\geq 64 \\pi$ to guarantee $R=R_\\pi \\geq \\bar{R}_{\\mu}$.\n\nFollowing this control over the perturbation term $6\\Gamma A((1-\\s_{i1})^2+Q_i)$, to conclude with the result, what remains is proving the comparison\n\\begin{align}\\label{desired comp}\n-\\frac{1}{n} \\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\in \\Tc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq - \\frac{1+0.5\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align}\nTo proceed, we split the problem into two scenarios. \n\n\\noindent\\textbf{Scenario 1:} $\\tf{\\Wb-\\Wm}\\leq \\eps=\\frac{\\pi}{4A\\Theta}$ for some $\\eps>0$.  In this scenario, for any $ t\\in \\Tc_i$ and $i\\in [n ]$, we have\n\\[\n|\\hb_{it}-\\hbm_{it}|=|\\x_{it}^\\top (\\Wb-\\Wm)  \\z_{it}|\\leq A\\Theta\\eps=\\frac{\\pi}{4}.\n\\]\nConsequently, we obtain \n\\[\n\\hb_{i1}-\\hb_{it}\\leq \\hbm_{i1}-\\hbm_{it}+2A\\Theta\\eps= 1+0.5\\pi.\n\\] \nSimilarly, $\\hb_{i1}-\\hb_{it}\\geq 1-0.5\\pi\\geq 0.5$. Since all terms $\\hb_{i1}-\\hb_{it},\\s_{it},\\bgam_{i1}-\\bgam_{it}$ in \\eqref{desired comp} are nonnegative and $(\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq (1+0.5\\pi)\\s_{it}(\\bgam_{i1}-\\bgam_{it})$, above implies the desired result in \\eqref{desired comp}.\n\n\\vspace{3pt}\n\\noindent\\textbf{Scenario 2:} $\\tf{\\Wb-\\Wm}\\geq \\eps=\\frac{\\pi}{4A\\Theta}$.  Since $\\Wb$ is not (locally) max-margin, in this scenario, for some $i \\in  [n]$, $\\nu=\\nu(\\eps)>0$, and $\\tau\\in\\Tc_i$, we have that\n\\begin{align*}\n\\hb_{i1}-\\hb_{i\\tau}\\leq 1-2\\nu.\n\\end{align*}\nHere $\\tau=\\arg\\max_{\\tau\\in\\Tc_i} \\x_{i\\tau}\\Wb \\z_i$ denotes the nearest point to $\\hb_{i1}$ (along the $\\Wb$ direction). Note that a non-neighbor $t\\in\\Tcb_i$ cannot be nearest because $\\Wb\\in \\cone_{\\mu}(\\ps)$ and \\eqref{cone-non-nei} holds. Recall that $\\s_i=\\sft{\\RR\\hb_i}$ where $\\RR=\\tf{\\W}\\Theta \\geq R\\Theta$. To proceed, let $ \\underline{\\hb}_i:=\\min_{t \\in\\mc{T}_i}\\hb_{i1}-\\hb_{it}$,\n\\begin{align*}\n\\mc{I}:=\\left\\{ i\\in[n]: \\underline{\\hb}_i \\leq 1-2\\nu \\right\\}, \\qquad [n]-\\mc{I}:=\\left\\{ i\\in[n]:  1-2\\nu  <  \\underline{\\hb}_i\\right\\}.\n\n\n\\end{align*}\nFor all $ i \\in [n]-\\mc{I}$,\n\\begin{equation}\\label{eqn:grad:difff2}\n\\begin{split}\n      \\sum_{t\\in \\Tc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it}) &- (1+0.5\\pi) \\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n      & \\leq  \\left(2A - (1+0.5\\pi)\\right)\\Gamma\\sum_{t\\in \\Tc_i,~\\hb_{i1}-\\hb_{it} \\geq 1+\\frac{\\pi}{2} } \\s_{it} \\\\\n      & \\leq  \\left(2A - (1+0.5\\pi)\\right)\\Gamma Te^{-\\RR(1+\\frac{\\pi}{2})} \\\\\n      &\\leq   2A\\Gamma  T e^{-\\RR(1+\\frac{\\pi}{2})}.\n      \n      \n      \n\\end{split}\n\\end{equation}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor all $ i \\in \\mc{I}$, split the tokens into two groups: Let $\\Nc_i$ be the group of tokens obeying $ \\hb_{i1}-\\hb_{it} \\leq 1-\\nu$ and $\\Tc_i-\\Nc_i$ be the rest of the neighbors. Observe that\n\\[\n\\frac{\\sum_{t\\in \\Tc_i-\\Nc_i}\\s_{it}}{\\sum_{t\\in\\Tc_i}\\s_{it}}\\leq  T\\frac{e^{\\nu \\RR}}{e^{2\\nu\\RR}}=Te^{-\\RR\\nu}.\n\\]\nUsing $|\\hb_{i1}-\\hb_{it}|\\leq 2A=2 \\max_{i\\in[n],t\\in[T]}\\tn{\\kb_{it}}/\\Theta$ and  $\\bggm=\\min_{i\\in[n]}\\bgg_i =\\min_{i\\in[n]} (\\bgam_{i1}-\\max_{t\\in\\Tc_i}\\bgam_{it})$, observe that \n\\[\n\\sum_{t\\in\\Tc_i-\\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq \\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm} \\sum_{t\\in \\Tc_i} \\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\]\nThus, \n\\begin{align*}\n  \\sum_{t\\in \\Tc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})&= \\sum_{t\\in \\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})+\\sum_{t\\in\\Tc_i-\\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\nonumber\\\\\n\n  &\\leq \\sum_{t\\in \\Nc_i} (1-\\nu)\\s_{it}(\\bgam_{i1}-\\bgam_{it})+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm} \\sum_{t\\in \\Tc_i} \\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n  &\\leq \\left(1-\\nu+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}\\right)\\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n &\\leq \\left(1+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}\\right)\\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align*}\nHence, choosing \n\\begin{align}\nR\\geq\\frac{1}{\\nu\\Theta}\\log\\left(\\frac{8\\Gamma AT}{\\bggm\\pi}\\right)\\label{R bound pi 1}\n\\end{align}\nresults in that\n\\begin{equation}\\label{eqn:grad:difff3}\n    \\begin{split}\n     &\\sum_{t\\in \\Tc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})  - (1+\\frac{\\pi}{2}) \\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}) \\\\\n   &\\leq\\left(\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}-\\frac{\\pi}{2}\\right)\\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n   &\\leq -\\frac{\\pi}{4}\\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n   &\\leq-\\frac{\\pi}{4T}\\bggm  e^{-\\bar{R} (1-2\\nu)}.      \n    \\end{split}\n\\end{equation}\nHere, the last inequality follows from the fact that $\\sum_{t\\in \\Tc_i}\\s_{it}\\geq \\max_{t\\in\\Tc_i}s_{it}\\geq\\frac{e^{-\\bar{R}(1-2\\nu)}}{\\sum_{t=1}^Te^{-\\bar{R}(\\hb_{i1}-\\hb_{it})}}\\geq e^{-\\bar{R}(1-2\\nu)}/T$.\n\n\nFrom Assumption~\\ref{assum:loss:prope}, we have $c_{\\min}\\leq-\\ell'\\leq c_{\\max}$ for some positive constants $c_{\\min}$ and $c_{\\max}$. It follows from  \\eqref{eqn:grad:difff2} and \\eqref{eqn:grad:difff3} that \n\\begin{align*}\n-\\frac{1}{n}\\sum_{i}^n \\ell_i' \\cdot&\\left(\n      \\sum_{t\\in \\Tc_i} (\\hb_{i1}-_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})- \\sum_{t\\in \\Tc_i} (1+0.5\\pi)\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\right)\\\\\n      & \\leq    c_{\\max}2A\\Gamma  T \\Gamma e^{-\\RR(1 +\\frac{\\pi}{2})}-\\frac{c_{\\min}}{nT}\\cdot\\frac{\\pi\\bggm}{4}e^{-\\bar{R} (1-2\\nu)}\\\\\n      \n      & \\leq 0.\n\\end{align*}\nCombing with \\eqref{R bound pi 1}, this is guaranteed by \nchoosing \n\n\n\n\\[\n  R\\geq \\max\\left\\{\\frac{1}{\\nu\\Theta}\\log\\left(\\frac{8\\Gamma AT}{\\bggm\\pi}\\right),\\frac{1}{(2\\nu+\\pi/2)\\Theta}\\log\\left(\\frac{8n\\Gamma AT^2 c_{\\max}}{c_{\\min}\\bggm\\pi}\\right)\\right\\},\n\\]\nwhere $\\nu=\\nu(\\frac{\\pi}{4A\\Theta})$ depends only on $\\pi$ and global problem variables. \n\nCombining this with the prior $R$ choice \\eqref{R boundC0} (by taking maximum), we conclude with the statement.\n\n\n\\end{proof}\n\n\n\n\\subsection{Proof of Theorem~\\ref{thm:local:gd}}\nThe proof of this theorem follows the proof of \\cite[Theorem 3]{tarzanagh2023margin}. Let us denote the initialization lower bound as $R^0_\\mu:=R$, where $R$ is given in the Theorem~\\ref{thm:local:gd}'s statement. Consider an arbitrary value of  $\\epsilon \\in (0, \\mu/2)$ and let $1/(1+\\pi)=1-\\epsilon$. We additionally denote $R_\\eps\\gets R_\\pi\\vee 1/2$ where $R_\\pi$ was defined in Lemma~\\ref{lem:local:corr}.  At initialization $\\W(0)$, we set $\\eps=\\mu/2$ to obtain $R^0_\\mu= R_{\\mu/2}$, and provide the proof in four steps:\n\\\\\n\\textbf{Step~1: There are no stationary points within $\\Cc_{\\mu,R^0_\\mu}(\\Ws)$.} We begin by proving that there are no stationary points within  $\\Cc_{\\mu,R^0_\\mu}(\\Ws)$. Let $(\\Tc_i)_{i=1}^n$ denote  the sets of \\neis as defined in Definition \\ref{def loc opt}. We define $\\Tcb_i=[T]-\\Tc_i-\\{\\alpha_i\\}$ as the tokens that are non-\\neis. Additionally, let $\\mu$ be defined as in \\eqref{mu choice}. Then, since $R^0_\\mu\\geq \\RR_\\mu$ per Lemma \\ref{lem:local:corr}, we can apply Lemma~\\ref{local cond} to find that: For all $\\V,\\W\\in  \\Sc_{\\mu}(\\Ws)$ with $\\tf{\\W} \\neq 0$ and $\\tf{\\W} \\geq R^0_\\mu$,  we have that $- \\li\\V, \\nabla \\Lc(\\W)\\ri$ is strictly positive.\n\\\\\n\\textbf{Step~2:}  It follows from Lemma~\\ref{lem:local:corr} that, there exists $ R_\\epsilon\\geq \\bar{R}_\\mu\\vee 1/2$ such that all  $ \\W \\in \\Cc_{\\mu,R_\\epsilon}(\\Wm)$ satisfy\n\\begin{align}\\label{eqn:neg:corr:local}\n\\iprod{-\\nabla\\mc{L}(\\W)}      {\\frac{\\Wm}{\\tf{\\Wm}}} \\geq (1-\\epsilon)    \\iprod{-\\nabla \\mc{L}(\\W)}{\\frac{\\W}{\\tf{\\W}}}.\n\\end{align}\nThe argument below applies to a general $\\eps\\in(0,\\mu/2)$. However, at initialization $\\W(0)$, we set $\\eps=\\mu/2$ and, recalling above, initialization lower bound was defined as $R^0_\\mu:= R_{\\mu/2}$. To proceed, for any $\\epsilon \\in (0, \\mu/2)$, we will show that after gradient descent enters the conic set $\\Cc_{\\mu,R_\\eps}(\\Ws)$ for the first time, it will never leave the set. Let $t_\\eps$ be the first time gradient descent enters $\\Cc_{\\mu,R_\\eps}(\\Ws)$. In \\textbf{Step 4}, we will prove that such $t_\\eps$ is guaranteed to exist. Additionally, for $\\eps\\gets\\mu/2$, note that $t_\\eps=0$ i.e.~the point of initialization.\n\n\\\\\n\n\\textbf{Step~3: Updates remain inside the cone $\\Cc_{\\mu,R_\\eps}(\\Ws)$.} \nBy leveraging the results from \\textbf{Step 1} and \\textbf{Step 2}, we demonstrate that the gradient iterates, with an appropriate constant step size, starting from $\\W(k_\\eps) \\in \\Cc_{\\mu,R_\\eps}(\\Ws)$, remain within this cone. \n\nWe proceed by induction. Suppose that the claim holds up to iteration $k \\geq k_\\eps$. This implies that $ \\W(k) \\in \\Cc_{\\mu,R_\\eps}(\\Ws)$. Hence, recalling cone definition, there exists scalar $\\mu=\\mu(\\bal) \\in (0,1)$  and $R $ such that  $\\tf{\\W(k)}\\geq R$, and\n\\begin{equation*}\n\\begin{split}\n\\left\\langle \\frac{\\W(k)}{\\tf{\\W(k)}},\\frac{\\Wm}{\\tf{\\Wm}} \\right\\rangle  \\geq 1-\\mu. \n\\end{split}\n\\end{equation*}\nFor all $k\\geq 1$, let\n\\begin{align}\\label{eqn:rho:def}\n\\rho (k) := - \\frac{1}{1-\\epsilon} \\iprod{\\nabla\\mc{L}(\\W(k))}\n     {\\frac{\\Wm}{\\tf{\\Wm}}}.\n\\end{align}\nNote that $\\rho (k) >0$ due to \\textbf{Step 1}. This together with the gradient descent update rule gives \n\\begin{subequations}\n\\begin{equation}\\label{eqn:localgd:1}\n\\begin{split}\n     \\left\\langle \\frac{\\W(k+1)}{\\tf{\\W(k)}},\\frac{\\Wm}{\\tf{\\Wm}} \\right\\rangle  &=    \\left\\langle \\frac{\\W(k)}{\\tf{\\W(k)}} -\\frac{\\eta}{\\tf{\\W(k)}}\\nabla \\mc{L}(\\W(k)), \\frac{\\Wm}{\\tf{\\Wm}} \\right\\rangle\\\\\n      &\\ge 1-\\mu- \\frac{\\eta}{\\tf{\\W(k)}}\\iprod{\\nabla \\mc{L}(\\W(k))} {\\frac{\\Wm}{\\tf{\\Wm}}} \\\\\n      & \\geq 1-\\mu +\\frac{\\eta \\rho (k)  (1-\\epsilon)}{\\tf{\\W(k)}}.\n\\end{split}\n\\end{equation}\nNote that from Lemma~\\ref{local cond}, we have $\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle<0$~ which implies that $\\tf{\\W(k+1)} \\geq \\tf{\\W(k)}$.  This together with  $R_\\eps$ definition and $\\tf{\\W(k)}\\geq 1/2$ implies that\n\\begin{align*}\n\\tf{\\W(k+1)}&\\leq\\frac{1}{{2\\tf{\\W(k)}}} \\left(\\tf{\\W(k+1)}^2+\\tf{\\W(k)}^2\\right)\\\\\n& = \\frac{1}{2\\tf{\\W(k)}} \\left(2\\tf{\\W(k)}^2-2\\eta\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle+\\eta^2\\tf{\\nabla \\Lc(\\W(k))}^2\\right)\\\\\n       &\\leq  \\tf{\\W(k)}- \\frac{\\eta}{\\tf{\\W(k)}}\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle + \\eta^2 \\tf{\\nabla \\Lc(\\W(k))}^2, \n\\end{align*}\n\n\nwhich gives\n\\begin{equation}\\label{eqn:localgd:2}\n\\begin{split}\n  \\frac{\\tf{\\W(k+1)}}{\\tf{\\W(k)}}& \\leq  1- \\frac{\\eta}{\\tf{\\W(k)}}\n       \\left\\langle \\nabla \\Lc(\\W(k)),\\frac{\\W(k)}{\\tf{\\W(k)}} \\right\\rangle + \\eta^2 \\frac{\\|\\nabla \\mc{L}(\\W(k))\\|^2}{\\tf{\\W(k)}}\\\\\n& \\leq 1- \\frac{\\eta}{(1-\\epsilon)\\tf{\\W(k)}}  \\iprod{\\nabla\\mc{L}(\\W(k))}\n     {\\frac{\\Wm}{\\tf{\\Wm}}}+ \\eta^2 \\frac{\\|\\nabla \\mc{L}(\\W(k))\\|^2}{\\tf{\\W(k)}}\\\\\n      & \\leq  1 + \\frac{\\eta \\rho (k) }{\\tf{\\W(k)}} + \\frac{\\eta^2\\|\\nabla \\mc{L}(\\W(k))\\|^2}{\\tf{\\W(k)}}=:C_1(\\rho (k) ,\\eta).\n\\end{split}\n\\end{equation}\n\\end{subequations}\nHere, the second inequality follows from \\eqref{eqn:neg:corr:local} and \\eqref{eqn:rho:def}.\n\nNow, it follows from \\eqref{eqn:localgd:1} and \\eqref{eqn:localgd:2} that   \n\\begin{equation}\\label{eqn:wt+1:cone}\n\\begin{split}\n\\left\\langle \\frac{\\W(k+1)}{\\|\\W(k+1)\\|},\\frac{\\Ws}{\\|\\Ws\\|} \\right\\rangle   &\\geq \\frac{1}{C_1(\\rho(k),\\eta)} \\left(1-\\mu +\\frac{\\eta \\rho(k)(1-\\epsilon)}{\\tf{\\W(k)}}\\right)\\\\\n& = 1-\\mu+ \\frac{1}{C_1(\\rho(k),\\eta)} \\left((1-\\mu)(1-C_1(\\rho(k),\\eta)) +\\frac{\\eta \\rho(k)(1-\\epsilon)}{\\tf{\\W(k)}}\\right)\\\\\n& = 1-\\mu+ \\frac{\\eta}{C_1(\\rho(k),\\eta)} \\left((\\mu-1)(\\frac{ \\rho(k)}{\\tf{\\W(k)}} + \\frac{\\eta\\|\\nabla \\mc{L}(\\W(k))\\|^2}{\\tf{\\W(k)}}) +\\frac{ \\rho(k)(1-\\epsilon)}{\\tf{\\W(k)}}\\right)\\\\\n& = 1-\\mu+\\frac{\\eta}{C_1(\\rho(k),\\eta)} \\left(\\frac{\\rho(k)(\\mu -\\epsilon)}{\\tf{\\W(k)}}\n-   \\eta (1-\\mu) \\frac{\\|\\nabla \\mc{L}(\\W(k))\\|^2 }{\\tf{\\W(k)}}\\right)\\\\\n& \\geq 1-\\mu, \n\\end{split}\n\\end{equation}\nwhere the last inequality uses our choice of stepsize $\\eta\\leq 1/L_W$ in Theorem~\\ref{thm:local:gd}'s statement. Specifically, we need $\\eta$ to be small to ensure the last inequality. We will guarantee this by choosing a proper $R_\\eps$ in Lemma \\ref{lem:local:corr}. Specifically, Lemma \\ref{lem:local:corr} leaves the choice of $C_0$ in $R_\\eps$ lower bound of \\eqref{R boundC0} open (it can always be chosen larger). Here, by choosing $C_0\\gtrsim 1/L_{\\W}$ will ensure $\\eta\\leq 1/L_W$ works well.\n\\begin{align}\\label{eqn:zeta:mu}\n\\nonumber\n\\eta &\\leq \\frac{\\mu}{2(1-\\mu)(1-\\frac{\\mu}{2})}\n \\frac{c}{C}  \\frac{\\Theta}{\\bar{A}}   \\frac{1}{\\bar{A}C T}    e^{R_\\mu^0\\Theta/2} \\\\\n& \\leq \\frac{\\mu-\\epsilon}{1-\\mu} \\cdot  \\frac{1}{1-\\epsilon} \\cdot \\frac{c}{C} \\cdot \\frac{\\Theta}{\\bar{A}} \\cdot  \\frac{1}{\\bar{A}C T}  e^{R_\\mu^0\\Theta/2} \\leq \\frac{(\\mu-\\epsilon)}{1-\\mu}  \\frac{\\rho(k) } { \\|\\nabla \\mc{L}(\\W(k))\\|^2_F}.\n\\end{align}\nHere, the first inequality uses our choice of $\\epsilon \\in (0, \\mu/2)$ (see \\textbf{Step 2}), and the last inequality is obtained from Lemma~\\ref{local cond} since\n\\begin{align*}\n    \\frac{\\rho(k) } { \\tf{\\nabla \\mc{L}(\\W(k))}} &= - \\frac{1}{1-\\epsilon} \\iprod{ \\frac{\\nabla\\mc{L}(\\W(k))}{\\tf{\\nabla \\mc{L}(\\W(k))}}}\n     {\\frac{\\Wm}{\\tf{\\Wm}}}  \\geq \\frac{1}{1-\\epsilon} \\cdot \\frac{c}{C} \\cdot \\frac{\\Theta}{\\bar{A}},\\\\\n         \\frac{1} { \\tf{\\nabla \\mc{L}(\\W(k))}} &\\geq \\frac{1}{\\bar{A}C \\cdot \\frac{1}{n} \\sum_{i=1}^n  \\left(1-\\s_{i\\alpha_i}\\right)}  \\geq    \\frac{1}{ \\bar{A} C T e^{-R_\\mu^{0}\\Theta/2}} \n\\end{align*}\nfor some data dependent constrants $c$ and $C$, $\\bar{A}=\\max_{i\\in[n],t,\\tau\\in[T]}\\tn{(\\x_{it}- \\x_{i\\tau})}~\\tn{\\z_i}$, and $\\Theta=1/\\tf{\\Ws}$.\n\nNext, we will demonstrate that the choice of $\\eta$ in \\eqref{eqn:zeta:mu} does indeed meet our step size condition as stated in the theorem, i.e., $\\eta \\leq 1/L_{\\W}$. Recall that $1/(1+\\pi)=1-\\epsilon$, which implies that $\\pi =\\epsilon/(1-\\epsilon)$. Combining this with \\eqref{R boundC0}, we obtain:\n\\begin{align}\\label{eqn:pitoC0}\nR_\\pi &\\geq\\frac{\\max(2,\\delta^{-1})}{\\Theta}\\log\\left(\\frac{C_0 T\\Gamma A}{\\pi\\bggm}\\right), \\quad \\textnormal{where} \\quad C_0 \\geq 64 \\pi.\\\\\n& \\Rightarrow R_\\epsilon \\geq\\frac{\\max(2,\\delta^{-1})}{\\Theta}\\log\\left(\\frac{ (1-\\epsilon)C_0 T\\Gamma A}{\\epsilon\\bggm}\\right),\\quad  \\textnormal{where}   \\quad C_0 \\geq 64  \\frac{\\epsilon}{1-\\epsilon}.\n\\end{align}\nOn the other hand, at the initialization, we have  $\\epsilon=\\mu/2$ which implies that \n\\begin{align}\\label{eqn:rmu:c0}\n  R_{\\mu}^0 \\geq \\frac{\\max(2,\\delta^{-1})}{\\Theta}\\log\\left(\\frac{ (2-\\mu)C_0 T\\Gamma A}{\\mu\\bggm}\\right),  \\quad  \\textnormal{where} \\quad  C_0 \\geq 64  \\frac{\\mu}{2(1-\\frac{\\mu}{2})}.\n\\end{align}\nIn the following, we will determine  a lower bound on  $C_0$ such that our step size condition in Theorem~\\ref{thm:local:gd}'s statement, i.e., $\\eta \\leq 1/L_{\\W}$, is satisfied. Note that for the choice of $\\eta$ in \\eqref{eqn:zeta:mu} to meet the condition $\\eta \\leq 1/L_{\\W}$, the following condition must hold:\n\\begin{equation}\n \\frac{1}{L_{\\W}}\\leq \n\\frac{\\mu}{ (2-\\mu)} \\frac{1}{C_2T} e^{R_\\mu^0\\Theta/2}  \\Rightarrow R_\\mu^0 \\geq  \\frac{2}{\\Theta} \\log  \\left(\\frac{1}{L_{\\W}}   \\frac{2-\\mu}{\\mu} C_2 T\\right).\n\\end{equation}\nwhere\n$C_2 = (1-\\mu)  \\frac{  \\bar{A}^2 C^2 }{ \\Theta c}$.\n\n \nThis together with \\eqref{eqn:rmu:c0} implies that \n\\begin{align}\\label{eqn:pitoC02}\n\\frac{C_0 \\Gamma A}{\\bggm}  & \\geq (1-\\mu) \\frac{C_2}{L_{\\W}}   \\Rightarrow  C_0 \\geq  \\max \\left( \\frac{(1-\\mu)C_2}{L_{\\W}}    \\frac{\\bggm }{\\Gamma A},  \\frac{64\\mu}{2-\\mu} \\right). \n\\end{align}\nTherefore, with this lower bound on $C_0$, the step size bound in \\eqref{eqn:zeta:mu} is sufficiently large to ensure that $\\eta \\leq 1/L_{\\W}$ guarantees \\eqref{eqn:wt+1:cone}. \n\nHence,  it follows from \\eqref{eqn:wt+1:cone} that $\\W(k+1) \\in \\Cc_{\\mu,R_\\eps}(\\Ws)$.\n\\\\\n\\textbf{Step 4: The correlation of $\\W(k)$ and $\\Wm$ increases over $k$.} \nFrom Step 3, we have that all iterates remain within the initial conic set i.e.~$\\W(k)\\in\\Cc_{\\mu,R^0_\\mu}(\\Ws)$ for all $k\\geq 0$. Note that it follows from Lemma~\\ref{local cond} that  $\\li\\nabla\\Lc(\\W), \\Ws/\\tf{\\Ws}\\ri<0$, for any finite $\\W \\in \\Cc_{\\mu,R^0_\\mu}(\\Ws)$. Hence, there are no finite critical points $\\W \\in \\Cc_{\\mu,R^0_\\mu}(\\Ws)$, for which $\\nabla \\mc{L} (\\W)=0$. Now, based on Lemma~\\ref{lem:grad:descent}, which guarantees that $\\nabla\\Lc(\\W(k))\\rightarrow 0$, this\nimplies that $\\left\\Vert \\W\\left(t\\right)\\right\\Vert_F\\rightarrow\\infty$. Consequently, for any choice of $\\eps\\in (0,\\mu/2)$ there is an iteration $k_\\eps$ such that, for all $k\\geq k_\\eps$, $\\W(k)\\in\\Cc_{\\mu,R_\\eps}(\\Ws)$. Once within $\\Cc_{\\mu,R_\\eps}(\\Ws)$, multiplying both sides \\eqref{eqn:neg:corr:local} by the stepsize $\\eta$ and using the gradient descent update, we get\n\\begin{equation*}\n\\begin{split}\n     \\left\\langle \\W(k+1)-\\W(k),\\frac{ \\Wm}{\\tf{\\Wm}} \\right\\rangle &\\geq  (1-\\epsilon) \\left\\langle \\W(k+1)-\\W(k), \\frac{\\W(k)}{\\tf{\\W(k)}}\\right\\rangle\\\\\n     &= \\frac{(1-\\epsilon)}{2\\tf{\\W(k)}}\\left(\\tf{\\W(k+1)}^2- \\tf{\\W(k)}^2-\\tf{\\W(k+1)-\\W(k)}^2\\right) \\\\\n     & \\geq (1-\\epsilon)\\left( \\frac{1}{2\\tf{\\W(k)}} \\left(\\tf{\\W(k+1)}^2- \\tf{\\W(k)}^2\\right)-\\tf{\\W(k+1)-\\W(k)}^2\\right) \\\\\n     & \\geq (1-\\epsilon)\\left(\\tf{\\W(k+1)}- \\tf{\\W(k)}-\\tf{\\W(k+1)-\\W(k)}^2\\right) \\\\\n          & \\geq (1-\\epsilon)\\Big(\\tf{\\W(k+1)}- \\tf{\\W(k)}- 2\\eta  \\left(\\mc{L}(\\W(k))-\\mc{L}(\\W(k+1))\\right) \\Big).\n\\end{split}\n\\end{equation*}\n\nHere, the second inequality is obtained from  $\\tf{\\W(k)}\\geq 1/2$; the third inequality follows since  for any $a, b >0$, we have $  (a^2-b^2)/(2b) -  (a-b) \\geq 0$; and the last inequality  uses Lemma~\\ref{lem:grad:descent}.\n\n\n\nSumming the above inequality over $k\\geq k_\\eps$ gives \n\\begin{align*}\n      \\left\\langle\\frac{\\W(k)}{\\tf{\\W(k)}}, \\frac{\\Wm}{\\tf{\\Wm}} \\right\\rangle \\ge1-\\epsilon+ \\frac{C(\\epsilon,\\eta)}{\\tf{\\W(k)}},    \\qquad \\W(k)\\in\\Cc_{\\mu,R_\\eps}(\\Ws),   \n\\end{align*}\nwhere $\\mathcal{L}_{\\star}\\leq\\mathcal{L}\\left(\\W\\left(k\\right)\\right)$ for all $k\\geq 0$, and \n\\begin{equation*}\nC(\\epsilon,\\eta)= \\left\\langle \\W(k_\\eps), \\frac{ \\Wm}{\\tf{\\Wm}}\\right\\rangle-(1-\\epsilon)\\tf{\\W(k_\\eps)} -2\\eta (1-\\epsilon) (\\mc{L}(\\W(k_\\eps))-\\mathcal{L}_{\\star}).\n\\end{equation*}\n\n\n\n\n\n\n\n\n\n\nConsequently, as $k\\rightarrow\\infty$\n    \\begin{align*}\n      \\liminf_{k\\to\\infty}\\iprod{\\frac{\\W(k)}{\\tf{\\W(k)}}}{\\frac{\\Wm}{\\tf{\\Wm}}}\\ge1-\\epsilon, \\qquad \\W(k)\\in\\Cc_{\\mu,R_\\eps}(\\Ws).\n    \\end{align*}\nSince  $\\epsilon \\in (0, \\mu/2)$  is arbitrary, we get $\\W(k)/\\tf{\\W(k)}\\to  \\Wm/\\tf{\\Wm}$.\n\n $\\qed$ \n\n\n==== END OF /2308.16898/supp/proof_convergence.tex ====\n==== BEGINNING OF /2308.16898/supp/great_leap_forward.tex ====\n\\section{Global Convergence with Good Initial Gradient}\\label{sec one step}\n\n\\begin{lemma}[Gradient Condition for Optimal Tokens]\\label{glocal cond} \nSuppose Assumption~\\ref{assum:loss:prope} holds and let $\\op=(\\op_i)_{i=1}^n$ be the unique globally-optimal indices with $\\Wm$ denoting the SVM solution. Define the margin $\\Theta=1/\\tf{\\Ws}$. \n\n\n\nGiven $\\mu\\geq 0$, consider the following subset of the sphere and its associated cone\n\n\n\n\n\\begin{subequations}\\label{eqn:con:nabla0}\n\\begin{align}\n&\\Sc_{\\mu}=\\left\\{\\W~\\Big|~  \\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\frac{\\W}{\\tf{\\W}}\\ri\\geq \\mu\\cdot\\Theta\\quad \\textnormal{for all}\\quad t\\neq \\op_i, \\quad  i\\in[n]\\right\\},\\\\\n&\\conb_{\\mu,R}=\\left\\{  \\W\\in\\Sc_\\mu ~\\Big|~   \\tf{\\W}\\geq R\\right\\}.\n\\end{align}\n\\end{subequations}\n\nFor any $\\mu>0$, there exists sufficiently large $R=R_\\mu=\\order{1/\\mu}$ (see \\eqref{R bound2}) such that:\n\\begin{enumerate}[label={\\textnormal{\\textbf{L\\arabic*.}}}, wide, labelwidth=!,itemindent=!, labelindent=5pt]\n\\item \\label{lem:gcond:l1} There is no stationary point within  $ \\conb_{\\mu,R}$.\n\n\\item\\label{lem:gcond:l2}  Let $s_i=\\sft{\\X_i\\W\\z_i}$. For all $\\V\\in \\Sc_{\\mu},\\W\\in\\conb_{\\mu,R}$, there exist $C,c>0$ such that \n\\begin{align*}\nC\\cdot\\max_{i\\in[n]}  \\left(1-s_{i\\op_i}\\right) \\geq -\\li\\nabla\\Lc(\\W),\\V\\ri\\geq c\\cdot\\mu\\cdot\\min_{i\\in[n]}  \\left(1-s_{i\\op_i}\\right).\n\\end{align*}\n\\end{enumerate}\n\\end{lemma}\n\n\\begin{proof} Let us introduce the norm upper bound\n\\begin{equation}\\label{mu choice2}\n\\begin{split}\n\n&A=\\max_{i\\in[n],t,\\tau\\in[T]} \\frac{(\\tn{\\x_{it}}\\vee\\tn{\\x_{it}-\\x_{i\\tau}})\\cdot\\tn{\\z_i}}{\\Theta}.\n\\end{split}\n\\end{equation}\n\nThe following inequalities hold for all $\\V\\in \\Sc_{\\mu},~\\tf{\\V}=\\tf{\\Wm}$ and all $i\\in[n], t\\neq \\op_i$:\n\\begin{equation}\\label{cone-A-eq}\n\\begin{split}\n\n\nA\\geq(\\x_{i\\op_i}-\\x_{it})^\\top \\V \\z_i &\\geq \\mu.\n\\end{split}\n\\end{equation}\n\n \n\nTo proceed, we write the gradient correlation following \\eqref{grad def} and \\eqref{grad def2}\n\\begin{align}\\label{grad def32}\n\\li\\nabla\\Lc(\\W),\\V\\ri&=\\frac{1}{n}\\sum_{i=1}^n\\ell'_i\\cdot\\li\\hb_i,\\sfp{\\hp_i}\\bgam_i\\ri,\n\\end{align}\nwhere we denoted $\\ell'_i=\\ell'(Y_i\\cdot \\vb^\\top \\X_i^\\top\\sft{\\hp_i})$, $\\hb_i=\\X_i\\V \\z_{i}$, $\\hp_i= \\X_i\\W \\z_{i}$, $\\s_i=\\sft{\\hp_i}$. It follows from \\eqref{mu choice2} that $A\\geq \\max_{i\\in[n],t\\in[T]}\\tn{\\hb_{it}}$. Using \\eqref{cone-A-eq}, we can bound the softmax probabilities $\\s_i=\\sft{\\hp_i}$ as follows, for all $i\\in[n]$:\n\\begin{align}\\label{soft prob bound2}\n&S_i:= \\sum_{\\tau\\neq \\op_i}\\s_{i\\tau}\\leq T e^{-R\\mu\\Theta}\\s_{i\\op_i}\\leq T e^{-R\\mu\\Theta}.\n\\end{align}\n\nRecall scores $\\bgam_{it}=Y_i\\cdot\\vb^\\top \\x_{it}$. Define the score gaps:\n\n\\begin{equation*}\n \\bgg_i=\\bgam_{i\\op_i}-\\max_{t\\neq\\op_i}\\bgam_{it},~~~ \\bgm_i=\\bgam_{i\\op_i}-\\min_{t\\neq\\op_i}\\bgam_{it},~~~\\textnormal{and}~~~\\Gamma=\\sup_{i\\in[n],t,\\tau\\in[T]}|\\bgam_{it}-\\bgam_{i\\tau}|. \n\\end{equation*}\nLet us focus on a fixed datapoint $i\\in[n]$, assume (without losing generality) $\\op_i=1$, and drop subscripts $i$.\n\nDirectly applying Lemma \\ref{lem:q_reduce}, we obtain\n\\[\n  \\big|\\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam-\\sum_{t\\geq 2}^T (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2\\Gamma A(1-\\s_1)^2.\n\\]\n\\noindent To proceed, let us upper/lower bound the gradient correlation. Since $A\\geq \\hb_1-\\hb_t\\geq \\mu>0$ from \\eqref{cone-A-eq}, setting $S:=\\sum_{t\\neq\\op_i}\\s_t=1-\\s_1$, we find\n\\begin{equation}\n A\\cdot S\\cdot \\bgm  \\geq\\sum_{t\\neq\\op} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\geq \\mu\\cdot S\\cdot \\bgg.\\label{aggregate2}\n\\end{equation}\nNext we show that $S=1-\\s_1$ dominates $(1-\\s_1)^2=S^2$ for large $R$. Specifically, we wish for \n\\begin{align}\\label{wishfor2}\n\\mu S \\bgg/2\\geq 2\\Gamma A(1-\\s_1)^2\\iff S\\geq \\frac{4}{\\mu}\\frac{\\Gamma A}{\\bgg}S^2\\iff S\\leq \\frac{\\mu\\bgg}{4\\Gamma A}.\n\\end{align}\nUsing \\eqref{soft prob bound2}, what we wish is ensured for all $i\\in[n]$, by guaranteeing $Te^{-R\\mu\\Theta}\\leq \\frac{\\mu\\bgg}{4\\Gamma A}$. That is, by choosing\n\\begin{align}\\label{R bound2}\nR:=R_\\mu= \\frac{1}{\\mu\\Theta}\\log\\big(\\frac{4T\\Gamma A}{\\mu\\bggm}\\big),\n\\end{align}\nwhere $\\bggm=\\sup_{i\\in[n]}\\bgg_i$ is the global scalar corresponding to the worst case score gap over all inputs. \n\nWith the above choice of $R$, we guaranteed\n\\[\n  2 A(1-\\s_1)\\cdot \\bgm\\geq 2A\\cdot S\\cdot \\bgm \\geq \\sum_{t\\neq\\op} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\geq\\frac{\\mu\\cdot S\\cdot \\bgg}{2}\\geq\\frac{\\mu(1-\\s_1) \\bgg}{2}.\n\\]\nvia \\eqref{wishfor2} and \\eqref{aggregate2}. Since this holds over all inputs, going back to the gradient correlation \\eqref{grad def32} and averaging above over all inputs $i\\in[n]$ and plugging back the indices $i$, we obtain the advertised bound by setting $q_i=1-\\s_{i\\op_i}$ (where we have used $\\op_i=1$ above, without losing generality)\n\\begin{align}\\label{pbb corr2}\n  \\frac{2A}{n}\\sum_{i\\in [n]} \\ell'_i\\cdot q_i\\cdot \\bgm_i\\geq \\li\\nabla\\Lc(\\W),\\V\\ri\\geq \\frac{\\mu}{2n}\\sum_{i\\in [n]} \\ell'_i\\cdot q_i\\cdot \\bgg_i.\n\\end{align}\n\\end{proof}\n\n\n\\begin{lemma}[Gradient Condition for Optimal Tokens]\\label{lem:glocal:corr} \nSuppose Assumption~\\ref{assum:loss:prope} holds and let $\\op=(\\op_i)_{i=1}^n$ be the unique optimal tokens with $ \\Wm$ denoting the SVM solution. Fix any $\\mu>0$ (per Lemma \\ref{glocal cond}). For any choice of $\\pi>0$, there exists $R:=R_{\\pi,\\mu}$ such that, for any $ \\W\\in \\conb_{\\mu,R}(\\Wm)$ we have\n\\[\n \\li \\nabla\\Lc(\\W), \\frac{\\W}{\\tf{\\W}} \\ri\\geq (1+\\pi)\\li \\nabla\\Lc(\\W), \\frac{\\Wm}{\\tf{\\Wm}}\\ri.\n\\]\n\\end{lemma}\n\\begin{proof}\n \nLet  $\\Wb= \\tf{\\W} \\W/\\tf{\\Wm}$, $\\hb_i=\\X_i\\Wb \\z_{i}$, $\\hbm_i= \\X_i\\Ws \\z_{i}$, and $\\s_i=\\sft{\\X_i\\W \\z_{i}}$. To establish the result, we will prove that, for sufficiently large $R=R_\\pi$, for any $\\W\\in \\conb_{\\mu,R}(\\Wm)$ and any $i\\in[n]$, \n\\begin{align}\\label{main local cond2}\n\\li\\hb_i,\\sfp{\\X_i\\W\\z_{i}}\\bgam_i\\ri\\leq (1+\\pi)\\li\\hbm_i,\\sfp{\\X_i\\W \\z_{i}}\\bgam_i\\ri.\n\\end{align}\nOnce \\eqref{main local cond2} holds for all $i$, the same conclusion will hold for the gradient correlations via \\eqref{grad def32}. Moving forward, we shall again focus on a single point $i\\in[n]$ and drop all subscripts $i$ i.e.~we will use vectors $\\hb,\\hbm,\\s$. Also assume $\\op=\\op_i=1$ without losing generality as above. Following \\eqref{aggregate2}, for all $\\V\\in \\Sc_\\mu$ with $\\tf{\\V}=\\tf{\\Wm}$ and $\\hp=\\X\\V \\z$, we have found\n\\begin{align}\n  \\big|\\hp^\\top\\diag{\\s}\\bgam-\\hp^\\top\\s\\s^\\top\\bgam-\\sum_{t\\neq\\op} (\\hp_1-\\hp_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2\\Gamma A(1-\\s_1)^2. \n\\end{align}\nRecalling $\\hbm_1-\\hbm_t\\geq 1$, we note that $\\sum_{t\\neq\\op}\\s_t(\\bgam_1-\\bgam_t)\\leq \\sum_{t\\neq\\op} (\\hbm_1-\\hbm_t)\\s_t(\\bgam_1-\\bgam_t)$. Now plugging in $\\hb,\\hbm$ in the bound above and assuming $\\pi\\leq 1$ (w.l.o.g.), \\eqref{main local cond2} is implied by the following stronger inequality\n\\begin{align*}\n6\\Gamma A(1-\\s_1)^2+ \\sum_{t\\neq\\op} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t) &\\leq (1+\\pi)\\sum_{t\\neq\\op}\\s_t(\\bgam_1-\\bgam_t)\\\\\n&\\leq (1+\\pi) \\sum_{t\\neq\\op} (\\hbm_1-\\hbm_t)\\s_t(\\bgam_1-\\bgam_t).\n\\end{align*}\nFirst, we claim that $0.5\\pi\\sum_{t\\neq\\op}\\s_t(\\bgam_1-\\bgam_t)\\geq 6\\Gamma A(1-\\s_1)^2$. The proof of this claim directly follows the earlier argument, namely following \\eqref{wishfor2}, \\eqref{R bound2}, \\eqref{soft prob bound2} we have that $1-\\s_1\\leq Te^{-R\\mu\\Theta}$ and $\\bgam_1-\\bgam_t\\geq \\bggm$. This leads to the choice (for $C=12$)\n\\begin{align}\nR_\\pi =\\frac{1}{\\mu\\Theta}\\log\\big(\\frac{C\\cdot T\\Gamma A}{\\pi\\bggm}\\big).\\label{Rpi choice2}\n\\end{align}\n\nFollowing this control over the perturbation term $6\\Gamma A(1-\\s_1)^2$, to conclude with the result, what remains is establishing the comparison\n\\begin{align}\\label{desired comp2}\n\\sum_{t\\neq\\op} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\leq (1+0.5\\pi) \\sum_{t\\neq\\op}\\s_t(\\bgam_1-\\bgam_t).\n\\end{align}\nTo proceed, we split the problem into two scenarios. \n\n\\noindent\\textbf{Scenario 1:} $\\tf{\\Wb-\\Wm}\\leq \\eps=\\frac{\\pi}{4A\\Theta}$ for some $\\eps>0$. In this scenario, for any token, we find that\n\\[\n|\\hb_t-\\hbm_t|\\leq A\\Theta\\eps=\\pi/4.\n\\]\nConsequently, we obtain \n\\[\n\\hb_1-\\hb_t\\leq \\hbm_1-\\hbm_t+2A\\Theta\\eps= 1+0.5\\pi.\n\\] \nSince $\\bgam_1-\\bgam_t\\geq 0$, for all $t\\neq 1$, we find $(\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\leq (1+0.5\\pi)\\s_t(\\bgam_1-\\bgam_t)$. This implies the desired result \\eqref{desired comp2}.\\smallskip\n\n\\noindent\\textbf{Scenario 2:} $\\tf{\\Wb-\\Wm}\\geq \\eps=\\frac{\\pi}{4A\\Theta}$. Since $\\Wb$ is not the max-margin solution, in this scenario, for some $\\nu=\\nu(\\eps)>0$ and $\\tau\\neq 1$, we have that $\\hb_1-\\hb_\\tau\\leq 1-2\\nu$. Here $\\tau=\\arg\\max_{\\tau\\neq 1} \\x_\\tau\\Wb \\z$ denotes the nearest point to $\\hb_1$ (along the $\\Wb$ direction). Recall that $\\s=\\sft{  C^\\svm\\hb}$, where  $C^\\svm=\\tf{\\W}/\\tf{\\Wm}$.  To proceed, split the tokens into two groups: Let $\\Nc$ be the group of tokens obeying $(\\x_1- \\x_\\tau) \\W \\z \\leq 1-\\nu$ and $[T]-\\{1\\}-\\Nc$ be the rest of the non-optimal tokens. Observe that\n\\[\n\\frac{\\sum_{t\\not\\in\\{\\op\\}\\cup\\Nc}\\s_t}{\\sum_{t\\neq\\op}\\s_t}\\leq\\frac{\\sum_{t\\not\\in\\{\\op\\}\\cup\\Nc}\\s_t}{\\sum_{t=\\tau}\\s_t}\\leq  T\\frac{e^{\\nu C^\\svm}}{e^{2\\nu C^\\svm}}=Te^{-C^\\svm\\nu}.\n\\]\nThus, using $|\\hb_1-\\hb_t|\\leq 2A$ and recalling the definition of $\\bgg$, observe that \n\\[\n\\sum_{t\\not\\in\\{\\op\\}\\cup\\Nc} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\leq \\frac{2\\Gamma A Te^{-C^\\svm\\nu}}{\\bgg} \\sum_{t\\neq\\op} \\s_t(\\bgam_1-\\bgam_t).\n\\]\nPlugging this into \\eqref{desired comp2}, we obtain\n\\begin{align*}\n  \\sum_{t\\neq\\op} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)&= \\sum_{t\\in \\Nc} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)+\\sum_{t\\not\\in\\{\\op\\}\\cup\\Nc} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\nonumber\\\\\n  &\\leq \\sum_{t\\in \\Nc} (1-\\nu)\\s_t(\\bgam_1-\\bgam_t)+\\sum_{t\\not\\in\\{\\op\\}\\cup\\Nc} 2A\\Gamma Te^{-C^\\svm\\nu}\\nonumber\\\\\n  &\\leq \\left(1-\\nu+\\frac{2\\Gamma A Te^{-C^\\svm\\nu}}{\\bgg}\\right)\\sum_{t\\neq\\op}\\s_t(\\bgam_1-\\bgam_t)\\\\\n  &\\leq \\left(1+\\frac{2\\Gamma A Te^{-C^\\svm\\nu}}{\\bgg}\\right)\\sum_{t\\neq\\op}\\s_t(\\bgam_1-\\bgam_t).\\\\\n\\end{align*}\nConsequently, the proof boils down to ensuring the perturbation term $\\frac{2\\Gamma A Te^{-C^\\svm\\nu}}{\\bgg}\\leq 0.5\\pi$. Since $C^\\svm\\geq R\\Theta$, this is guaranteed for all inputs $i\\in[n]$ by recalling $\\bggm=\\min_{i\\in[n]}\\bgg_i$ and choosing \n\\[\n  R\\geq R_\\pi= \\frac{1}{\\nu\\Theta}\\log(\\frac{4\\Gamma AT}{\\bggm\\pi}),\n\\]\nwhere $\\nu=\\nu(\\frac{\\pi}{4A\\Theta})$ depends only on $\\pi$ and global problem variables. \n\nCombining this with the prior $R_\\pi$ choice of \\eqref{Rpi choice2} (by taking their maximum), we conclude with the statement.   \n\\end{proof}\n\\subsection{Proof of Theorem~\\ref{conv:gd:w:global:nabla0}}\n\\begin{proof}\nWe provide the proof in four steps:\\\\\n\\textbf{Step~1: There are no stationary points within $\\conb_{\\mu,R_\\mu}(\\Wm)$ for sufficiently large $R_\\mu$.} \nThis step directly follows from Lemma~\\ref{glocal cond}: for all $\\V,\\W\\in \\Sc_{\\mu}(\\Wm)$ with $\\tf{\\V}=\\tf{\\Wm}$ and $\\tf{\\W}\\geq R_\\mu$, it follows from Lemma~\\ref{glocal cond} that there exists  $R_\\mu$ such that $\\iprod{\\V}{ -\\nabla \\Lc(\\W)}$ is strictly positive.\n\\\\\n\\textbf{Step~2:}  Consider an arbitrary value of  $\\epsilon \\in (0, \\mu/2)$ and let $1/(1+\\pi)=1-\\epsilon$. It follows from Lemma~\\ref{lem:glocal:corr} that, there exists $R_\\epsilon$ such that all  $ \\W \\in \\conb_{\\mu,R_\\epsilon}(\\Wm)$ satisfy\n\\begin{align}\\label{eqn:neg:corr:local:nabla0}\n\\iprod{-\\nabla\\mc{L}(\\W)}      {\\frac{\\Wm}{\\tf{\\Wm}}} \\geq (1-\\epsilon)    \\iprod{-\\nabla \\mc{L}(\\W)}{\\frac{\\W}{\\tf{\\W}}}.\n\\end{align}\n\n\n\\\\\n\\textbf{Step~3: Updates remain inside the cone $\\conb_{\\mu,R_\\mu}(\\Wm)$.}  By leveraging the results from \\textbf{Step 1} and \\textbf{Step 2}, we demonstrate that the gradient iterates remain within this cone. Note that by our assumption and for sufficiently large $\\eta_0$, we get $\\W(1) \\in \\conb_{\\mu,R}(\\Wm)$. Here, $R$ is chosen such that \n\\begin{align}\\label{eqn:bound:R:nabla0}\nR \\geq R_\\mu \\vee R_\\epsilon \\vee 1/2.\n\\end{align}\n We provide the proof by induction. By the above argument, $\\W(1) \\in \\conb_{\\mu,R}(\\Wm)$. Let $\\W(k) \\in \\conb_{\\mu,R}(\\Wm)$. For all $k\\geq 1$, we have that \n\n\n\n\\begin{subequations}\\label{eqn:rho:def:nabla0}\n\\begin{align}\n& \\min_{t\\neq \\op_i,~i\\in[n]} ~~   \\left\\langle  (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top, - \\nabla \\mc{L}(\\W(k)) \\right\\rangle \\geq \\bar{\\rho} , \n\\label{eqn:bar:rho}\n\\\\\n &\\frac{1}{1-\\epsilon} \\iprod{ {\\frac{\\Wm}{\\tf{\\Wm}}}}{-\\nabla\\mc{L}(\\W(k))} \\geq \\rho.\n\\end{align}\n\\end{subequations}\nfor some positive constants $\\bar{\\rho}$ and $\\rho$.  \n\n\nTo proceed, for all $t\\neq \\op_i,~i\\in[n]$, we obtain \n\\begin{equation}\\label{eqn:localgd:1:nabla0}\n    \\begin{split}\n   \\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top,    \\frac{\\W(k+1)}{\\tf{\\W(k)}} \\right\\rangle &=   \\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top,    \\frac{\\W(k)}{\\tf{\\W(k)}} -\\frac{\\eta}{\\tf{\\W(k)}}\\nabla \\mc{L}(\\W(k)) \\right\\rangle\\\\   \n   &\\ge \\mu \\Theta- \\frac{\\eta}{\\tf{\\W(k)}} \\left\\langle   (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top, \\nabla \\mc{L}(\\W(k)) \\right\\rangle  \\\\\n\n      & \\geq \\mu \\Theta +\\frac{\\eta\\bar{\\rho}}{\\tf{\\W(k)}}.\n    \\end{split}\n\\end{equation}\nHere, the first inequality follows from the induction assumption $\\W(k) \\in \\conb_{\\mu,R}(\\Wm)$, and the second inequality uses \\eqref{eqn:bar:rho}.\n\nFrom Lemma~\\ref{glocal cond}, we have $\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle<0$~ which implies that $\\tf{\\W(k+1)} \\geq \\tf{\\W(k)}$. Hence, \n\\begin{align*}\n\\tf{\\W(k+1)}&\\leq\\frac{1}{{2\\tf{\\W(k)}}} \\left(\\tf{\\W(k+1)}^2+\\tf{\\W(k)}^2\\right)\\\\\n& = \\frac{1}{2\\tf{\\W(k)}} \\left(2\\tf{W(t)}^2-2\\eta\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle+\\eta^2\\tf{\\nabla \\Lc(\\W(k))}^2\\right)\\\\\n       &\\leq  \\tf{\\W(k)}- \\frac{\\eta}{\\tf{\\W(k)}}\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle + \\eta^2 \\|\\nabla \\Lc(\\W(k))\\|^2, \n\\end{align*}\nwhere the last inequality follows from $\\tf{\\W(k)} \\geq R \\geq 1/2 $. \n\nThus,\n\\begin{equation}\\label{eqn:localgd:2:nabla0}\n\\begin{split}\n  \\frac{\\tf{\\W(k+1)}}{\\tf{\\W(k)}}& \\leq  1- \\eta\n       \\left\\langle \\nabla \\Lc(\\W(k)),\\frac{\\W(k)}{\\tf{\\W(k)}} \\right\\rangle + \\eta^2 \\frac{\\|\\nabla \\mc{L}(\\W(k))\\|^2}{\\tf{\\W(k)}}\\\\\n& \\leq 1- \\frac{\\eta}{1-\\epsilon}  \\iprod{\\nabla\\mc{L}(\\W(k))}\n     {\\frac{\\Wm}{\\tf{\\Wm}}}+ \\eta^2 \\frac{\\|\\nabla \\mc{L}(\\W(k))\\|^2}{\\tf{\\W(k)}}\\\\\n      & \\leq  1 + \\frac{\\eta \\rho}{\\tf{\\W(k)}} + \\frac{\\eta^2\\|\\nabla \\mc{L}(\\W(k))\\|^2}{\\tf{\\W(k)}}=C_1(\\rho,\\eta).\n\\end{split}\n\\end{equation}\nNow, it follows from \\eqref{eqn:localgd:1:nabla0} and \\eqref{eqn:localgd:2:nabla0} that   \n\\begin{equation}\\label{eqn:localgd:3:nabla0}\n\\begin{split}\n\\min_{t\\neq \\op_i,~i\\in[n]} ~~  \\left\\langle  \\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top, \\frac{\\W(k+1)}{\\tf{\\W(k+1)}}\\right\\rangle   &\\geq \\frac{1}{C_1(\\bar{\\rho},\\eta)} \\left(\\mu \\Theta+\\frac{\\eta \\bar{\\rho}}{\\tf{\\W(k)}}\\right)\\\\\n& \\geq \\mu \\Theta+\\frac{\\eta}{C_1(\\bar{\\rho},\\eta)} \\left(\\frac{\\bar{\\rho}-\\rho\\mu \\Theta}{\\tf{\\W(k)}}\n-   \\eta \\mu \\Theta \\frac{\\|\\nabla \\mc{L}(\\W(k))\\|^2 }{\\tf{\\W(k)}}\\right)\\\\\n& \\geq \\mu \\Theta.\n\\end{split}\n\\end{equation}\nHere, we leverage the fact that the parameter $\\mu$ can be chosen arbitrarily, and then we select the step size $\\eta$ to satisfy the following condition:\n$$\\eta \\leq \\frac{\\bar{\\rho}-\\rho\\mu \\Theta }{\\mu\\Theta} \\frac{1} {\\|\\nabla \\mc{L}(\\W(k))\\|^2}.\n$$\nHence, \\eqref{eqn:localgd:3:nabla0} gives $\\W(k+1) \\in \\conb_{\\mu,R}$.\n\\\\\n\\textbf{Step 4: The correlation of $\\W(k)$ and $\\Wm$ increases over $k$.} \nIt follows from Theorem~\\ref{diverg:norm:w} that \n$\\left\\Vert \\W\\left(k\\right)\\right\\Vert \\rightarrow\\infty$. Hence, we can choose $k_0$ such that for any $k\\ge k_0$, it holds that $\\tf{\\W(k)}>  R$ for some $R \\geq R_\\mu \\vee R_\\epsilon \\vee 1/2$. Now, following similar steps in \\eqref{eqn:decpath:1} and \\eqref{eqn:decpath:22},  for  some constant $C(\\epsilon,\\eta)$, we obtain\n\\begin{align*}\n      \\left\\langle\\frac{\\W(k)}{\\tf{\\W(k)}}, \\frac{\\Wm}{\\tf{\\Wm}} \\right\\rangle \\ge1-\\epsilon+ \\frac{C(\\epsilon,\\eta)}{\\tf{\\W(k)}}.      \n\\end{align*}\nConsequently,\n    \\begin{align*}\n      \\liminf_{t\\to\\infty}\\iprod{\\frac{\\W(k)}{\\tf{\\W(k)}}}{\\frac{\\Wm}{\\tf{\\Wm}}}\\ge1-\\epsilon.\n    \\end{align*}\nSince  $\\epsilon \\in (0, \\mu/2)$  is arbitrary, we get $\\W(k)/\\tf{\\W(k)}\\to  \\Wm/\\tf{\\Wm}$.\n\\end{proof} \n==== END OF /2308.16898/supp/great_leap_forward.tex ====\n==== BEGINNING OF /2308.16898/supp/QK_max_margin.tex ====\n\\subsection{Convergence of Gradient Descent  for (K,Q)-Parameterization}\\label{sec:KQ convergence proof}\n\nIn the following, our objective is to establish analogous assurances for Algorithm \\ref{GD-QK} when applied to \\eqref{eqn:erm:kq}. Nevertheless, unlike Theorem~\\ref{diverg:norm:w}, where a fixed step size of $O(1/L_{\\W})$ is utilized, we now necessitate an adaptive step size configuration for the $(\\Kb, \\Qb)$ decomposition.  This is because $(1/L_\\Kb, 1/L_\\Qb)$ tends to  $0$ as the number of iterations approaches infinity. To proceed, for any $R > 0$, we introduce the set $\\mc{S}(R)$ as follows:\n\\begin{equation}\\label{eqn:kq:set}\n \\mc{S}(R):= \\left\\{(\\Kb,\\Qb)~~\\big|~~\\tf{\\Qb} \\leq R,~~\\tf{\\Kb} \\leq R \\right\\}.   \n\\end{equation}\nThe following lemma demonstrates that given any $R>0$, the number of iterations taken by Algorithm~\\ref{GD-QK} within $\\Sc(R)$ is finite. \n\\begin{lemma}\\label{lem:out:S}\nSuppose Assumptions~\\ref{assum:loss:prope} and \\ref{assum:token} hold. Assume the gradient at the initialization $(\\Kb(0), \\Qb(0))$ is nonzero and $\\Lc(\\Kb(0), \\Qb(0)) \\leq \\Lc(0, 0)$. Consider Algorithm~\\ref{GD-QK} with a step size $\\eta=1/L(R)$, where $L(R):=RL_{\\W}$. Then for any $R>0$, there exists an iteration index $k$ at which $(\\Kb(k),\\Qb(k)) \\notin \\mc{S} (R)$.\n\\end{lemma}\n\\begin{proof}\nLet us select a value for $R$ and fix the step size to $\\eta=1/L(R)$. Assuming that both $(\\Kb({k+1}), \\Qb(k+1))$ and $(\\Kb({k}), \\Qb(k))$ reside within the set $\\mc{S}(R)$, we can establish a descent of objective analogous to Lemma \\ref{lem:grad:descent}. Specifically, we obtain the following result: \n\\begin{align}\\label{eq:descent:obj}\n\\nonumber\n\\mathcal{L}(\\Kb({k+1}), \\Qb({k+1}))-\\mathcal{L}(\\Kb({k}),\\Qb(k)) &\\leq-\\frac{1}{2 L(R)} \\tf{\\nabla \\mathcal{L}(\\Kb({k}), \\Qb(k))}^2 \\\\\n& =-\\frac{\\eta}{2} \\tf{\\nabla \\mathcal{L}(\\Kb({k}), \\Qb(k))}^2.\n\\end{align}\nBy our assumption, $\\tf{\\nabla \\Lc (\\Kb(0), \\Qb(0))} \\neq 0$, which implies that  \n\\begin{equation}\\label{eqn:obj:less0}\n    \\mathcal{L}(\\Kb({1}),\\Qb(1)) \\leq  \\mathcal{L}(\\Kb({0}),\\Qb(0)) - \\frac{1}{ 2 L(R)} \\tf{\\nabla \\mathcal{L}(\\Kb({0}), \\Qb(0))}^2 <    \\mathcal{L}(\\Kb(0),\\Qb(0)) \\leq  \\mathcal{L}(0,0).\n\\end{equation}\nNext, we claim that for any $R>0$, there exists a constant $\\epsilon(R)>0$, such that \n\\begin{align}\\label{eqn:grad:low:eps}\n\\forall k\\ge1\\,:\\,  \\quad (\\Kb (k),\\Qb(k)) \\in \\mathcal{S}, \\qquad \\tf{\\nabla_{\\Qb} \\Lc(\\Kb(k),\\Qb(k))}\\geq \\epsilon(R). \n\\end{align}\nFix an arbitrary $R>0$. If \\eqref{eqn:grad:low:eps} is not true, then for any $\\epsilon>0$, there exists some $k\\ge1$ such that $\\|\\Kb(k)\\|_F\\le R$ and  $\\tf{\\Qb(k)} \\leq R$ while  \\eqref{grad def KQ} gives\n\\begin{align}\\label{eqn:qgrad}\n\\nonumber\n\\left\\|\\nabla_{\\Qb} \\Lc(\\Kb (k),\\Qb(k))\\right\\|_F^2 & =\\left\\|\\frac{1}{n}\\sum_{i=1}^n \\ell' \\left(\\bgam_i^\\top \\sft{\\g_i}\\right) \\cdot \\X_i^\\top  \\sfp{\\g_i}  \\bgam_i  \\z_{i}^\\top \\Kb(k)\\right\\|_F^2 \\\\\n&=  \\tf{\\nabla \\Lc (\\W(k))}^2 \\left\\|\\Kb(k)\\right\\|^2_F \\leq \\epsilon^2,   \n\\end{align}\nwhere $\\g_{i}=\\X_i\\Kb(k) \\Qb(k)^\\top\\z_{i}=\\X_i\\W(k)\\z_{i}$.\n\nIt follows from Lemma~\\ref{global des lem} that $\\li\\nabla\\Lc(\\W (k)),\\Wm/\\|\\Wm\\|_F\\ri \\leq  - c <0$ for some positive constants $c$.  \n\nHence, $\\tf{\\nabla \\Lc (\\W(k))}  \\geq c$ and $\\tf{\\Kb(k)} \\leq \\epsilon/c$.  This together with $\\tf{\\Qb(k)} \\leq R$ implies that   $\\tf{ \\Kb(k) \\Qb(k)^\\top} \\leq  \\epsilon R/ c$.  In other words,  after $k=1$, $\\tf{ \\Kb(k) \\Qb(k)^\\top}$  may be arbitrarily small, which implies $\\Lc(\\Kb(k), \\Qb(k))$ can be arbitrarily close to $ \\mathcal{L}(\\Kb(0),\\Qb(0))$. \nThis is a contradiction to \\eqref{eqn:obj:less0}. Hence, \\eqref{eqn:grad:low:eps} holds.\n\nNow,  it follows from \\eqref{eq:descent:obj} and \\eqref{eqn:grad:low:eps}  that \n\\begin{align*}\n  \\mathcal{L}(\\Kb(0),\\Qb(0))\n         \\ge   \\mathcal{L}(\\Kb(0),\\Qb(0)) -\\mathcal{L}_{\\star} \\geq  \\frac{1}{2 L(R)}\\sum_{k=0}^{\\infty} \\left\\|\\nabla_{\\Qb} \\Lc(\\Kb(k),\\Qb(k)) \\right\\|_{F}^2\n         \n        \n          \\geq \\infty,\n    \\end{align*}\nwhich is a contradiction. Hence,  $(\\Kb(k),\\Qb(k))$ must go out of $\\mc{S} (R)$.\n\\end{proof}\n\nNote that in Lemma~\\ref{lem:out:S},  we assume that the gradients at the initialization $(\\Kb(0), \\Qb(0))$ are nonzero. Requiring the initialization not to be a critical point is reasonable because if it were, gradient descent would be unable to make any progress.  From Lemma~\\ref{lem:out:S}, we see that $(\\Kb(k),\\Qb(k))$ can go out of the set $\\mc{S} (R)$.  However, we can address this matter by dynamically increasing $R$ while proportionally decreasing the step sizes, as formalized in the following theorem:\n\n\\begin{theorem}\\label{diverg:norm:qk}\nSuppose Assumption~\\ref{assum:loss:prope} on the loss function $\\ell$ and Assumption \\ref{assum:token} on the tokens hold. Assume the initialization $(\\Kb(0), \\Qb(0))$ satisfies $\\nabla \\Lc (\\Kb(0),\\Qb(0)) \\neq 0$. Let $\\eta_k=\\min\\{1/L(R_k),1\\}$, where $R_k$ is chosen such that $(\\Kb(k),\\Qb(k))\\in \\mc{S}(R_k-1)$, and if $(\\Kb(k+1),\\Qb(k+1))\\in \\mc{S} (R_k-1)$, then $R_{k+1}=R_k$.  Then, the following statements hold:\n\\begin{itemize}\n\\item \nThere is no $\\Kb,\\Qb\\in\\R^{d\\times m}$ satisfying $\\nabla \\Lc(\\Kb,\\Qb)=0$.\n\\item Algorithm~\\ref{GD-QK} with the step size $\\eta_k$  satisfies  $\\lim_{k \\rightarrow \\infty} \\tf{\\nabla \\Lc_\\Kb(\\Kb(k), \\Qb(k))}\\vee\\tf{\\nabla\\Lc_\\Qb(\\Kb(k),\\Qb(k))}=0$, and  $\\lim_{k\\rightarrow\\infty} \\tf{\\Kb(k)}\\wedge\\tf{\\Qb(k)}=\\infty$.\n\\end{itemize}\n\\end{theorem}\n\n\\begin{proof}\nSince $(\\Kb(k),\\Qb(k))\\in \\mc{S}(R_k)$, \nwe have \n    \\begin{align}\n        \\|\\Qb(k+1)\\|_F \\le\\|\\Qb(k)\\|_F+\\eta_k\\tf{ \\nabla_{\\Qb} \\Lc(\\Kb(k),\\Qb(k))} \\nonumber    & \\le\\|\\Qb(k)\\|_F+\\frac{1}{L(R_k)}\\tf{\\nabla_{\\Qb} \\Lc(\\Kb(k),\\Qb(k))} \\nonumber \\\\\n\n         & \\le\\|\\Qb(k)\\|_F+1. \\label{eq:gd_inc}\n    \\end{align}\n\n    \nSince $R_k\\to\\infty$ by Lemma~\\ref{lem:out:S} and $R_{k+1}=R_k$ as long as $ (\\Kb(k+1),\\Qb(k+1))\\in \\mc{S}(R_k-1)$, we obtain\n\\begin{equation}\\label{eqn:c1}\n \\max\\{ \\|\\Qb(k)\\|_F,  \\|\\Kb(k)\\|_F \\} \\quad    \\textnormal{is unbounded}. \\tag{C1}\n\\end{equation}\n\n    \nIt then follows that for any $k$, by Cauchy-Schwarz,\n\\begin{align*}\n\\left(\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}\\right) \\left(\\sum_{\\tau=0}^{k-1}\\eta_{\\tau} \\|\\nabla \\Lc(\\Kb(\\tau),\\Qb(\\tau))\\|^2_F\\right) & \n\\ge \\left(\\sum_{\\tau=0}^{k-1}\\eta_{\\tau} \\|\\nabla \\Lc(\\Kb(\\tau),\\Qb(\\tau))\\|^2_F\\right)^2\\to\\infty.\n\\end{align*}\nSince by  \\eqref{eq:descent:obj},\n\\begin{align*}\n        \\sum_{\\tau=0}^{k-1}\\eta_{\\tau} \\left\\|\\nabla \\Lc(\\Kb(\\tau),\\Qb(\\tau))\\right\\|^2_F\\le 2\\Lc(\\Kb(0),\\Qb(0))-2\\Lc(\\Kb(k),\\Qb(k) )\\le2 \\Lc(\\Kb(0),\\Qb(0)),\n\\end{align*}\nwe have $\\sum_{t=0}^{\\infty}\\eta_k=\\infty$.\n\nSince gradient descent never increases the risk,  for $(\\Kb(k),\\Qb(k))\\in \\mc{S}(R)$, $\\|\\partial\\Lc/\\partial \\Qb(k)\\|_F\\ge\\epsilon(R)$ for some constant $\\epsilon(R)>0$.  Following similar steps  as the proof of \\eqref{eqn:grad:low:eps},   we get that $\\sum_{k:(\\Kb(k),\\Qb(k))\\in \\mc{S}(R)}^{}\\eta_k<\\infty$.  \n\n\nFor simplicity, we  replace $\\Qb^\\top$ with $\\Qb$ in \\eqref{eqn:erm:kq}. \n\n\n\n For gradient descent iterates \\ref{GD-QK}, summing from $0$ to $k-1$, we get\n\\begin{align}\\label{eqn:qk:recur}\n& \\quad\\Qb(k)^\\top\\Qb(k)-\\Qb(0)^\\top\\Qb(0)+\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\nabla_{\\Qb} \\Lc(\\Kb(\\tau),\\Qb(\\tau))^\\top \\nabla_{\\Qb} \\Lc(\\Kb(\\tau),\\Qb(\\tau)) \\nonumber \\\\\n= & \\quad \\Kb(k)\\Kb^{\\top}(k)-\\Kb(0)\\Kb^{\\top}(0)+\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2  \\nabla_{\\Kb} \\Lc(\\Kb(\\tau),\\Qb(\\tau)) \\nabla_{\\Kb} \\Lc(\\Kb(\\tau),\\Qb(\\tau))^\\top.\n\\end{align}\nLet\n\\begin{subequations}\n    \\begin{align*}\n       P_{\\Qb}&=\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\nabla_{\\Qb} \\Lc(\\Kb,\\Qb) \\nabla_{\\Qb} \\Lc(\\Kb(\\tau), \\Qb (\\tau)))^\\top,\\\\\n   P_{\\Kb}&:=\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\nabla_{\\Kb} \\Lc(\\Kb,\\Qb)  \\nabla_{\\Qb} \\Lc(\\Kb(\\tau), \\Kb (\\tau)))^\\top,\n    \\end{align*}\n    and\n\\begin{align*}\nS_{\\Qb}(k)=\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\nabla_{\\Qb} \\Lc(\\Kb(\\tau), \\Qb (\\tau)))^\\top \\nabla_{\\Qb} \\Lc(\\Kb(\\tau), \\Qb (\\tau))),\\\\\nS_{\\Kb}(k)=\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\nabla_{\\Kb} \\Lc(\\Kb(\\tau), \\Qb (\\tau)))^\\top \\nabla_{\\Kb} \\Lc(\\Kb(\\tau), \\Qb (\\tau))) .\n    \\end{align*}\n\\end{subequations}\nWe obtain\n\\begin{subequations}\n    \\begin{align}\\label{eq:qk:tr:b1}\n           \\tr(P_{\\Kb}(k))+ \\tr(P_{\\Qb}(k))  & =\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\|\\nabla \\Lc(\\Kb(\\tau), \\Qb (\\tau))\\|^2_F \\nonumber \\\\\n         & \\le \\sum_{\\tau=0}^{k-1}\\eta_{\\tau} \\|\\nabla \\Lc(\\Kb(\\tau), \\Qb (\\tau))\\|^2_F  \\nonumber \\\\\n         &  \\leq 2 \\Lc(\\W(0)) -2 \\Lc(\\W(k))  \\nonumber\\\\\n            &  \\leq 2 \\Lc(\\W(0)) -2 \\Lc_\\star.\n    \\end{align}\nIt follows from \\eqref{eqn:qk:recur} that\n\\begin{align}\\label{eq:qk:tr:b2}\n\\|\\Kb(k)\\|_F^2=\\|\\Qb(k)\\|_F^2+\\|\\Kb(0)\\|_F^2-\\|\\Qb(0)\\|_F^2-\\tr(P_{\\Kb}(k))+\\tr(S_{\\Qb}(k)).\n\\end{align}    \n\\end{subequations}\nIn other words, the difference between the squares of Frobenius norms of $(\\Kb,\\Qb)$ is still bounded.\n\n\nNow, combining \\eqref{eq:qk:tr:b1} and \\eqref{eq:qk:tr:b2}, we get\n \\begin{align*}\n   2 \\Lc(\\W(0))\n           \\ge\\sum_{t=0}^{\\infty} \\eta_k \\left\\|\\nabla_{\\Qb} \\Lc(\\Kb (k),\\Qb(k))\\right\\|_F^2 \n          \n          \n           \\geq \\infty,\n     \\end{align*}\n     which is a contradiction. This implies $\\tf{\\Kb(k)}\\to\\infty$, since $\\Lc(\\Kb,\\Qb)$ has no finite optimum.\n\\end{proof}\n\nTheorem~\\ref{diverg:norm:qk} is the formal version of the $(\\Kb,\\Qb)$-statement in Theorem \\ref{diverg:norm:w}. It provides similar guarantees to the $\\W$-statements of Theorem \\ref{diverg:norm:w}, namely the absence of finite stationary points and the divergence of the parameter norm to infinity. It is important to mention that the step size can be appropriately determined using methods such as a line search. The line search ensures that the \\ref{GD-QK} update is not excessively aggressive, thus allowing the boundary $R$ to be increased as needed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n==== END OF /2308.16898/supp/QK_max_margin.tex ====\n==== BEGINNING OF /2308.16898/supp/separation.tex ====\n\n\\section{Proof of Theorem \\ref{thm:separation}: Separability Under Mild Over-Parameterization}\\label{app sep}\n\n\n\n\nWe denote Kronecker product of two matrices via $\\kron$. Additionally, given $\\W\\in\\R^{d\\times d}$, let us denote its vectorization $\\w=\\text{vec}(\\W)\\in\\R^{d^2}$. We first note that separation is implied by the linear independence of the constraints. Specifically, we are interested in guaranteeing\n\\[\n\\li\\w,\\fb_\\itt\\ri\\geq 1\\quad \\text{for all}\\quad i\\in[n],~~t\\neq\\bal_i, ~~ \\textnormal{where} ~~\\fb_\\itt:=(\\x_{i\\bal_i}-\\x_\\itt)\\kron \\z_i.\n\\]\nNote that, the inequality constraints above are feasible as soon as $\\fb_\\itt$'s are linearly independent. Thus, we will instead prove linear independence of the vectors $(\\fb_\\itt)_{i\\in[n],t\\neq\\bal_i}$. Also note that, since there are finitely many $\\bal$ choices, if we show almost sure separation for a fixed but arbitrary $\\bal$ choice, through union bound, we recover the result for all $\\bal$. Thus, we prove the result for a fixed $\\bal$ choice.\n\n\nWe will prove this result inductively.  Let $\\M_{n-1}\\in\\R^{(n-1)(T-1)\\times d^2}$ denote the matrix whose rows are given by the features $(\\fb_\\itt)_{i\\in[n-1],t\\neq\\bal_i}$. Suppose the result is correct for $n-1$, thus, $\\M_{n-1}$ is full row-rank almost surely (post random Gaussian perturbation). Now, fix $\\M_{n-1}$ and, conditioned on $\\M_{n-1}$ being full row-rank, let us show that $\\M_n$ is also full row-rank almost surely. To prove this, consider the $n$'th example $(\\X_n,\\z_n)$. Let $(\\g_t)_{t=1}^T,\\hb\\in\\R^d$ be random vectors with i.i.d.~$\\Nn(0,\\sigma^2)$ entries. Consider the perturbed input $\\X'_n \\in\\R^{T\\times d}$ with tokens $\\x'_{nt}=\\x_{nt}+\\g_t$ and $\\z'_n=\\z_n+\\hb$. Note that for self-attention, we set $\\z_n=\\x_{n1}$ and $\\hb=\\g_1$. From these, create the matrix $\\tilde{\\M}_n \\in \\R^{(T-1)\\times d^2}$ with rows $(\\fb'_{nt})_{t\\neq\\bal_n}$ where $\\fb'_{nt}=(\\x'_{n\\bal_n}-\\x'_{nt})\\kron \\z'_n$. Observe that $\\M_n=\\begin{bmatrix}\\tilde{\\M}_n\\\\\\M_{n-1}\\end{bmatrix}$. To conclude with the result, we will apply Lemma \\ref{lem add up}. To apply this lemma, we have two claims.\n\n\\noindent\\textbf{Claim 1:} Let $\\bar{\\z}_n$ be the projection of $\\z'_n$ on the orthogonal complement of $(\\z_i)_{i=1}^{n-1}$. Consider the matrix $\\bar{\\M}_n$ with rows $\\bar{\\fb}_{nt}=(\\x'_{n\\bal_n}-\\x'_{nt})\\kron \\bar{\\z}_n$ for $t\\neq \\bal_n$. $\\bar{\\M}_n$ is rank $T-1$ almost surely whenever $d\\geq \\max(T-1,n)$.\n\nTo see this claim, first denote the orthogonal complement of the span of the vectors $(\\z_i)_{i=1}^{n-1}$ by $Z_{n-1}$. The span of the vectors $(\\z_i)_{i=1}^{n-1}$ is at most $n-1$ dimensional and, since $d\\geq n$, $\\text{dim}(Z_{n-1})\\geq 1$. Consequently, $\\bar{\\z}_n\\neq 0$ almost surely because the Gaussian variable $\\z_n+\\hb$ will have nonzero projection on $Z_{n-1}$ almost surely. Secondly, let $\\bar{\\X}\\in\\R^{(T-1)\\times d}$ be the matrix whose rows are equal to $\\x'_{n\\bal_n}-\\x'_{nt}$ for $t\\neq \\bal_n$. $\\bar{\\X}$ is full row-rank almost surely, this is because conditioned on $\\g_{\\bal_n}$, the matrix $\\bar{\\X}$ is written as $\\bar{\\X}=\\tilde{\\X}+\\Gb$ where $\\tilde{\\X}$ is deterministic and $\\Gb$ is i.i.d. Gaussian. The latter perturbation ensures full row-rank almost surely whenever $T-1\\leq d$. Finally, note that $\\bar{\\M}_n=\\bar{\\X}\\kron \\bar{\\z}_n$. Since the rank of the Kronecker product is multiplicative, we conclude with the claim.\n\n\\noindent\\textbf{Claim 2:} Let $S_{n-1}\\subset\\R^{d^2}$ be the null space of $\\M_{n-1}$. There exists a subspace $P\\subseteq S_{n-1}$ such that rows of $\\bar{\\M}_n$ are projections of the rows of $\\M_n$ on $P$, that is, $\\bar{\\fb}_{nt}=\\Pi_{P}(\\fb'_{nt})$ where $\\Pi$ denotes set projection.\n\nTo show this claim, let us consider the matrix forms of the vectorized features i.e.~let us work with $\\R^{d\\times d}$ rather than $\\R^{d^2}$. Denote the notation change as $\\Fb_\\itt=(\\x_{i\\bal_i}-\\x_\\itt) \\z_i^\\top\\leftrightarrow \\fb_\\itt=(\\x_{i\\bal_i}-\\x_\\itt)\\kron \\z_i$. Recall that $Z_{n-1}$ denotes the orthogonal complement of $(\\z_i)_{i=1}^{n-1}$. Define $Q$ to be the set of matrices in $\\R^{d\\times d}$ whose column space lies in $Z_{n-1}$ and $P$ to be the vectorization of $Q$. We first show that $P$ is a subset of the null space of $S_{n-1}$. To see this, fix any matrix $\\A\\in P$ and a row $\\fb_\\itt$ from $\\M_{n-1}$. Matricized $\\fb_\\itt$ can be written as $\\Fb_\\itt=\\ab\\z_i^\\top$ for $\\z_i\\in Z_{n-1}^\\perp$. Since $\\A\\in Q$, this implies $\\li\\Fb_\\itt,\\A\\ri=\\ab^\\top\\A\\z_i=0$ as $\\A\\z_i=0$. This holds for all $\\Fb_\\itt$, thus, $\\texttt{vectorized}(\\A)\\in\\texttt{null}(S_{n-1})$.\n\nNext, we need to show that $\\bar{\\fb}_{nt}$ is the projection of $\\fb'_{nt}$ on $P$. To see this, we will show that $\\bar{\\fb}_{nt}\\in P$ whereas $\\fb'_{nt}-\\bar{\\fb}_{nt}\\in P^\\perp$ for all $t$. Write $\\Fb'_{nt}=\\texttt{matricized}(\\fb'_{nt})=\\ab{\\z'}_n^\\top$. We have that $\\bar{\\Fb}_{nt}=\\texttt{matricized}(\\bar{\\fb}_{nt})=\\ab\\bar{\\z}_n^\\top$ where $\\bar{\\z}_n=\\Pi_{Z_{n-1}}(\\z'_n)$. This implies $\\bar{\\Fb}_{nt}\\in Q$ and $\\bar{\\fb}_{nt}\\in P$. Similarly, since $\\z'_n-\\bar{\\z}_n\\in Z_{n-1}^\\perp$ which implies $\\Fb'_{nt}-\\bar{\\Fb}_{nt}\\in Q^\\perp$.  \n\nTo conclude with the proof, observe that, through Claims 1 and 2, $\\M_n=\\begin{bmatrix}\\tilde{\\M}_n\\\\\\M_{n-1}\\end{bmatrix}$ satisfies the requirements of Lemma \\ref{lem add up} almost surely, namely, projection of $\\tilde{\\M}_n$ onto a subset of the null space of $\\M_{n-1}$ being full rank. Thus, $\\M_n$ is full rank almost surely.\n\n $\\qed$ \n \n\\begin{lemma}\\label{lem add up}Let $\\A\\in\\R^{n\\times p},\\B\\in\\R^{m\\times p}$. Suppose $n+m\\leq p$ and $\\A$ is full row-rank. Denote the null space of $\\A$ by $S_\\A^\\perp$. Let $P$ be a subspace that is its subset i.e.~$P\\subseteq S_\\A^\\perp$. Let $\\B'$ be the matrix obtained by projecting each of row of $\\B$ on $P$ and suppose $\\B'$ is full rank. Then, the concatenation $\\Cb= [\\A; \\B ]$\n\nis full row-rank.\n\\end{lemma}\n\\begin{proof} Let $(\\ab_i)_{i=1}^n$, $(\\bb_i)_{i=1}^m$, $(\\bb'_i)_{i=1}^m$ be the rows of $\\A,\\B,\\B'$, respectively. Suppose the set of rows of $\\A$ and $\\B$ are linearly dependent. Then, for some $(c_i)_{i=1}^{n},(c'_i)_{i=1}^{m}$ (which are not all-zeros), we have that\n\\begin{align}\n\\sum_{i=1}^n c_i\\ab_i+\\sum_{i=1}^m c'_i\\bb_i=0.\\label{cs are nonzero}\n\\end{align}\nWe now rewrite this as follows to decouple $P$ and $P^\\perp$:\n\\[\n\\sum_{i=1}^n c_i\\ab_i+\\sum_{i=1}^m c'_i\\bb'_i+\\sum_{i=1}^m c'_i(\\bb'_i-\\bb_i)=0.\n\\]\n\nProjecting above inequality to $P$, we find that $\\sum_{i=1}^m c'_i\\bb'_i=0$. Since $(\\bb'_i)_{i=1}^m$ are linearly independent, we find $c'_i=0$ for all $i\\in[m]$. This implies $\\sum_{i=1}^n c_i\\ab_i=0$. Since $(\\ab_i)_{i=1}^n$ are linearly independent, this implies $c_i=0$ for all $i\\in[n]$. Thus, \\eqref{cs are nonzero} can only hold if all coefficients are zero which is a contradiction.\n\\end{proof}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n==== END OF /2308.16898/supp/separation.tex ====\n==== BEGINNING OF /2308.16898/supp/reg_path_glob.tex ====\n\nCorollary \\ref{cor global reg path} already proves Theorem \\ref{thm global reg path} through the more general Theorem \\ref{local RP thm}. Below, we provide a self contained proof of Theorem \\ref{thm global reg path} for clarity.\n\n\\begin{proof} Throughout $\\dm$ denotes either Frobenius norm or nuclear norm. We will prove that $\\wrb{R}$ asymptotically aligns with the set of globally-optimal directions and also $\\td{\\wrb{R}}\\rightarrow \\infty$. $\\Rcm\\subseteq\\R^{d\\times d}$ denote the manifold of rank $\\leq$$m$ matrices.\n\\\\\n\\noindent\\textbf{Step 1:} Let us first prove that $\\wrb{R}$ achieves the optimal risk as $R\\rightarrow\\infty$ -- rather than problem having finite optima. Define $\\xdm=1/\\td{\\Wm}$ and norm-normalized $\\Wsb=\\xdm\\Wm$. Note that $\\Wm$ separates tokens $\\op$ from rest of the tokens for each $i \\in[n]$. Thus, we have that\n\\begin{align}\n\\lim_{R\\rightarrow\\infty}\\Lc(\\wrb{R})\\leq\\lim_{R\\rightarrow\\infty}\\Lc(R\\cdot\\Wsb):=\\Lc_\\star= \\frac{1}{n}\\sum_{i=1}^n \\ell(\\bgam^{\\op}_i).\\label{glob:asymp loss}\n\\end{align}\nOn the other hand, for any $\\W\\in \\Rcm$, define the softmax probabilities $\\s^{(i)}=\\sft{\\X_i\\W\\z_i}$ and attention features $\\x^{\\W}_i=\\sum_{t=1}^T \\s^{(i)}_t\\x_t$. Decompose $\\x^{\\W}_i$ as \n$\n\\x^{\\W}_i=\\s^{(i)}_{\\op_i}\\x_{i\\opt_i}+\\sum_{t\\neq \\op_i}\\s^{(i)}_t\\x_\\itt.\n$ Set $\\bgg_\\itt=\\bgam^{\\op}_i-\\bgam_\\itt=Y_i\\cdot \\vb^\\top(\\x_{i\\opt_i}-\\x_\\itt)>0$, and define\n\\begin{align}\n&B:=\\max_{i\\in[n]}\\max_{t,\\tau\\in[T]}\\tn{\\vb}\\cdot \\tn{\\x_\\itt-\\x_\\ittt}\\geq \\bgg_\\itt.\\label{glob:BB eq}\n\\end{align}\n\n\n\n\n\n\n\nDefine $c_\\op=\\min_{i\\in[n],t\\neq\\op_i}\\bgg_\\itt>0$ and $\\bgam^{\\W}_i=Y_i\\cdot \\vb^\\top\\x^{\\W}_i$. We obtain the following score inequalities\n\\begin{align}\\label{glob:score decomp}\n&\\bgam^{\\W}_i\\leq \\bgam^{\\op}_i-c_\\op (1-\\s^{(i)}_{\\op_i})<\\bgam^{\\op}_i,\\\\\n\n&|\\bgam^{\\W}_i-\\bgam^{\\op}_i|\\leq \\tn{\\vb}\\cdot\\tn{\\x^{\\W}_i-\\xa_i}\\leq \\tn{\\vb} \\sum_{t\\neq \\op_i}\\s^{(i)}_t\\tn{\\x_\\itt-\\xa_i}\\leq B (1-\\s^{(i)}_{\\op_i}).\\nonumber\n\n\n\\end{align}\nWe will use the $\\bgam^{\\W}_i-\\bgam^{\\op}_i$ term in \\eqref{glob:score decomp} to evaluate $\\W$ against the reference loss $\\Lc_\\star$ of \\eqref{glob:asymp loss}. \n\n\n\n\n\nUsing the strictly-decreasing nature of $\\ell$, we conclude with the fact that for all (finite) $\\W\\in \\Rcm$, \n\\[\n\\Lc(\\W)=\\frac{1}{n}\\sum_{i=1}^n \\ell(\\bgam^{\\W}_i)> \\Lc_\\st=\\frac{1}{n}\\sum_{i=1}^n \\ell(\\bgam^{\\op}_i),\n\\]\nwhich implies $\\td{\\wrb{R}}\\rightarrow\\infty$ together with \\eqref{glob:asymp loss}.\n\n\\noindent\\textbf{Step 2:} To proceed, we show that $\\wrb{R}$ converges in direction to $\\Wcs$, which denotes the set of SVM minima. Suppose this is not the case and~convergence fails. We will obtain a contradiction by showing that $\\Wsb_R=R\\cdot\\Wsb$ achieves a strictly superior loss compared to $\\wrb{R}$. Let us introduce the normalized parameters $\\wrt{R}=\\frac{\\wrb{R}}{R\\xdm}$ and $\\W'=\\frac{\\wrb{R}}{\\td{\\wrb{R}}\\xdm}$. Note that $\\wrt{R}$ is obtained by scaling down $\\W'$ since $\\td{\\wrb{R}}\\leq R$ and $\\W'$ obeys $\\td{\\W'}=\\td{\\Wm}$.\nSince $\\wrt{R}$ fails to converge to $\\Wcs$, for some $\\delta>0$, there exists arbitrarily large $R>0$ such that $\\dist{\\wrt{R},\\Wcs}\\geq \\delta$. This translates to the suboptimality in terms of the margin constraints as follows: First, since nuclear norm dominates Frobenius, distance with respect to the $\\dm$-norm obeys $\\distd{\\wrt{R},\\Wcs}\\geq \\delta$. Secondly, using triangle inequality,\n\\[ \n\\text{this implies that either~~~}\\td{\\wrt{R}}\\leq \\td{\\Wm}-\\delta/2\\text{~~~or~~~}\\distd{\\W',\\Wcs}\\geq \\delta/2.\n\\]\nIn either scenario, $\\wrt{R}$ strictly violates one of the margin constraints of \\eqref{seqattnsvm}: If $\\td{\\wrt{R}}\\leq \\td{\\Wm}-\\delta/2$, then, since the optimal SVM objective is $\\td{\\Wm}$, there exists a constraint $i,t\\neq\\op_i$ for which $\\inn{(\\x^\\op_i-\\x_\\itt)\\z_i^\\top,\\wrt{R}}\\leq 1-\\frac{\\delta}{2\\td{\\Wm}}$. If $\\distd{\\W',\\Wcs}\\geq \\delta/2$, then, $\\W'$ has the same SVM objective but it is strictly bounded away from the solution set. Thus, for some $\\eps:=\\eps(\\delta)>0$, $\\W'$ and its scaled down version $\\wrt{R}$ strictly violate an SVM constraint achieving margin $\\leq 1-\\eps$. Without losing generality, suppose $\\wrt{R}$ violates the first constraint $i=1$. Thus, for a properly updated $\\delta>0$ (that is function of the initial $\\delta>0$) and for $i=1$ and some \\nei $\\tau\\in \\Tc_1$,\n\\begin{align}\n\\inn{ (\\x^\\op_1-\\x_{1t})\\z_1^\\top,\\wrt{R}}\\leq 1-\\delta.\\label{margin violate:glob}\n\n\\end{align}\nNow, we will argue that this leads to a contradiction by proving $\\Lc(\\Wsb_R)<\\Lc(\\wrb{R})$ for sufficiently large $R$.\n\nTo obtain the result, we establish a refined softmax probability control as in Step 1 by studying distance to $\\Lc_\\star$. Following \\eqref{glob:score decomp}, denote the score function at $\\wrb{R}$ via $\\bgam^R_i:=\\bgam_i^{\\wrb{R}}$. Similarly, let $\\sir_i=\\sft{\\abr_i}$ with $\\abr_i=\\X_i\\wrb{R}\\z_i$. Set the corresponding notation for the reference parameter $\\Wsb_R$ as $\\bgam^\\st_i,\\s^\\st_i,\\ab^\\st_i$.  Recall that $R\\geq \\td{\\wrb{R}}$ and $\\xdm:=1/\\td{\\Wm}$. We note the following softmax inequalities \n\\begin{align}\n&\\s^\\st_{i\\op_i}\\geq \\frac{1}{1+Te^{-R\\xdm}}\\geq 1-Te^{-R\\xdm}\\quad \\text{for all}\\quad i\\in[n], \\label{glob:salpha bounds}\\\\\n&s^R_{i\\op_i}\\leq \\frac{1}{1+e^{-(1-\\delta)\\td{\\wrb{R}}\\xdm}}\\leq \\frac{1}{1+e^{-(1-\\delta)R\\xdm}}\\quad\\text{for}\\quad i=1.\\nn\n\\end{align}\nThe former inequality is thanks to $\\Wm$ achieving $\\geq$$1$ margins on all tokens $[T]-\\op_i$ and the latter arises from the $\\delta$-margin violation of $\\wrb{R}$ at $i=1$ i.e.~Eq.~\\eqref{margin violate:glob}. Since $\\ell$ is strictly decreasing with Lipschitz derivative and the scores are upper/lower bounded by an absolute constant (as tokens are bounded and fixed), we have that $\\cop\\geq -\\ell'(\\bgam_i^{\\W})\\geq \\cdn$ for some constants $\\cop>\\cdn>0$. Thus, following Eq.~\\eqref{glob:BB eq}, the score decomposition \\eqref{glob:score decomp}, and \\eqref{glob:salpha bounds} we can write\n\\begin{align}\\label{glob:ineq prl}\n\\Lc(\\wrb{R})-\\Lc_\\star&\\geq \\frac{1}{n}[\\ell(\\bgam_1^{\\wrb{R}})-\\ell(\\bgam^{\\op}_1)]\\geq \\frac{\\cdn}{n}(\\bgam^{\\op}_{1}-\\bgam_1^{\\wrb{R}})\\\\\n\\nonumber\n&\\geq \\frac{\\cdn}{n}c_\\op (1-\\s^R_{1\\op_1}).\n\\\\\n\\nonumber \n&\\geq \\frac{\\cdn c_\\op}{n}\\frac{1}{1+e^{(1-\\delta)R\\xdm}}.\n\\end{align}\nConversely, we upper bound the difference between $\\Lc(\\Wsb_R)$ and $\\Lc_\\star$ as follows. Define the worst-case loss difference for $\\wrb{R}$ as $j=\\arg\\max_{i\\in[n]}[\\ell(\\bgam_i^\\st)-\\ell(\\bgam^{\\op}_i)]$. Using \\eqref{glob:score decomp}\\&\\eqref{glob:salpha bounds}, we write\n\n\\begin{equation*}\n\\begin{aligned}\n\\Lc(\\Wsb_R)-\\Lc_\\star&\\leq \\max_{i\\in[n]}[\\ell(\\bgam_i^\\st)-\\ell(\\bgam^{\\op}_i)]\\leq \\cop\\cdot(\\bgam^{\\op}_{j}-\\bgam^\\st_{j})\\\\\n&\\leq \\cop\\cdot(1-\\s^\\st_{j\\op_j})B\\\\\n&\\leq \\cop\\cdot Te^{-R\\xdm}B.\n\\end{aligned}\n\\end{equation*}\nCombining the last inequality and \\eqref{glob:ineq prl}, we conclude that $\\Lc(\\Wsb_R)<\\Lc(\\wrb{R})$ whenever\n\\[\n\\cop T\\cdot e^{-R\\xdm}B<\\frac{\\cdn\\cdot c_\\op }{n}\\frac{1}{1+e^{(1-\\delta)R\\xdm}}\\iff \\frac{e^{R\\xdm}}{1+e^{(1-\\delta)R\\xdm}}> \\frac{\\cop Tn B}{\\cdn c_\\op }.\n\\]\nThe left hand-side inequality holds for all sufficiently large $R$: Specifically, as soon as $R$ obeys $R>\\frac{1}{\\delta\\xdm}\\log(\\frac{2\\cop Tn B}{\\cdn c_\\op})$. This completes the proof of the theorem by contradiction since we obtained $\\Lc(\\wrb{R})>\\Lc(\\Wsb_R)$.\n\\end{proof}\n\n\n\n\n\n==== END OF /2308.16898/supp/reg_path_glob.tex ====",
            "statements": {
                "definitions": [
                    {
                        "statement_id": "8b4381d3-4f77-44ff-b28b-1cb33455102f",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 7,
                        "library_name": "Definition 7",
                        "title": "Locally-Optimal Indices and Direction",
                        "statement_original_tex": "\\begin{definition}[\\NEIS and Locally-Optimal Direction]\\label{def loc opt} \nFix token indices $\\bal=(\\alpha_i)_{i=1}^n$. Solve \\eqref{eqn:sattnsvm} with $ (\\opt_i)_{i=1}^n$ replaced with $\\boldsymbol{\\alpha} = (\\alpha_i)_{i=1}^n$ to obtain $\\Wma$. Consider the set $\\Tc_i\\subset[T]$ such that $(\\x_{i\\alpha_i}-\\x_{it})^\\top \\Wma \\z_i=1$ for all $t\\in\\Tc_i$. We refer to $(\\Tc_i)_{i=1}^n$ as the \\neis of $\\bal$. Additionally, if for all $i\\in[n]$ and $t\\in\\Tc_i$ scores per Definition~\\ref{score def} obey $\\bgam_{i\\alpha_i}>\\bgam_{it}$, indices $\\bal=(\\alpha_i)_{i=1}^n$ are called \\emph{locally-optimal} and $\\Wma$ is called a \\emph{locally-optimal direction}.\n\\end{definition}",
                        "statement_html": "Fix token indices $\\bal=(\\alpha_i)_{i=1}^n$. Solve (<a href=\"https://arxiv.org/pdf/2308.16898#equation.2.3\">Att-SVM</a>) with $(\\opt_i)_{i=1}^n$ replaced with $\\boldsymbol{\\alpha} = (\\alpha_i)_{i=1}^n$ to obtain $\\Wma$. Consider the set $\\Tc_i\\subset[T]$ such that $(\\x_{i\\alpha_i}-\\x_{it})^\\top \\Wma \\z_i=1$ for all $t\\in\\Tc_i$. We refer to $(\\Tc_i)_{i=1}^n$ as the $\\neis$ of $\\bal$. Additionally, if for all $i\\in[n]$ and $t\\in\\Tc_i$ scores per <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/definitions/definition_8/index.html#score+def\">Definition 8</a> obey $\\bgam_{i\\alpha_i}>\\bgam_{it}$, indices $\\bal=(\\alpha_i)_{i=1}^n$ are called $\\emph{locally-optimal}$ and $\\Wma$ is called a $\\emph{locally-optimal direction}$.",
                        "statement_type": "definition",
                        "statement_motivation_html": "Understanding the concept of $\\emph{locally-optimal direction}$ and $\\neis$ is crucial in optimization problems, particularly in machine learning and support vector machines (SVMs). When dealing with high-dimensional data, identifying locally-optimal directions helps in efficiently navigating the solution space to find optimal or near-optimal solutions. This can significantly reduce computational complexity and improve the performance of algorithms in practice.",
                        "html_url": "library/definitions/definition_7/index.html"
                    },
                    {
                        "statement_id": "7c093de6-8552-483f-90e0-5b6f7ea177b5",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 8,
                        "library_name": "Definition 8",
                        "title": "Token Scoring and Optimality Criterion",
                        "statement_original_tex": "\\begin{definition}[Token Score and Optimality]\\label{score def}\nGiven a prediction head $\\vb\\in\\R^d$, the score of a token $\\x_{it}$ of input $\\X_i$ is defined as $\\bgam_{it} = Y_i \\cdot \\vb^\\top\\x_{it}$. The optimal token for each input $\\X_i$ is given by the index $\\op_i \\in \\arg\\max_{t \\in [T]} \\bgam_{it}$ for all $i \\in [n]$.\n\\end{definition}",
                        "statement_html": "Given a prediction head $\\vb\\in\\R^d$, the score of a token $\\x_{it}$ of input $\\X_i$ is defined as $\\bgam_{it} = Y_i \\cdot \\vb^\\top\\x_{it}$. The optimal token for each input $\\X_i$ is given by the index $\\op_i \\in \\arg\\max_{t \\in [T]} \\bgam_{it}$ for all $i \\in [n]$.",
                        "statement_type": "definition",
                        "statement_motivation_html": "The given statement is useful in the context of machine learning and natural language processing, particularly in tasks involving token-level predictions. By defining the score of each token and identifying the optimal token for each input, this approach allows for the selection of the most relevant or important token based on the prediction head. This can be applied in various scenarios such as named entity recognition, sentiment analysis, and other sequence labeling tasks where identifying key tokens is crucial for accurate predictions.",
                        "html_url": "library/definitions/definition_8/index.html"
                    },
                    {
                        "statement_id": "66d850ce-ac5b-4e5d-aff3-19ee0ab7bb5d",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 9,
                        "library_name": "Definition 9",
                        "title": "Low and High Score Token Separation Criterion",
                        "statement_original_tex": "\\begin{definition}[Low\\&High Score Tokens and Separating Cone]\\label{HL cone def main} Given $\\al\\in[T]$, input sequence $\\X$ with label $Y$, $h(\\cdot):\\R^d\\rightarrow\\R$, and score $\\bgam_t=Y\\cdot h(\\x_t)$ for all $t\\in[T]$, define the low and high score tokens as\n\\[\n\\low:=\\left\\{t\\in[T]\\bgl \\bgam_t<\\bgam_\\al\\right\\},\\quad \\high:=\\left\\{t\\in[T]-\\{\\alpha\\}\\bgl \\bgam_t\\geq \\bgam_\\al\\right\\}.\n\\]\nFor input $\\X_i$ and index $\\alpha_i$, we use the shorthand notations $\\texttt{low}^\\alpha_i$ and $\\texttt{high}^\\alpha_i$. Finally define $\\con{\\bal}$ as\n\\begin{align}\n\\con{\\bal}:=\\left\\{\\texttt{rank}(W)\\leq m\\bgl \\min_{i\\in[n]}\\max_{t\\in\\texttt{low}^\\alpha_i}\\min_{\\tau\\in\\texttt{high}^\\alpha_i} (\\x_\\itt-\\x_\\ittt)^\\top\\W\\z_i\\geq \\eps\\tf{\\W}\\right\\}.\\label{cone alpha eq1}\n\\end{align}\n\\end{definition}",
                        "statement_html": "Given $\\alpha \\in [T]$, input sequence $\\mathbf{X}$ with label $Y$, $h(\\cdot): \\mathbb{R}^d \\rightarrow \\mathbb{R}$, and score $\\boldsymbol{\\gamma}_t = Y \\cdot h(\\mathbf{x}_t)$ for all $t \\in [T]$, define the low and high score tokens as\n\\[\n\\texttt{low} := \\left\\{ t \\in [T] \\mid \\boldsymbol{\\gamma}_t < \\boldsymbol{\\gamma}_\\alpha \\right\\}, \\quad \\texttt{high} := \\left\\{ t \\in [T] - \\{\\alpha\\} \\mid \\boldsymbol{\\gamma}_t \\geq \\boldsymbol{\\gamma}_\\alpha \\right\\}.\n\\]\nFor input $\\mathbf{X}_i$ and index $\\alpha_i$, we use the shorthand notations $\\texttt{low}^\\alpha_i$ and $\\texttt{high}^\\alpha_i$. Finally define $\\mathcal{C}(\\boldsymbol{\\alpha})$ as\n\\begin{align}\n\\mathcal{C}(\\boldsymbol{\\alpha}) := \\left\\{ \\texttt{rank}(W) \\leq m \\mid \\min_{i \\in [n]} \\max_{t \\in \\texttt{low}^\\alpha_i} \\min_{\\tau \\in \\texttt{high}^\\alpha_i} (\\mathbf{x}_{it} - \\mathbf{x}_{i\\tau})^\\top W \\mathbf{z}_i \\geq \\epsilon \\|W\\| \\right\\}. \\label{cone alpha eq1}\n\\end{align}",
                        "statement_type": "definition",
                        "statement_motivation_html": "The definition of $\\mathcal{C}(\\boldsymbol{\\alpha})$ is crucial in the context of ranking and classification problems. It helps in identifying a set of weight matrices $W$ that can effectively separate low and high score tokens for each input sequence. This is particularly useful in machine learning tasks where the goal is to ensure that the model can distinguish between different classes or ranks with a certain margin $\\epsilon$. By ensuring that the condition holds for all input sequences, we can guarantee a level of robustness and accuracy in the model's predictions.",
                        "html_url": "library/definitions/definition_9/index.html"
                    },
                    {
                        "statement_id": "e95f020a-7716-4370-a0de-0d4bbaeedc65",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 10,
                        "library_name": "Definition 10",
                        "title": "Toy Distribution for Self-Attention",
                        "statement_original_tex": "\\begin{definition} [Toy Distribution for Self-Attn]\\label{def data model} Data $(\\X,Y)$ is generated according to $\\Dc_{\\texttt{data}}$ as follows: Let $\\rho >1$ be the index of $\\x_1$'s \\emph{relevant token} (that is allowed to be random).\n\\begin{itemize}\n\\item \\textbf{Relevant token:} $(\\x_1,\\x_\\rho)$ has a uniform distribution over $r$ values $(\\ab_j,\\bb_j)_{j=1}^r$ with associated labels $(y_j)_{j=1}^r$. That is, whenever $(\\x_1,\\x_\\rho)=(\\ab_j,\\bb_j)$, the output label is $Y=y_j$. \n\\item \\textbf{Fixed score:} For some $\\gamma_1,\\gamma_\\rho$ and all $j\\in[r]$: $y_j \\vb^\\top\\ab_j=\\gamma_1$, $y_j \\vb^\\top\\bb_j=\\gamma_\\rho>0$.\n\\item \\textbf{Other tokens} $t\\not\\in\\{\\rho,1\\}$ are bounded, independent of the rest, and $\\E[\\x_t]=\\E[\\x_t\\x_t^\\top]\\vb=0$.\n\\end{itemize}\n\\end{definition}",
                        "statement_html": "Data $(\\X,Y)$ is generated according to $\\Dc_{\\texttt{data}}$ as follows: Let $\\rho >1$ be the index of $\\x_1$'s $\\emph{relevant token}$ (that is allowed to be random).\n<ul>\n<li><strong>Relevant token:</strong> $(\\x_1,\\x_\\rho)$ has a uniform distribution over $r$ values $(\\ab_j,\\bb_j)_{j=1}^r$ with associated labels $(y_j)_{j=1}^r$. That is, whenever $(\\x_1,\\x_\\rho)=(\\ab_j,\\bb_j)$, the output label is $Y=y_j$.</li>\n<li><strong>Fixed score:</strong> For some $\\gamma_1,\\gamma_\\rho$ and all $j\\in[r]$: $y_j \\vb^\\top\\ab_j=\\gamma_1$, $y_j \\vb^\\top\\bb_j=\\gamma_\\rho>0$.</li>\n<li><strong>Other tokens</strong> $t\\not\\in\\{\\rho,1\\}$ are bounded, independent of the rest, and $\\E[\\x_t]=\\E[\\x_t\\x_t^\\top]\\vb=0$.</li>\n</ul>",
                        "statement_type": "definition",
                        "statement_motivation_html": "This statement is useful in the context of machine learning and data generation. It provides a structured way to generate data with specific properties, such as relevant tokens and fixed scores, which can be crucial for testing and validating algorithms. By understanding the distribution and relationships between tokens and labels, one can design better models and improve prediction accuracy. Use this when you need to simulate data with controlled characteristics for experiments or when analyzing the behavior of learning algorithms under specific conditions.",
                        "html_url": "library/definitions/definition_10/index.html"
                    },
                    {
                        "statement_id": "38a2ca6a-b47e-4ac8-bbcf-c37cee0c13ef",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 11,
                        "library_name": "Definition 11",
                        "title": "Low and High Score Token Separation",
                        "statement_original_tex": "\\begin{definition}[Low\\&High Score Tokens and Separating Cone]\\label{HL cone def} Given $\\al\\in[T]$, input sequence $\\X$ with label $Y$, $h(\\cdot):\\R^d\\rightarrow\\R$, and score $\\bgam_t=Y\\cdot h(\\x_t)$ for all $t\\in[T]$, define the low and high score tokens as\n\\[\n\\low=\\left\\{t\\in[T]\\bgl \\bgam_t<\\bgam_\\al\\},\\quad \\high=\\{t\\in[T]-\\{\\alpha\\}\\bgl \\bgam_t\\geq \\bgam_\\al \\right\\}.\n\\]\nFor input $\\X_\\ik$ and index $\\alpha_\\ik$, we use the shorthand notations $\\lowi,\\higi$. Finally define $\\con{\\bal}$ as\n\\begin{align}\n\\con{\\bal}=\\left\\{\\W\\in\\Rcm\\bgl \\min_{i\\in[n]}\\max_{t\\in\\lowi}\\min_{\\tau\\in\\higi} \\inn{\\F_\\ikt-\\F_\\iktt,\\W}\\geq \\eps\\tf{\\W} \\right\\}.\\label{cone alpha eq}\n\\end{align}\n\\end{definition}",
                        "statement_html": "Given $\\alpha \\in [T]$, input sequence $\\mathbf{X}$ with label $Y$, $h(\\cdot): \\mathbb{R}^d \\rightarrow \\mathbb{R}$, and score $\\boldsymbol{\\gamma}_t = Y \\cdot h(\\mathbf{x}_t)$ for all $t \\in [T]$, define the low and high score tokens as\n\\[\n\\low = \\left\\{ t \\in [T] \\mid \\boldsymbol{\\gamma}_t < \\boldsymbol{\\gamma}_\\alpha \\right\\}, \\quad \\high = \\{ t \\in [T] - \\{\\alpha\\} \\mid \\boldsymbol{\\gamma}_t \\geq \\boldsymbol{\\gamma}_\\alpha \\}.\n\\]\nFor input $\\mathbf{X}_i$ and index $\\alpha_i$, we use the shorthand notations $\\lowi, \\higi$. Finally define $\\con{\\boldsymbol{\\alpha}}$ as\n\\begin{align}\n\\con{\\boldsymbol{\\alpha}} = \\left\\{ \\mathbf{W} \\in \\mathbb{R}^{c \\times m} \\mid \\min_{i \\in [n]} \\max_{t \\in \\lowi} \\min_{\\tau \\in \\higi} \\langle \\mathbf{F}_{i,t} - \\mathbf{F}_{i,\\tau}, \\mathbf{W} \\rangle \\geq \\epsilon \\|\\mathbf{W}\\| \\right\\}.\n\\end{align}",
                        "statement_type": "definition",
                        "statement_motivation_html": "The definition of $\\con{\\boldsymbol{\\alpha}}$ is crucial in the context of machine learning and optimization, particularly when dealing with score-based token classification. It helps in identifying a set of weight matrices $\\mathbf{W}$ that ensure a certain margin $\\epsilon$ between low and high score tokens. This can be particularly useful in designing robust classifiers that maintain a minimum separation between different classes, thereby improving the model's generalization and performance.",
                        "html_url": "library/definitions/definition_11/index.html"
                    },
                    {
                        "statement_id": "2c42c01e-0346-4000-96e8-2ab03bddf31b",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 12,
                        "library_name": "Definition 12",
                        "title": "Locally-Optimal Token Selection",
                        "statement_original_tex": "\\begin{definition}[\\Neis and Locally-Optimal Indices]\\label{seq loc opt} Fix token indices $\\bal=(\\alpha_\\ik)\\ikix$ for which \\eqref{seqattnsvm} is feasible to obtain $\\Wma:= \\Wm_{\\dm,\\bal}$. Define token scores as\n\\[\n\\bgam_\\ikt=Y_\\ik\\cdot h_k(\\x_\\itt),\\quad \\bga_\\ik:=\\bgam_{ik\\alpha_\\ik}=Y_\\ik\\cdot h_k(\\xa_\\ik).\n\\]\nConsider tokens $\\Tc_\\ik\\subset[T]$ such that $\\inn{\\Fa_\\ik-\\F_\\ikt,\\Wma}=1$ for all $t\\in\\Tc_\\ik$. $\\Tc_\\ik$ is allowed to be an empty set. We refer to $\\Tc_\\ik$ as \\neis of $\\Fa_\\ik=\\xa_\\ik\\z_\\ik^\\top$ and define its complement $\\Tcb_\\ik=[T]-\\Tc_\\ik-\\{\\al_\\ik\\}$.  Additionally, token indices $\\bal=(\\alpha_\\ik)\\ikix$ are called locally-optimal if for all $i\\in[n],k\\in[K]$ and  $t\\in\\Tc_\\ik$, token scores obey $\\bga_\\ik>\\bgam_\\ikt$. Associated $\\Wma$ is called a locally-optimal direction. Finally, let $\\op_\\ik\\in\\arg\\max_{t\\in[T]}\\bgam_\\ikt$ be the optimal indices and define the associated $\\Ws(\\op)$ to be a globally-optimal direction.\n\\end{definition}",
                        "statement_html": "Fix token indices $\\bal=(\\alpha_\\ik)\\ikix$ for which (<a href=\"https://arxiv.org/pdf/2308.16898#equation.E.91\">SAtt-SVM</a>) is feasible to obtain $\\Wma:= \\Wm_{\\dm,\\bal}$. Define token scores as\n\\[\n\\bgam_\\ikt=Y_\\ik\\cdot h_k(\\x_\\itt),\\quad \\bga_\\ik:=\\bgam_{ik\\alpha_\\ik}=Y_\\ik\\cdot h_k(\\xa_\\ik).\n\\]\nConsider tokens $\\Tc_\\ik\\subset[T]$ such that $\\inn{\\Fa_\\ik-\\F_\\ikt,\\Wma}=1$ for all $t\\in\\Tc_\\ik$. $\\Tc_\\ik$ is allowed to be an empty set. We refer to $\\Tc_\\ik$ as $\\neis$ of $\\Fa_\\ik=\\xa_\\ik\\z_\\ik^\\top$ and define its complement $\\Tcb_\\ik=[T]-\\Tc_\\ik-\\{\\al_\\ik\\}$. Additionally, token indices $\\bal=(\\alpha_\\ik)\\ikix$ are called locally-optimal if for all $i\\in[n],k\\in[K]$ and $t\\in\\Tc_\\ik$, token scores obey $\\bga_\\ik>\\bgam_\\ikt$. Associated $\\Wma$ is called a locally-optimal direction. Finally, let $\\op_\\ik\\in\\arg\\max_{t\\in[T]}\\bgam_\\ikt$ be the optimal indices and define the associated $\\Ws(\\op)$ to be a globally-optimal direction.",
                        "statement_type": "definition",
                        "statement_motivation_html": "Understanding the concept of locally-optimal and globally-optimal directions is crucial in optimization problems, particularly in machine learning and data analysis. Locally-optimal directions help in identifying the best possible solution within a neighborhood, which can be useful for iterative algorithms that refine solutions step-by-step. On the other hand, globally-optimal directions provide the best possible solution across the entire dataset, ensuring the most accurate and efficient outcomes. This distinction is essential when designing algorithms that need to balance between computational efficiency and solution accuracy.",
                        "html_url": "library/definitions/definition_12/index.html"
                    }
                ],
                "axioms": [],
                "lemmas": [
                    {
                        "statement_id": "a1634479-9af0-472b-ab5b-3ffdccc1a613",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 21,
                        "library_name": "Lemma 21",
                        "title": "Optimal Token Separation Lemma",
                        "statement_original_tex": "\\begin{lemma}[Optimal Tokens Minimize Training Loss]\\label{lem min risk} Suppose Assumption \\ref{assum:loss:prope} (i)-(ii) hold, and not all tokens are optimal per Definition~\\ref{score def}. Then, training risk obeys $\\Lc(\\W)>\\Lc_\\st:=\\frac{1}{n}\\sum_{i=1}^n \\ell(\\bgam_{i\\op_i})$. Additionally, suppose there are optimal indices $(\\op_i)_{i=1}^n$ for which \\eqref{eqn:sattnsvm} is feasible, i.e.~there exists a $\\W$ separating optimal tokens. This $\\W$ choice obeys $\\lim_{R\\rightarrow\\infty}\\Lc(R\\cdot\\W)=\\Lc_\\st$.\n\\end{lemma}",
                        "statement_html": "Suppose Assumption A (i)-(ii) [in <a href=\"https://arxiv.org/pdf/2308.16898#assumption.1\">original paper</a>] hold, and not all tokens are optimal per <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/definitions/definition_8/index.html#score+def\">Definition 8</a>. Then, training risk obeys $\\Lc(\\W)>\\Lc_\\st:=\\frac{1}{n}\\sum_{i=1}^n \\ell(\\bgam_{i\\op_i})$. Additionally, suppose there are optimal indices $(\\op_i)_{i=1}^n$ for which (<a href=\"https://arxiv.org/pdf/2308.16898#equation.2.3\">Att-SVM</a>) is feasible, i.e.~there exists a $\\W$ separating optimal tokens. This $\\W$ choice obeys $\\lim_{R\\rightarrow\\infty}\\Lc(R\\cdot\\W)=\\Lc_\\st$.",
                        "statement_type": "lemma",
                        "statement_motivation_html": "Understanding the behavior of training risk is crucial in machine learning, especially when not all tokens are optimal. This statement provides a lower bound for the training risk, ensuring that it is greater than a certain value $\\Lc_\\st$. Additionally, it highlights the conditions under which a weight matrix $\\W$ can separate optimal tokens, and how scaling this matrix affects the training risk. This is useful for evaluating and improving model performance, particularly in scenarios where optimal solutions are sought.",
                        "html_url": "library/lemmas/lemma_21/index.html",
                        "corollary_ids": [],
                        "proof": null
                    },
                    {
                        "statement_id": "bee0dd5e-94e1-44c9-8d3d-bed68c439491",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 22,
                        "library_name": "Lemma 22",
                        "title": "Equivalence of Solution Sets in Constrained Optimization",
                        "statement_original_tex": "\\begin{lemma} \nLet $\\Wc^\\svm_\\star$ be the solution set of \\eqref{eqn:sattnsvmst} with nuclear norm achieving objective $C_\\st$. Further let $\\Wcs_{\\texttt{cvx}}$ be the solution set of \\eqref{eqn:sattnsvmst} with $m=d$ achieving objective $C_{\\texttt{cvx}}$. If $\\Wcs_\\st\\cap \\Wcs_{\\texttt{cvx}}\\neq\\emptyset$, then $C_\\st=C_{\\texttt{cvx}}$ and $\\Wcs_\\st\\subseteq \\Wcs_{\\texttt{cvx}}$. Also, if the elements of $\\Wcs_{\\texttt{cvx}}$ have rank at most $m$, then,  $\\Wcs_\\st=\\Wcs_{\\texttt{cvx}}$.  \n\\end{lemma}",
                        "statement_html": "Let $\\Wc^\\svm_\\star$ be the solution set of (<a href=\"https://arxiv.org/pdf/2308.16898#equation.2.3\">Att-SVM$_*$</a>) with nuclear norm achieving objective $C_\\st$. Further let $\\Wcs_{\\texttt{cvx}}$ be the solution set of (<a href=\"https://arxiv.org/pdf/2308.16898#equation.2.3\">Att-SVM$_*$</a>) with $m=d$ achieving objective $C_{\\texttt{cvx}}$. If $\\Wcs_\\st\\cap \\Wcs_{\\texttt{cvx}}\\neq\\emptyset$, then $C_\\st=C_{\\texttt{cvx}}$ and $\\Wcs_\\st\\subseteq \\Wcs_{\\texttt{cvx}}$. Also, if the elements of $\\Wcs_{\\texttt{cvx}}$ have rank at most $m$, then, $\\Wcs_\\st=\\Wcs_{\\texttt{cvx}}$.",
                        "statement_type": "lemma",
                        "statement_motivation_html": "This result is useful in the context of optimization problems involving nuclear norms. It provides conditions under which the solution sets of two different formulations of the problem coincide. Specifically, it tells us that if the intersection of the solution sets is non-empty, then the objective values are equal and one solution set is contained within the other. This can be particularly useful when trying to simplify or solve complex optimization problems by leveraging the properties of convexity and rank constraints.",
                        "html_url": "library/lemmas/lemma_22/index.html",
                        "corollary_ids": [],
                        "proof": null
                    },
                    {
                        "statement_id": "3f18c03f-226b-4ca9-9165-8868c28c9907",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 23,
                        "library_name": "Lemma 23",
                        "title": "Lipschitz Continuity of Gradients Lemma",
                        "statement_original_tex": "\\begin{lemma}\\label{lem:lip}\nUnder Assumption~\\ref{assum:loss:prope}, $ \\nabla\\Lc(\\W)$,   $ \\nabla_{\\Kb} \\Lc(\\Kb,\\Qb)$, and  $\\nabla_{\\Qb} \\Lc(\\Kb,\\Qb)$ are $L_{\\W}$,  $L_{\\Kb}$, $L_{\\Qb}$--Lipschitz continuous, respectively, where $a_i=\\|\\vb\\|~\\|\\z_i\\|^2 \\|\\X_i \\|^3$,  $b_i= M_0\\|\\vb\\|~\\|\\X_i\\|+ 3  M_1 $ for all $i\\in[n]$,\n\\begin{align}\\label{eqn:lip:cons:erm}\nL_{\\W}:=\\frac{1}{n}\\sum_{i=1}^{n} a_i b_i, \\quad L_{\\Kb}:= \\|\\Qb\\|L_{\\W}, \\quad \\textnormal{and} \\quad L_{\\Qb}:= \\|\\Kb\\|L_{\\W}.\n\\end{align}\n\\end{lemma}",
                        "statement_html": "Under Assumption A [in <a href=\"https://arxiv.org/pdf/2308.16898#assumption.1\">original paper</a>], $ \\nabla\\Lc(\\W)$, $ \\nabla_{\\Kb} \\Lc(\\Kb,\\Qb)$, and $\\nabla_{\\Qb} \\Lc(\\Kb,\\Qb)$ are $L_{\\W}$, $L_{\\Kb}$, $L_{\\Qb}$--Lipschitz continuous, respectively, where $a_i=\\|\\vb\\|~\\|\\z_i\\|^2 \\|\\X_i \\|^3$, $b_i= M_0\\|\\vb\\|~\\|\\X_i\\|+ 3  M_1 $ for all $i\\in[n]$,\n\\begin{align}\\label{eqn:lip:cons:erm}\nL_{\\W}:=\\frac{1}{n}\\sum_{i=1}^{n} a_i b_i, \\quad L_{\\Kb}:= \\|\\Qb\\|L_{\\W}, \\quad \\textnormal{and} \\quad L_{\\Qb}:= \\|\\Kb\\|L_{\\W}.\n\\end{align}",
                        "statement_type": "lemma",
                        "statement_motivation_html": "Understanding the Lipschitz continuity of gradients, as stated in the given lemma, is crucial for analyzing the convergence properties of optimization algorithms. Specifically, knowing that $ \\nabla\\Lc(\\W)$, $ \\nabla_{\\Kb} \\Lc(\\Kb,\\Qb)$, and $\\nabla_{\\Qb} \\Lc(\\Kb,\\Qb)$ are Lipschitz continuous with constants $L_{\\W}$, $L_{\\Kb}$, and $L_{\\Qb}$, respectively, helps in setting appropriate step sizes for gradient-based methods. This ensures that the optimization process is stable and converges efficiently. Use this lemma when you need to guarantee the smoothness of the loss function's gradients in machine learning and optimization problems.",
                        "html_url": "library/lemmas/lemma_23/index.html",
                        "corollary_ids": [],
                        "proof": null
                    },
                    {
                        "statement_id": "3c2b211c-19ba-4dbe-a6dd-feefddb48c30",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 24,
                        "library_name": "Lemma 24",
                        "title": "Rank Bound Lemma",
                        "statement_original_tex": "\\begin{lemma}\\label{lem:rank} Any optimal solution of \\eqref{eqn:sattnsvm} or \\eqref{eqn:sattnsvmst} is at most rank $n$. More precisely, the  row space of $\\Ws$ or $\\Ws_\\st$ lies within $\\texttt{span}(\\{\\z_i\\}_{i=1}^n)$.\n\\end{lemma}",
                        "statement_html": "Any optimal solution of (<a href=\"https://arxiv.org/pdf/2308.16898#equation.2.3\">Att-SVM</a>) or (<a href=\"https://arxiv.org/pdf/2308.16898#equation.2.3\">Att-SVM$_*$</a>) is at most rank $n$. More precisely, the row space of $\\Ws$ or $\\Ws_\\st$ lies within $\\texttt{span}(\\{\\z_i\\}_{i=1}^n)$.",
                        "statement_type": "lemma",
                        "statement_motivation_html": "The statement that any optimal solution of (<a href=\"https://arxiv.org/pdf/2308.16898#equation.2.3\">Att-SVM</a>) or (<a href=\"https://arxiv.org/pdf/2308.16898#equation.2.3\">Att-SVM$_*$</a>) is at most rank $n$ is crucial in the context of support vector machines (SVMs). This result implies that the complexity of the solution is inherently limited by the number of training samples, $n$. Consequently, it ensures that the solution can be efficiently computed and stored, making it practical for large-scale machine learning applications. Additionally, knowing that the row space of $\\Ws$ or $\\Ws_\\st$ lies within $\\texttt{span}(\\{\\z_i\\}_{i=1}^n)$ helps in understanding the structure of the solution and can be leveraged to optimize the algorithm further.",
                        "html_url": "library/lemmas/lemma_24/index.html",
                        "corollary_ids": [],
                        "proof": null
                    },
                    {
                        "statement_id": "06fd87b2-4fe5-4edc-b881-dc9bce163025",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 25,
                        "library_name": "Lemma 25",
                        "title": "Cone Membership Lemma",
                        "statement_original_tex": "\\begin{lemma} \\label{lemma cone main}Suppose \\eqref{dmattnsvm} is feasible. If indices $\\bal$ are locally-optimal, $\\Wma\\in \\con{\\bal}$ for all sufficiently small $\\eps>0$. Otherwise, $\\Wma\\not\\in \\con{\\bal}$ for all $\\eps>0$. Additionally, suppose optimal indices $\\op_i\\in\\arg\\max_{t\\in[T]}\\bgam_\\itt$ are unique and set $\\bal\\gets\\op$. Then, $\\con{\\opt}$ is the set of all rank-$\\leq$$m$ matrices (i.e.~global set).\n\\end{lemma}",
                        "statement_html": "Suppose (<a href=\"https://arxiv.org/pdf/2308.16898#equation.5.6\">$\\diamond$-SVM</a>) is feasible. If indices $\\bal$ are locally-optimal, $\\Wma \\in \\con{\\bal}$ for all sufficiently small $\\eps > 0$. Otherwise, $\\Wma \\not\\in \\con{\\bal}$ for all $\\eps > 0$. Additionally, suppose optimal indices $\\op_i \\in \\arg\\max_{t \\in [T]} \\bgam_\\itt$ are unique and set $\\bal \\gets \\op$. Then, $\\con{\\opt}$ is the set of all rank-$\\leq m$ matrices (i.e. global set).",
                        "statement_type": "lemma",
                        "statement_motivation_html": "Understanding the feasibility of (<a href=\"https://arxiv.org/pdf/2308.16898#equation.5.6\">$\\diamond$-SVM</a>) and the behavior of indices $\\bal$ is crucial in optimization problems involving rank constraints. This statement helps determine whether a given set of indices is locally optimal and provides conditions under which the global set of rank-$\\leq m$ matrices can be identified. This is particularly useful in applications such as matrix completion, signal processing, and machine learning, where rank constraints are common.",
                        "html_url": "library/lemmas/lemma_25/index.html",
                        "corollary_ids": [],
                        "proof": null
                    },
                    {
                        "statement_id": "72962460-6103-4d8c-b1de-88a92a770852",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 26,
                        "library_name": "Lemma 26",
                        "title": "Sparse Softmax Optimization Lemma",
                        "statement_original_tex": "\\begin{lemma}\\label{example dataset} Given $\\vb\\in\\R^d$, recall the score vector $\\bgam=\\X\\vb$. Without losing generality, assume $\\bgam$ is non-increasing. Define the vector of score gaps $\\bbg\\in\\R^{T-1}$ with entries $\\bbg_t=\\bgam_{t}-\\bgam_{t+1}$. Suppose all tokens within the input sequence are orthonormal and for some $\\tau\\geq 2$, we have that \n\\begin{align}\n\\tau\\bbg_\\tau/2>\\bbg_1.\\label{tau description}\n\\end{align}\nSet $h(\\x)=\\vb^\\top\\x-\\la\\tn{\\x}^2$ where $\\tau\\bbg_\\tau/2>\\la>\\bbg_1$, $\\ell(x)=-x$, and $Y=1$. Let $\\Bal_T$ denote the $T$-dimensional simplex. Define the unconstrained softmax optimization associated to the objective $h$ where we make $\\s:=\\sft{\\X\\W\\z}$ a free variable, namely,\n\\begin{align} \n\\min_{\\s\\in\\Bal_T}\\ell(h(\\X\\s))=\\min_{\\s\\in\\Bal_T}\\la \\tn{\\X^\\top \\s}^2-\\vb^\\top\\X^\\top \\s.\\label{direct opt}\n\\end{align}\nThen, the optimal solution $\\s^\\st$ contains at least $2$ and at most $\\tau$ nonzero entries.\n\\end{lemma}",
                        "statement_html": "Given $\\vb \\in \\mathbb{R}^d$, recall the score vector $\\bgam = \\mathbf{X} \\vb$. Without losing generality, assume $\\bgam$ is non-increasing. Define the vector of score gaps $\\bbg \\in \\mathbb{R}^{T-1}$ with entries $\\bbg_t = \\bgam_t - \\bgam_{t+1}$. Suppose all tokens within the input sequence are orthonormal and for some $\\tau \\geq 2$, we have that \n\\begin{align}\n\\tau \\bbg_\\tau / 2 > \\bbg_1. \\label{tau description}\n\\end{align}\nSet $h(\\mathbf{x}) = \\vb^\\top \\mathbf{x} - \\lambda \\|\\mathbf{x}\\|^2$ where $\\tau \\bbg_\\tau / 2 > \\lambda > \\bbg_1$, $\\ell(x) = -x$, and $Y = 1$. Let $\\Delta_T$ denote the $T$-dimensional simplex. Define the unconstrained softmax optimization associated to the objective $h$ where we make $\\mathbf{s} := \\text{softmax}(\\mathbf{X} \\mathbf{W} \\mathbf{z})$ a free variable, namely,\n\\begin{align} \n\\min_{\\mathbf{s} \\in \\Delta_T} \\ell(h(\\mathbf{X} \\mathbf{s})) = \\min_{\\mathbf{s} \\in \\Delta_T} \\lambda \\|\\mathbf{X}^\\top \\mathbf{s}\\|^2 - \\vb^\\top \\mathbf{X}^\\top \\mathbf{s}. \\label{direct opt}\n\\end{align}\nThen, the optimal solution $\\mathbf{s}^\\ast$ contains at least $2$ and at most $\\tau$ nonzero entries.",
                        "statement_type": "lemma",
                        "statement_motivation_html": "This result is useful in the context of optimization problems involving the softmax function. It provides a bound on the number of nonzero entries in the optimal solution $\\mathbf{s}^\\ast$, which can simplify the problem by reducing the dimensionality of the solution space. This is particularly beneficial when dealing with high-dimensional data, as it allows for more efficient computations and can lead to faster convergence in optimization algorithms.",
                        "html_url": "library/lemmas/lemma_26/index.html",
                        "corollary_ids": [],
                        "proof": null
                    },
                    {
                        "statement_id": "a165a686-200b-4f67-8bc3-4961e665fdf3",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 27,
                        "library_name": "Lemma 27",
                        "title": "Cone Membership Lemma",
                        "statement_original_tex": "\\begin{lemma}\\label{lemma cone} Consider the cone definition of \\eqref{cone alpha eq} and suppose an SVM solution $\\Wma$ exists. If indices $\\bal$ are locally-optimal, $\\Wma\\in \\con{\\bal}$ for all sufficiently small $\\eps>0$. Otherwise, $\\Wma\\not\\in \\con{\\bal}$ for all $\\eps>0$. Additionally, suppose optimal indices $\\op_\\ik\\in\\arg\\max_{t\\in[T]}\\bgam_\\ikt$ are unique. Then, $\\con{\\opt}=\\Rcm$.\n\\end{lemma}",
                        "statement_html": "Consider the cone definition of (<a href=\"/library/definitions/definition_11/index.html\">2</a>) [in <a href=\"/library/definitions/definition_11/index.html\">Definition 11</a>] and suppose an SVM solution $\\Wma$ exists. If indices $\\bal$ are locally-optimal, $\\Wma \\in \\con{\\bal}$ for all sufficiently small $\\eps > 0$. Otherwise, $\\Wma \\not\\in \\con{\\bal}$ for all $\\eps > 0$. Additionally, suppose optimal indices $\\op_\\ik \\in \\arg\\max_{t \\in [T]} \\bgam_\\ikt$ are unique. Then, $\\con{\\opt} = \\Rcm$.",
                        "statement_type": "lemma",
                        "statement_motivation_html": "This result is crucial in the context of Support Vector Machines (SVMs) and optimization. It provides a clear criterion for determining whether a given solution lies within a specific cone, which is essential for understanding the geometry of the solution space. This can be particularly useful when analyzing the stability and robustness of the SVM solution, as well as when performing sensitivity analysis.",
                        "html_url": "library/lemmas/lemma_27/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "2d608f00-aa31-4e7c-b84c-4d2a8fe23516",
                            "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof} Suppose $\\bal$ is locally optimal. Observe that, thanks to local optimality, $\\Wma$ obeys\n\\[\n\\min_{t\\in\\Tc_\\ik}\\inn{\\F_\\ikt,\\Wma}>\\max_{\\tau\\not\\in\\Tc_\\ik\\cup\\{\\al_\\ik\\}}\\inn{\\F_\\iktt,\\Wma},\n\\]\nfor all $i\\in[n]$. Next, observe that $\\Tc_\\ik\\subseteq \\lowi$ and $\\higi\\subseteq\\Tcb_\\ik=[T]-\\Tc_\\ik-\\{\\al_\\ik\\}$. Thus, the inequality \\eqref{cone alpha eq} holds for small enough $\\eps>0$.\n\nConversely, suppose $\\bal$ is not locally-optimal. Fix \\nei $t\\in\\Tc_\\ik$ with $t\\in \\higi$. Since $t\\in\\Tc_\\ik$, observe that\n\\[\n\\inn{\\F_\\ikt,\\Wma}\\geq \\max_{\\tau\\neq \\al_\\ik}\\inn{\\F_\\iktt,\\Wma}.\n\\]\nIn other words, for this $i\\in[n]$, we found\n\\[\n\\max_{\\tau\\in\\lowi} \\inn{\\F_\\iktt-\\F_\\ikt,\\Wma}\\leq 0,\n\\]\nviolating \\eqref{cone alpha eq} definition for any $\\eps>0$. To show the final claim, observe that, setting $\\bal:=\\op$, we have that $\\higi=\\emptyset$ for all $i\\in[n]$ as $\\op_\\ik$ are unique optimal indices. Thus, there is no constraint enforced on the cone definition in \\eqref{cone alpha eq} making it equal to the rank-$m$ manifold $\\Rcm$.\n\\end{proof}",
                            "statement_html": "Suppose $\\bal$ is locally optimal. Observe that, thanks to local optimality, $\\Wma$ obeys\n\\[\n\\min_{t\\in\\Tc_\\ik}\\inn{\\F_\\ikt,\\Wma}>\\max_{\\tau\\not\\in\\Tc_\\ik\\cup\\{\\al_\\ik\\}}\\inn{\\F_\\iktt,\\Wma},\n\\]\nfor all $i\\in[n]$. Next, observe that $\\Tc_\\ik\\subseteq \\lowi$ and $\\higi\\subseteq\\Tcb_\\ik=[T]-\\Tc_\\ik-\\{\\al_\\ik\\}$. Thus, the inequality (<a href=\"/library/definitions/definition_11/index.html\">2</a>) [in <a href=\"/library/definitions/definition_11/index.html\">Definition 11</a>] holds for small enough $\\eps>0$.\n\nConversely, suppose $\\bal$ is not locally-optimal. Fix \\nei $t\\in\\Tc_\\ik$ with $t\\in \\higi$. Since $t\\in\\Tc_\\ik$, observe that\n\\[\n\\inn{\\F_\\ikt,\\Wma}\\geq \\max_{\\tau\\neq \\al_\\ik}\\inn{\\F_\\iktt,\\Wma}.\n\\]\nIn other words, for this $i\\in[n]$, we found\n\\[\n\\max_{\\tau\\in\\lowi} \\inn{\\F_\\iktt-\\F_\\ikt,\\Wma}\\leq 0,\n\\]\nviolating (<a href=\"/library/definitions/definition_11/index.html\">2</a>) [in <a href=\"/library/definitions/definition_11/index.html\">Definition 11</a>] definition for any $\\eps>0$. To show the final claim, observe that, setting $\\bal:=\\op$, we have that $\\higi=\\emptyset$ for all $i\\in[n]$ as $\\op_\\ik$ are unique optimal indices. Thus, there is no constraint enforced on the cone definition in (<a href=\"/library/definitions/definition_11/index.html\">2</a>) [in <a href=\"/library/definitions/definition_11/index.html\">Definition 11</a>] making it equal to the rank-$m$ manifold $\\Rcm$.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n<br>\n<br>1. <i>Local Optimality Condition</i>: The proof begins by assuming that \\(\\bal\\) is locally optimal. This implies that for all \\(i \\in [n]\\), the following inequality holds:\n<br>   \\[\n   \\min_{t\\in\\Tc_\\ik}\\inn{\\F_\\ikt,\\Wma}>\\max_{\\tau\\not\\in\\Tc_\\ik\\cup\\{\\al_\\ik\\}}\\inn{\\F_\\iktt,\\Wma}\n   \\]\n<br>\n<br>2. <i>Subset Relationships</i>: Next, it is noted that \\(\\Tc_\\ik \\subseteq \\lowi\\) and \\(\\higi \\subseteq \\Tcb_\\ik = [T] - \\Tc_\\ik - \\{\\al_\\ik\\}\\). This relationship helps in establishing that the inequality (<a href=\"/library/definitions/definition_11/index.html\">2</a>) [in <a href=\"/library/definitions/definition_11/index.html\">Definition 11</a>] holds for sufficiently small \\(\\eps > 0\\).\n<br>\n<br>3. <i>Non-Local Optimality Condition</i>: Conversely, if \\(\\bal\\) is not locally optimal, we fix a \\(t \\in \\Tc_\\ik\\) with \\(t \\in \\higi\\). Since \\(t \\in \\Tc_\\ik\\), we have:\n<br>   \\[\n   \\inn{\\F_\\ikt,\\Wma} \\geq \\max_{\\tau \\neq \\al_\\ik} \\inn{\\F_\\iktt,\\Wma}\n   \\]\n<br>\n<br>4. <i>Violation of Inequality</i>: This implies that for this \\(i \\in [n]\\), we find:\n<br>   \\[\n   \\max_{\\tau \\in \\lowi} \\inn{\\F_\\iktt - \\F_\\ikt, \\Wma} \\leq 0\n   \\]\n<br>   This violates the definition of (<a href=\"/library/definitions/definition_11/index.html\">2</a>) [in <a href=\"/library/definitions/definition_11/index.html\">Definition 11</a>] for any \\(\\eps > 0\\).\n<br>\n<br>5. <i>Final Claim</i>: To show the final claim, we set \\(\\bal := \\op\\). In this case, \\(\\higi = \\emptyset\\) for all \\(i \\in [n]\\) because \\(\\op_\\ik\\) are unique optimal indices. Therefore, there is no constraint enforced on the cone definition in (<a href=\"/library/definitions/definition_11/index.html\">2</a>) [in <a href=\"/library/definitions/definition_11/index.html\">Definition 11</a>], making it equal to the rank-\\(m\\) manifold \\(\\Rcm\\).\n<br>\n<br>Thus, the proof demonstrates the conditions under which \\(\\bal\\) is locally optimal and the implications of these conditions."
                        }
                    },
                    {
                        "statement_id": "e69fcfe5-c221-4494-a431-7c607dd91ae5",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 28,
                        "library_name": "Lemma 28",
                        "title": "Equivalence of Regularization Paths",
                        "statement_original_tex": "\\begin{lemma}[Mapping regularization path of $(\\Kb,\\Qb)$ to $\\W$]\\label{kqw mapping} Let $\\Kb,\\Qb\\in\\R^{d\\times m}$ and consider regularization path solutions of \\eqref{serm-w} and \\eqref{serm-kq}\n\\begin{align}\n&\\Wb_R\\in\\underset{\\W\\in\\Rcm:\\tnuc{\\W}\\leq R}{\\arg\\min}\\Lc(\\W)\\label{Wpath}\\\\\n&\\Kbb_R,\\Qbb_R\\in\\underset{\\tf{\\Kb}^2+\\tf{\\Qb}^2\\leq 2R}{\\arg\\min}\\Lc(\\Kb,\\Qb).\\label{KQpath}\n\\end{align}\nFor all $R\\geq 0$, there is a one-to-one map between the set of solutions $\\Wb_R$ of \\eqref{Wpath} and $\\Kbb_R\\Qbb_R^\\top$ of \\eqref{KQpath}.\n\n\\end{lemma}",
                        "statement_html": "Let $\\Kb,\\Qb\\in\\R^{d\\times m}$ and consider regularization path solutions of (<a href=\"https://arxiv.org/pdf/2308.16898#appendix.E\">SERM-W</a>) and (<a href=\"https://arxiv.org/pdf/2308.16898#appendix.E\">SERM-KQ</a>)\n\\begin{align}\n&\\Wb_R\\in\\underset{\\W\\in\\Rcm:\\tnuc{\\W}\\leq R}{\\arg\\min}\\Lc(\\W)\\label{Wpath}\\\\\n&\\Kbb_R,\\Qbb_R\\in\\underset{\\tf{\\Kb}^2+\\tf{\\Qb}^2\\leq 2R}{\\arg\\min}\\Lc(\\Kb,\\Qb).\\label{KQpath}\n\\end{align}\nFor all $R\\geq 0$, there is a one-to-one map between the set of solutions $\\Wb_R$ of $\\eqref{Wpath}$ and $\\Kbb_R\\Qbb_R^\\top$ of $\\eqref{KQpath}$.",
                        "statement_type": "lemma",
                        "statement_motivation_html": "The one-to-one mapping between the set of solutions $\\Wb_R$ of $\\eqref{Wpath}$ and $\\Kbb_R\\Qbb_R^\\top$ of $\\eqref{KQpath}$ is useful in optimization problems involving matrix factorization. This relationship allows us to switch between different formulations of the problem, potentially simplifying the computational process or providing deeper insights into the structure of the solutions.",
                        "html_url": "library/lemmas/lemma_28/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "4aa7f3b1-d43a-4cb2-a034-87d11ade8673",
                            "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof} To prove the mapping, first fix a $\\Wb_R$ solution with rank $m$, set $\\Lc_F=\\Lc(\\Wb_R)$ and show the existence of $\\Kb,\\Qb$ with $\\Kb\\Qb^\\top=\\Wb_R$ feasible for \\eqref{KQpath} and $\\Lc(\\Kb,\\Qb)\\leq \\Lc_F$. Use the singular value decomposition $\\Wb_R=\\Ub\\bSi\\Vb^\\top$ with $\\bSi\\in\\R^{m\\times m}$ being diagonal matrix of singular values. Set $\\Kb=\\Ub\\sqrt{\\bSi}$ and $\\Qb=\\Vb\\sqrt{\\bSi}$. Observe that $\\Kb\\Qb^\\top=\\W$ and\n\\[\n\\tf{\\Kb}^2=\\tf{\\Qb}^2=\\sum_{i=1}^m\\sqrt{\\bSi_{ii}}^2=\\tnuc{\\Wb_R}\\leq R.\n\\]\nThus, $\\Kb,\\Qb$ achieves $\\Lc(\\Kb,\\Qb)=\\Lc_F$. Conversely, given $\\Kbb_R,\\Qbb_R$ with $\\Lc_\\st=\\Lc(\\Kbb_R,\\Qbb_R)$, $\\W=\\Kbb_R\\Qbb_R^\\top$ obeys $\\Lc(\\W)=\\Lc_\\st$ and, using the standard nuclear norm inequality, we have\n\\[\n\\tnuc{\\W}=\\tnuc{\\Kbb_R\\Qbb_R^\\top}\\leq \\frac{1}{2}(\\tf{\\Kbb_R}^2+\\tf{\\Qbb_R}^2)=R.\n\\]\nThis shows $\\W$ is feasible for \\eqref{Wpath}. Combining the two findings above, we find that optimal costs are equal ($\\Lc_\\st=\\Lc_F$) and for any $(\\Kbb_R,\\Qbb_R)$ solution there exists a $\\Wb_R$ solution and vice versa.\n\\end{proof}",
                            "statement_html": "To prove the mapping, first fix a $\\Wb_R$ solution with rank $m$, set $\\Lc_F=\\Lc(\\Wb_R)$ and show the existence of $\\Kb,\\Qb$ with $\\Kb\\Qb^\\top=\\Wb_R$ feasible for $\\eqref{KQpath}$ and $\\Lc(\\Kb,\\Qb)\\leq \\Lc_F$. Use the singular value decomposition $\\Wb_R=\\Ub\\bSi\\Vb^\\top$ with $\\bSi\\in\\R^{m\\times m}$ being diagonal matrix of singular values. Set $\\Kb=\\Ub\\sqrt{\\bSi}$ and $\\Qb=\\Vb\\sqrt{\\bSi}$. Observe that $\\Kb\\Qb^\\top=\\W$ and\n\\[\n\\tf{\\Kb}^2=\\tf{\\Qb}^2=\\sum_{i=1}^m\\sqrt{\\bSi_{ii}}^2=\\tnuc{\\Wb_R}\\leq R.\n\\]\nThus, $\\Kb,\\Qb$ achieves $\\Lc(\\Kb,\\Qb)=\\Lc_F$. Conversely, given $\\Kbb_R,\\Qbb_R$ with $\\Lc_\\st=\\Lc(\\Kbb_R,\\Qbb_R)$, $\\W=\\Kbb_R\\Qbb_R^\\top$ obeys $\\Lc(\\W)=\\Lc_\\st$ and, using the standard nuclear norm inequality, we have\n\\[\n\\tnuc{\\W}=\\tnuc{\\Kbb_R\\Qbb_R^\\top}\\leq \\frac{1}{2}(\\tf{\\Kbb_R}^2+\\tf{\\Qbb_R}^2)=R.\n\\]\nThis shows $\\W$ is feasible for $\\eqref{Wpath}$. Combining the two findings above, we find that optimal costs are equal ($\\Lc_\\st=\\Lc_F$) and for any $(\\Kbb_R,\\Qbb_R)$ solution there exists a $\\Wb_R$ solution and vice versa.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n<br>\n<br>1. <i>Fixing a Solution</i>: The proof begins by fixing a solution \\(\\Wb_R\\) with rank \\(m\\) and setting \\(\\Lc_F = \\Lc(\\Wb_R)\\). The goal is to show the existence of matrices \\(\\Kb\\) and \\(\\Qb\\) such that \\(\\Kb\\Qb^\\top = \\Wb_R\\) and \\(\\Lc(\\Kb, \\Qb) \\leq \\Lc_F\\).\n<br>\n<br>2. <i>Singular Value Decomposition</i>: The next step involves using the singular value decomposition (SVD) of \\(\\Wb_R\\). This is expressed as:\n<br>   \\[\n   \\Wb_R = \\Ub \\bSi \\Vb^\\top\n   \\]\n   where \\(\\bSi \\in \\R^{m \\times m}\\) is a diagonal matrix of singular values.\n<br>\n<br>3. <i>Constructing \\(\\Kb\\) and \\(\\Qb\\)</i>: Define \\(\\Kb = \\Ub \\sqrt{\\bSi}\\) and \\(\\Qb = \\Vb \\sqrt{\\bSi}\\). It is observed that:\n<br>   \\[\n   \\Kb \\Qb^\\top = \\Wb_R\n   \\]\n   and\n<br>   \\[\n   \\tf{\\Kb}^2 = \\tf{\\Qb}^2 = \\sum_{i=1}^m \\sqrt{\\bSi_{ii}}^2 = \\tnuc{\\Wb_R} \\leq R.\n   \\]\n   Thus, \\(\\Kb\\) and \\(\\Qb\\) achieve \\(\\Lc(\\Kb, \\Qb) = \\Lc_F\\).\n<br>\n<br>4. <i>Conversely</i>: Given \\(\\Kbb_R\\) and \\(\\Qbb_R\\) with \\(\\Lc_\\st = \\Lc(\\Kbb_R, \\Qbb_R)\\), it follows that \\(\\W = \\Kbb_R \\Qbb_R^\\top\\) satisfies \\(\\Lc(\\W) = \\Lc_\\st\\). Using the standard nuclear norm inequality, we have:\n<br>   \\[\n   \\tnuc{\\W} = \\tnuc{\\Kbb_R \\Qbb_R^\\top} \\leq \\frac{1}{2} (\\tf{\\Kbb_R}^2 + \\tf{\\Qbb_R}^2) = R.\n   \\]\n   This shows that \\(\\W\\) is feasible for the original problem.\n<br>\n<br>5. <i>Combining Results</i>: Combining the findings above, it is concluded that the optimal costs are equal (\\(\\Lc_\\st = \\Lc_F\\)). For any \\((\\Kbb_R, \\Qbb_R)\\) solution, there exists a \\(\\Wb_R\\) solution and vice versa."
                        }
                    },
                    {
                        "statement_id": "8fd6e58d-c3bf-49fd-bfcf-e6af7b7bf276",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 29,
                        "library_name": "Lemma 29",
                        "title": "'Quadratic Reduction Lemma'",
                        "statement_original_tex": "\\begin{lemma}\\label{lem:q_reduce} For any $\\X \\in\\R^{T\\times d}$, $\\W,\\V \\in \\R^{d\\times d}$ and $\\z, \\vb \\in \\R^{d}$, let $\\ab= \\X\\V \\z$, $\\s=\\sft{\\X\\W\\z}$, and $\\bgam=\\X\\vb$. Set\n\\begin{equation*}\n\\Gamma=\\sup_{t,\\tau\\in[T]}|\\bgam_t-\\bgam_\\tau|~~~\\textnormal{and}~~~A=\\sup_{t\\in[T]}\\tn{\\ab_t}.\n\\end{equation*}\nWe have that\n  \\[\n    \\left|\\ab^\\top\\textnormal{diag}(\\s) \\bgam-\\ab^\\top\\s\\s^\\top\\bgam-\\sum_{t\\geq 2}^T (\\ab_1-\\ab_t)\\s_t(\\bgam_1-\\bgam_t)\\right|\\leq 2\\Gamma A(1-\\s_1)^2.\n  \\]\n\\end{lemma}",
                        "statement_html": "For any $\\X \\in\\R^{T\\times d}$, $\\W,\\V \\in \\R^{d\\times d}$ and $\\z, \\vb \\in \\R^{d}$, let $\\ab= \\X\\V \\z$, $\\s=\\sft{\\X\\W\\z}$, and $\\bgam=\\X\\vb$. Set\n\\begin{equation*}\n\\Gamma=\\sup_{t,\\tau\\in[T]}|\\bgam_t-\\bgam_\\tau|~~~\\textnormal{and}~~~A=\\sup_{t\\in[T]}\\tn{\\ab_t}.\n\\end{equation*}\nWe have that\n\\[\n\\left|\\ab^\\top\\textnormal{diag}(\\s) \\bgam-\\ab^\\top\\s\\s^\\top\\bgam-\\sum_{t\\geq 2}^T (\\ab_1-\\ab_t)\\s_t(\\bgam_1-\\bgam_t)\\right|\\leq 2\\Gamma A(1-\\s_1)^2.\n\\]",
                        "statement_type": "lemma",
                        "statement_motivation_html": "This inequality is useful in the context of analyzing the behavior of certain matrix and vector operations, particularly in high-dimensional spaces. It provides a bound on the difference between a specific weighted sum and a more complex expression involving matrix-vector products and differences. This can be particularly helpful in optimization problems, machine learning algorithms, and numerical analysis where such bounds are necessary to ensure stability and convergence.",
                        "html_url": "library/lemmas/lemma_29/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "53794786-0136-47c4-9387-c7077f323e3c",
                            "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\nThe proof is similar to \\cite[Lemma~4]{tarzanagh2023margin}, but for the sake of completeness, we provide it here.  Set $\\gamb=\\sum_{t=1}^T \\bgam_t\\s_t$.  We have \n\\begin{align*}\n\\bgam_1-\\gamb=\\sum_{t\\geq 2}^T (\\bgam_1-\\bgam_t)\\s_t,~~\\textnormal{and}~~|\\bgam_1-\\gamb|\\leq \\Gamma (1-\\s_1).\n\\end{align*}    \nThen,\n  \\begin{align} \n  \\nonumber \n    \\ab^\\top\\diag{\\s}\\bgam-\\ab^\\top\\s\\s^\\top\\bgam&=\\sum_{t=1}^T \\ab_t\\bgam_t\\s_t-\\sum_{t=1}^T \\ab_t\\s_t\\sum_{t=1}^T \\bgam_t\\s_t\\\\\n    &=\\ab_1\\s_1(\\bgam_1-\\gamb)-\\sum_{t\\geq 2}^T\\ab_t\\s_t(\\gamb-\\bgam_t). \\label{grad def step3}\n  \\end{align}\nSince \n$$\n\\left|\\sum_{t\\geq 2}^T\\ab_t\\s_t(\\gamb-\\bgam_t)-\\sum_{t\\geq 2}^T\\ab_t\\s_t(\\bgam_1-\\bgam_t)\\right|\\leq A\\Gamma (1-\\s_1)^2,\n$$\nwe obtain\\footnote{For simplicity, we use $\\pm$ on the right hand side to denote the upper and lower bounds.}\n  \\begin{align*}  \n    \\ab^\\top\\diag{\\s}\\bgam-\\ab^\\top\\s\\s^\\top\\bgam&=\\ab_1\\s_1(\\bgam_1-\\gamb)-\\sum_{t\\geq 2}^T\\ab_t\\s_t(\\bgam_1-\\bgam_t)\\pm A\\Gamma (1-\\s_1)^2\\\\\n    &=\\ab_1\\s_1\\sum_{t\\geq 2}^T (\\bgam_1-\\bgam_t)\\s_t-\\sum_{t\\geq 2}^T\\ab_t\\s_t(\\bgam_1-\\bgam_t)\\pm A\\Gamma (1-\\s_1)^2\\\\\n    &=\\sum_{t\\geq 2}^T (\\ab_1\\s_1-\\ab_t)\\s_t(\\bgam_1-\\bgam_t)\\pm A\\Gamma (1-\\s_1)^2\\\\\n    &=\\sum_{t\\geq 2}^T (\\ab_1-\\ab_t)\\s_t(\\bgam_1-\\bgam_t)\\pm 2A\\Gamma (1-\\s_1)^2.\n    \n  \\end{align*}\nHere,  $\\pm$ on the right handside uses the fact that\n  \\[\n  \\left|\\sum_{t\\geq 2}^T (\\ab_1\\s_1-\\ab_1)\\s_t(\\bgam_1-\\bgam_t)\\right|\\leq (1-\\s_1)A\\Gamma\\sum_{t\\geq 2}^T\\s_t=(1-\\s_1)^2A\\Gamma.\n  \\]\n\\end{proof}",
                            "statement_html": "The proof is similar to [<a href=\"https://arxiv.org/pdf/2308.16898#cite.tarzanagh2023margin\">TLZO23</a>, Lemma 4], but for the sake of completeness, we provide it here. Set $\\gamb=\\sum_{t=1}^T \\bgam_t\\s_t$. We have \n\\begin{align*}\n\\bgam_1-\\gamb=\\sum_{t\\geq 2}^T (\\bgam_1-\\bgam_t)\\s_t,~~\\textnormal{and}~~|\\bgam_1-\\gamb|\\leq \\Gamma (1-\\s_1).\n\\end{align*}    \nThen,\n\\begin{align} \n\\nonumber \n\\ab^\\top\\diag{\\s}\\bgam-\\ab^\\top\\s\\s^\\top\\bgam&=\\sum_{t=1}^T \\ab_t\\bgam_t\\s_t-\\sum_{t=1}^T \\ab_t\\s_t\\sum_{t=1}^T \\bgam_t\\s_t\\\\\n&=\\ab_1\\s_1(\\bgam_1-\\gamb)-\\sum_{t\\geq 2}^T\\ab_t\\s_t(\\gamb-\\bgam_t). \\label{grad def step3}\n\\end{align}\nSince \n$$\n\\left|\\sum_{t\\geq 2}^T\\ab_t\\s_t(\\gamb-\\bgam_t)-\\sum_{t\\geq 2}^T\\ab_t\\s_t(\\bgam_1-\\bgam_t)\\right|\\leq A\\Gamma (1-\\s_1)^2,\n$$\nwe obtain\\footnote{For simplicity, we use $\\pm$ on the right hand side to denote the upper and lower bounds.}\n\\begin{align*}  \n\\ab^\\top\\diag{\\s}\\bgam-\\ab^\\top\\s\\s^\\top\\bgam&=\\ab_1\\s_1(\\bgam_1-\\gamb)-\\sum_{t\\geq 2}^T\\ab_t\\s_t(\\bgam_1-\\bgam_t)\\pm A\\Gamma (1-\\s_1)^2\\\\\n&=\\ab_1\\s_1\\sum_{t\\geq 2}^T (\\bgam_1-\\bgam_t)\\s_t-\\sum_{t\\geq 2}^T\\ab_t\\s_t(\\bgam_1-\\bgam_t)\\pm A\\Gamma (1-\\s_1)^2\\\\\n&=\\sum_{t\\geq 2}^T (\\ab_1\\s_1-\\ab_t)\\s_t(\\bgam_1-\\bgam_t)\\pm A\\Gamma (1-\\s_1)^2\\\\\n&=\\sum_{t\\geq 2}^T (\\ab_1-\\ab_t)\\s_t(\\bgam_1-\\bgam_t)\\pm 2A\\Gamma (1-\\s_1)^2.\n\\end{align*}\nHere, $\\pm$ on the right hand side uses the fact that\n\\[\n\\left|\\sum_{t\\geq 2}^T (\\ab_1\\s_1-\\ab_1)\\s_t(\\bgam_1-\\bgam_t)\\right|\\leq (1-\\s_1)A\\Gamma\\sum_{t\\geq 2}^T\\s_t=(1-\\s_1)^2A\\Gamma.\n\\]",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n<br>\n<br>1. <i>Initial Setup</i>: The proof starts by defining \\(\\gamb\\) as the sum of \\(\\bgam_t\\) weighted by \\(\\s_t\\):\n<br>   \\[\n   \\gamb = \\sum_{t=1}^T \\bgam_t\\s_t\n   \\]\n<br>   It then expresses \\(\\bgam_1 - \\gamb\\) and provides an upper bound:\n<br>   \\[\n   \\bgam_1 - \\gamb = \\sum_{t \\geq 2}^T (\\bgam_1 - \\bgam_t)\\s_t, \\quad \\text{and} \\quad |\\bgam_1 - \\gamb| \\leq \\Gamma (1 - \\s_1)\n   \\]\n<br>\n<br>2. <i>Difference of Products</i>: The next step involves the difference between two products involving \\(\\ab\\), \\(\\s\\), and \\(\\bgam\\):\n<br>   \\[\n   \\ab^\\top\\diag{\\s}\\bgam - \\ab^\\top\\s\\s^\\top\\bgam\n   \\]\n<br>   This is expanded as:\n<br>   \\[\n   \\sum_{t=1}^T \\ab_t\\bgam_t\\s_t - \\sum_{t=1}^T \\ab_t\\s_t\\sum_{t=1}^T \\bgam_t\\s_t\n   \\]\n<br>\n<br>3. <i>Substitution and Simplification</i>: By substituting \\(\\gamb\\) and simplifying, we get:\n<br>   \\[\n   \\ab_1\\s_1(\\bgam_1 - \\gamb) - \\sum_{t \\geq 2}^T \\ab_t\\s_t(\\gamb - \\bgam_t)\n   \\]\n<br>   Using the given bound, we further simplify:\n<br>   \\[\n   \\left|\\sum_{t \\geq 2}^T \\ab_t\\s_t(\\gamb - \\bgam_t) - \\sum_{t \\geq 2}^T \\ab_t\\s_t(\\bgam_1 - \\bgam_t)\\right| \\leq A\\Gamma (1 - \\s_1)^2\n   \\]\n<br>\n<br>4. <i>Final Simplification</i>: Combining the terms and using the bound, we obtain:\n<br>   \\[\n   \\ab^\\top\\diag{\\s}\\bgam - \\ab^\\top\\s\\s^\\top\\bgam = \\ab_1\\s_1(\\bgam_1 - \\gamb) - \\sum_{t \\geq 2}^T \\ab_t\\s_t(\\bgam_1 - \\bgam_t) \\pm A\\Gamma (1 - \\s_1)^2\n   \\]\n<br>   This is further simplified to:\n<br>   \\[\n   \\sum_{t \\geq 2}^T (\\ab_1\\s_1 - \\ab_t)\\s_t(\\bgam_1 - \\bgam_t) \\pm 2A\\Gamma (1 - \\s_1)^2\n   \\]\n<br>   Finally, using the fact that:\n<br>   \\[\n   \\left|\\sum_{t \\geq 2}^T (\\ab_1\\s_1 - \\ab_1)\\s_t(\\bgam_1 - \\bgam_t)\\right| \\leq (1 - \\s_1)A\\Gamma\\sum_{t \\geq 2}^T\\s_t = (1 - \\s_1)^2A\\Gamma\n   \\]\n<br>   We conclude:\n<br>   \\[\n   \\sum_{t \\geq 2}^T (\\ab_1 - \\ab_t)\\s_t(\\bgam_1 - \\bgam_t) \\pm 2A\\Gamma (1 - \\s_1)^2\n   \\]"
                        }
                    },
                    {
                        "statement_id": "24f524c6-9779-4b5c-9456-f6210fef911c",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 30,
                        "library_name": "Lemma 30",
                        "title": "Gradient Descent Convergence Lemma",
                        "statement_original_tex": "\\begin{lemma}[Descent Lemma]\\label{lem:grad:descent}\nUnder Assumption \\ref{assum:loss:prope}, if $\\eta \\leq 1/L_{\\W}$, then for any initialization $\\W(0)$, Algorithm~\\ref{GD-W} satisfies:\n\\begin{align}\\label{eq:descent:obj new}\n\\mathcal{L}(\\W(k+1))-\\mathcal{L}(\\W(k))\\leq-\\frac{\\eta}{2} \\tf{\\nabla \\mathcal{L}(\\W(k))}^2,\n\\end{align}\nfor all $k\\ge0$. Additionally, it holds that $\\sum_{k=0}^{\\infty} \\tf{\\nabla\\mathcal{L}\\left(\\W(k)\\right)}^{2}<\\infty$, and $\\lim_{k\\rightarrow \\infty}\n\\tf{\\nabla\\mathcal{L}\\left(\\W\\left(k\\right)\\right)}^{2}=0$.\n\\end{lemma}",
                        "statement_html": "Under Assumption  A [in <a href=\"https://arxiv.org/pdf/2308.16898#assumption.1\">original paper</a>], if $\\eta \\leq 1/L_{\\W}$, then for any initialization $\\W(0)$, Algorithm W-GD [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.2.3\">original paper</a>] satisfies:\n\\begin{align}\\label{eq:descent:obj new}\n\\mathcal{L}(\\W(k+1))-\\mathcal{L}(\\W(k))\\leq-\\frac{\\eta}{2} \\tf{\\nabla \\mathcal{L}(\\W(k))}^2,\n\\end{align}\nfor all $k\\ge0$. Additionally, it holds that $\\sum_{k=0}^{\\infty} \\tf{\\nabla\\mathcal{L}\\left(\\W(k)\\right)}^{2}<\\infty$, and $\\lim_{k\\rightarrow \\infty}\n\\tf{\\nabla\\mathcal{L}\\left(\\W\\left(k\\right)\\right)}^{2}=0$.",
                        "statement_type": "lemma",
                        "statement_motivation_html": "This result is crucial in the context of gradient descent optimization. It guarantees that, under certain conditions, the gradient descent algorithm will make progress towards minimizing the loss function $\\mathcal{L}$. Specifically, it ensures that the gradient norm will decrease over iterations, eventually approaching zero. This is particularly useful for proving the convergence of gradient descent in machine learning and optimization problems.",
                        "html_url": "library/lemmas/lemma_30/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "5bbd661b-27de-4e82-b9fa-fd8d2e898dd1",
                            "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\nThe proof is similar to \\cite[Lemma~6]{tarzanagh2023margin}.\n\n\\end{proof}",
                            "statement_html": "The proof is similar to [<a href=\"https://arxiv.org/pdf/2308.16898#cite.tarzanagh2023margin\">TLZO23</a>, Lemma 6].",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "The proof is similar to [<a href=\"https://arxiv.org/pdf/2308.16898#cite.tarzanagh2023margin\">TLZO23</a>, Lemma 6]."
                        }
                    },
                    {
                        "statement_id": "d47ee3c7-8de8-4689-9c13-c19a0b937d1e",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 31,
                        "library_name": "Lemma 31",
                        "title": "Global Descent Lemma",
                        "statement_original_tex": "\\begin{lemma}\\label{global des lem} \nLet $ \\Wm$ be the SVM solution of \\eqref{eqn:sattnsvm}. Suppose Assumptions \\ref{assum:loss:prope} and \\ref{assum:token} hold.  Then,  for all $\\W\\in\\R^{d\\times d}$, the training loss \\eqref{eqn:erm:w} obeys $\\li\\nabla\\Lc(\\W),\\Wm\\ri<0$. \n\\end{lemma}",
                        "statement_html": "Let $\\Wm$ be the SVM solution of (<a href=\"https://arxiv.org/pdf/2308.16898#equation.2.3\">Att-SVM</a>). Suppose Assumptions A [in <a href=\"https://arxiv.org/pdf/2308.16898#assumption.1\">original paper</a>] and B [in <a href=\"https://arxiv.org/pdf/2308.16898#assumption.2\">original paper</a>] hold. Then, for all $\\W\\in\\R^{d\\times d}$, the training loss (<a href=\"https://arxiv.org/pdf/2308.16898#figure.3\">W-ERM</a>) obeys $\\li\\nabla\\Lc(\\W),\\Wm\\ri<0$.",
                        "statement_type": "lemma",
                        "statement_motivation_html": "This result is useful in the context of Support Vector Machines (SVMs) and optimization. It provides a condition under which the training loss function decreases, ensuring that the solution $\\Wm$ is indeed minimizing the loss. This is particularly important for validating the effectiveness of the SVM model and ensuring that the optimization process is correctly guiding the model towards better performance.",
                        "html_url": "library/lemmas/lemma_31/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "38f24589-43b1-45f4-834d-aa51a50a11e4",
                            "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\nLet\n\\begin{equation}\n\\hbm_i=  \\X_{i} \\Wm \\z_i, ~~~\\bgam_i=Y_i\\cdot \\X_i\\vb,~~~\\textnormal{and}~~~\n \\hb_i=\\X_i\\W \\z_{i}.    \n\\end{equation}\nLet us recall the gradient evaluated at $\\W$ which is given by \n\\begin{align}\\label{grad def new}\n\\nabla\\Lc(\\W)=\\frac{1}{n}\\sum_{i=1}^n\\ell' \\left(\\bgam_i^\\top \\sft{\\hb_i}\\right) \\cdot \\X_i^\\top  \\sfp{\\hb_i}  \\bgam_i  \\z_{i}^\\top,\n\\end{align}\n which implies that \n\\begin{equation}\\label{eqn:grad:prod:p}\n    \\begin{split}\n\\li\\nabla\\Lc(\\W),\\Wm\\ri&= \\frac{1}{n}\\sum_{i=1}^n \\ell' \\left(\\bgam_i^\\top \\sft{\\hb_i}\\right)  \\cdot \\iprod{ \\X_i^\\top  \\sfp{\\hb_i}  \\bgam_i  \\z_{i}^\\top}{\\Wm}\\\\\n&= \\frac{1}{n}\\sum_{i=1}^n\n\\ell'_i  \\cdot \\tr\\left(  (\\Wm)^\\top  \\X_{i}^\\top \\sfp{\\hb_i} \\bgam_i  \\z_{i}^\\top\\right)\\\\\n\n\n\n\n\n\n&= \\frac{1}{n}\\sum_{i=1}^n\n\\ell'_i \\cdot \\hbm_{i}^\\top \\sfp{\\hb_i} \\bgam_i \\\\\n&=  \\frac{1}{n}\\sum_{i=1}^n\n\\ell'_i \\cdot  \\left(\\hbm^\\top_i\\diag{\\s_i}\\bgam_i-\\hbm^\\top_i\\s_i\\s^\\top_i\\bgam_i\\right).        \n    \\end{split}\n\\end{equation}\nHere, let $\\ell'_i:=\\ell'(\\bgam_i^\\top\\sft{\\hb_i})$, $\\s_i=\\sft{\\hb_i}$ and the third equality uses $\\tr\\left(\\bb\\ab^\\top\\right) = \\ab^\\top \\bb$.\n\nIn order to move forward, we will establish the following result, with a focus on the equal score condition (Assumption B.2 [in <a href=\"https://arxiv.org/pdf/2308.16898#Item.5\">original paper</a>]): Let $\\gamma=\\bgam_{t\\geq 2}$ be a constant, and let $\\bgam_1$ and $\\bar{\\hb}_1$ represent the largest indices of vectors $\\bgam$ and $\\hbm$ respectively. For any vector $\\s$ that satisfies $\\sum_{t\\in[T]}\\s_t=1$ and $\\s_t> 0$, we aim to prove that $\\hbm^\\top\\diag{\\s}\\bgam-\\hbm^\\top\\s\\s^\\top\\bgam>0$. To demonstrate this, we proceed by writing the following:\n\\begin{equation}\\label{grad def2}\n\\begin{split}\n\\hbm^\\top\\diag{\\s}\\bgam-\\hbm^\\top\\s\\s^\\top\\bgam&=\\sum_{t=1}^T \\bar{\\hb}_t\\bgam_t \\s_t-\\sum_{t=1}^T  \\bar{\\hb}_t \\s_t\\sum_{t=1}^T \\bgam_t \\s_t\\\\\n&=\\left(\\bar{\\hb}_1\\bgam_1\\s_1+\\gamma\\sum_{t\\geq 2}^T\\bar{\\hb}_t\\s_t\\right)-\\Big(\\bgam_1\\s_1+\\gamma(1-\\s_1)\\Big)\\left(\\bar{\\hb}_1\\s_1+\\sum_{t\\geq 2}^T \\bar{\\hb}_t\\s_t\\right)\\\\\n&=\\bar{\\hb}_1(\\bgam_1-\\gamma) \\s_1(1-\\s_1)-(\\bgam_1-\\gamma)\\s_1\\sum_{t\\geq 2}^T \\bar{\\hb}_t \\s_t\\\\\n&=(\\bgam_1-\\gamma)(1- \\s_1) \\s_1\\left[\\bar{\\hb}_1-\\frac{ \\sum_{t\\geq 2}^T \\bar{\\hb}_t \\s_t}{\\sum_{t\\geq 2}^T\\s_t}\\right]\\\\\n&\\geq(\\bgam_1-\\gamma)(1- \\s_1) \\s_1 (\\bar{\\hb}_1-\\max_{t\\geq 2}\\bar{\\hb}_t).\n\\end{split}\n\\end{equation}\nTo proceed, define\n\\begin{equation*}\n\\bgag^i=\\bgam_{i\\opt_i}-\\max_{t\\neq\\opt_i}\\bgam_{it}~~\\textnormal{and}~~\\bhbg^i=\\bar \\hb_{i\\opt_i}-\\max_{t\\neq\\opt_i}\\bar \\hb_{it}.\n\\end{equation*}\nWith these, we obtain \n\n\n\n\\begin{equation}\\label{eqn:al:lem}\n\n\\hbm^\\top_i\\diag{\\s_i}\\bgam_i-\\hbm^\\top_i\\s_i\\s^\\top_i\\bgam_i\\geq\\bgag^i\\bhbg^i(1-\\s_{i\\opt_i})\\s_{i\\opt_i}.\n\\end{equation}\nNote that \n\\begin{equation*}\n\\begin{split}\n& \\bhbg^i=\\min_{t\\neq \\opt_i}~(\\x_{i \\opt_i}-\\x_{it})^\\top\\Wm \\z_i \\geq1,  \\\\\n&\\bgag^i=\\min_{t\\neq \\opt_i}~\\bgam_{i\\opt_i}-\\bgam_{it} >0,\\\\\n&\\s_{i\\opt_i}(1-\\s_{i\\opt_i}) > 0.    \n\\end{split}\n\\end{equation*}\n\\begin{comment}\n    Hence,\n\\begin{equation}\\label{eqn:lower}\nc_0:=\\min_{i \\in [n]}\\left\\{ \\left(\\min_{t\\neq \\opt_i}~(\\x_{i \\opt_i}-\\x_{it})^\\top\\Wm \\z_i\\right) \\cdot \\left(\\min_{t\\neq \\opt_i}~\\bgam_{i\\opt_i}-\\bgam_{it}\\right) \\cdot \\s_{i\\opt_i}(1-\\s_{i\\opt_i}) \\right\\}>0.\n\\end{equation}\nIt follows from  \\eqref{eqn:al:lem} and \\eqref{eqn:lower} that \n\\begin{equation}\\label{eqn:al:lem:2}\n\n\\min_{i \\in [n]}\\left\\{\\hbm^\\top_i\\diag{\\s_i}\\bgam_i-\\hbm^\\top_i\\s_i\\s^\\top_i\\bgam_i \\right\\} \\geq c_0>0.\n\\end{equation}\nFurther, by our assumption $\\ell'_i<0$.  \nSince by Assumption \\ref{assum:loss:prope}, $\\ell'$ is continuous and the domain is bounded, the maximum is attained and negative, and thus  \n\\begin{equation}\\label{eqn:bound:lprim}\n-c_1= \\max_{x} \\ell'(x), \\qquad  \\textnormal{for some} \\quad c_1>0.     \n\\end{equation}\nHence, using \\eqref{eqn:al:lem:2} and \\eqref{eqn:bound:lprim} in  \\eqref{eqn:grad:prod:p}, we obtain \n\\begin{equation}\\label{eqn:grad:prod:p:fin}\n    \\begin{split}\n\\li\\nabla\\Lc(\\W),\\Wm\\ri \\leq  - c<0, \\quad \\textnormal{where} \\quad c=c_1 \\cdot c_0.\n    \\end{split}\n\\end{equation}\n\n\n\n\n\n\n\n\n\nIn the scenario that Assumption~\\ref{assum:token:supp} holds (all tokens are support), $\\hbm_t=\\x_{it}^\\top\\Wm \\z_i $ is constant for all $t\\geq 2$. Hence, following similar steps as in \\eqref{grad def2} completes the proof. \n\\end{comment}\nHence,\n\\begin{equation}\\label{eqn:lower}\n\\min_{i \\in [n]}\\left\\{ \\left(\\min_{t\\neq \\opt_i}~(\\x_{i \\opt_i}-\\x_{it})^\\top\\Wm \\z_i\\right) \\cdot \\left(\\min_{t\\neq \\opt_i}~\\bgam_{i\\opt_i}-\\bgam_{it}\\right) \\cdot \\s_{i\\opt_i}(1-\\s_{i\\opt_i}) \\right\\}>0.\n\\end{equation}\nIt follows from  \\eqref{eqn:al:lem} and \\eqref{eqn:lower} that \n\\begin{equation}\\label{eqn:al:lem:2}\n\n\\min_{i \\in [n]}\\left\\{\\hbm^\\top_i\\diag{\\s_i}\\bgam_i-\\hbm^\\top_i\\s_i\\s^\\top_i\\bgam_i \\right\\}>0.\n\\end{equation}\nFurther, by Assumption \\ref{assum:loss:prope}, $\\ell'_i<0$, $\\ell'$ is continuous and the domain is bounded, the maximum is attained and negative, and thus  \n\\begin{equation}\\label{eqn:bound:lprim}\n\\max_{x} \\ell'(x)<0.    \n\\end{equation}\nHence, using \\eqref{eqn:al:lem:2} and \\eqref{eqn:bound:lprim} in  \\eqref{eqn:grad:prod:p}, we obtain \n\\begin{equation}\\label{eqn:grad:prod:p:fin}\n    \\begin{split}\n\\li\\nabla\\Lc(\\W),\\Wm\\ri <0.\n    \\end{split}\n\\end{equation}\n\n\n\n\n\n\n\n\n\nIn the scenario that Assumption~\\ref{assum:token:supp} holds (all tokens are support), $\\hbm_t=\\x_{it}^\\top\\Wm \\z_i $ is constant for all $t\\geq 2$. Hence, following similar steps as in \\eqref{grad def2} completes the proof. \n\n\\end{proof}",
                            "statement_html": "Let\n\\begin{equation}\n\\hbm_i=  \\X_{i} \\Wm \\z_i, ~~~\\bgam_i=Y_i\\cdot \\X_i\\vb,~~~\\textnormal{and}~~~\n \\hb_i=\\X_i\\W \\z_{i}.    \n\\end{equation}\nLet us recall the gradient evaluated at $\\W$ which is given by \n\\begin{align}\\label{grad def new}\n\\nabla\\Lc(\\W)=\\frac{1}{n}\\sum_{i=1}^n\\ell' \\left(\\bgam_i^\\top \\sft{\\hb_i}\\right) \\cdot \\X_i^\\top  \\sfp{\\hb_i}  \\bgam_i  \\z_{i}^\\top,\n\\end{align}\n which implies that \n\\begin{equation}\\label{eqn:grad:prod:p}\n    \\begin{split}\n\\li\\nabla\\Lc(\\W),\\Wm\\ri&= \\frac{1}{n}\\sum_{i=1}^n \\ell' \\left(\\bgam_i^\\top \\sft{\\hb_i}\\right)  \\cdot \\iprod{ \\X_i^\\top  \\sfp{\\hb_i}  \\bgam_i  \\z_{i}^\\top}{\\Wm}\\\\\n&= \\frac{1}{n}\\sum_{i=1}^n\n\\ell'_i  \\cdot \\tr\\left(  (\\Wm)^\\top  \\X_{i}^\\top \\sfp{\\hb_i} \\bgam_i  \\z_{i}^\\top\\right)\\\\\n&= \\frac{1}{n}\\sum_{i=1}^n\n\\ell'_i \\cdot \\hbm_{i}^\\top \\sfp{\\hb_i} \\bgam_i \\\\\n&=  \\frac{1}{n}\\sum_{i=1}^n\n\\ell'_i \\cdot  \\left(\\hbm^\\top_i\\diag{\\s_i}\\bgam_i-\\hbm^\\top_i\\s_i\\s^\\top_i\\bgam_i\\right).        \n    \\end{split}\n\\end{equation}\nHere, let $\\ell'_i:=\\ell'(\\bgam_i^\\top\\sft{\\hb_i})$, $\\s_i=\\sft{\\hb_i}$ and the third equality uses $\\tr\\left(\\bb\\ab^\\top\\right) = \\ab^\\top \\bb$.\n\nIn order to move forward, we will establish the following result, with a focus on the equal score condition (Assumption B.2 [in <a href=\"https://arxiv.org/pdf/2308.16898#Item.5\">original paper</a>]): Let $\\gamma=\\bgam_{t\\geq 2}$ be a constant, and let $\\bgam_1$ and $\\bar{\\hb}_1$ represent the largest indices of vectors $\\bgam$ and $\\hbm$ respectively. For any vector $\\s$ that satisfies $\\sum_{t\\in[T]}\\s_t=1$ and $\\s_t> 0$, we aim to prove that $\\hbm^\\top\\diag{\\s}\\bgam-\\hbm^\\top\\s\\s^\\top\\bgam>0$. To demonstrate this, we proceed by writing the following:\n\\begin{equation}\\label{grad def2}\n\\begin{split}\n\\hbm^\\top\\diag{\\s}\\bgam-\\hbm^\\top\\s\\s^\\top\\bgam&=\\sum_{t=1}^T \\bar{\\hb}_t\\bgam_t \\s_t-\\sum_{t=1}^T  \\bar{\\hb}_t \\s_t\\sum_{t=1}^T \\bgam_t \\s_t\\\\\n&=\\left(\\bar{\\hb}_1\\bgam_1\\s_1+\\gamma\\sum_{t\\geq 2}^T\\bar{\\hb}_t\\s_t\\right)-\\Big(\\bgam_1\\s_1+\\gamma(1-\\s_1)\\Big)\\left(\\bar{\\hb}_1\\s_1+\\sum_{t\\geq 2}^T \\bar{\\hb}_t\\s_t\\right)\\\\\n&=\\bar{\\hb}_1(\\bgam_1-\\gamma) \\s_1(1-\\s_1)-(\\bgam_1-\\gamma)\\s_1\\sum_{t\\geq 2}^T \\bar{\\hb}_t \\s_t\\\\\n&=(\\bgam_1-\\gamma)(1- \\s_1) \\s_1\\left[\\bar{\\hb}_1-\\frac{ \\sum_{t\\geq 2}^T \\bar{\\hb}_t \\s_t}{\\sum_{t\\geq 2}^T\\s_t}\\right]\\\\\n&\\geq(\\bgam_1-\\gamma)(1- \\s_1) \\s_1 (\\bar{\\hb}_1-\\max_{t\\geq 2}\\bar{\\hb}_t).\n\\end{split}\n\\end{equation}\nTo proceed, define\n\\begin{equation*}\n\\bgag^i=\\bgam_{i\\opt_i}-\\max_{t\\neq\\opt_i}\\bgam_{it}~~\\textnormal{and}~~\\bhbg^i=\\bar \\hb_{i\\opt_i}-\\max_{t\\neq\\opt_i}\\bar \\hb_{it}.\n\\end{equation*}\nWith these, we obtain \n\\begin{equation}\\label{eqn:al:lem}\n\\hbm^\\top_i\\diag{\\s_i}\\bgam_i-\\hbm^\\top_i\\s_i\\s^\\top_i\\bgam_i\\geq\\bgag^i\\bhbg^i(1-\\s_{i\\opt_i})\\s_{i\\opt_i}.\n\\end{equation}\nNote that \n\\begin{equation*}\n\\begin{split}\n& \\bhbg^i=\\min_{t\\neq \\opt_i}~(\\x_{i \\opt_i}-\\x_{it})^\\top\\Wm \\z_i \\geq1,  \\\\\n&\\bgag^i=\\min_{t\\neq \\opt_i}~\\bgam_{i\\opt_i}-\\bgam_{it} >0,\\\\\n&\\s_{i\\opt_i}(1-\\s_{i\\opt_i}) > 0.    \n\\end{split}\n\\end{equation*}\nHence,\n\\begin{equation}\\label{eqn:lower}\n\\min_{i \\in [n]}\\left\\{ \\left(\\min_{t\\neq \\opt_i}~(\\x_{i \\opt_i}-\\x_{it})^\\top\\Wm \\z_i\\right) \\cdot \\left(\\min_{t\\neq \\opt_i}~\\bgam_{i\\opt_i}-\\bgam_{it}\\right) \\cdot \\s_{i\\opt_i}(1-\\s_{i\\opt_i}) \\right\\}>0.\n\\end{equation}\nIt follows from  \\eqref{eqn:al:lem} and \\eqref{eqn:lower} that \n\\begin{equation}\\label{eqn:al:lem:2}\n\\min_{i \\in [n]}\\left\\{\\hbm^\\top_i\\diag{\\s_i}\\bgam_i-\\hbm^\\top_i\\s_i\\s^\\top_i\\bgam_i \\right\\}>0.\n\\end{equation}\nFurther, by Assumption A [in <a href=\"https://arxiv.org/pdf/2308.16898#assumption.1\">original paper</a>], $\\ell'_i<0$, $\\ell'$ is continuous and the domain is bounded, the maximum is attained and negative, and thus  \n\\begin{equation}\\label{eqn:bound:lprim}\n\\max_{x} \\ell'(x)<0.    \n\\end{equation}\nHence, using \\eqref{eqn:al:lem:2} and \\eqref{eqn:bound:lprim} in  (15) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.C.26\">original paper</a>], we obtain \n\\begin{equation}\\label{eqn:grad:prod:p:fin}\n    \\begin{split}\n\\li\\nabla\\Lc(\\W),\\Wm\\ri <0.\n    \\end{split}\n\\end{equation}\nIn the scenario that Assumption  B.1 [in <a href=\"https://arxiv.org/pdf/2308.16898#Item.4\">original paper</a>] holds (all tokens are support), $\\hbm_t=\\x_{it}^\\top\\Wm \\z_i $ is constant for all $t\\geq 2$. Hence, following similar steps as in (27) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.C.27\">original paper</a>] completes the proof.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n<br>\n<br>1. <i>Definition of Variables</i>: The proof starts by defining several key variables:\n<br>   \\[\n   \\hbm_i=  \\X_{i} \\Wm \\z_i, ~~~\\bgam_i=Y_i\\cdot \\X_i\\vb,~~~\\textnormal{and}~~~ \\hb_i=\\X_i\\W \\z_{i}.\n   \\]\n<br>\n<br>2. <i>Gradient Evaluation</i>: The gradient evaluated at \\(\\W\\) is given by:\n<br>   \\[\n   \\nabla\\Lc(\\W)=\\frac{1}{n}\\sum_{i=1}^n\\ell' \\left(\\bgam_i^\\top \\sft{\\hb_i}\\right) \\cdot \\X_i^\\top  \\sfp{\\hb_i}  \\bgam_i  \\z_{i}^\\top,\n   \\]\n<br>   which implies:\n<br>   \\[\n   \\li\\nabla\\Lc(\\W),\\Wm\\ri= \\frac{1}{n}\\sum_{i=1}^n \\ell' \\left(\\bgam_i^\\top \\sft{\\hb_i}\\right)  \\cdot \\iprod{ \\X_i^\\top  \\sfp{\\hb_i}  \\bgam_i  \\z_{i}^\\top}{\\Wm}\n   \\]\n<br>\n<br>3. <i>Trace and Inner Product</i>: The expression is further simplified using the trace and inner product properties:\n<br>   \\[\n   \\frac{1}{n}\\sum_{i=1}^n \\ell'_i  \\cdot \\tr\\left(  (\\Wm)^\\top  \\X_{i}^\\top \\sfp{\\hb_i} \\bgam_i  \\z_{i}^\\top\\right) = \\frac{1}{n}\\sum_{i=1}^n \\ell'_i \\cdot \\hbm_{i}^\\top \\sfp{\\hb_i} \\bgam_i\n   \\]\n<br>\n<br>4. <i>Diagonal and Simplification</i>: The term is then expressed in terms of diagonal matrices and simplified:\n<br>   \\[\n   \\frac{1}{n}\\sum_{i=1}^n \\ell'_i \\cdot  \\left(\\hbm^\\top_i\\diag{\\s_i}\\bgam_i-\\hbm^\\top_i\\s_i\\s^\\top_i\\bgam_i\\right)\n   \\]\n<br>   Here, \\(\\ell'_i:=\\ell'(\\bgam_i^\\top\\sft{\\hb_i})\\) and \\(\\s_i=\\sft{\\hb_i}\\).\n<br>\n<br>5. <i>Equal Score Condition</i>: To proceed, the proof establishes a result under the equal score condition:\n<br>   \\[\n   \\hbm^\\top\\diag{\\s}\\bgam-\\hbm^\\top\\s\\s^\\top\\bgam>0\n   \\]\n<br>   This is demonstrated by:\n<br>   \\[\n   \\begin{split}\n   \\hbm^\\top\\diag{\\s}\\bgam-\\hbm^\\top\\s\\s^\\top\\bgam &= \\sum_{t=1}^T \\bar{\\hb}_t\\bgam_t \\s_t-\\sum_{t=1}^T  \\bar{\\hb}_t \\s_t\\sum_{t=1}^T \\bgam_t \\s_t \\\\\n   &= (\\bgam_1-\\gamma)(1- \\s_1) \\s_1\\left[\\bar{\\hb}_1-\\frac{ \\sum_{t\\geq 2}^T \\bar{\\hb}_t \\s_t}{\\sum_{t\\geq 2}^T\\s_t}\\right] \\\\\n   &\\geq (\\bgam_1-\\gamma)(1- \\s_1) \\s_1 (\\bar{\\hb}_1-\\max_{t\\geq 2}\\bar{\\hb}_t)\n   \\end{split}\n   \\]\n<br>\n<br>6. <i>Lower Bound Establishment</i>: The proof then defines:\n<br>   \\[\n   \\bgag^i=\\bgam_{i\\opt_i}-\\max_{t\\neq\\opt_i}\\bgam_{it}~~\\textnormal{and}~~\\bhbg^i=\\bar \\hb_{i\\opt_i}-\\max_{t\\neq\\opt_i}\\bar \\hb_{it}\n   \\]\n<br>   and shows:\n<br>   \\[\n   \\hbm^\\top_i\\diag{\\s_i}\\bgam_i-\\hbm^\\top_i\\s_i\\s^\\top_i\\bgam_i\\geq\\bgag^i\\bhbg^i(1-\\s_{i\\opt_i})\\s_{i\\opt_i}\n   \\]\n<br>\n<br>7. <i>Final Inequality</i>: Using the established bounds and assumptions, the proof concludes:\n<br>   \\[\n   \\li\\nabla\\Lc(\\W),\\Wm\\ri <0\n   \\]\n<br>   This completes the proof under the given assumptions."
                        }
                    },
                    {
                        "statement_id": "88506101-38eb-4532-8e56-86f786b8f886",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 32,
                        "library_name": "Lemma 32",
                        "title": "Global Optimality Conditions",
                        "statement_original_tex": "\\begin{lemma}\n\\label{glocal cond} \nSuppose Assumption~\\ref{assum:loss:prope} holds and let $\\op=(\\op_i)_{i=1}^n$ be the unique globally-optimal indices with $\\Wm$ denoting the \\ref{eqn:sattnsvm} solution. Define the margin $\\Theta=1/\\tf{\\Ws}$. Let $\\s_{i}=\\sft{\\X_i\\W\\z_i}$. For any $\\mu>0$, there exists a sufficiently large $\\RR_\\mu=\\order{1/\\mu}$ (see \\eqref{R bound2}) such that:\n\\begin{enumerate}[label={\\textnormal{\\textbf{L\\arabic*.}}}, wide, labelwidth=!,itemindent=!, labelindent=5pt]\n\\item \\label{lem:gcond:l1} There is no stationary point within  $ \\conb_{\\mu,\\RR_\\mu}(\\Ws)$, where $\\conb_{\\mu,\\RR_\\mu} (\\Ws)$ is defined in \\eqref{eqn:con:nabla0}. \n\\item\\label{lem:gcond:l2} For all $\\V\\in \\Scc_{\\mu} (\\Ws)$ with $\\tf{\\V}=\\tf{\\Wm}$  and $\\W\\in\\conb_{\\mu,\\RR_\\mu}(\\Wm)$, there exist dataset dependent constants $C,c>0$ such that \n\\begin{subequations}\\label{zero:g:lbound}\n\\begin{align}\n&C\\cdot \\frac{1}{n}\\sum_{i=1}^n \\left(1-\\s_{i\\op_i}\\right) \\geq -\\Big\\langle\\nabla\\Lc(\\W),\\V \\Big\\rangle\\geq c\\cdot \\mu\\cdot  \\frac{1}{n} \\sum_{i=1}^n  \\left(1-\\s_{i\\op_i}\\right)>0, \\label{zero1:g:bound} \\\\\n& -\\li\\frac{\\V}{\\tf{\\V}},\\frac{\\nabla\\Lc(\\W)}{\\tf{\\nabla\\Lc(\\W)}}\\ri \\geq  \\frac{c}{C} \\cdot \\frac{\\Theta}{\\bar{A}}>0, \\label{zero2:g:bound}\\\\\n&\\tf{\\nabla\\Lc(\\W)}\\leq \\bar{A}C \\cdot \\frac{1}{n} \\sum_{i=1}^n  \\left(1-\\s_{i\\op_i}\\right). \\label{zero3:g:bound}\n\\end{align}\n\\end{subequations}\nHere,  $\\s_{i\\opt_i}=(\\sft{\\X_i\\W \\z_{i}})_{\\opt_i}$, $\\bar{A}=\\max_{i\\in[n],t,\\tau\\in[T]}\\tn{\\x_{it}- \\x_{i\\tau}}~\\tn{\\z_i}$, and $\\Theta=1/\\tf{\\Ws}$.\n\n\n\n\n\n \n\n\n\n\n\n\\end{enumerate}\n\\end{lemma}",
                        "statement_html": "Suppose Assumption A [in <a href=\"https://arxiv.org/pdf/2308.16898#assumption.1\">original paper</a>] holds and let $\\op=(\\op_i)_{i=1}^n$ be the unique globally-optimal indices with $\\Wm$ denoting the (<a href=\"https://arxiv.org/pdf/2308.16898#equation.5.6\">Att-SVM</a>) solution. Define the margin $\\Theta=1/\\tf{\\Ws}$. Let $\\s_{i}=\\sft{\\X_i\\W\\z_i}$. For any $\\mu>0$, there exists a sufficiently large $\\RR_\\mu=\\order{1/\\mu}$ (see $(41) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.C.41\">original paper</a>]$) such that:\n<ol style=\"list-style-type: none;\">\n    <li><b>L1.</b> There is no stationary point within $ \\conb_{\\mu,\\RR_\\mu}(\\Ws)$, where $\\conb_{\\mu,\\RR_\\mu} (\\Ws)$ is defined in (33) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.C.33\">original paper</a>]. </li>\n    <li><b>L2.</b> For all $\\V\\in \\Scc_{\\mu} (\\Ws)$ with $\\tf{\\V}=\\tf{\\Wm}$  and $\\W\\in\\conb_{\\mu,\\RR_\\mu}(\\Wm)$, there exist dataset dependent constants $C,c>0$ such that \n    \\begin{subequations}\\label{zero:g:lbound}\n    \\begin{align}\n    &C\\cdot \\frac{1}{n}\\sum_{i=1}^n \\left(1-\\s_{i\\op_i}\\right) \\geq -\\Big\\langle\\nabla\\Lc(\\W),\\V \\Big\\rangle\\geq c\\cdot \\mu\\cdot  \\frac{1}{n} \\sum_{i=1}^n  \\left(1-\\s_{i\\op_i}\\right)>0, \\label{zero1:g:bound} \\\\\n    & -\\li\\frac{\\V}{\\tf{\\V}},\\frac{\\nabla\\Lc(\\W)}{\\tf{\\nabla\\Lc(\\W)}}\\ri \\geq  \\frac{c}{C} \\cdot \\frac{\\Theta}{\\bar{A}}>0, \\label{zero2:g:bound}\\\\\n    &\\tf{\\nabla\\Lc(\\W)}\\leq \\bar{A}C \\cdot \\frac{1}{n} \\sum_{i=1}^n  \\left(1-\\s_{i\\op_i}\\right). \\label{zero3:g:bound}\n    \\end{align}\n    \\end{subequations}\n    Here,  $\\s_{i\\opt_i}=(\\sft{\\X_i\\W \\z_{i}})_{\\opt_i}$, $\\bar{A}=\\max_{i\\in[n],t,\\tau\\in[T]}\\tn{\\x_{it}- \\x_{i\\tau}}~\\tn{\\z_i}$, and $\\Theta=1/\\tf{\\Ws}$.\n</li>\n</ol>",
                        "statement_type": "lemma",
                        "statement_motivation_html": "This lemma is crucial for understanding the behavior of the optimization landscape in machine learning models, particularly in the context of support vector machines (SVMs). It provides conditions under which no stationary points exist within a certain region, ensuring that the optimization process does not get stuck in suboptimal points. Additionally, it establishes bounds on the gradient, which are essential for analyzing the convergence properties of the optimization algorithm. Use this lemma when you need to guarantee the effectiveness and efficiency of your optimization procedure in high-dimensional spaces.",
                        "html_url": "library/lemmas/lemma_32/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "04261db5-c5b9-466c-874c-7a9bf012120b",
                            "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof} For simplicity let $R=\\RR_\\mu$, $\\W\\in\\conb_{\\mu,R}(\\Ws)$ and \n\\begin{equation}\\label{mu choice2}\n\\begin{split}\n\n&A=\\max_{i\\in[n],t,\\tau\\in[T]} \\frac{(\\tn{\\x_{it}}\\vee\\tn{\\x_{it}-\\x_{i\\tau}})\\cdot\\tn{\\z_i}}{\\Theta}.\n\\end{split}\n\\end{equation}\n\nThe following inequalities hold for all $\\V\\in \\Scc_{\\mu},~\\tf{\\V}=\\tf{\\Wm}$ and all $i\\in[n], t\\neq \\op_i$:\n\\begin{equation}\\label{cone-A-eq}\n\\begin{split}\n\n\nA\\geq(\\x_{i\\op_i}-\\x_{it})^\\top \\V \\z_i &\\geq \\mu.\n\\end{split}\n\\end{equation}\n\n \n\nTo proceed, we write the gradient correlation following \\eqref{grad def} and \\eqref{eqn:grad:prod:p}\n\\begin{align}\\label{grad def32}\n\\li\\nabla\\Lc(\\W),\\V\\ri&=\\frac{1}{n}\\sum_{i=1}^n\\ell'_i\\cdot\\li\\hb_i,\\sfp{\\hp_i}\\bgam_i\\ri,\n\\end{align}\nwhere we denoted $\\ell'_i=\\ell'(Y_i\\cdot \\vb^\\top \\X_i^\\top\\sft{\\hp_i})$, $\\hb_i=\\X_i\\V \\z_{i}$, $\\hp_i= \\X_i\\W \\z_{i}$, $\\s_i=\\sft{\\hp_i}$. \n\nIt follows from \\eqref{mu choice2} that $A\\geq \\max_{i\\in[n],t\\in[T]}\\tn{\\hb_{it}}$. Using \\eqref{cone-A-eq}, we can bound the softmax probabilities $\\s_i=\\sft{\\hp_i}$ as follows, for all $i\\in[n]$:\n\\begin{align}\\label{soft prob bound2}\n&S_i:= \\sum_{\\tau\\neq \\op_i}\\s_{i\\tau}\\leq T e^{-R\\mu\\Theta}\\s_{i\\op_i}\\leq T e^{-R\\mu\\Theta}.\n\\end{align}\n\nRecall scores $\\bgam_{it}=Y_i\\cdot\\vb^\\top \\x_{it}$. Define the score gaps:\n\\begin{equation*}\n \\bgg_i=\\bgam_{i\\op_i}-\\max_{t\\neq\\op_i}\\bgam_{it},~~~ \\bgm_i=\\bgam_{i\\op_i}-\\min_{t\\neq\\op_i}\\bgam_{it},~~~\\textnormal{and}~~~\\Gamma=\\sup_{i\\in[n],t,\\tau\\in[T]}|\\bgam_{it}-\\bgam_{i\\tau}|. \n\\end{equation*}\nLet us focus on a fixed datapoint $i\\in[n]$, assume (without losing generality) $\\op_i=1$, and drop subscripts $i$.\n\nDirectly applying Lemma \\ref{lem:q_reduce}, we obtain\n\\[\n  \\big|\\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam-\\sum_{t\\geq 2}^T (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2\\Gamma A(1-\\s_1)^2.\n\\]\n\\noindent To proceed, let us upper/lower bound the gradient correlation. Since $A\\geq \\hb_1-\\hb_t\\geq \\mu>0$ from \\eqref{cone-A-eq}, setting $S:=\\sum_{t\\neq\\op_i}\\s_t=1-\\s_1$, we find\n\\begin{equation}\n A\\cdot S\\cdot \\bgm  \\geq\\sum_{t\\neq\\op} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\geq \\mu\\cdot S\\cdot \\bgg.\\label{aggregate2}\n\\end{equation}\nNext we show that $S=1-\\s_1$ dominates $(1-\\s_1)^2=S^2$ for large $R$. Specifically, we wish for \n\\begin{align}\\label{wishfor2}\n\\mu S \\bgg/2\\geq 2\\Gamma A(1-\\s_1)^2\\iff S\\geq \\frac{4}{\\mu}\\frac{\\Gamma A}{\\bgg}S^2\\iff S\\leq \\frac{\\mu\\bgg}{4\\Gamma A}.\n\\end{align}\nUsing \\eqref{soft prob bound2}, what we wish is ensured for all $i\\in[n]$, by guaranteeing $Te^{-R\\mu\\Theta}\\leq \\frac{\\mu\\bgg}{4\\Gamma A}$. That is, by choosing\n\\begin{align}\\label{R bound2}\nR\\geq \\frac{1}{\\mu\\Theta}\\log\\left(\\frac{4T\\Gamma A}{\\mu\\bggm}\\right),\n\\end{align}\nwhere $\\bggm=\\min_{i\\in[n]}\\bgg_i$ is the global scalar corresponding to the worst case score gap over all inputs. \n\nWith the above choice of $R$, we guaranteed\n\\[\n  2 A(1-\\s_1)\\cdot \\bgm\\geq 2A\\cdot S\\cdot \\bgm \\geq \\sum_{t\\neq\\op} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\geq\\frac{\\mu\\cdot S\\cdot \\bgg}{2}\\geq\\frac{\\mu(1-\\s_1) \\bgg}{2},\n\\]\nvia \\eqref{wishfor2} and \\eqref{aggregate2}. \n\n\nSince this holds over all inputs, going back to the gradient correlation \\eqref{grad def32} and averaging above over all inputs $i\\in[n]$ and plugging back the indices $i$, we obtain the advertised bound \n\\begin{align}\\label{pbb corr2}\n  \n  \\frac{2A}{n}\\sum_{i\\in [n]} -\\ell'_i\\cdot S_i\\cdot \\bgm_i\\geq -\\li\\nabla\\Lc(\\W),\\V\\ri\\geq \\frac{\\mu}{2n}\\sum_{i\\in [n]} -\\ell'_i\\cdot S_i\\cdot \\bgg_i.\n\\end{align}\nLet $-\\ell'_{\\min/\\max}$ be the min/max values negative loss derivative admits over the ball $[-A,A]$ and note that $\\max_{i\\in[n]}\\bgm_i>0$ and $\\min_{i\\in[n]}\\bgg_i>0$ are dataset dependent constants. Then, we declare the constants $C=-2A\\ell'_{\\max}\\cdot \\max_{i\\in[n]}\\bgm_i>0,c=-(1/2)\\ell'_{\\min}\\cdot \\min_{i\\in[n]}\\bgg_i>0$ to obtain the bound \\eqref{zero1:g:bound}. \n\nThe proof of  \\eqref{zero2:g:bound} and \\eqref{zero3:g:bound} follows similarly as the proof of Lemma \\ref{local cond}. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\end{proof}",
                            "statement_html": "For simplicity let $R=\\RR_\\mu$, $\\W\\in\\conb_{\\mu,R}(\\Ws)$ and \n\\begin{equation}\\label{mu choice2}\n\\begin{split}\n&A=\\max_{i\\in[n],t,\\tau\\in[T]} \\frac{(\\tn{\\x_{it}}\\vee\\tn{\\x_{it}-\\x_{i\\tau}})\\cdot\\tn{\\z_i}}{\\Theta}.\n\\end{split}\n\\end{equation}\n\nThe following inequalities hold for all $\\V\\in \\Scc_{\\mu},~\\tf{\\V}=\\tf{\\Wm}$ and all $i\\in[n], t\\neq \\op_i$:\n\\begin{equation}\\label{cone-A-eq}\n\\begin{split}\nA\\geq(\\x_{i\\op_i}-\\x_{it})^\\top \\V \\z_i &\\geq \\mu.\n\\end{split}\n\\end{equation}\n\nTo proceed, we write the gradient correlation following (15) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.B.15\">original paper</a>] and (15) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.C.26\">original paper</a>]\n\\begin{align}\\label{grad def32}\n\\li\\nabla\\Lc(\\W),\\V\\ri&=\\frac{1}{n}\\sum_{i=1}^n\\ell'_i\\cdot\\li\\hb_i,\\sfp{\\hp_i}\\bgam_i\\ri,\n\\end{align}\nwhere we denoted $\\ell'_i=\\ell'(Y_i\\cdot \\vb^\\top \\X_i^\\top\\sft{\\hp_i})$, $\\hb_i=\\X_i\\V \\z_{i}$, $\\hp_i= \\X_i\\W \\z_{i}$, $\\s_i=\\sft{\\hp_i}$. \n\nIt follows from \\eqref{mu choice2} that $A\\geq \\max_{i\\in[n],t\\in[T]}\\tn{\\hb_{it}}$. Using \\eqref{cone-A-eq}, we can bound the softmax probabilities $\\s_i=\\sft{\\hp_i}$ as follows, for all $i\\in[n]$:\n\\begin{align}\\label{soft prob bound2}\n&S_i:= \\sum_{\\tau\\neq \\op_i}\\s_{i\\tau}\\leq T e^{-R\\mu\\Theta}\\s_{i\\op_i}\\leq T e^{-R\\mu\\Theta}.\n\\end{align}\n\nRecall scores $\\bgam_{it}=Y_i\\cdot\\vb^\\top \\x_{it}$. Define the score gaps:\n\\begin{equation*}\n \\bgg_i=\\bgam_{i\\op_i}-\\max_{t\\neq\\op_i}\\bgam_{it},~~~ \\bgm_i=\\bgam_{i\\op_i}-\\min_{t\\neq\\op_i}\\bgam_{it},~~~\\textnormal{and}~~~\\Gamma=\\sup_{i\\in[n],t,\\tau\\in[T]}|\\bgam_{it}-\\bgam_{i\\tau}|. \n\\end{equation*}\nLet us focus on a fixed datapoint $i\\in[n]$, assume (without losing generality) $\\op_i=1$, and drop subscripts $i$.\n\nDirectly applying <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_29/index.html#lem%3Aq_reduce\">Lemma 29</a>, we obtain\n\\[\n  \\big|\\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam-\\sum_{t\\geq 2}^T (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2\\Gamma A(1-\\s_1)^2.\n\\]\nTo proceed, let us upper/lower bound the gradient correlation. Since $A\\geq \\hb_1-\\hb_t\\geq \\mu>0$ from \\eqref{cone-A-eq}, setting $S:=\\sum_{t\\neq\\op_i}\\s_t=1-\\s_1$, we find\n\\begin{equation}\n A\\cdot S\\cdot \\bgm  \\geq\\sum_{t\\neq\\op} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\geq \\mu\\cdot S\\cdot \\bgg.\\label{aggregate2}\n\\end{equation}\nNext we show that $S=1-\\s_1$ dominates $(1-\\s_1)^2=S^2$ for large $R$. Specifically, we wish for \n\\begin{align}\\label{wishfor2}\n\\mu S \\bgg/2\\geq 2\\Gamma A(1-\\s_1)^2\\iff S\\geq \\frac{4}{\\mu}\\frac{\\Gamma A}{\\bgg}S^2\\iff S\\leq \\frac{\\mu\\bgg}{4\\Gamma A}.\n\\end{align}\nUsing (38) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.C.38\">original paper</a>], what we wish is ensured for all $i\\in[n]$, by guaranteeing $Te^{-R\\mu\\Theta}\\leq \\frac{\\mu\\bgg}{4\\Gamma A}$. That is, by choosing\n\\begin{align}\\label{R bound2}\nR\\geq \\frac{1}{\\mu\\Theta}\\log\\left(\\frac{4T\\Gamma A}{\\mu\\bggm}\\right),\n\\end{align}\nwhere $\\bggm=\\min_{i\\in[n]}\\bgg_i$ is the global scalar corresponding to the worst case score gap over all inputs. \n\nWith the above choice of $R$, we guaranteed\n\\[\n  2 A(1-\\s_1)\\cdot \\bgm\\geq 2A\\cdot S\\cdot \\bgm \\geq \\sum_{t\\neq\\op} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\geq\\frac{\\mu\\cdot S\\cdot \\bgg}{2}\\geq\\frac{\\mu(1-\\s_1) \\bgg}{2},\n\\]\nvia (40) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.C.40\">original paper</a>] and \\eqref{aggregate2}. \n\nSince this holds over all inputs, going back to the gradient correlation \\eqref{grad def32} and averaging above over all inputs $i\\in[n]$ and plugging back the indices $i$, we obtain the advertised bound \n\\begin{align}\\label{pbb corr2}\n  \\frac{2A}{n}\\sum_{i\\in [n]} -\\ell'_i\\cdot S_i\\cdot \\bgm_i\\geq -\\li\\nabla\\Lc(\\W),\\V\\ri\\geq \\frac{\\mu}{2n}\\sum_{i\\in [n]} -\\ell'_i\\cdot S_i\\cdot \\bgg_i.\n\\end{align}\nLet $-\\ell'_{\\min/\\max}$ be the min/max values negative loss derivative admits over the ball $[-A,A]$ and note that $\\max_{i\\in[n]}\\bgm_i>0$ and $\\min_{i\\in[n]}\\bgg_i>0$ are dataset dependent constants. Then, we declare the constants $C=-2A\\ell'_{\\max}\\cdot \\max_{i\\in[n]}\\bgm_i>0,c=-(1/2)\\ell'_{\\min}\\cdot \\min_{i\\in[n]}\\bgg_i>0$ to obtain the bound \\eqref{zero1:g:bound}. \n\nThe proof of  \\eqref{zero2:g:bound} and \\eqref{zero3:g:bound} follows similarly as the proof of <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_34/index.html#local+cond\">Lemma 34</a>.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n<br>\n<br>1. <i></i>Definition of Constants<i></i>: The proof starts by defining constants and variables for simplicity. Specifically, \\( R = \\RR_\\mu \\) and \\( \\W \\in \\conb_{\\mu,R}(\\Ws) \\). The constant \\( A \\) is defined as:\n<br>   \\[\n   A = \\max_{i \\in [n], t, \\tau \\in [T]} \\frac{(\\tn{\\x_{it}} \\vee \\tn{\\x_{it} - \\x_{i\\tau}}) \\cdot \\tn{\\z_i}}{\\Theta}.\n   \\]\n<br>\n<br>2. <i></i>Inequalities for \\(\\V\\)<i></i>: The proof establishes inequalities for all \\(\\V \\in \\Scc_{\\mu}\\) with \\(\\tf{\\V} = \\tf{\\Wm}\\) and for all \\(i \\in [n], t \\neq \\op_i\\):\n<br>   \\[\n   A \\geq (\\x_{i\\op_i} - \\x_{it})^\\top \\V \\z_i \\geq \\mu.\n   \\]\n<br>\n<br>3. <i></i>Gradient Correlation<i></i>: The gradient correlation is written using the definitions from previous equations:\n<br>   \\[\n   \\li \\nabla \\Lc(\\W), \\V \\ri = \\frac{1}{n} \\sum_{i=1}^n \\ell'_i \\cdot \\li \\hb_i, \\sfp{\\hp_i} \\bgam_i \\ri,\n   \\]\n<br>   where \\(\\ell'_i = \\ell'(Y_i \\cdot \\vb^\\top \\X_i^\\top \\sft{\\hp_i})\\), \\(\\hb_i = \\X_i \\V \\z_i\\), \\(\\hp_i = \\X_i \\W \\z_i\\), and \\(\\s_i = \\sft{\\hp_i}\\).\n<br>\n<br>4. <i></i>Bounding Softmax Probabilities<i></i>: Using the earlier inequalities, the softmax probabilities \\(\\s_i = \\sft{\\hp_i}\\) are bounded:\n<br>   \\[\n   S_i := \\sum_{\\tau \\neq \\op_i} \\s_{i\\tau} \\leq T e^{-R\\mu\\Theta} \\s_{i\\op_i} \\leq T e^{-R\\mu\\Theta}.\n   \\]\n<br>\n<br>5. <i></i>Score Gaps<i></i>: The proof defines score gaps and a global scalar for the worst-case score gap:\n<br>   \\[\n   \\bgg_i = \\bgam_{i\\op_i} - \\max_{t \\neq \\op_i} \\bgam_{it}, \\quad \\bgm_i = \\bgam_{i\\op_i} - \\min_{t \\neq \\op_i} \\bgam_{it}, \\quad \\Gamma = \\sup_{i \\in [n], t, \\tau \\in [T]} |\\bgam_{it} - \\bgam_{i\\tau}|.\n   \\]\n<br>\n<br>6. <i></i>Application of Lemma<i></i>: By applying Lemma 29, the proof derives an inequality involving \\(\\hb\\), \\(\\s\\), and \\(\\bgam\\):\n<br>   \\[\n   \\big|\\hb^\\top \\diag{\\s} \\bgam - \\hb^\\top \\s \\s^\\top \\bgam - \\sum_{t \\geq 2}^T (\\hb_1 - \\hb_t) \\s_t (\\bgam_1 - \\bgam_t)\\big| \\leq 2\\Gamma A(1 - \\s_1)^2.\n   \\]\n<br>\n<br>7. <i></i>Bounding Gradient Correlation<i></i>: The proof bounds the gradient correlation by setting \\( S = 1 - \\s_1 \\):\n<br>   \\[\n   A \\cdot S \\cdot \\bgm \\geq \\sum_{t \\neq \\op} (\\hb_1 - \\hb_t) \\s_t (\\bgam_1 - \\bgam_t) \\geq \\mu \\cdot S \\cdot \\bgg.\n   \\]\n<br>\n<br>8. <i></i>Ensuring Dominance<i></i>: The proof ensures that \\( S = 1 - \\s_1 \\) dominates \\( (1 - \\s_1)^2 = S^2 \\) for large \\( R \\):\n<br>   \\[\n   S \\leq \\frac{\\mu \\bgg}{4 \\Gamma A}.\n   \\]\n<br>   This is guaranteed by choosing:\n<br>   \\[\n   R \\geq \\frac{1}{\\mu \\Theta} \\log\\left(\\frac{4T \\Gamma A}{\\mu \\bggm}\\right),\n   \\]\n<br>   where \\(\\bggm = \\min_{i \\in [n]} \\bgg_i\\).\n<br>\n<br>9. <i></i>Final Bound<i></i>: With the chosen \\( R \\), the proof guarantees:\n<br>   \\[\n   2 A (1 - \\s_1) \\cdot \\bgm \\geq \\sum_{t \\neq \\op} (\\hb_1 - \\hb_t) \\s_t (\\bgam_1 - \\bgam_t) \\geq \\frac{\\mu (1 - \\s_1) \\bgg}{2}.\n   \\]\n<br>   Averaging over all inputs \\( i \\in [n] \\) and plugging back the indices, the proof obtains the bound:\n<br>   \\[\n   \\frac{2A}{n} \\sum_{i \\in [n]} -\\ell'_i \\cdot S_i \\cdot \\bgm_i \\geq -\\li \\nabla \\Lc(\\W), \\V \\ri \\geq \\frac{\\mu}{2n} \\sum_{i \\in [n]} -\\ell'_i \\cdot S_i \\cdot \\bgg_i.\n   \\]\n<br>\n<br>10. <i></i>Constants Declaration<i></i>: Finally, the proof declares constants \\( C \\) and \\( c \\) to obtain the bound:\n<br>   \\[\n   C = -2A \\ell'_{\\max} \\cdot \\max_{i \\in [n]} \\bgm_i > 0, \\quad c = -\\frac{1}{2} \\ell'_{\\min} \\cdot \\min_{i \\in [n]} \\bgg_i > 0.\n   \\]\n<br>   This leads to the bound \\eqref{zero1:g:bound}.\n<br>\n<br>The proof of \\eqref{zero2:g:bound} and \\eqref{zero3:g:bound} follows similarly as the proof of Lemma 34."
                        }
                    },
                    {
                        "statement_id": "694f89ee-59e8-458c-881a-23ca7d9a82a8",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 33,
                        "library_name": "Lemma 33",
                        "title": "Cone Gradient Inequality Lemma",
                        "statement_original_tex": "\\begin{lemma}\n\\label{lem:glocal:corr} \nSuppose Assumption~\\ref{assum:loss:prope} holds and let $\\op=(\\op_i)_{i=1}^n$ be the unique optimal tokens with $ \\Wm$ denoting the SVM solution. Fix any $\\mu>0$ (per Lemma \\ref{glocal cond}). For any choice of $\\pi>0$, there exists $R:=R_{\\pi} \\geq \\bar{R}_\\mu$ such that, for any $ \\W\\in \\conb_{\\mu,R}(\\Wm)$, we have\n\\[\n \\li \\nabla\\Lc(\\W), \\frac{\\W}{\\tf{\\W}} \\ri\\geq (1+\\pi)\\li \\nabla\\Lc(\\W), \\frac{\\Wm}{\\tf{\\Wm}}\\ri.\n\\]\nHere, $\\conb_{\\mu,R}(\\Wm)$ is the cone defined at  \\eqref{eqn:con:nabla0}.\n\\end{lemma}",
                        "statement_html": "Suppose Assumption A [in <a href=\"https://arxiv.org/pdf/2308.16898#assumption.1\">original paper</a>] holds and let $\\op=(\\op_i)_{i=1}^n$ be the unique optimal tokens with $\\Wm$ denoting the SVM solution. Fix any $\\mu>0$ (per <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_36/index.html#glocal+cond\">Lemma 36</a>). For any choice of $\\pi>0$, there exists $R:=R_{\\pi} \\geq \\bar{R}_\\mu$ such that, for any $\\W\\in \\conb_{\\mu,R}(\\Wm)$, we have\n\\[\n \\li \\nabla\\Lc(\\W), \\frac{\\W}{\\tf{\\W}} \\ri\\geq (1+\\pi)\\li \\nabla\\Lc(\\W), \\frac{\\Wm}{\\tf{\\Wm}}\\ri.\n\\]\nHere, $\\conb_{\\mu,R}(\\Wm)$ is the cone defined at (33) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.C.33\">original paper</a>].",
                        "statement_type": "lemma",
                        "statement_motivation_html": "This result is useful in the context of Support Vector Machines (SVM) optimization. It provides a condition under which the gradient of the loss function, when projected onto the direction of the current weight vector $\\W$, is greater than its projection onto the direction of the optimal weight vector $\\Wm$. This can be particularly helpful in ensuring that the optimization process is making sufficient progress towards the optimal solution, especially when working within the defined cone $\\conb_{\\mu,R}(\\Wm)$.",
                        "html_url": "library/lemmas/lemma_33/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "56368fd6-dcfe-471d-a49e-1400d4fa9777",
                            "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\n \nLet  $\\Wb= \\tf{\\Wm} \\W/\\tf{\\W}$, $\\hb_i=\\X_i\\Wb \\z_{i}$, $\\hbm_i= \\X_i\\Ws \\z_{i}$, and $\\s_i=\\sft{\\X_i\\W \\z_{i}}$. To establish the result, we will prove that, for sufficiently large $R$ and for any $\\W\\in \\conb_{\\mu,R}(\\Wm)$:\n\\begin{align}\\label{main local cond2}\n\\nonumber \n\\li -\\nabla\\Lc(\\W),\\frac{\\W}{\\tf{\\W}}\\ri&= -\\frac{1}{n}\\sum_{i=1}^n\\ell'_i \\cdot  \\li \\hb_i, \\sfp{\\X_i\\W \\z_{i}}\\bgam_i\\ri\\\\\n&\\leq - \\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot  \\li\\hbm_i, \\sfp{\\X_i\\W \\z_{i}}\\bgam_i\\ri=(1+\\pi)\\li-\\nabla\\Lc(\\W), \\frac{\\ps}{\\tf{\\Ws}}\\ri.\n\\end{align}\nDirectly applying Lemma \\ref{lem:q_reduce}, for all $\\V\\in \\Scc_\\mu$ with $\\tf{\\V}=\\tf{\\Wm}$ and $\\hp_i=\\X_i\\V \\z_i$, we have found\n\\begin{align}\n  \\big|\\hp^\\top_i\\diag{\\s_i}\\bgam_i-\\hp^\\top_i\\s_i\\s^\\top_i\\bgam_i-\\sum_{t\\neq\\op_i} (\\hp_{i1}-\\hp_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\big|\\leq 2\\Gamma A(1-\\s_{i1})^2.\n\\end{align}\nRecalling $\\hbm_{i1}-\\hbm_{it}\\geq 1$, we note that $\\sum_{t\\neq\\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq \\sum_{t\\neq\\op_i} (\\hbm_{i1}-\\hbm_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})$. Now plugging in $\\hb,\\hbm$ in the bound above and assuming $\\pi\\leq 1$ (w.l.o.g.), \\eqref{main local cond2} is implied by the following stronger inequality\n\\begin{align*}\n-\\frac{1}{n}&\\sum_{i=1}^n\\ell'_i \\cdot \\left(6\\Gamma A(1-\\s_{i1})^2+ \\sum_{t\\neq \\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it}) \\right)\\\\\n&\\leq -\\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i  \\cdot \\sum_{t\\neq \\op_i} (\\hbm_{i1}-\\hbm_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n&\\leq-\\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\neq \\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align*}\nFirst, we claim that $0.5\\pi\\sum_{t\\in \\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\geq 6\\Gamma A(1-\\s_{i1})^2$ for all $i \\in [n]$.  \nThe proof of this claim directly follows the argument in Lemma~\\ref{glocal cond}, \n(namely following \\eqref{soft prob bound2}, \\eqref{wishfor2}, \\eqref{R bound2}) \nwe have that $1-\\s_{i1}\\leq Te^{-R\\mu\\Theta}$ and $\\bgam_{i1}-\\bgam_{it}\\geq \\bggm$ for all $i \\in [n]$. This leads to the choice (for $D_0\\geq 12$)\n\\begin{align}\n  R\\geq R_\\pi =\\frac{1}{\\mu\\Theta}\\log\\left(\\frac{D_{0}\\cdot T\\Gamma A}{\\pi\\bggm}\\right).\\label{Rpi choice2}\n\\end{align}\nWe shall choose $D_0$ sufficiently large such that $R_{\\pi}\\geq \\bar{R}_{\\mu}$, where $\\bar{R}_{\\mu}$ is defined in Lemma \\ref{glocal cond}.\n\nFollowing this control over the perturbation term $6\\Gamma A(1-\\s_{i1})^2$, to conclude with the result, what remains is proving the comparison\n\\begin{align}\\label{desired comp2}\n-\\frac{1}{n} \\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\neq\\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq - \\frac{1+0.5\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\neq\\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align}\n\\noindent\\textbf{Scenario 1:} $\\tf{\\Wb-\\Wm}\\leq \\eps=\\frac{\\pi}{4A\\Theta}$ for some $\\eps>0$.  In this scenario, for any $ t\\neq \\op_i$ and $i\\in [n ]$, we have\n\\[\n|\\hb_{it}-\\hbm_{it}|=|\\x_{it}^\\top (\\Wb-\\Wm)  \\z_{i}|\\leq A\\Theta\\eps=\\frac{\\pi}{4}.\n\\]\nConsequently, we obtain \n\\[\n\\hb_{i1}-\\hb_{it}\\leq \\hbm_{i1}-\\hbm_{it}+2A\\Theta\\eps= 1+0.5\\pi.\n\\] \nSimilarly, $\\hb_{i1}-\\hb_{it}\\geq 1-0.5\\pi\\geq 0.5$. Since all terms $\\hb_{i1}-\\hb_{it},\\s_{it},\\bgam_{i1}-\\bgam_{it}$ in \\eqref{desired comp2} are nonnegative, we  obtain \\eqref{desired comp2}. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\noindent\\textbf{Scenario 2:} $\\tf{\\Wb-\\Wm}\\geq \\eps=\\frac{\\pi}{4A\\Theta}$.  Since $\\Wb$ is not max-margin solution, in this scenario, for some $i \\in  [n]$, $\\nu=\\nu(\\eps)>0$, and $\\tau\\neq\\op_i$, we have that\n\\begin{align*}\n\\hb_{i1}-\\hb_{i\\tau}\\leq 1-2\\nu.\n\\end{align*}\nHere $\\tau=\\arg\\max_{\\tau\\neq\\op_i} \\x_{i\\tau}\\Wb \\z_i$ denotes the nearest point to $\\hb_{i1}$ (along the $\\Wb$ direction). Recall that $\\s=\\sft{  \\RR\\hb}$, where  $\\RR=R\\Theta=\\tf{\\W}/\\tf{\\Wm}$. To proceed, let $ \\underline{\\hb}_i:=\\min_{t \\neq\\op_i}\\hb_{i1}-\\hb_{it}$,\n\\begin{align*}\n\\mc{I}:=\\left\\{ i\\in[n]: \\underline{\\hb}_i \\leq 1-2\\nu \\right\\}, \\qquad [n]-\\mc{I}:=\\left\\{ i\\in[n]:  1-2\\nu  <  \\underline{\\hb}_i\\right\\}.\n\n\n\\end{align*}\nFor all $ i \\in [n]-\\mc{I}$,\n\\begin{equation}\\label{eqn:grad:difff0}\n\\begin{split}\n      \\sum_{t\\neq \\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it}) &- (1+0.5\\pi) \\sum_{t\\neq \\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n      & \\leq  \\left(2A - (1+0.5\\pi)\\right)\\Gamma\\sum_{t\\neq \\op_i,~\\hb_{i1}-\\hb_{it} \\geq 1+\\frac{\\pi}{2} } \\s_{it} \\\\\n      & \\leq  \\left(2A - (1+0.5\\pi)\\right)\\Gamma Te^{-\\RR(1+\\frac{\\pi}{2})} \\\\\n      &\\leq   2A\\Gamma  T e^{-\\RR(1+\\frac{\\pi}{2})}.\n      \n      \n      \n\\end{split}\n\\end{equation}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor all $ i \\in \\mc{I}$, split the tokens into two groups: Let $\\Nc_i$ be the group of tokens obeying $ \\hb_{i1}-\\hb_{it} \\leq 1-\\nu$ and $\\bar\\Nc_i:=[T]-\\{\\op_i\\}-\\Nc_i$ be the rest of the neighbors. Observe that\n\\[\n\\frac{\\sum_{t\\in\\bar{\\Nc}_i}\\s_{it}}{\\sum_{t\\neq\\op_i}\\s_{it}}\\leq  T\\frac{e^{\\nu \\RR}}{e^{2\\nu\\RR}}=Te^{-\\RR\\nu}.\n\\]\nUsing $|\\hb_{i1}-\\hb_{it}|\\leq 2A$ and  $\\bggm=\\min_{i\\in[n]}\\bgg_i =\\min_{i\\in[n]} (\\bgam_{i1}-\\max_{t\\neq\\op_i}\\bgam_{it})$, observe that \n\\[\n\\sum_{t\\in\\bar\\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq \\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm} \\sum_{t\\neq\\opt_i} \\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\]\nThus, \n\\begin{align*}\n  \\sum_{t\\neq \\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})&= \\sum_{t\\in \\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})+\\sum_{t\\in\\bar\\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\nonumber\\\\\n\n  &\\leq \\sum_{t\\in \\Nc_i} (1-\\nu)\\s_{it}(\\bgam_{i1}-\\bgam_{it})+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm} \\sum_{t\\neq \\op_i} \\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n  &\\leq \\left(1-\\nu+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}\\right)\\sum_{t\\neq \\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n &\\leq \\left(1+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}\\right)\\sum_{t\\neq \\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align*}\nHence, choosing \n\\begin{align}\nR\\geq\\frac{1}{\\nu\\Theta}\\log\\left(\\frac{8\\Gamma AT}{\\bggm\\pi}\\right)\\label{R bound pi}\n\\end{align}\nresults in that\n\\begin{equation}\\label{eqn:grad:difff1}\n    \\begin{split}\n     &\\sum_{t\\neq\\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})  - \\left(1+\\frac{\\pi}{2}\\right) \\sum_{t\\neq\\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}) \\\\\n   &\\leq\\left(\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}-\\frac{\\pi}{2}\\right)\\sum_{t\\neq\\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n   &\\leq -\\frac{\\pi}{4}\\sum_{t\\neq\\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n   &\\leq-\\frac{\\pi}{4T}\\bggm  e^{-\\bar{R} (1-2\\nu)}.      \n    \\end{split}\n\\end{equation}\nHere, the last inequality follows from the fact that $\\sum_{t\\neq\\op_i}\\s_{it}\\geq \\max_{t\\neq\\op_i}\\s_{it}\\geq\\frac{e^{-\\bar{R}(1-2\\nu)}}{\\sum_{t=1}^Te^{-\\bar{R}(\\hb_{i1}-\\hb_{it})}}\\geq e^{-\\bar{R}(1-2\\nu)}/T$.\n\n\nFrom Assumption~\\ref{assum:loss:prope}, we have $c_{\\min}\\leq-\\ell'\\leq c_{\\max}$ for some positive constants $c_{\\min}$ and $c_{\\max}$. It follows from  \\eqref{eqn:grad:difff0} and \\eqref{eqn:grad:difff1} that \n\\begin{align*}\n-\\frac{1}{n}\\sum_{i}^n \\ell_i' \\cdot&\\left(\n      \\sum_{t\\neq\\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})- \\sum_{t\\neq\\op_i} (1+0.5\\pi)\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\right)\\\\\n      & \\leq    c_{\\max}2A\\Gamma  T \\Gamma e^{-\\RR(1 +\\frac{\\pi}{2})}-\\frac{c_{\\min}}{nT}\\cdot\\frac{\\pi\\bggm}{4}e^{-\\bar{R} (1-2\\nu)}\\\\\n      \n      & \\leq 0.\n\\end{align*}\nCombing with \\eqref{R bound pi}, this is guaranteed by \nchoosing \n\n\n\n\\[\n  R\\geq \\max\\left\\{\\frac{1}{\\nu\\Theta}\\log\\left(\\frac{8\\Gamma AT}{\\bggm\\pi}\\right),\\frac{1}{(2\\nu+\\pi/2)\\Theta}\\log\\left(\\frac{8n\\Gamma AT^2 c_{\\max}}{c_{\\min}\\bggm\\pi}\\right)\\right\\},\n\\]\nwhere $\\nu=\\nu(\\frac{\\pi}{4A\\Theta})$ depends only on $\\pi$ and global problem variables. \n\nCombining this with the prior $R$ choice \\eqref{Rpi choice2} (by taking maximum), we conclude with the statement.\n\\end{proof}",
                            "statement_html": "Let $\\Wb= \\tf{\\Wm} \\W/\\tf{\\W}$, $\\hb_i=\\X_i\\Wb \\z_{i}$, $\\hbm_i= \\X_i\\Ws \\z_{i}$, and $\\s_i=\\sft{\\X_i\\W \\z_{i}}$. To establish the result, we will prove that, for sufficiently large $R$ and for any $\\W\\in \\conb_{\\mu,R}(\\Wm)$:\n\\begin{align}\\label{main local cond2}\n\\nonumber \n\\li -\\nabla\\Lc(\\W),\\frac{\\W}{\\tf{\\W}}\\ri&= -\\frac{1}{n}\\sum_{i=1}^n\\ell'_i \\cdot  \\li \\hb_i, \\sfp{\\X_i\\W \\z_{i}}\\bgam_i\\ri\\\\\n&\\leq - \\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot  \\li\\hbm_i, \\sfp{\\X_i\\W \\z_{i}}\\bgam_i\\ri=(1+\\pi)\\li-\\nabla\\Lc(\\W), \\frac{\\ps}{\\tf{\\Ws}}\\ri.\n\\end{align}\nDirectly applying <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_29/index.html#lem%3Aq_reduce\">Lemma 29</a>, for all $\\V\\in \\Scc_\\mu$ with $\\tf{\\V}=\\tf{\\Wm}$ and $\\hp_i=\\X_i\\V \\z_i$, we have found\n\\begin{align}\n  \\big|\\hp^\\top_i\\diag{\\s_i}\\bgam_i-\\hp^\\top_i\\s_i\\s^\\top_i\\bgam_i-\\sum_{t\\neq\\op_i} (\\hp_{i1}-\\hp_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\big|\\leq 2\\Gamma A(1-\\s_{i1})^2.\n\\end{align}\nRecalling $\\hbm_{i1}-\\hbm_{it}\\geq 1$, we note that $\\sum_{t\\neq\\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq \\sum_{t\\neq\\op_i} (\\hbm_{i1}-\\hbm_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})$. Now plugging in $\\hb,\\hbm$ in the bound above and assuming $\\pi\\leq 1$ (w.l.o.g.), (43) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.C.43\">original paper</a>] is implied by the following stronger inequality\n\\begin{align*}\n-\\frac{1}{n}&\\sum_{i=1}^n\\ell'_i \\cdot \\left(6\\Gamma A(1-\\s_{i1})^2+ \\sum_{t\\neq \\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it}) \\right)\\\\\n&\\leq -\\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i  \\cdot \\sum_{t\\neq \\op_i} (\\hbm_{i1}-\\hbm_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n&\\leq-\\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\neq \\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align*}\nFirst, we claim that $0.5\\pi\\sum_{t\\in \\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\geq 6\\Gamma A(1-\\s_{i1})^2$ for all $i \\in [n]$.  \nThe proof of this claim directly follows the argument in <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_36/index.html#glocal+cond\">Lemma 36</a>, \n(namely following (38) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.C.38\">original paper</a>], (40) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.C.40\">original paper</a>], (41) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.C.41\">original paper</a>]) \nwe have that $1-\\s_{i1}\\leq Te^{-R\\mu\\Theta}$ and $\\bgam_{i1}-\\bgam_{it}\\geq \\bggm$ for all $i \\in [n]$. This leads to the choice (for $D_0\\geq 12$)\n\\begin{align}\n  R\\geq R_\\pi =\\frac{1}{\\mu\\Theta}\\log\\left(\\frac{D_{0}\\cdot T\\Gamma A}{\\pi\\bggm}\\right).\\label{Rpi choice2}\n\\end{align}\nWe shall choose $D_0$ sufficiently large such that $R_{\\pi}\\geq \\bar{R}_{\\mu}$, where $\\bar{R}_{\\mu}$ is defined in <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_36/index.html#glocal+cond\">Lemma 36</a>.\n\nFollowing this control over the perturbation term $6\\Gamma A(1-\\s_{i1})^2$, to conclude with the result, what remains is proving the comparison\n\\begin{align}\\label{desired comp2}\n-\\frac{1}{n} \\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\neq\\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq - \\frac{1+0.5\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\neq\\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align}\n<b>Scenario 1:</b> $\\tf{\\Wb-\\Wm}\\leq \\eps=\\frac{\\pi}{4A\\Theta}$ for some $\\eps>0$.  In this scenario, for any $ t\\neq \\op_i$ and $i\\in [n ]$, we have\n\\[\n|\\hb_{it}-\\hbm_{it}|=|\\x_{it}^\\top (\\Wb-\\Wm)  \\z_{i}|\\leq A\\Theta\\eps=\\frac{\\pi}{4}.\n\\]\nConsequently, we obtain \n\\[\n\\hb_{i1}-\\hb_{it}\\leq \\hbm_{i1}-\\hbm_{it}+2A\\Theta\\eps= 1+0.5\\pi.\n\\] \nSimilarly, $\\hb_{i1}-\\hb_{it}\\geq 1-0.5\\pi\\geq 0.5$. Since all terms $\\hb_{i1}-\\hb_{it},\\s_{it},\\bgam_{i1}-\\bgam_{it}$ in \\eqref{desired comp2} are nonnegative, we  obtain \\eqref{desired comp2}. \n\n<b>Scenario 2:</b> $\\tf{\\Wb-\\Wm}\\geq \\eps=\\frac{\\pi}{4A\\Theta}$.  Since $\\Wb$ is not max-margin solution, in this scenario, for some $i \\in  [n]$, $\\nu=\\nu(\\eps)>0$, and $\\tau\\neq\\op_i$, we have that\n\\begin{align*}\n\\hb_{i1}-\\hb_{i\\tau}\\leq 1-2\\nu.\n\\end{align*}\nHere $\\tau=\\arg\\max_{\\tau\\neq\\op_i} \\x_{i\\tau}\\Wb \\z_i$ denotes the nearest point to $\\hb_{i1}$ (along the $\\Wb$ direction). Recall that $\\s=\\sft{  \\RR\\hb}$, where  $\\RR=R\\Theta=\\tf{\\W}/\\tf{\\Wm}$. To proceed, let $ \\underline{\\hb}_i:=\\min_{t \\neq\\op_i}\\hb_{i1}-\\hb_{it}$,\n\\begin{align*}\n\\mc{I}:=\\left\\{ i\\in[n]: \\underline{\\hb}_i \\leq 1-2\\nu \\right\\}, \\qquad [n]-\\mc{I}:=\\left\\{ i\\in[n]:  1-2\\nu  <  \\underline{\\hb}_i\\right\\}.\n\\end{align*}\nFor all $ i \\in [n]-\\mc{I}$,\n\\begin{equation}\\label{eqn:grad:difff0}\n\\begin{split}\n      \\sum_{t\\neq \\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it}) &- (1+0.5\\pi) \\sum_{t\\neq \\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n      & \\leq  \\left(2A - (1+0.5\\pi)\\right)\\Gamma\\sum_{t\\neq \\op_i,~\\hb_{i1}-\\hb_{it} \\geq 1+\\frac{\\pi}{2} } \\s_{it} \\\\\n      & \\leq  \\left(2A - (1+0.5\\pi)\\right)\\Gamma Te^{-\\RR(1+\\frac{\\pi}{2})} \\\\\n      &\\leq   2A\\Gamma  T e^{-\\RR(1+\\frac{\\pi}{2})}.\n\\end{split}\n\\end{equation}\n\nFor all $ i \\in \\mc{I}$, split the tokens into two groups: Let $\\Nc_i$ be the group of tokens obeying $ \\hb_{i1}-\\hb_{it} \\leq 1-\\nu$ and $\\bar\\Nc_i:=[T]-\\{\\op_i\\}-\\Nc_i$ be the rest of the neighbors. Observe that\n\\[\n\\frac{\\sum_{t\\in\\bar{\\Nc}_i}\\s_{it}}{\\sum_{t\\neq\\op_i}\\s_{it}}\\leq  T\\frac{e^{\\nu \\RR}}{e^{2\\nu\\RR}}=Te^{-\\RR\\nu}.\n\\]\nUsing $|\\hb_{i1}-\\hb_{it}|\\leq 2A$ and  $\\bggm=\\min_{i\\in[n]}\\bgg_i =\\min_{i\\in[n]} (\\bgam_{i1}-\\max_{t\\neq\\op_i}\\bgam_{it})$, observe that \n\\[\n\\sum_{t\\in\\bar\\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq \\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm} \\sum_{t\\neq\\opt_i} \\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\]\nThus, \n\\begin{align*}\n  \\sum_{t\\neq \\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})&= \\sum_{t\\in \\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})+\\sum_{t\\in\\bar\\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\nonumber\\\\\n\n  &\\leq \\sum_{t\\in \\Nc_i} (1-\\nu)\\s_{it}(\\bgam_{i1}-\\bgam_{it})+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm} \\sum_{t\\neq \\op_i} \\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n  &\\leq \\left(1-\\nu+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}\\right)\\sum_{t\\neq \\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n &\\leq \\left(1+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}\\right)\\sum_{t\\neq \\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align*}\nHence, choosing \n\\begin{align}\nR\\geq\\frac{1}{\\nu\\Theta}\\log\\left(\\frac{8\\Gamma AT}{\\bggm\\pi}\\right)\\label{R bound pi}\n\\end{align}\nresults in that\n\\begin{align}\\label{eqn:grad:difff1}\n    \n     &\\sum_{t\\neq\\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})  - \\left(1+\\frac{\\pi}{2}\\right) \\sum_{t\\neq\\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}) \\\\\n   &\\leq\\left(\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}-\\frac{\\pi}{2}\\right)\\sum_{t\\neq\\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n   &\\leq -\\frac{\\pi}{4}\\sum_{t\\neq\\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n   &\\leq-\\frac{\\pi}{4T}\\bggm  e^{-\\bar{R} (1-2\\nu)}.      \n    \n\\end{align}\nHere, the last inequality follows from the fact that $\\sum_{t\\neq\\op_i}\\s_{it}\\geq \\max_{t\\neq\\op_i}\\s_{it}\\geq\\frac{e^{-\\bar{R}(1-2\\nu)}}{\\sum_{t=1}^Te^{-\\bar{R}(\\hb_{i1}-\\hb_{it})}}\\geq e^{-\\bar{R}(1-2\\nu)}/T$.\n\nFrom Assumption A [in <a href=\"https://arxiv.org/pdf/2308.16898#assumption.1\">original paper</a>], we have $c_{\\min}\\leq-\\ell'\\leq c_{\\max}$ for some positive constants $c_{\\min}$ and $c_{\\max}$. It follows from  \\eqref{eqn:grad:difff0} and \\eqref{eqn:grad:difff1} that \n\\begin{align*}\n-\\frac{1}{n}\\sum_{i}^n \\ell_i' \\cdot&\\left(\n      \\sum_{t\\neq\\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})- \\sum_{t\\neq\\op_i} (1+0.5\\pi)\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\right)\\\\\n      & \\leq    c_{\\max}2A\\Gamma  T \\Gamma e^{-\\RR(1 +\\frac{\\pi}{2})}-\\frac{c_{\\min}}{nT}\\cdot\\frac{\\pi\\bggm}{4}e^{-\\bar{R} (1-2\\nu)}\\\\\n      \n      & \\leq 0.\n\\end{align*}\nCombing with \\eqref{R bound pi}, this is guaranteed by \nchoosing \n\n\\[\n  R\\geq \\max\\left\\{\\frac{1}{\\nu\\Theta}\\log\\left(\\frac{8\\Gamma AT}{\\bggm\\pi}\\right),\\frac{1}{(2\\nu+\\pi/2)\\Theta}\\log\\left(\\frac{8n\\Gamma AT^2 c_{\\max}}{c_{\\min}\\bggm\\pi}\\right)\\right\\},\n\\]\nwhere $\\nu=\\nu(\\frac{\\pi}{4A\\Theta})$ depends only on $\\pi$ and global problem variables. \n\nCombining this with the prior $R$ choice \\eqref{Rpi choice2} (by taking maximum), we conclude with the statement.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n<br>\n<br>1. <i>Initial Definitions</i>: The proof starts by defining several variables: \n<br>   \\[\n   \\Wb= \\frac{\\tf{\\Wm} \\W}{\\tf{\\W}}, \\quad \\hb_i=\\X_i\\Wb \\z_{i}, \\quad \\hbm_i= \\X_i\\Ws \\z_{i}, \\quad \\s_i=\\sft{\\X_i\\W \\z_{i}}\n   \\]\n<br>\n<br>2. <i>Main Inequality</i>: The goal is to prove the inequality for sufficiently large \\(R\\) and any \\(\\W\\in \\conb_{\\mu,R}(\\Wm)\\):\n<br>   \\[\n   \\li -\\nabla\\Lc(\\W),\\frac{\\W}{\\tf{\\W}}\\ri \\leq (1+\\pi)\\li-\\nabla\\Lc(\\W), \\frac{\\ps}{\\tf{\\Ws}}\\ri\n   \\]\n<br>\n<br>3. <i>Application of Lemma 29</i>: Using Lemma 29, the proof establishes a bound involving \\(\\hp_i\\):\n<br>   \\[\n   \\big|\\hp^\\top_i\\diag{\\s_i}\\bgam_i-\\hp^\\top_i\\s_i\\s^\\top_i\\bgam_i-\\sum_{t\\neq\\op_i} (\\hp_{i1}-\\hp_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\big|\\leq 2\\Gamma A(1-\\s_{i1})^2\n   \\]\n<br>\n<br>4. <i>Substitution and Simplification</i>: By substituting \\(\\hb\\) and \\(\\hbm\\) and assuming \\(\\pi\\leq 1\\), the proof simplifies the inequality:\n<br>   \\[\n   -\\frac{1}{n}\\sum_{i=1}^n\\ell'_i \\cdot \\left(6\\Gamma A(1-\\s_{i1})^2+ \\sum_{t\\neq \\op_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it}) \\right) \\leq -\\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\neq \\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\n   \\]\n<br>\n<br>5. <i>Claim and Lemma 36</i>: The proof claims that \\(0.5\\pi\\sum_{t\\in \\op_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\geq 6\\Gamma A(1-\\s_{i1})^2\\) for all \\(i \\in [n]\\), following the argument in Lemma 36.\n<br>\n<br>6. <i>Choice of \\(R\\)</i>: The proof chooses \\(R\\) such that:\n<br>   \\[\n   R\\geq R_\\pi =\\frac{1}{\\mu\\Theta}\\log\\left(\\frac{D_{0}\\cdot T\\Gamma A}{\\pi\\bggm}\\right)\n   \\]\n<br>\n<br>7. <i>Scenario Analysis</i>: The proof considers two scenarios:\n<br>   <b>Scenario 1:</b> \\(\\tf{\\Wb-\\Wm}\\leq \\eps=\\frac{\\pi}{4A\\Theta}\\)\n<br>   <b>Scenario 2:</b> \\(\\tf{\\Wb-\\Wm}\\geq \\eps=\\frac{\\pi}{4A\\Theta}\\)\n<br>\n<br>8. <i>Final Comparison</i>: The proof concludes by comparing the terms and ensuring the inequality holds by choosing:\n<br>   \\[\n   R\\geq \\max\\left\\{\\frac{1}{\\nu\\Theta}\\log\\left(\\frac{8\\Gamma AT}{\\bggm\\pi}\\right),\\frac{1}{(2\\nu+\\pi/2)\\Theta}\\log\\left(\\frac{8n\\Gamma AT^2 c_{\\max}}{c_{\\min}\\bggm\\pi}\\right)\\right\\}\n   \\]\n<br>\n<br>Thus, the proof shows that the desired inequality holds for sufficiently large \\(R\\)."
                        }
                    },
                    {
                        "statement_id": "d108ee94-cb8a-43bf-9ea6-1760f23ab71c",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 34,
                        "library_name": "Lemma 34",
                        "title": "Local Optimality Conditions for SVM Solutions",
                        "statement_original_tex": "\\begin{lemma}\n\\label{local cond} \nSuppose Assumption~\\ref{assum:loss:prope} on the loss function $\\ell$ holds, and let $\\bal=(\\alpha_i)_{i=1}^n$ be locally optimal tokens according to Definition \\ref{def loc opt}. Let $ \\Wm= \\Wm_\\bal$ denote the SVM solution obtained via \\eqref{eqn:sattnsvm} by applying the Frobenius norm and replacing $(\\opt_i)_{i=1}^n$ with $\\boldsymbol{\\alpha} = (\\alpha_i)_{i=1}^n$. \n\n\n\n\n\n\nThere exists a scalar $\\mu=\\mu(\\bal)>0$ such that for sufficiently large $\\RR_\\mu$:\n\\begin{enumerate}[label={\\textnormal{\\textbf{L\\arabic*.}}}, wide, labelwidth=!,itemindent=!, labelindent=5pt]\n\\item \\label{lem:cond:l1} There is no stationary point within  $ \\Cc_{\\mu,\\RR_\\mu} (\\Wm)$.\n\n\\item\\label{lem:cond:l2} For all $\\V\\in \\Sc_{\\mu}(\\Wm)$ with $\\tf{\\V}=\\tf{\\Wm}$  and $\\W\\in\\Cc_{\\mu,\\RR_\\mu}(\\Wm)$, there exist dataset dependent constants $C,c>0$ such that \n\\begin{subequations}\\label{local:g:lbound}\n\\begin{align}\n&C\\cdot \\frac{1}{n}\\sum_{i=1}^n \\left(1-\\s_{i\\alpha_i}\\right) \\geq -\\Big\\langle\\nabla\\Lc(\\W),\\V \\Big\\rangle\\geq c\\cdot  \\frac{1}{n} \\sum_{i=1}^n  \\left(1-\\s_{i\\alpha_i}\\right)>0, \\label{local1:g:bound} \\\\\n&\\tf{\\nabla\\Lc(\\W)}\\leq \\bar{A}C \\cdot \\frac{1}{n} \\sum_{i=1}^n  \\left(1-\\s_{i\\alpha_i}\\right), \\label{local2:g:bound}\\\\\n& -\\li\\frac{\\V}{\\tf{\\V}},\\frac{\\nabla\\Lc(\\W)}{\\tf{\\nabla\\Lc(\\W)}}\\ri \\geq  \\frac{c}{C} \\cdot \\frac{\\Theta}{\\bar{A}}>0. \\label{local3:g:bound}\n\\end{align}\n\\end{subequations}\nHere, $\\s_{i\\alpha_i}= (\\sft{\\X_i\\W \\z_{i}})_{\\alpha_i}$, $\\bar{A}=\\max_{i\\in[n],t,\\tau\\in[T]}\\tn{(\\x_{it}- \\x_{i\\tau})}~\\tn{\\z_i}$, and $\\Theta=1/\\tf{\\Ws}$.\n\n\n\n\n\n\\end{enumerate}\n\\end{lemma}",
                        "statement_html": "Suppose Assumption A [in <a href=\"https://arxiv.org/pdf/2308.16898#assumption.1\">original paper</a>] on the loss function $\\ell$ holds, and let $\\bal=(\\alpha_i)_{i=1}^n$ be locally optimal tokens according to <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/definitions/definition_7/index.html#def+loc+opt\">Definition 7</a>. Let $\\Wm= \\Wm_\\bal$ denote the SVM solution obtained via (<a href=\"https://arxiv.org/pdf/2308.16898#equation.2.3\">Att-SVM</a>) by applying the Frobenius norm and replacing $(\\opt_i)_{i=1}^n$ with $\\boldsymbol{\\alpha} = (\\alpha_i)_{i=1}^n$. \n\nThere exists a scalar $\\mu=\\mu(\\bal)>0$ such that for sufficiently large $\\RR_\\mu$:\n<ol>\n    <li><b>L1.</b> There is no stationary point within $\\Cc_{\\mu,\\RR_\\mu} (\\Wm)$.</li>\n    <li><b>L2.</b> For all $\\V\\in \\Sc_{\\mu}(\\Wm)$ with $\\tf{\\V}=\\tf{\\Wm}$ and $\\W\\in\\Cc_{\\mu,\\RR_\\mu}(\\Wm)$, there exist dataset dependent constants $C,c>0$ such that \n    \\begin{subequations}\\label{local:g:lbound}\n    \\begin{align}\n    &C\\cdot \\frac{1}{n}\\sum_{i=1}^n \\left(1-\\s_{i\\alpha_i}\\right) \\geq -\\Big\\langle\\nabla\\Lc(\\W),\\V \\Big\\rangle\\geq c\\cdot  \\frac{1}{n} \\sum_{i=1}^n  \\left(1-\\s_{i\\alpha_i}\\right)>0, \\label{local1:g:bound} \\\\\n    &\\tf{\\nabla\\Lc(\\W)}\\leq \\bar{A}C \\cdot \\frac{1}{n} \\sum_{i=1}^n  \\left(1-\\s_{i\\alpha_i}\\right), \\label{local2:g:bound}\\\\\n    & -\\li\\frac{\\V}{\\tf{\\V}},\\frac{\\nabla\\Lc(\\W)}{\\tf{\\nabla\\Lc(\\W)}}\\ri \\geq  \\frac{c}{C} \\cdot \\frac{\\Theta}{\\bar{A}}>0. \\label{local3:g:bound}\n    \\end{align}\n    \\end{subequations}\n    Here, $\\s_{i\\alpha_i}= (\\sft{\\X_i\\W \\z_{i}})_{\\alpha_i}$, $\\bar{A}=\\max_{i\\in[n],t,\\tau\\in[T]}\\tn{(\\x_{it}- \\x_{i\\tau})}~\\tn{\\z_i}$, and $\\Theta=1/\\tf{\\Ws}$.\n    </li>\n</ol>",
                        "statement_type": "lemma",
                        "statement_motivation_html": "This lemma is crucial for understanding the behavior of the SVM solution in the context of locally optimal tokens. It provides conditions under which there are no stationary points within a certain region, ensuring that the optimization process does not get stuck in suboptimal points. Additionally, it establishes bounds on the gradient and inner product terms, which are essential for analyzing the convergence and stability of the SVM solution. This lemma is particularly useful when dealing with large datasets and complex loss functions, as it helps in guaranteeing the robustness and effectiveness of the SVM model.",
                        "html_url": "library/lemmas/lemma_34/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "0273151b-f2e8-4106-9576-273f8f98cd44",
                            "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\nLet $R=\\RR_\\mu$, $(\\Tc_i)_{i=1}^n$ be the set of all \\neis per Definition \\ref{def loc opt}. Let $\\Tcb_i=[T]-\\Tc_i-\\{\\alpha_i\\}$ be the non-\\neis. Let\n\\begin{equation}\\label{mu choice}\n\\begin{split}\n&\\Theta=1/\\tf{\\Wm},\\\\\n&\\delta= \\frac{1}{2}\\min_{i\\in[n]}\\min_{t\\in\\Tc_i,\\tau\\in\\Tcb_i}(\\x_{it}-\\x_{i\\tau})^\\top \\Wm \\z_{i},\\\\\n&A=\\max_{i\\in[n],t\\in[T]} \\frac{\\tf{\\x_{it} \\z_i^\\top}}{\\Theta},\\\\\n& \\mu\\leq \\mu(\\delta)=\\frac{1}{8}\\left(\\frac{\\min(0.5,\\delta)}{A}\\right)^2.\n\\end{split}\n\\end{equation}\n\n\nSince $\\Wm$ is the max-margin model ensuring $(\\x_{i\\alpha_i}-\\x_{it})^\\top\\Wm \\z_i\\geq 1$, the following inequalities hold for all $\\W\\in \\cone_\\mu(\\Wm),~\\tf{\\W}=\\tf{\\Wm}$ and all $i\\in[n], t\\in\\Tc_i,\\tau\\in\\Tcb_i$:\n\\begin{equation}\\label{cone-non-nei}\n\\begin{split}\n(\\x_{it}-\\x_{i\\tau})^\\top \\W \\z_i&\\geq \\delta>0,\\\\\n(\\x_{i\\alpha_i}-\\x_{i\\tau})^\\top \\W \\z_i&\\geq 1+\\delta,\\\\\n\\frac{3}{2}\\geq(\\x_{i\\alpha_i}-\\x_{it})^\\top \\W \\z_i &\\geq \\frac{1}{2}.\n\\end{split}\n\\end{equation}\nHere, we used $\\tf{\\W-\\Wm}^2/\\tf{\\Wm}^2\\leq 2\\mu$ which implies $\\tf{\\W-\\Wm}\\leq \\sqrt{2\\mu}/\\Theta$.\n\n\n\n\n\nTo proceed, we write the gradient correlation following \\eqref{grad def} and \\eqref{grad def2}\n\\begin{align}\\label{grad def3}\n\\li\\nabla\\Lc(\\W),\\V\\ri&=\\frac{1}{n}\\sum_{i=1}^n\\ell'_i\\cdot\\hb_i^\\top\\sfp{\\hp_i}\\bgam_i,\n\\end{align}\nwhere we denoted $\\ell'_i=\\ell'(Y_i\\cdot \\vb^\\top \\X_i^\\top\\sft{\\hp_i})$, $\\hb_i=\\X_i\\V \\z_{i}$, $\\hp_i= \\X_i\\W \\z_{i}$, and $\\s_i=\\sft{\\hp_i}$.  \n\nUsing \\eqref{cone-non-nei}, for all $t\\in\\Tc_i,\\tau\\in \\Tcb_i$, for all $\\W\\in \\Cc_{\\mu,R}(\\Wm)$, we have that\n\\begin{align*}\n&\\hp_{it}-\\hp_{i\\tau}\\geq R\\Theta\\delta,\\\\\n&\\hp_{i\\alpha_i}-\\hp_{i\\tau}\\geq R\\Theta(1+\\delta),\\\\\n&\\hp_{i\\alpha_i}-\\hp_{it}\\geq R\\Theta/2.    \n\\end{align*}\nConsequently, we can bound the softmax probabilities $\\s_i=\\sft{\\hp_i}$ over non-\\neis as follows: For all $i\\in[n]$ and any $t_i\\in \\Tc_i$\n\\begin{subequations}\n\\begin{align}\\label{soft prob bound}\n&S_i:=\\sum_{\\tau\\in\\Tc_i}\\s_{i\\tau} \n\n\\leq T e^{-R\\Theta/2}\\s_{i\\alpha_i}\\leq T e^{-R\\Theta/2},\\\\\n&Q_i:=\\sum_{\\tau\\in\\Tcb_i}\\s_{i\\tau} \\leq T e^{-R\\Theta\\delta}\\s_{it_i}\\leq T e^{-R\\Theta\\delta}S_i.\n\\end{align}\n\\end{subequations}\n\nRecall scores $\\bgam_{it}=Y_i\\cdot\\vb^\\top \\x_{it}$. Define the score gaps over \\neis:\n\\begin{equation*}\n \\bgg_i=\\bgam_{i\\alpha_i}-\\max_{t\\in\\Tc_i}\\bgam_{it}~~~ \\textnormal{and}~~~ \\bgm_i=\\bgam_{i\\alpha_i}-\\min_{t\\in\\Tc_i}\\bgam_{it}. \n\\end{equation*}\nIt follows from \\eqref{mu choice} that \n\\begin{align*}\n&A=\\max_{i\\in[n],t\\in[T]} \\frac{\\tf{\\x_{it} \\z_i^\\top}}{\\Theta}\\geq \\max_{i\\in[n],t\\in[T]}\\tn{\\hb_{it}}.\n\\end{align*}\nDefine the $\\bal$-dependent global scalar $\\Gamma=\\sup_{i\\in[n],t,\\tau\\in[T]}|\\bgam_{it}-\\bgam_{i\\tau}|$.\n\n\nLet us focus on a fixed datapoint $i\\in[n]$, assume (without losing generality) $\\alpha_i=1$, and drop subscripts $i$.\n\nDirectly applying Lemma \\ref{lem:q_reduce}, we obtain\n\\[\n  \\big|\\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam-\\sum_{t\\geq 2}^T (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2\\Gamma A(1-\\s_1)^2.\n\\]\nTo proceed, let us decouple the non-\\neis within $\\sum_{t\\geq 2}^T (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)$ via\n\\[\n\\big|\\sum_{t\\in\\Tcb} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2Q\\Gamma A.\n\\]\nAggregating these, we found\n\\begin{align}\n  \\big|\\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam-\\sum_{t\\in \\Tc} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2\\Gamma A((1-\\s_1)^2+Q).\\label{aggregate}\n\\end{align}\n\nTo proceed, let us upper/lower bound the gradient correlation.  We use two bounds depending on $\\V\\in\\Sc_{\\mu}(\\Ws)$ (\\textbf{Case 1}) or general $\\V\\in\\R^{d\\times d}$ (\\textbf{Case 2}).\n\n\n\\noindent$\\bullet$ \\textbf{Case 1:  $\\V\\in\\Sc_{\\mu}(\\Ws)$.} Since $1.5\\geq \\hb_1-\\hb_t\\geq 0.5$ following \\eqref{cone-non-nei}, we find\n\\[\n 1.5\\cdot S\\cdot \\bgm  \\geq\\sum_{t\\in \\Tc} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\geq 0.5\\cdot S\\cdot \\bgg,\n\\]\nwhere recall the definition of $S$ (having dropped subscripts) in \\eqref{soft prob bound}. \n\n\\noindent$\\bullet$ \\textbf{Case 2: $\\Vb\\in\\R^{d\\times d}$ and $\\tf{\\V}=\\tf{\\Wm}$.}  Define $\\bar{A}=\\max_{i\\in[n],t,\\tau\\in[T]}\\tn{\\x_{it}-\\x_{i\\tau}}~\\tn{\\z_i}$. For any $\\tf{\\V}=\\tn{\\Ws}$, we use the fact that\n$$\\tn{\\hb_1-\\hb_t}\\leq \\tf{(\\x_{it}-\\x_{i\\tau}) \\z_i^\\top}\\cdot\\tf{\\V}\\leq \\frac{\\bar{A}}{\\Theta}.$$\nNote that by definition $ \\frac{\\bar{A}}{\\Theta} \\geq 1$. To proceed, we can upper bound\n\\begin{align}\n\\frac{\\bar{A}}{\\Theta}\\cdot S\\cdot \\bgm  \\geq\\sum_{t\\in \\Tc} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t).\\label{wishwish2}\n\\end{align}\n\nNext we claim that for both cases, $S$ dominates $((1-\\s_1)^2+Q)$ for large $R$. Specifically, we wish for \n\\begin{align}\\label{wishfor}\n\\frac{S\\cdot \\bgg}{4}\\geq 4\\Gamma A\\max((1-\\s_1)^2,Q)\\iff S\\geq 16\\frac{\\Gamma A}{\\bgg}\\max((1-\\s_1)^2,Q).\n\\end{align}\nNow choose $R\\geq \\delta^{-1}\\log(T)/\\Theta$  to ensure $Q\\leq S$ since $Q\\leq Te^{-R\\Theta\\delta}S$ from \\eqref{soft prob bound}. Consequently\n\\[\n(1-\\s_1)^2=(Q+S)^2\\leq 4S^2\\leq 4STe^{-R\\Theta/2}.\n\\]\nCombining these, what we wish is ensured by guaranteeing\n\\begin{align}\\label{s bound}\n  S\\geq 16\\frac{\\Gamma A}{\\bgg}\\max(4STe^{-R\\Theta/2},Te^{-R\\Theta\\delta}S).\n\\end{align}\nThis in turn is ensured for all inputs $i\\in[n]$ by choosing \n\\begin{align}\\label{R bound}\nR\\geq \\frac{\\max(2,\\delta^{-1})}{\\Theta}\\log\\left(\\frac{64T\\Gamma A}{\\bggm}\\right),\n\\end{align}\nwhere $\\bggm=\\min_{i\\in[n]}\\bgg_i$ is the global scalar which is the worst case score gap over all inputs. \n\\\\\n$\\bullet$ \\textbf{Case 1: $\\V\\in\\Sc_{\\mu}(\\Ws)$}. With the above choice of $R$, we guaranteed\n\\[\n  2 (1-\\s_1)\\cdot \\bgm\\geq 2\\cdot S\\cdot \\bgm \\geq \\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam\\geq\\frac{S\\cdot \\bgg}{4}\\geq\\frac{(1-\\s_1) \\bgg}{8}.\n\\]\nvia \\eqref{wishfor} and \\eqref{aggregate}. \n\nSince this holds over all inputs, going back to the gradient correlation \\eqref{grad def3} and averaging above over all inputs $i\\in[n]$ and plugging back the indices $i$, we obtain the advertised bound \n\\begin{align}\\label{pbb corr}\n\n\\frac{2}{n}\\sum_{i\\in [n]} -\\ell'_i\\cdot S_i\\cdot \\bgm_i\\geq -\\li\\nabla\\Lc(\\W),\\V\\ri\\geq \\frac{1}{8n}\\sum_{i\\in [n]} -\\ell'_i\\cdot S_i\\cdot \\bgg_i.\n\\end{align}\nLet $-\\ell'_{\\min/\\max}$ be the min/max values negative loss derivative admits over the ball $[-A,A]$ and note that $\\max_{i\\in[n]}\\bgm_i>0$ and $\\min_{i\\in[n]}\\bgg_i>0$ are dataset dependent constants. Then, we declare the constants $C=-2\\ell'_{\\max}\\cdot \\max_{i\\in[n]}\\bgm_i>0,c=-(1/8)\\ell'_{\\min}\\cdot \\min_{i\\in[n]}\\bgg_i>0$ to obtain the bound \\eqref{local1:g:bound}. \n\\vspace{.2cm}\n\\\\\n\\noindent$\\bullet$ \\textbf{Case 2: $\\Vb\\in\\R^{d\\times d}$ and $\\tf{\\V}=\\tf{\\Wm}$.} Next, we show \\eqref{local2:g:bound} and \\eqref{local3:g:bound}. For any $\\V \\in \\mathbb{R}^{d \\times d}$ satisfying $\\tf{\\V}=\\tf{\\Ws}$, using \\eqref{wishwish2} and the  choice of $R$ in \\eqref{R bound} similarly guarantees \n$$\n\\frac{2\\bar{A}}{\\Theta }(1-\\s_1) \\bgm\\geq \\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam,\n$$\nfor fixed input. Going back to the gradient correlation \\eqref{grad def3} and averaging above over all inputs $i\\in[n]$, with the same definition of $C>0$, we obtain\n\\begin{align}\n\\frac{ \\bar{A} C}{  \\Theta n}\\sum_{i\\in [n]} (1-\\s_{i\\alpha_i})\\geq -\\li\\nabla\\Lc(\\W),\\V\\ri.\\label{local lamma general upper}\n\\end{align}\nTo proceed, since \\eqref{local lamma general upper} holds for any $\\V\\in\\R^{d\\times d}$, we observe that when setting $\\V=\\frac{\\tf{\\Ws}}{\\tf{\\nabla\\Lc(\\W)}}\\cdot \\nabla\\Lc(\\W)$, this implies that\n\\[ \n\\li\\nabla\\Lc(\\W),\\V\\ri = \\tf{\\nabla\\Lc(\\W)}\\cdot \\tf{\\Ws}\\leq \\frac{\\bar{A} C}{\\Theta \n n}\\sum_{i\\in [n]} (1-\\s_{i\\alpha_i}).\n\\]\nSimplifying $\\Theta=1/\\tf{\\Ws}$ on both sides gives \\eqref{local2:g:bound}. \n\\\\\nCombining the above inequality with \\eqref{pbb corr}, we obtain that for all $\\V,\\W\\in\\Sc_{\\mu}(\\Ws)$\n\\[ \n-\\li\\frac{\\V}{\\tf{\\V}},\\frac{\\nabla\\Lc(\\W)}{\\tf{\\nabla\\Lc(\\W)}}\\ri\\geq \\frac{c \\Theta }{C\\bar{A}},\n\\]\nwhich gives \\eqref{local3:g:bound}.\n\n\n\n\n\\end{proof}",
                            "statement_html": "Let $R=\\RR_\\mu$, $(\\Tc_i)_{i=1}^n$ be the set of all $\\neis$ per <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/definitions/definition_7/index.html#def+loc+opt\">Definition 7</a>. Let $\\Tcb_i=[T]-\\Tc_i-\\{\\alpha_i\\}$ be the non-$\\neis$. Let\n\\begin{equation}\\label{mu choice}\n\\begin{split}\n&\\Theta=1/\\tf{\\Wm},\\\\\n&\\delta= \\frac{1}{2}\\min_{i\\in[n]}\\min_{t\\in\\Tc_i,\\tau\\in\\Tcb_i}(\\x_{it}-\\x_{i\\tau})^\\top \\Wm \\z_{i},\\\\\n&A=\\max_{i\\in[n],t\\in[T]} \\frac{\\tf{\\x_{it} \\z_i^\\top}}{\\Theta},\\\\\n& \\mu\\leq \\mu(\\delta)=\\frac{1}{8}\\left(\\frac{\\min(0.5,\\delta)}{A}\\right)^2.\n\\end{split}\n\\end{equation}\n\nSince $\\Wm$ is the max-margin model ensuring $(\\x_{i\\alpha_i}-\\x_{it})^\\top\\Wm \\z_i\\geq 1$, the following inequalities hold for all $\\W\\in \\cone_\\mu(\\Wm),~\\tf{\\W}=\\tf{\\Wm}$ and all $i\\in[n], t\\in\\Tc_i,\\tau\\in\\Tcb_i$:\n\\begin{equation}\\label{cone-non-nei}\n\\begin{split}\n(\\x_{it}-\\x_{i\\tau})^\\top \\W \\z_i&\\geq \\delta>0,\\\\\n(\\x_{i\\alpha_i}-\\x_{i\\tau})^\\top \\W \\z_i&\\geq 1+\\delta,\\\\\n\\frac{3}{2}\\geq(\\x_{i\\alpha_i}-\\x_{it})^\\top \\W \\z_i &\\geq \\frac{1}{2}.\n\\end{split}\n\\end{equation}\nHere, we used $\\tf{\\W-\\Wm}^2/\\tf{\\Wm}^2\\leq 2\\mu$ which implies $\\tf{\\W-\\Wm}\\leq \\sqrt{2\\mu}/\\Theta$.\n\nTo proceed, we write the gradient correlation following (15) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.B.15\">original paper</a>] and (27) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.C.27\">original paper</a>]\n\\begin{align}\\label{grad def3}\n\\li\\nabla\\Lc(\\W),\\V\\ri&=\\frac{1}{n}\\sum_{i=1}^n\\ell'_i\\cdot\\hb_i^\\top\\sfp{\\hp_i}\\bgam_i,\n\\end{align}\nwhere we denoted $\\ell'_i=\\ell'(Y_i\\cdot \\vb^\\top \\X_i^\\top\\sft{\\hp_i})$, $\\hb_i=\\X_i\\V \\z_{i}$, $\\hp_i= \\X_i\\W \\z_{i}$, and $\\s_i=\\sft{\\hp_i}$.  \n\nUsing $\\eqref{cone-non-nei}$, for all $t\\in\\Tc_i,\\tau\\in \\Tcb_i$, for all $\\W\\in \\Cc_{\\mu,R}(\\Wm)$, we have that\n\\begin{align*}\n&\\hp_{it}-\\hp_{i\\tau}\\geq R\\Theta\\delta,\\\\\n&\\hp_{i\\alpha_i}-\\hp_{i\\tau}\\geq R\\Theta(1+\\delta),\\\\\n&\\hp_{i\\alpha_i}-\\hp_{it}\\geq R\\Theta/2.    \n\\end{align*}\nConsequently, we can bound the softmax probabilities $\\s_i=\\sft{\\hp_i}$ over non-$\\neis$ as follows: For all $i\\in[n]$ and any $t_i\\in \\Tc_i$\n\\begin{subequations}\n\\begin{align}\\label{soft prob bound}\n&S_i:=\\sum_{\\tau\\in\\Tc_i}\\s_{i\\tau} \n\n\\leq T e^{-R\\Theta/2}\\s_{i\\alpha_i}\\leq T e^{-R\\Theta/2},\\\\\n&Q_i:=\\sum_{\\tau\\in\\Tcb_i}\\s_{i\\tau} \\leq T e^{-R\\Theta\\delta}\\s_{it_i}\\leq T e^{-R\\Theta\\delta}S_i.\n\\end{align}\n\\end{subequations}\n\nRecall scores $\\bgam_{it}=Y_i\\cdot\\vb^\\top \\x_{it}$. Define the score gaps over $\\neis$:\n\\begin{equation*}\n \\bgg_i=\\bgam_{i\\alpha_i}-\\max_{t\\in\\Tc_i}\\bgam_{it}~~~ \\textnormal{and}~~~ \\bgm_i=\\bgam_{i\\alpha_i}-\\min_{t\\in\\Tc_i}\\bgam_{it}. \n\\end{equation*}\nIt follows from $\\eqref{mu choice}$ that \n\\begin{align*}\n&A=\\max_{i\\in[n],t\\in[T]} \\frac{\\tf{\\x_{it} \\z_i^\\top}}{\\Theta}\\geq \\max_{i\\in[n],t\\in[T]}\\tn{\\hb_{it}}.\n\\end{align*}\nDefine the $\\bal$-dependent global scalar $\\Gamma=\\sup_{i\\in[n],t,\\tau\\in[T]}|\\bgam_{it}-\\bgam_{i\\tau}|$.\n\nLet us focus on a fixed datapoint $i\\in[n]$, assume (without losing generality) $\\alpha_i=1$, and drop subscripts $i$.\n\nDirectly applying <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_29/index.html#lem%3Aq_reduce\">Lemma 29</a>, we obtain\n\\[\n  \\big|\\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam-\\sum_{t\\geq 2}^T (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2\\Gamma A(1-\\s_1)^2.\n\\]\nTo proceed, let us decouple the non-$\\neis$ within $\\sum_{t\\geq 2}^T (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)$ via\n\\[\n\\big|\\sum_{t\\in\\Tcb} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2Q\\Gamma A.\n\\]\nAggregating these, we found\n\\begin{align}\n  \\big|\\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam-\\sum_{t\\in \\Tc} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2\\Gamma A((1-\\s_1)^2+Q).\\label{aggregate}\n\\end{align}\n\nTo proceed, let us upper/lower bound the gradient correlation.  We use two bounds depending on $\\V\\in\\Sc_{\\mu}(\\Ws)$ ($\\textbf{Case 1}$) or general $\\V\\in\\R^{d\\times d}$ ($\\textbf{Case 2}$).\n<br>\n$\\bullet$ $\\textbf{Case 1: }  \\V\\in\\Sc_{\\mu}(\\Ws)$. Since $1.5\\geq \\hb_1-\\hb_t\\geq 0.5$ following $\\eqref{cone-non-nei}$, we find\n\\[\n 1.5\\cdot S\\cdot \\bgm  \\geq\\sum_{t\\in \\Tc} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\geq 0.5\\cdot S\\cdot \\bgg,\n\\]\nwhere recall the definition of $S$ (having dropped subscripts) in $\\eqref{soft prob bound}$. \n<br>\n$\\bullet$ $\\textbf{Case 2:} \\Vb\\in\\R^{d\\times d} \\textbf{ and } \\tf{\\V}=\\tf{\\Wm}$.  Define $\\bar{A}=\\max_{i\\in[n],t,\\tau\\in[T]}\\tn{\\x_{it}-\\x_{i\\tau}}~\\tn{\\z_i}$. For any $\\tf{\\V}=\\tn{\\Ws}$, we use the fact that\n$$\\tn{\\hb_1-\\hb_t}\\leq \\tf{(\\x_{it}-\\x_{i\\tau}) \\z_i^\\top}\\cdot\\tf{\\V}\\leq \\frac{\\bar{A}}{\\Theta}.$$\nNote that by definition $ \\frac{\\bar{A}}{\\Theta} \\geq 1$. To proceed, we can upper bound\n\\begin{align}\n\\frac{\\bar{A}}{\\Theta}\\cdot S\\cdot \\bgm  \\geq\\sum_{t\\in \\Tc} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t).\\label{wishwish2}\n\\end{align}\n\nNext we claim that for both cases, $S$ dominates $((1-\\s_1)^2+Q)$ for large $R$. Specifically, we wish for \n\\begin{align}\\label{wishfor}\n\\frac{S\\cdot \\bgg}{4}\\geq 4\\Gamma A\\max((1-\\s_1)^2,Q)\\iff S\\geq 16\\frac{\\Gamma A}{\\bgg}\\max((1-\\s_1)^2,Q).\n\\end{align}\nNow choose $R\\geq \\delta^{-1}\\log(T)/\\Theta$  to ensure $Q\\leq S$ since $Q\\leq Te^{-R\\Theta\\delta}S$ from $\\eqref{soft prob bound}$. Consequently\n\\[\n(1-\\s_1)^2=(Q+S)^2\\leq 4S^2\\leq 4STe^{-R\\Theta/2}.\n\\]\nCombining these, what we wish is ensured by guaranteeing\n\\begin{align}\\label{s bound}\n  S\\geq 16\\frac{\\Gamma A}{\\bgg}\\max(4STe^{-R\\Theta/2},Te^{-R\\Theta\\delta}S).\n\\end{align}\nThis in turn is ensured for all inputs $i\\in[n]$ by choosing \n\\begin{align}\\label{R bound}\nR\\geq \\frac{\\max(2,\\delta^{-1})}{\\Theta}\\log\\left(\\frac{64T\\Gamma A}{\\bggm}\\right),\n\\end{align}\nwhere $\\bggm=\\min_{i\\in[n]}\\bgg_i$ is the global scalar which is the worst case score gap over all inputs. \n<br>\n$\\bullet$ $\\textbf{Case 1: }  \\V\\in\\Sc_{\\mu}(\\Ws)$. With the above choice of $R$, we guaranteed\n\\[\n  2 (1-\\s_1)\\cdot \\bgm\\geq 2\\cdot S\\cdot \\bgm \\geq \\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam\\geq\\frac{S\\cdot \\bgg}{4}\\geq\\frac{(1-\\s_1) \\bgg}{8}.\n\\]\nvia $\\eqref{wishfor}$ and $\\eqref{aggregate}$. \n\nSince this holds over all inputs, going back to the gradient correlation $\\eqref{grad def3}$ and averaging above over all inputs $i\\in[n]$ and plugging back the indices $i$, we obtain the advertised bound \n\\begin{align}\\label{pbb corr}\n\n\\frac{2}{n}\\sum_{i\\in [n]} -\\ell'_i\\cdot S_i\\cdot \\bgm_i\\geq -\\li\\nabla\\Lc(\\W),\\V\\ri\\geq \\frac{1}{8n}\\sum_{i\\in [n]} -\\ell'_i\\cdot S_i\\cdot \\bgg_i.\n\\end{align}\nLet $-\\ell'_{\\min/\\max}$ be the min/max values negative loss derivative admits over the ball $[-A,A]$ and note that $\\max_{i\\in[n]}\\bgm_i>0$ and $\\min_{i\\in[n]}\\bgg_i>0$ are dataset dependent constants. Then, we declare the constants $C=-2\\ell'_{\\max}\\cdot \\max_{i\\in[n]}\\bgm_i>0,c=-(1/8)\\ell'_{\\min}\\cdot \\min_{i\\in[n]}\\bgg_i>0$ to obtain the bound $\\eqref{local1:g:bound}$. \n$\\vspace{.2cm}$\n<br>\n$\\bullet$ $\\textbf{Case 2:} \\Vb\\in\\R^{d\\times d} \\textbf{ and } \\tf{\\V}=\\tf{\\Wm}$. Next, we show $\\eqref{local2:g:bound}$ and $\\eqref{local3:g:bound}$. For any $\\V \\in \\mathbb{R}^{d \\times d}$ satisfying $\\tf{\\V}=\\tf{\\Ws}$, using $\\eqref{wishwish2}$ and the  choice of $R$ in $\\eqref{R bound}$ similarly guarantees \n$$\n\\frac{2\\bar{A}}{\\Theta }(1-\\s_1) \\bgm\\geq \\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam,\n$$\nfor fixed input. Going back to the gradient correlation $\\eqref{grad def3}$ and averaging above over all inputs $i\\in[n]$, with the same definition of $C>0$, we obtain\n\\begin{align}\n\\frac{ \\bar{A} C}{  \\Theta n}\\sum_{i\\in [n]} (1-\\s_{i\\alpha_i})\\geq -\\li\\nabla\\Lc(\\W),\\V\\ri.\\label{local lamma general upper}\n\\end{align}\nTo proceed, since $\\eqref{local lamma general upper}$ holds for any $\\V\\in\\R^{d\\times d}$, we observe that when setting $\\V=\\frac{\\tf{\\Ws}}{\\tf{\\nabla\\Lc(\\W)}}\\cdot \\nabla\\Lc(\\W)$, this implies that\n\\[ \n\\li\\nabla\\Lc(\\W),\\V\\ri = \\tf{\\nabla\\Lc(\\W)}\\cdot \\tf{\\Ws}\\leq \\frac{\\bar{A} C}{\\Theta \n n}\\sum_{i\\in [n]} (1-\\s_{i\\alpha_i}).\n\\]\nSimplifying $\\Theta=1/\\tf{\\Ws}$ on both sides gives $\\eqref{local2:g:bound}$. \n\nCombining the above inequality with $\\eqref{pbb corr}$, we obtain that for all $\\V,\\W\\in\\Sc_{\\mu}(\\Ws)$\n\\[ \n-\\li\\frac{\\V}{\\tf{\\V}},\\frac{\\nabla\\Lc(\\W)}{\\tf{\\nabla\\Lc(\\W)}}\\ri\\geq \\frac{c \\Theta }{C\\bar{A}},\n\\]\nwhich gives $\\eqref{local3:g:bound}$.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n<br>\n<br>1. <i>Definitions and Initial Setup</i>: The proof begins by defining several key variables and parameters:\n<br>   \\[\n   \\Theta=1/\\tf{\\Wm}, \\quad \\delta= \\frac{1}{2}\\min_{i\\in[n]}\\min_{t\\in\\Tc_i,\\tau\\in\\Tcb_i}(\\x_{it}-\\x_{i\\tau})^\\top \\Wm \\z_{i}, \\quad A=\\max_{i\\in[n],t\\in[T]} \\frac{\\tf{\\x_{it} \\z_i^\\top}}{\\Theta}, \\quad \\mu\\leq \\mu(\\delta)=\\frac{1}{8}\\left(\\frac{\\min(0.5,\\delta)}{A}\\right)^2.\n   \\]\n<br>\n<br>2. <i>Max-Margin Model and Inequalities</i>: The proof uses the max-margin model to establish several inequalities for all \\(\\W\\in \\cone_\\mu(\\Wm)\\):\n<br>   \\[\n   (\\x_{it}-\\x_{i\\tau})^\\top \\W \\z_i\\geq \\delta>0, \\quad (\\x_{i\\alpha_i}-\\x_{i\\tau})^\\top \\W \\z_i\\geq 1+\\delta, \\quad \\frac{3}{2}\\geq(\\x_{i\\alpha_i}-\\x_{it})^\\top \\W \\z_i \\geq \\frac{1}{2}.\n   \\]\n<br>\n<br>3. <i>Gradient Correlation</i>: The gradient correlation is written as:\n<br>   \\[\n   \\li\\nabla\\Lc(\\W),\\V\\ri=\\frac{1}{n}\\sum_{i=1}^n\\ell'_i\\cdot\\hb_i^\\top\\sfp{\\hp_i}\\bgam_i,\n   \\]\n<br>   where \\(\\ell'_i=\\ell'(Y_i\\cdot \\vb^\\top \\X_i^\\top\\sft{\\hp_i})\\), \\(\\hb_i=\\X_i\\V \\z_{i}\\), \\(\\hp_i= \\X_i\\W \\z_{i}\\), and \\(\\s_i=\\sft{\\hp_i}\\).\n<br>\n<br>4. <i>Softmax Probability Bounds</i>: Using the inequalities, the softmax probabilities over non-neighbors are bounded:\n<br>   \\[\n   S_i:=\\sum_{\\tau\\in\\Tc_i}\\s_{i\\tau} \\leq T e^{-R\\Theta/2}\\s_{i\\alpha_i}\\leq T e^{-R\\Theta/2}, \\quad Q_i:=\\sum_{\\tau\\in\\Tcb_i}\\s_{i\\tau} \\leq T e^{-R\\Theta\\delta}\\s_{it_i}\\leq T e^{-R\\Theta\\delta}S_i.\n   \\]\n<br>\n<br>5. <i>Score Gaps and Global Scalar</i>: Define the score gaps over neighbors and the global scalar:\n<br>   \\[\n   \\bgg_i=\\bgam_{i\\alpha_i}-\\max_{t\\in\\Tc_i}\\bgam_{it}, \\quad \\bgm_i=\\bgam_{i\\alpha_i}-\\min_{t\\in\\Tc_i}\\bgam_{it}, \\quad \\Gamma=\\sup_{i\\in[n],t,\\tau\\in[T]}|\\bgam_{it}-\\bgam_{i\\tau}|.\n   \\]\n<br>\n<br>6. <i>Applying Lemma and Aggregation</i>: Applying Lemma 29 and aggregating terms, we get:\n<br>   \\[\n   \\big|\\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam-\\sum_{t\\in \\Tc} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2\\Gamma A((1-\\s_1)^2+Q).\n   \\]\n<br>\n<br>7. <i>Bounding Gradient Correlation</i>: The gradient correlation is bounded for two cases:\n<br>   \\[\n   \\text{Case 1: } \\V\\in\\Sc_{\\mu}(\\Ws), \\quad \\text{Case 2: } \\Vb\\in\\R^{d\\times d} \\text{ and } \\tf{\\V}=\\tf{\\Wm}.\n   \\]\n<br>   For both cases, the bounds are derived using the inequalities and the choice of \\(R\\).\n<br>\n<br>8. <i>Final Bound</i>: The final bound on the gradient correlation is obtained:\n<br>   \\[\n   \\frac{2}{n}\\sum_{i\\in [n]} -\\ell'_i\\cdot S_i\\cdot \\bgm_i\\geq -\\li\\nabla\\Lc(\\W),\\V\\ri\\geq \\frac{1}{8n}\\sum_{i\\in [n]} -\\ell'_i\\cdot S_i\\cdot \\bgg_i.\n   \\]\n<br>   Constants \\(C\\) and \\(c\\) are defined to simplify the bound.\n<br>\n<br>Thus, the proof shows the desired bounds on the gradient correlation."
                        }
                    },
                    {
                        "statement_id": "bd49c7be-7b04-4027-a0a4-c0ebb4ce989e",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 35,
                        "library_name": "Lemma 35",
                        "title": "Local Correlation Inequality",
                        "statement_original_tex": "\\begin{lemma}\n\\label{lem:local:corr} \nSuppose Assumption~\\ref{assum:loss:prope} on the loss function $\\ell$ holds, and let $\\bal=(\\alpha_i)_{i=1}^n$ be locally optimal tokens according to Definition \\ref{def loc opt}. Let $ \\Wm= \\Wm_\\bal$ denote the SVM solution obtained via \\eqref{eqn:sattnsvm} by replacing $(\\opt_i)_{i=1}^n$ with $\\boldsymbol{\\alpha} = (\\alpha_i)_{i=1}^n$. Let $\\mu=\\mu(\\bal)>0$ and $\\bar{R}_{\\mu}$ be defined as in Lemma~\\ref{local cond}. For any choice of $\\pi>0$, there exists $R_\\pi \\geq \\bar{R}_{\\mu}$ such that, for any $ \\W\\in \\Cc_{\\mu,R_\\pi}(\\Wm)$, we have\n\\[\n \\li \\nabla\\Lc(\\W), \\frac{\\W}{\\tf{\\W}} \\ri\\geq (1+\\pi)\\li \\nabla\\Lc(\\W), \\frac{\\Wm}{\\tf{\\Wm}}\\ri.\n\\]\n\\end{lemma}",
                        "statement_html": "Suppose Assumption A [in <a href=\"https://arxiv.org/pdf/2308.16898#assumption.1\">original paper</a>] on the loss function $\\ell$ holds, and let $\\bal=(\\alpha_i)_{i=1}^n$ be locally optimal tokens according to <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/definitions/definition_7/index.html#def+loc+opt\">Definition 7</a>. Let $ \\Wm= \\Wm_\\bal$ denote the SVM solution obtained via (<a href=\"https://arxiv.org/pdf/2308.16898#equation.2.3\">Att-SVM</a>) by replacing $(\\opt_i)_{i=1}^n$ with $\\boldsymbol{\\alpha} = (\\alpha_i)_{i=1}^n$. Let $\\mu=\\mu(\\bal)>0$ and $\\bar{R}_{\\mu}$ be defined as in <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_34/index.html#local+cond\">Lemma 34</a>. For any choice of $\\pi>0$, there exists $R_\\pi \\geq \\bar{R}_{\\mu}$ such that, for any $ \\W\\in \\Cc_{\\mu,R_\\pi}(\\Wm)$, we have\n\\[\n \\li \\nabla\\Lc(\\W), \\frac{\\W}{\\tf{\\W}} \\ri\\geq (1+\\pi)\\li \\nabla\\Lc(\\W), \\frac{\\Wm}{\\tf{\\Wm}}\\ri.\n\\]",
                        "statement_type": "lemma",
                        "statement_motivation_html": "Understanding the relationship between the SVM solution $\\Wm$ and the locally optimal tokens $\\bal$ is crucial for improving the performance of machine learning models. This statement provides a bound on the gradient of the loss function $\\Lc$ in terms of $\\W$ and $\\Wm$, ensuring that the solution $\\W$ is within a certain range of the optimal solution $\\Wm$. This is particularly useful when fine-tuning models to achieve better generalization and robustness.",
                        "html_url": "library/lemmas/lemma_35/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "fac81b15-e50c-4359-bbbb-a620de4a29ca",
                            "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\nLet  $R=R_{\\pi}$, $\\Wb=\\tf{\\Wm} \\W/\\tf{\\W} $, $\\hb_i=\\X_i\\Wb \\z_{i}$, and $\\hbm_i= \\X_i \\Wm \\z_{i}$.   To establish the result, we will prove that, for sufficiently large $R$ and for any $\\W\\in \\Cc_{\\mu,R}(\\Wm)$:\n\\begin{align}\\label{main local cond}\n\\nonumber \n\\li -\\nabla\\Lc(\\W),\\frac{\\W}{\\tf{\\W}}\\ri&= -\\frac{1}{n}\\sum_{i=1}^n\\ell'_i \\cdot  \\li \\hb_i, \\sfp{\\X_i\\W \\z_{i}}\\bgam_i\\ri\\\\\n&\\leq - \\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot  \\li\\hbm_i, \\sfp{\\X_i\\W \\z_{i}}\\bgam_i\\ri=(1+\\pi)\\li-\\nabla\\Lc(\\W), \\frac{\\ps}{\\tf{\\Ws}}\\ri.\n\\end{align}\n\nFollowing (67) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.D.67\">original paper</a>], for all $\\W\\in \\Sc_{\\mu}(\\Wm)$ with $\\tf{\\W}=\\tf{\\Wm}$, $\\hp=\\X\\W \\z$, and $\\s=\\sft{\\hp}$, we have found\n\\begin{align}\n  \\big|\\hp^\\top_i\\diag{\\s_i}\\bgam_i-\\hp^\\top_i\\s_i\\s^\\top_i\\bgam_i-\\sum_{t\\in \\Tc_i} (\\hp_{i1}-\\hp_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\big|\\leq 2\\Gamma A((1-\\s_{i1})^2+Q_i), \n\\end{align}\nwhere $\\Tc_i$ is the set of support indices. \n\n\nPlugging in $\\hb,\\hbm$ in the bound above and assuming $\\pi\\leq 1$ (w.l.o.g.), \\eqref{main local cond} is implied by the following stronger inequality\n\\begin{align*}\n-\\frac{1}{n}&\\sum_{i=1}^n\\ell'_i \\cdot \\left(6\\Gamma A((1-\\s_{i1})^2+Q_i)+ \\sum_{t\\in \\Tc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it}) \\right)\\\\\n&\\leq -\\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i  \\cdot \\sum_{t\\in \\Tc_i} (\\hbm_{i1}-\\hbm_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n&=-\\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align*}\nFirst, we claim that $0.5\\pi\\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\geq 6\\Gamma A((1-\\s_{i1})^2+Q_i)$ for all $i \\in [n]$.  The proof of this claim directly follows the earlier argument, namely, following \\eqref{wishfor}, \\eqref{s bound}, and \\eqref{R bound}  which leads to the choice \n\\begin{equation}\\label{R boundC0}\nR \\ge\\frac{\\max(2,\\delta^{-1})}{\\Theta}\\log\\left(\\frac{C_0\\cdot T\\Gamma A}{\\pi\\bggm}\\right),    \n\\end{equation}\nfor some constant $C_0>0$. Using \\eqref{R bound}, we choose $C_0 \\geq 64 \\pi$ to guarantee $R=R_\\pi \\geq \\bar{R}_{\\mu}$.\n\nFollowing this control over the perturbation term $6\\Gamma A((1-\\s_{i1})^2+Q_i)$, to conclude with the result, what remains is proving the comparison\n\\begin{align}\\label{desired comp}\n-\\frac{1}{n} \\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\in \\Tc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq - \\frac{1+0.5\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align}\nTo proceed, we split the problem into two scenarios. \n\n\\noindent\\textbf{Scenario 1:} $\\tf{\\Wb-\\Wm}\\leq \\eps=\\frac{\\pi}{4A\\Theta}$ for some $\\eps>0$.  In this scenario, for any $ t\\in \\Tc_i$ and $i\\in [n ]$, we have\n\\[\n|\\hb_{it}-\\hbm_{it}|=|\\x_{it}^\\top (\\Wb-\\Wm)  \\z_{it}|\\leq A\\Theta\\eps=\\frac{\\pi}{4}.\n\\]\nConsequently, we obtain \n\\[\n\\hb_{i1}-\\hb_{it}\\leq \\hbm_{i1}-\\hbm_{it}+2A\\Theta\\eps= 1+0.5\\pi.\n\\] \nSimilarly, $\\hb_{i1}-\\hb_{it}\\geq 1-0.5\\pi\\geq 0.5$. Since all terms $\\hb_{i1}-\\hb_{it},\\s_{it},\\bgam_{i1}-\\bgam_{it}$ in \\eqref{desired comp} are nonnegative and $(\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq (1+0.5\\pi)\\s_{it}(\\bgam_{i1}-\\bgam_{it})$, above implies the desired result in \\eqref{desired comp}.\n\n\\vspace{3pt}\n\\noindent\\textbf{Scenario 2:} $\\tf{\\Wb-\\Wm}\\geq \\eps=\\frac{\\pi}{4A\\Theta}$.  Since $\\Wb$ is not (locally) max-margin, in this scenario, for some $i \\in  [n]$, $\\nu=\\nu(\\eps)>0$, and $\\tau\\in\\Tc_i$, we have that\n\\begin{align*}\n\\hb_{i1}-\\hb_{i\\tau}\\leq 1-2\\nu.\n\\end{align*}\nHere $\\tau=\\arg\\max_{\\tau\\in\\Tc_i} \\x_{i\\tau}\\Wb \\z_i$ denotes the nearest point to $\\hb_{i1}$ (along the $\\Wb$ direction). Note that a non-neighbor $t\\in\\Tcb_i$ cannot be nearest because $\\Wb\\in \\cone_{\\mu}(\\ps)$ and \\eqref{cone-non-nei} holds. Recall that $\\s_i=\\sft{\\RR\\hb_i}$ where $\\RR=\\tf{\\W}\\Theta \\geq R\\Theta$. To proceed, let $ \\underline{\\hb}_i:=\\min_{t \\in\\mc{T}_i}\\hb_{i1}-\\hb_{it}$,\n\\begin{align*}\n\\mc{I}:=\\left\\{ i\\in[n]: \\underline{\\hb}_i \\leq 1-2\\nu \\right\\}, \\qquad [n]-\\mc{I}:=\\left\\{ i\\in[n]:  1-2\\nu  <  \\underline{\\hb}_i\\right\\}.\n\n\n\\end{align*}\nFor all $ i \\in [n]-\\mc{I}$,\n\\begin{equation}\\label{eqn:grad:difff2}\n\\begin{split}\n      \\sum_{t\\in \\Tc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it}) &- (1+0.5\\pi) \\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n      & \\leq  \\left(2A - (1+0.5\\pi)\\right)\\Gamma\\sum_{t\\in \\Tc_i,~\\hb_{i1}-\\hb_{it} \\geq 1+\\frac{\\pi}{2} } \\s_{it} \\\\\n      & \\leq  \\left(2A - (1+0.5\\pi)\\right)\\Gamma Te^{-\\RR(1+\\frac{\\pi}{2})} \\\\\n      &\\leq   2A\\Gamma  T e^{-\\RR(1+\\frac{\\pi}{2})}.\n      \n      \n      \n\\end{split}\n\\end{equation}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor all $ i \\in \\mc{I}$, split the tokens into two groups: Let $\\Nc_i$ be the group of tokens obeying $ \\hb_{i1}-\\hb_{it} \\leq 1-\\nu$ and $\\Tc_i-\\Nc_i$ be the rest of the neighbors. Observe that\n\\[\n\\frac{\\sum_{t\\in \\Tc_i-\\Nc_i}\\s_{it}}{\\sum_{t\\in\\Tc_i}\\s_{it}}\\leq  T\\frac{e^{\\nu \\RR}}{e^{2\\nu\\RR}}=Te^{-\\RR\\nu}.\n\\]\nUsing $|\\hb_{i1}-\\hb_{it}|\\leq 2A=2 \\max_{i\\in[n],t\\in[T]}\\tn{\\kb_{it}}/\\Theta$ and  $\\bggm=\\min_{i\\in[n]}\\bgg_i =\\min_{i\\in[n]} (\\bgam_{i1}-\\max_{t\\in\\Tc_i}\\bgam_{it})$, observe that \n\\[\n\\sum_{t\\in\\Tc_i-\\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq \\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm} \\sum_{t\\in \\Tc_i} \\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\]\nThus, \n\\begin{align*}\n  \\sum_{t\\in \\Tc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})&= \\sum_{t\\in \\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})+\\sum_{t\\in\\Tc_i-\\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\nonumber\\\\\n\n  &\\leq \\sum_{t\\in \\Nc_i} (1-\\nu)\\s_{it}(\\bgam_{i1}-\\bgam_{it})+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm} \\sum_{t\\in \\Tc_i} \\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n  &\\leq \\left(1-\\nu+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}\\right)\\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n &\\leq \\left(1+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}\\right)\\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align*}\nHence, choosing \n\\begin{align}\nR\\geq\\frac{1}{\\nu\\Theta}\\log\\left(\\frac{8\\Gamma AT}{\\bggm\\pi}\\right)\\label{R bound pi 1}\n\\end{align}\nresults in that\n\\begin{equation}\\label{eqn:grad:difff3}\n    \\begin{split}\n     &\\sum_{t\\in \\Tc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})  - (1+\\frac{\\pi}{2}) \\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}) \\\\\n   &\\leq\\left(\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}-\\frac{\\pi}{2}\\right)\\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n   &\\leq -\\frac{\\pi}{4}\\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n   &\\leq-\\frac{\\pi}{4T}\\bggm  e^{-\\bar{R} (1-2\\nu)}.      \n    \\end{split}\n\\end{equation}\nHere, the last inequality follows from the fact that $\\sum_{t\\in \\Tc_i}\\s_{it}\\geq \\max_{t\\in\\Tc_i}s_{it}\\geq\\frac{e^{-\\bar{R}(1-2\\nu)}}{\\sum_{t=1}^Te^{-\\bar{R}(\\hb_{i1}-\\hb_{it})}}\\geq e^{-\\bar{R}(1-2\\nu)}/T$.\n\n\nFrom Assumption~\\ref{assum:loss:prope}, we have $c_{\\min}\\leq-\\ell'\\leq c_{\\max}$ for some positive constants $c_{\\min}$ and $c_{\\max}$. It follows from  \\eqref{eqn:grad:difff2} and \\eqref{eqn:grad:difff3} that \n\\begin{align*}\n-\\frac{1}{n}\\sum_{i}^n \\ell_i' \\cdot&\\left(\n      \\sum_{t\\in \\Tc_i} (\\hb_{i1}-_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})- \\sum_{t\\in \\Tc_i} (1+0.5\\pi)\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\right)\\\\\n      & \\leq    c_{\\max}2A\\Gamma  T \\Gamma e^{-\\RR(1 +\\frac{\\pi}{2})}-\\frac{c_{\\min}}{nT}\\cdot\\frac{\\pi\\bggm}{4}e^{-\\bar{R} (1-2\\nu)}\\\\\n      \n      & \\leq 0.\n\\end{align*}\nCombing with \\eqref{R bound pi 1}, this is guaranteed by \nchoosing \n\n\n\n\\[\n  R\\geq \\max\\left\\{\\frac{1}{\\nu\\Theta}\\log\\left(\\frac{8\\Gamma AT}{\\bggm\\pi}\\right),\\frac{1}{(2\\nu+\\pi/2)\\Theta}\\log\\left(\\frac{8n\\Gamma AT^2 c_{\\max}}{c_{\\min}\\bggm\\pi}\\right)\\right\\},\n\\]\nwhere $\\nu=\\nu(\\frac{\\pi}{4A\\Theta})$ depends only on $\\pi$ and global problem variables. \n\nCombining this with the prior $R$ choice \\eqref{R boundC0} (by taking maximum), we conclude with the statement.\n\n\n\\end{proof}",
                            "statement_html": "Let $R=R_{\\pi}$, $\\Wb=\\tf{\\Wm} \\W/\\tf{\\W} $, $\\hb_i=\\X_i\\Wb \\z_{i}$, and $\\hbm_i= \\X_i \\Wm \\z_{i}$. To establish the result, we will prove that, for sufficiently large $R$ and for any $\\W\\in \\Cc_{\\mu,R}(\\Wm)$:\n\\begin{align}\\label{main local cond}\n\\nonumber \n\\li -\\nabla\\Lc(\\W),\\frac{\\W}{\\tf{\\W}}\\ri&= -\\frac{1}{n}\\sum_{i=1}^n\\ell'_i \\cdot  \\li \\hb_i, \\sfp{\\X_i\\W \\z_{i}}\\bgam_i\\ri\\\\\n&\\leq - \\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot  \\li\\hbm_i, \\sfp{\\X_i\\W \\z_{i}}\\bgam_i\\ri=(1+\\pi)\\li-\\nabla\\Lc(\\W), \\frac{\\ps}{\\tf{\\Ws}}\\ri.\n\\end{align}\n\nFollowing (67) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.D.67\">original paper</a>], for all $\\W\\in \\Sc_{\\mu}(\\Wm)$ with $\\tf{\\W}=\\tf{\\Wm}$, $\\hp=\\X\\W \\z$, and $\\s=\\sft{\\hp}$, we have found\n\\begin{align}\n  \\big|\\hp^\\top_i\\diag{\\s_i}\\bgam_i-\\hp^\\top_i\\s_i\\s^\\top_i\\bgam_i-\\sum_{t\\in \\Tc_i} (\\hp_{i1}-\\hp_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\big|\\leq 2\\Gamma A((1-\\s_{i1})^2+Q_i), \n\\end{align}\nwhere $\\Tc_i$ is the set of support indices. \n\nPlugging in $\\hb,\\hbm$ in the bound above and assuming $\\pi\\leq 1$ (w.l.o.g.), (74) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.D.74\">original paper</a>] is implied by the following stronger inequality\n\\begin{align*}\n-\\frac{1}{n}&\\sum_{i=1}^n\\ell'_i \\cdot \\left(6\\Gamma A((1-\\s_{i1})^2+Q_i)+ \\sum_{t\\in \\Tc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it}) \\right)\\\\\n&\\leq -\\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i  \\cdot \\sum_{t\\in \\Tc_i} (\\hbm_{i1}-\\hbm_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n&=-\\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align*}\nFirst, we claim that $0.5\\pi\\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\geq 6\\Gamma A((1-\\s_{i1})^2+Q_i)$ for all $i \\in [n]$. The proof of this claim directly follows the earlier argument, namely, following (69) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.D.71\">original paper</a>], (70) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.D.70\">original paper</a>], and (71) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.D.71\">original paper</a>] which leads to the choice \n\\begin{equation}\\label{R boundC0}\nR \\ge\\frac{\\max(2,\\delta^{-1})}{\\Theta}\\log\\left(\\frac{C_0\\cdot T\\Gamma A}{\\pi\\bggm}\\right),    \n\\end{equation}\nfor some constant $C_0>0$. Using (71) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.D.71\">original paper</a>], we choose $C_0 \\geq 64 \\pi$ to guarantee $R=R_\\pi \\geq \\bar{R}_{\\mu}$.\n\nFollowing this control over the perturbation term $6\\Gamma A((1-\\s_{i1})^2+Q_i)$, to conclude with the result, what remains is proving the comparison\n\\begin{align}\\label{desired comp}\n-\\frac{1}{n} \\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\in \\Tc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq - \\frac{1+0.5\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align}\nTo proceed, we split the problem into two scenarios. \n<br>\n<b>Scenario 1:</b> $\\tf{\\Wb-\\Wm}\\leq \\eps=\\frac{\\pi}{4A\\Theta}$ for some $\\eps>0$. In this scenario, for any $ t\\in \\Tc_i$ and $i\\in [n ]$, we have\n\\[\n|\\hb_{it}-\\hbm_{it}|=|\\x_{it}^\\top (\\Wb-\\Wm)  \\z_{it}|\\leq A\\Theta\\eps=\\frac{\\pi}{4}.\n\\]\nConsequently, we obtain \n\\[\n\\hb_{i1}-\\hb_{it}\\leq \\hbm_{i1}-\\hbm_{it}+2A\\Theta\\eps= 1+0.5\\pi.\n\\] \nSimilarly, $\\hb_{i1}-\\hb_{it}\\geq 1-0.5\\pi\\geq 0.5$. Since all terms $\\hb_{i1}-\\hb_{it},\\s_{it},\\bgam_{i1}-\\bgam_{it}$ in \\eqref{desired comp} are nonnegative and $(\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq (1+0.5\\pi)\\s_{it}(\\bgam_{i1}-\\bgam_{it})$, above implies the desired result in \\eqref{desired comp}.\n\n<br>\n<b>Scenario 2:</b> $\\tf{\\Wb-\\Wm}\\geq \\eps=\\frac{\\pi}{4A\\Theta}$. Since $\\Wb$ is not (locally) max-margin, in this scenario, for some $i \\in  [n]$, $\\nu=\\nu(\\eps)>0$, and $\\tau\\in\\Tc_i$, we have that\n\\begin{align*}\n\\hb_{i1}-\\hb_{i\\tau}\\leq 1-2\\nu.\n\\end{align*}\nHere $\\tau=\\arg\\max_{\\tau\\in\\Tc_i} \\x_{i\\tau}\\Wb \\z_i$ denotes the nearest point to $\\hb_{i1}$ (along the $\\Wb$ direction). Note that a non-neighbor $t\\in\\Tcb_i$ cannot be nearest because $\\Wb\\in \\cone_{\\mu}(\\ps)$ and (64) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.D.64\">original paper</a>] holds. Recall that $\\s_i=\\sft{\\RR\\hb_i}$ where $\\RR=\\tf{\\W}\\Theta \\geq R\\Theta$. To proceed, let $ \\underline{\\hb}_i:=\\min_{t \\in\\mc{T}_i}\\hb_{i1}-\\hb_{it}$,\n\\begin{align*}\n\\mc{I}:=\\left\\{ i\\in[n]: \\underline{\\hb}_i \\leq 1-2\\nu \\right\\}, \\qquad [n]-\\mc{I}:=\\left\\{ i\\in[n]:  1-2\\nu  <  \\underline{\\hb}_i\\right\\}.\n\\end{align*}\nFor all $ i \\in [n]-\\mc{I}$,\n\\begin{equation}\\label{eqn:grad:difff2}\n\\begin{split}\n      \\sum_{t\\in \\Tc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it}) &- (1+0.5\\pi) \\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n      & \\leq  \\left(2A - (1+0.5\\pi)\\right)\\Gamma\\sum_{t\\in \\Tc_i,~\\hb_{i1}-\\hb_{it} \\geq 1+\\frac{\\pi}{2} } \\s_{it} \\\\\n      & \\leq  \\left(2A - (1+0.5\\pi)\\right)\\Gamma Te^{-\\RR(1+\\frac{\\pi}{2})} \\\\\n      &\\leq   2A\\Gamma  T e^{-\\RR(1+\\frac{\\pi}{2})}.\n\\end{split}\n\\end{equation}\n\nFor all $ i \\in \\mc{I}$, split the tokens into two groups: Let $\\Nc_i$ be the group of tokens obeying $ \\hb_{i1}-\\hb_{it} \\leq 1-\\nu$ and $\\Tc_i-\\Nc_i$ be the rest of the neighbors. Observe that\n\\[\n\\frac{\\sum_{t\\in \\Tc_i-\\Nc_i}\\s_{it}}{\\sum_{t\\in\\Tc_i}\\s_{it}}\\leq  T\\frac{e^{\\nu \\RR}}{e^{2\\nu\\RR}}=Te^{-\\RR\\nu}.\n\\]\nUsing $|\\hb_{i1}-\\hb_{it}|\\leq 2A=2 \\max_{i\\in[n],t\\in[T]}\\tn{\\kb_{it}}/\\Theta$ and  $\\bggm=\\min_{i\\in[n]}\\bgg_i =\\min_{i\\in[n]} (\\bgam_{i1}-\\max_{t\\in\\Tc_i}\\bgam_{it})$, observe that \n\\[\n\\sum_{t\\in\\Tc_i-\\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\leq \\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm} \\sum_{t\\in \\Tc_i} \\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\]\nThus, \n\\begin{align*}\n  \\sum_{t\\in \\Tc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})&= \\sum_{t\\in \\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})+\\sum_{t\\in\\Tc_i-\\Nc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\nonumber\\\\\n\n  &\\leq \\sum_{t\\in \\Nc_i} (1-\\nu)\\s_{it}(\\bgam_{i1}-\\bgam_{it})+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm} \\sum_{t\\in \\Tc_i} \\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n  &\\leq \\left(1-\\nu+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}\\right)\\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n &\\leq \\left(1+\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}\\right)\\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}).\n\\end{align*}\nHence, choosing \n\\begin{align}\nR\\geq\\frac{1}{\\nu\\Theta}\\log\\left(\\frac{8\\Gamma AT}{\\bggm\\pi}\\right)\\label{R bound pi 1}\n\\end{align}\nresults in that\n\\begin{align}\\label{eqn:grad:difff3}\n    \n     &\\sum_{t\\in \\Tc_i} (\\hb_{i1}-\\hb_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})  - (1+\\frac{\\pi}{2}) \\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it}) \\\\\n   &\\leq\\left(\\frac{2\\Gamma A Te^{-\\RR\\nu}}{\\bggm}-\\frac{\\pi}{2}\\right)\\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n   &\\leq -\\frac{\\pi}{4}\\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\\\\n   &\\leq-\\frac{\\pi}{4T}\\bggm  e^{-\\bar{R} (1-2\\nu)}.      \n    \n\\end{align}\nHere, the last inequality follows from the fact that $\\sum_{t\\in \\Tc_i}\\s_{it}\\geq \\max_{t\\in\\Tc_i}s_{it}\\geq\\frac{e^{-\\bar{R}(1-2\\nu)}}{\\sum_{t=1}^Te^{-\\bar{R}(\\hb_{i1}-\\hb_{it})}}\\geq e^{-\\bar{R}(1-2\\nu)}/T$.\n\nFrom Assumption A [in <a href=\"https://arxiv.org/pdf/2308.16898#assumption.1\">original paper</a>], we have $c_{\\min}\\leq-\\ell'\\leq c_{\\max}$ for some positive constants $c_{\\min}$ and $c_{\\max}$. It follows from  \\eqref{eqn:grad:difff2} and \\eqref{eqn:grad:difff3} that \n\\begin{align*}\n-\\frac{1}{n}\\sum_{i}^n \\ell_i' \\cdot&\\left(\n      \\sum_{t\\in \\Tc_i} (\\hb_{i1}-_{it})\\s_{it}(\\bgam_{i1}-\\bgam_{it})- \\sum_{t\\in \\Tc_i} (1+0.5\\pi)\\s_{it}(\\bgam_{i1}-\\bgam_{it})\\right)\\\\\n      & \\leq    c_{\\max}2A\\Gamma  T \\Gamma e^{-\\RR(1 +\\frac{\\pi}{2})}-\\frac{c_{\\min}}{nT}\\cdot\\frac{\\pi\\bggm}{4}e^{-\\bar{R} (1-2\\nu)}\\\\\n      \n      & \\leq 0.\n\\end{align*}\nCombing with \\eqref{R bound pi 1}, this is guaranteed by \nchoosing \n\n\\[\n  R\\geq \\max\\left\\{\\frac{1}{\\nu\\Theta}\\log\\left(\\frac{8\\Gamma AT}{\\bggm\\pi}\\right),\\frac{1}{(2\\nu+\\pi/2)\\Theta}\\log\\left(\\frac{8n\\Gamma AT^2 c_{\\max}}{c_{\\min}\\bggm\\pi}\\right)\\right\\},\n\\]\nwhere $\\nu=\\nu(\\frac{\\pi}{4A\\Theta})$ depends only on $\\pi$ and global problem variables. \n\nCombining this with the prior $R$ choice \\eqref{R boundC0} (by taking maximum), we conclude with the statement.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n<br>\n<br>1. <i>Initial Setup</i>: The proof begins by defining several variables and stating the goal. Specifically, it introduces \\( R = R_{\\pi} \\), \\(\\Wb = \\frac{\\tf{\\Wm} \\W}{\\tf{\\W}} \\), \\(\\hb_i = \\X_i \\Wb \\z_{i} \\), and \\(\\hbm_i = \\X_i \\Wm \\z_{i} \\). The aim is to show that for sufficiently large \\( R \\) and any \\(\\W \\in \\Cc_{\\mu,R}(\\Wm)\\):\n<br>   \\[\n   \\li -\\nabla\\Lc(\\W),\\frac{\\W}{\\tf{\\W}}\\ri \\leq (1+\\pi)\\li-\\nabla\\Lc(\\W), \\frac{\\ps}{\\tf{\\Ws}}\\ri.\n   \\]\n<br>\n<br>2. <i>Intermediate Bound</i>: Using previous results, the proof establishes an intermediate bound for all \\(\\W \\in \\Sc_{\\mu}(\\Wm)\\) with \\(\\tf{\\W} = \\tf{\\Wm}\\):\n<br>   \\[\n   \\big|\\hp^\\top_i\\diag{\\s_i}\\bgam_i - \\hp^\\top_i\\s_i\\s^\\top_i\\bgam_i - \\sum_{t\\in \\Tc_i} (\\hp_{i1} - \\hp_{it})\\s_{it}(\\bgam_{i1} - \\bgam_{it})\\big| \\leq 2\\Gamma A((1-\\s_{i1})^2 + Q_i).\n   \\]\n<br>\n<br>3. <i>Stronger Inequality</i>: The proof then aims to show a stronger inequality that implies the main local condition:\n<br>   \\[\n   -\\frac{1}{n}\\sum_{i=1}^n\\ell'_i \\cdot \\left(6\\Gamma A((1-\\s_{i1})^2 + Q_i) + \\sum_{t\\in \\Tc_i} (\\hb_{i1} - \\hb_{it})\\s_{it}(\\bgam_{i1} - \\bgam_{it}) \\right) \\leq -\\frac{1+\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\in \\Tc_i} (\\hbm_{i1} - \\hbm_{it})\\s_{it}(\\bgam_{i1} - \\bgam_{it}).\n   \\]\n<br>\n<br>4. <i>Claim and Bound</i>: The proof claims that \\(0.5\\pi\\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1} - \\bgam_{it}) \\geq 6\\Gamma A((1-\\s_{i1})^2 + Q_i)\\) for all \\(i \\in [n]\\). This is supported by choosing:\n<br>   \\[\n   R \\ge \\frac{\\max(2, \\delta^{-1})}{\\Theta}\\log\\left(\\frac{C_0 \\cdot T \\Gamma A}{\\pi \\bggm}\\right),\n   \\]\n<br>   for some constant \\(C_0 > 0\\).\n<br>\n<br>5. <i>Comparison</i>: The proof then focuses on proving the comparison:\n<br>   \\[\n   -\\frac{1}{n} \\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\in \\Tc_i} (\\hb_{i1} - \\hb_{it})\\s_{it}(\\bgam_{i1} - \\bgam_{it}) \\leq - \\frac{1+0.5\\pi}{n}\\sum_{i=1}^n\\ell'_i \\cdot \\sum_{t\\in \\Tc_i}\\s_{it}(\\bgam_{i1} - \\bgam_{it}).\n   \\]\n<br>\n<br>6. <i>Scenario Analysis</i>: The proof splits into two scenarios:\n<br>   <b>Scenario 1:</b> \\(\\tf{\\Wb - \\Wm} \\leq \\eps = \\frac{\\pi}{4A\\Theta}\\). Here, it shows that:\n<br>   \\[\n   \\hb_{i1} - \\hb_{it} \\leq \\hbm_{i1} - \\hbm_{it} + 2A\\Theta\\eps = 1 + 0.5\\pi.\n   \\]\n<br>   <b>Scenario 2:</b> \\(\\tf{\\Wb - \\Wm} \\geq \\eps = \\frac{\\pi}{4A\\Theta}\\). Here, it shows that for some \\(i \\in [n]\\) and \\(\\tau \\in \\Tc_i\\):\n<br>   \\[\n   \\hb_{i1} - \\hb_{i\\tau} \\leq 1 - 2\\nu.\n   \\]\n<br>\n<br>7. <i>Final Bound</i>: The proof combines the results from both scenarios and chooses:\n<br>   \\[\n   R \\geq \\max\\left\\{\\frac{1}{\\nu\\Theta}\\log\\left(\\frac{8\\Gamma AT}{\\bggm\\pi}\\right), \\frac{1}{(2\\nu + \\pi/2)\\Theta}\\log\\left(\\frac{8n\\Gamma AT^2 c_{\\max}}{c_{\\min}\\bggm\\pi}\\right)\\right\\},\n   \\]\n<br>   ensuring the desired inequality holds.\n<br>\n<br>Thus, the proof concludes by showing that for sufficiently large \\(R\\), the main local condition is satisfied."
                        }
                    },
                    {
                        "statement_id": "a632ebee-b8f8-4fec-8945-2c8e11909cdd",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 36,
                        "library_name": "Lemma 36",
                        "title": "Gradient Cone Lemma",
                        "statement_original_tex": "\\begin{lemma}[Gradient Condition for Optimal Tokens]\\label{glocal cond} \nSuppose Assumption~\\ref{assum:loss:prope} holds and let $\\op=(\\op_i)_{i=1}^n$ be the unique globally-optimal indices with $\\Wm$ denoting the SVM solution. Define the margin $\\Theta=1/\\tf{\\Ws}$. \n\n\n\nGiven $\\mu\\geq 0$, consider the following subset of the sphere and its associated cone\n\n\n\n\n\\begin{subequations}\\label{eqn:con:nabla0}\n\\begin{align}\n&\\Sc_{\\mu}=\\left\\{\\W~\\Big|~  \\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\frac{\\W}{\\tf{\\W}}\\ri\\geq \\mu\\cdot\\Theta\\quad \\textnormal{for all}\\quad t\\neq \\op_i, \\quad  i\\in[n]\\right\\},\\\\\n&\\conb_{\\mu,R}=\\left\\{  \\W\\in\\Sc_\\mu ~\\Big|~   \\tf{\\W}\\geq R\\right\\}.\n\\end{align}\n\\end{subequations}\n\nFor any $\\mu>0$, there exists sufficiently large $R=R_\\mu=\\order{1/\\mu}$ (see \\eqref{R bound2}) such that:\n\\begin{enumerate}[label={\\textnormal{\\textbf{L\\arabic*.}}}, wide, labelwidth=!,itemindent=!, labelindent=5pt]\n\\item \\label{lem:gcond:l1} There is no stationary point within  $ \\conb_{\\mu,R}$.\n\n\\item\\label{lem:gcond:l2}  Let $s_i=\\sft{\\X_i\\W\\z_i}$. For all $\\V\\in \\Sc_{\\mu},\\W\\in\\conb_{\\mu,R}$, there exist $C,c>0$ such that \n\\begin{align*}\nC\\cdot\\max_{i\\in[n]}  \\left(1-s_{i\\op_i}\\right) \\geq -\\li\\nabla\\Lc(\\W),\\V\\ri\\geq c\\cdot\\mu\\cdot\\min_{i\\in[n]}  \\left(1-s_{i\\op_i}\\right).\n\\end{align*}\n\\end{enumerate}\n\\end{lemma}",
                        "statement_html": "Suppose Assumption A [in <a href=\"https://arxiv.org/pdf/2308.16898#assumption.1\">original paper</a>] holds and let $\\op=(\\op_i)_{i=1}^n$ be the unique globally-optimal indices with $\\Wm$ denoting the SVM solution. Define the margin $\\Theta=1/\\tf{\\Ws}$.\n\nGiven $\\mu\\geq 0$, consider the following subset of the sphere and its associated cone\n\n\\begin{subequations}\\label{eqn:con:nabla0}\n\\begin{align}\n&\\Sc_{\\mu}=\\left\\{\\W~\\Big|~  \\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\frac{\\W}{\\tf{\\W}}\\ri\\geq \\mu\\cdot\\Theta\\quad \\textnormal{for all}\\quad t\\neq \\op_i, \\quad  i\\in[n]\\right\\},\\\\\n&\\conb_{\\mu,R}=\\left\\{  \\W\\in\\Sc_\\mu ~\\Big|~   \\tf{\\W}\\geq R\\right\\}.\n\\end{align}\n\\end{subequations}\n\nFor any $\\mu>0$, there exists sufficiently large $R=R_\\mu=\\order{1/\\mu}$ (see (41) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.C.41\">original paper</a>]) such that:\n<ol>\n    <li><b>L1.</b> There is no stationary point within $\\conb_{\\mu,R}$.</li>\n    <li><b>L2.</b> Let $s_i=\\sft{\\X_i\\W\\z_i}$. For all $\\V\\in \\Sc_{\\mu},\\W\\in\\conb_{\\mu,R}$, there exist $C,c>0$ such that \n    \\begin{align*}\n    C\\cdot\\max_{i\\in[n]}  \\left(1-s_{i\\op_i}\\right) \\geq -\\li\\nabla\\Lc(\\W),\\V\\ri\\geq c\\cdot\\mu\\cdot\\min_{i\\in[n]}  \\left(1-s_{i\\op_i}\\right).\n    \\end{align*}\n    </li>\n</ol>",
                        "statement_type": "lemma",
                        "statement_motivation_html": "Understanding the properties of the subset $\\Sc_{\\mu}$ and its associated cone $\\conb_{\\mu,R}$ is crucial in the context of Support Vector Machines (SVM) and optimization. These definitions help in identifying regions where no stationary points exist, which is essential for ensuring convergence to a global optimum. Specifically, the conditions provided in L1 and L2 offer insights into the behavior of the gradient and the margin, guiding the selection of parameters $\\mu$ and $R$ to achieve desired optimization outcomes. This is particularly useful when dealing with large-scale machine learning problems where efficient and effective optimization is key.",
                        "html_url": "library/lemmas/lemma_36/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "38b8cc7a-c311-4169-8f66-36a0c710d05e",
                            "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof} Let us introduce the norm upper bound\n\\begin{equation}\\label{mu choice2}\n\\begin{split}\n\n&A=\\max_{i\\in[n],t,\\tau\\in[T]} \\frac{(\\tn{\\x_{it}}\\vee\\tn{\\x_{it}-\\x_{i\\tau}})\\cdot\\tn{\\z_i}}{\\Theta}.\n\\end{split}\n\\end{equation}\n\nThe following inequalities hold for all $\\V\\in \\Sc_{\\mu},~\\tf{\\V}=\\tf{\\Wm}$ and all $i\\in[n], t\\neq \\op_i$:\n\\begin{equation}\\label{cone-A-eq}\n\\begin{split}\n\n\nA\\geq(\\x_{i\\op_i}-\\x_{it})^\\top \\V \\z_i &\\geq \\mu.\n\\end{split}\n\\end{equation}\n\n \n\nTo proceed, we write the gradient correlation following \\eqref{grad def} and \\eqref{grad def2}\n\\begin{align}\\label{grad def32}\n\\li\\nabla\\Lc(\\W),\\V\\ri&=\\frac{1}{n}\\sum_{i=1}^n\\ell'_i\\cdot\\li\\hb_i,\\sfp{\\hp_i}\\bgam_i\\ri,\n\\end{align}\nwhere we denoted $\\ell'_i=\\ell'(Y_i\\cdot \\vb^\\top \\X_i^\\top\\sft{\\hp_i})$, $\\hb_i=\\X_i\\V \\z_{i}$, $\\hp_i= \\X_i\\W \\z_{i}$, $\\s_i=\\sft{\\hp_i}$. It follows from \\eqref{mu choice2} that $A\\geq \\max_{i\\in[n],t\\in[T]}\\tn{\\hb_{it}}$. Using \\eqref{cone-A-eq}, we can bound the softmax probabilities $\\s_i=\\sft{\\hp_i}$ as follows, for all $i\\in[n]$:\n\\begin{align}\\label{soft prob bound2}\n&S_i:= \\sum_{\\tau\\neq \\op_i}\\s_{i\\tau}\\leq T e^{-R\\mu\\Theta}\\s_{i\\op_i}\\leq T e^{-R\\mu\\Theta}.\n\\end{align}\n\nRecall scores $\\bgam_{it}=Y_i\\cdot\\vb^\\top \\x_{it}$. Define the score gaps:\n\n\\begin{equation*}\n \\bgg_i=\\bgam_{i\\op_i}-\\max_{t\\neq\\op_i}\\bgam_{it},~~~ \\bgm_i=\\bgam_{i\\op_i}-\\min_{t\\neq\\op_i}\\bgam_{it},~~~\\textnormal{and}~~~\\Gamma=\\sup_{i\\in[n],t,\\tau\\in[T]}|\\bgam_{it}-\\bgam_{i\\tau}|. \n\\end{equation*}\nLet us focus on a fixed datapoint $i\\in[n]$, assume (without losing generality) $\\op_i=1$, and drop subscripts $i$.\n\nDirectly applying Lemma \\ref{lem:q_reduce}, we obtain\n\\[\n  \\big|\\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam-\\sum_{t\\geq 2}^T (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2\\Gamma A(1-\\s_1)^2.\n\\]\n\\noindent To proceed, let us upper/lower bound the gradient correlation. Since $A\\geq \\hb_1-\\hb_t\\geq \\mu>0$ from \\eqref{cone-A-eq}, setting $S:=\\sum_{t\\neq\\op_i}\\s_t=1-\\s_1$, we find\n\\begin{equation}\n A\\cdot S\\cdot \\bgm  \\geq\\sum_{t\\neq\\op} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\geq \\mu\\cdot S\\cdot \\bgg.\\label{aggregate2}\n\\end{equation}\nNext we show that $S=1-\\s_1$ dominates $(1-\\s_1)^2=S^2$ for large $R$. Specifically, we wish for \n\\begin{align}\\label{wishfor2}\n\\mu S \\bgg/2\\geq 2\\Gamma A(1-\\s_1)^2\\iff S\\geq \\frac{4}{\\mu}\\frac{\\Gamma A}{\\bgg}S^2\\iff S\\leq \\frac{\\mu\\bgg}{4\\Gamma A}.\n\\end{align}\nUsing \\eqref{soft prob bound2}, what we wish is ensured for all $i\\in[n]$, by guaranteeing $Te^{-R\\mu\\Theta}\\leq \\frac{\\mu\\bgg}{4\\Gamma A}$. That is, by choosing\n\\begin{align}\\label{R bound2}\nR:=R_\\mu= \\frac{1}{\\mu\\Theta}\\log\\big(\\frac{4T\\Gamma A}{\\mu\\bggm}\\big),\n\\end{align}\nwhere $\\bggm=\\sup_{i\\in[n]}\\bgg_i$ is the global scalar corresponding to the worst case score gap over all inputs. \n\nWith the above choice of $R$, we guaranteed\n\\[\n  2 A(1-\\s_1)\\cdot \\bgm\\geq 2A\\cdot S\\cdot \\bgm \\geq \\sum_{t\\neq\\op} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\geq\\frac{\\mu\\cdot S\\cdot \\bgg}{2}\\geq\\frac{\\mu(1-\\s_1) \\bgg}{2}.\n\\]\nvia \\eqref{wishfor2} and \\eqref{aggregate2}. Since this holds over all inputs, going back to the gradient correlation \\eqref{grad def32} and averaging above over all inputs $i\\in[n]$ and plugging back the indices $i$, we obtain the advertised bound by setting $q_i=1-\\s_{i\\op_i}$ (where we have used $\\op_i=1$ above, without losing generality)\n\\begin{align}\\label{pbb corr2}\n  \\frac{2A}{n}\\sum_{i\\in [n]} \\ell'_i\\cdot q_i\\cdot \\bgm_i\\geq \\li\\nabla\\Lc(\\W),\\V\\ri\\geq \\frac{\\mu}{2n}\\sum_{i\\in [n]} \\ell'_i\\cdot q_i\\cdot \\bgg_i.\n\\end{align}\n\\end{proof}",
                            "statement_html": "Let us introduce the norm upper bound\n\\begin{equation}\\label{mu choice2}\n\\begin{split}\n&A=\\max_{i\\in[n],t,\\tau\\in[T]} \\frac{(\\tn{\\x_{it}}\\vee\\tn{\\x_{it}-\\x_{i\\tau}})\\cdot\\tn{\\z_i}}{\\Theta}.\n\\end{split}\n\\end{equation}\n\nThe following inequalities hold for all $\\V\\in \\Sc_{\\mu},~\\tf{\\V}=\\tf{\\Wm}$ and all $i\\in[n], t\\neq \\op_i$:\n\\begin{equation}\\label{cone-A-eq}\n\\begin{split}\nA\\geq(\\x_{i\\op_i}-\\x_{it})^\\top \\V \\z_i &\\geq \\mu.\n\\end{split}\n\\end{equation}\n\nTo proceed, we write the gradient correlation following (15) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.B.15\">original paper</a>] and (27) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.C.27\">original paper</a>]\n\\begin{align}\\label{grad def32}\n\\li\\nabla\\Lc(\\W),\\V\\ri&=\\frac{1}{n}\\sum_{i=1}^n\\ell'_i\\cdot\\li\\hb_i,\\sfp{\\hp_i}\\bgam_i\\ri,\n\\end{align}\nwhere we denoted $\\ell'_i=\\ell'(Y_i\\cdot \\vb^\\top \\X_i^\\top\\sft{\\hp_i})$, $\\hb_i=\\X_i\\V \\z_{i}$, $\\hp_i= \\X_i\\W \\z_{i}$, $\\s_i=\\sft{\\hp_i}$. It follows from \\eqref{mu choice2} that $A\\geq \\max_{i\\in[n],t\\in[T]}\\tn{\\hb_{it}}$. Using \\eqref{cone-A-eq}, we can bound the softmax probabilities $\\s_i=\\sft{\\hp_i}$ as follows, for all $i\\in[n]$:\n\\begin{align}\\label{soft prob bound2}\n&S_i:= \\sum_{\\tau\\neq \\op_i}\\s_{i\\tau}\\leq T e^{-R\\mu\\Theta}\\s_{i\\op_i}\\leq T e^{-R\\mu\\Theta}.\n\\end{align}\n\nRecall scores $\\bgam_{it}=Y_i\\cdot\\vb^\\top \\x_{it}$. Define the score gaps:\n\n\\begin{equation*}\n \\bgg_i=\\bgam_{i\\op_i}-\\max_{t\\neq\\op_i}\\bgam_{it},~~~ \\bgm_i=\\bgam_{i\\op_i}-\\min_{t\\neq\\op_i}\\bgam_{it},~~~\\textnormal{and}~~~\\Gamma=\\sup_{i\\in[n],t,\\tau\\in[T]}|\\bgam_{it}-\\bgam_{i\\tau}|. \n\\end{equation*}\nLet us focus on a fixed datapoint $i\\in[n]$, assume (without losing generality) $\\op_i=1$, and drop subscripts $i$.\n\nDirectly applying <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_29/index.html#lem%3Aq_reduce\">Lemma 29</a>, we obtain\n\\[\n  \\big|\\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam-\\sum_{t\\geq 2}^T (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2\\Gamma A(1-\\s_1)^2.\n\\]\nTo proceed, let us upper/lower bound the gradient correlation. Since $A\\geq \\hb_1-\\hb_t\\geq \\mu>0$ from \\eqref{cone-A-eq}, setting $S:=\\sum_{t\\neq\\op_i}\\s_t=1-\\s_1$, we find\n\\begin{equation}\n A\\cdot S\\cdot \\bgm  \\geq\\sum_{t\\neq\\op} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\geq \\mu\\cdot S\\cdot \\bgg.\\label{aggregate2}\n\\end{equation}\nNext we show that $S=1-\\s_1$ dominates $(1-\\s_1)^2=S^2$ for large $R$. Specifically, we wish for \n\\begin{align}\\label{wishfor2}\n\\mu S \\bgg/2\\geq 2\\Gamma A(1-\\s_1)^2\\iff S\\geq \\frac{4}{\\mu}\\frac{\\Gamma A}{\\bgg}S^2\\iff S\\leq \\frac{\\mu\\bgg}{4\\Gamma A}.\n\\end{align}\nUsing (38) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.C.38\">original paper</a>], what we wish is ensured for all $i\\in[n]$, by guaranteeing $Te^{-R\\mu\\Theta}\\leq \\frac{\\mu\\bgg}{4\\Gamma A}$. That is, by choosing\n\\begin{align}\\label{R bound2}\nR:=R_\\mu= \\frac{1}{\\mu\\Theta}\\log\\big(\\frac{4T\\Gamma A}{\\mu\\bggm}\\big),\n\\end{align}\nwhere $\\bggm=\\sup_{i\\in[n]}\\bgg_i$ is the global scalar corresponding to the worst case score gap over all inputs. \n\nWith the above choice of $R$, we guaranteed\n\\[\n  2 A(1-\\s_1)\\cdot \\bgm\\geq 2A\\cdot S\\cdot \\bgm \\geq \\sum_{t\\neq\\op} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\geq\\frac{\\mu(1-\\s_1) \\bgg}{2}.\n\\]\nvia (40) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.C.40\">original paper</a>] and \\eqref{aggregate2}. Since this holds over all inputs, going back to the gradient correlation \\eqref{grad def32} and averaging above over all inputs $i\\in[n]$ and plugging back the indices $i$, we obtain the advertised bound by setting $q_i=1-\\s_{i\\op_i}$ (where we have used $\\op_i=1$ above, without losing generality)\n\\begin{align}\\label{pbb corr2}\n  \\frac{2A}{n}\\sum_{i\\in [n]} \\ell'_i\\cdot q_i\\cdot \\bgm_i\\geq \\li\\nabla\\Lc(\\W),\\V\\ri\\geq \\frac{\\mu}{2n}\\sum_{i\\in [n]} \\ell'_i\\cdot q_i\\cdot \\bgg_i.\n\\end{align}",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n<br>\n<br>1. <i>Norm Upper Bound</i>: The proof starts by introducing an upper bound for the norm:\n<br>   \\[\n   A=\\max_{i\\in[n],t,\\tau\\in[T]} \\frac{(\\tn{\\x_{it}}\\vee\\tn{\\x_{it}-\\x_{i\\tau}})\\cdot\\tn{\\z_i}}{\\Theta}.\n   \\]\n<br>\n<br>2. <i>Inequalities for \\(\\V\\)</i>: It establishes inequalities for all \\(\\V\\in \\Sc_{\\mu}\\) and \\(i\\in[n], t\\neq \\op_i\\):\n<br>   \\[\n   A\\geq(\\x_{i\\op_i}-\\x_{it})^\\top \\V \\z_i \\geq \\mu.\n   \\]\n<br>\n<br>3. <i>Gradient Correlation</i>: The gradient correlation is written using the definitions:\n<br>   \\[\n   \\li\\nabla\\Lc(\\W),\\V\\ri=\\frac{1}{n}\\sum_{i=1}^n\\ell'_i\\cdot\\li\\hb_i,\\sfp{\\hp_i}\\bgam_i\\ri,\n   \\]\n<br>   where \\(\\ell'_i=\\ell'(Y_i\\cdot \\vb^\\top \\X_i^\\top\\sft{\\hp_i})\\), \\(\\hb_i=\\X_i\\V \\z_{i}\\), \\(\\hp_i= \\X_i\\W \\z_{i}\\), \\(\\s_i=\\sft{\\hp_i}\\).\n<br>\n<br>4. <i>Softmax Probabilities Bound</i>: Using the norm upper bound, the softmax probabilities are bounded:\n<br>   \\[\n   S_i:= \\sum_{\\tau\\neq \\op_i}\\s_{i\\tau}\\leq T e^{-R\\mu\\Theta}\\s_{i\\op_i}\\leq T e^{-R\\mu\\Theta}.\n   \\]\n<br>\n<br>5. <i>Score Gaps</i>: The score gaps are defined as:\n<br>   \\[\n   \\bgg_i=\\bgam_{i\\op_i}-\\max_{t\\neq\\op_i}\\bgam_{it},~~~ \\bgm_i=\\bgam_{i\\op_i}-\\min_{t\\neq\\op_i}\\bgam_{it},~~~\\textnormal{and}~~~\\Gamma=\\sup_{i\\in[n],t,\\tau\\in[T]}|\\bgam_{it}-\\bgam_{i\\tau}|.\n   \\]\n<br>\n<br>6. <i>Application of Lemma 29</i>: Applying Lemma 29, we get:\n<br>   \\[\n   \\big|\\hb^\\top\\diag{\\s}\\bgam-\\hb^\\top\\s\\s^\\top\\bgam-\\sum_{t\\geq 2}^T (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\big|\\leq 2\\Gamma A(1-\\s_1)^2.\n   \\]\n<br>\n<br>7. <i>Upper/Lower Bound Gradient Correlation</i>: The gradient correlation is bounded:\n<br>   \\[\n   A\\cdot S\\cdot \\bgm  \\geq\\sum_{t\\neq\\op} (\\hb_1-\\hb_t)\\s_t(\\bgam_1-\\bgam_t)\\geq \\mu\\cdot S\\cdot \\bgg.\n   \\]\n<br>\n<br>8. <i>Dominance of \\(S\\)</i>: It is shown that \\(S=1-\\s_1\\) dominates \\((1-\\s_1)^2\\) for large \\(R\\):\n<br>   \\[\n   S\\leq \\frac{\\mu\\bgg}{4\\Gamma A}.\n   \\]\n<br>\n<br>9. <i>Choice of \\(R\\)</i>: The choice of \\(R\\) is made to ensure the desired inequality:\n<br>   \\[\n   R:=R_\\mu= \\frac{1}{\\mu\\Theta}\\log\\big(\\frac{4T\\Gamma A}{\\mu\\bggm}\\big),\n   \\]\n<br>   where \\(\\bggm=\\sup_{i\\in[n]}\\bgg_i\\).\n<br>\n<br>10. <i>Final Bound</i>: With the chosen \\(R\\), the final bound is obtained:\n<br>   \\[\n   \\frac{2A}{n}\\sum_{i\\in [n]} \\ell'_i\\cdot q_i\\cdot \\bgm_i\\geq \\li\\nabla\\Lc(\\W),\\V\\ri\\geq \\frac{\\mu}{2n}\\sum_{i\\in [n]} \\ell'_i\\cdot q_i\\cdot \\bgg_i.\n   \\]"
                        }
                    },
                    {
                        "statement_id": "446ace68-6d7b-476b-8a21-a4bb0e9c9009",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 37,
                        "library_name": "Lemma 37",
                        "title": "Gradient Inequality for Optimal Tokens",
                        "statement_original_tex": "\\begin{lemma}[Gradient Condition for Optimal Tokens]\\label{lem:glocal:corr} \nSuppose Assumption~\\ref{assum:loss:prope} holds and let $\\op=(\\op_i)_{i=1}^n$ be the unique optimal tokens with $ \\Wm$ denoting the SVM solution. Fix any $\\mu>0$ (per Lemma \\ref{glocal cond}). For any choice of $\\pi>0$, there exists $R:=R_{\\pi,\\mu}$ such that, for any $ \\W\\in \\conb_{\\mu,R}(\\Wm)$ we have\n\\[\n \\li \\nabla\\Lc(\\W), \\frac{\\W}{\\tf{\\W}} \\ri\\geq (1+\\pi)\\li \\nabla\\Lc(\\W), \\frac{\\Wm}{\\tf{\\Wm}}\\ri.\n\\]\n\\end{lemma}",
                        "statement_html": "Suppose Assumption A [in <a href=\"https://arxiv.org/pdf/2308.16898#assumption.1\">original paper</a>] holds and let $\\op=(\\op_i)_{i=1}^n$ be the unique optimal tokens with $ \\Wm$ denoting the SVM solution. Fix any $\\mu>0$ (per <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_36/index.html#glocal+cond\">Lemma 36</a>). For any choice of $\\pi>0$, there exists $R:=R_{\\pi,\\mu}$ such that, for any $ \\W\\in \\conb_{\\mu,R}(\\Wm)$ we have\n\\[\n \\li \\nabla\\Lc(\\W), \\frac{\\W}{\\tf{\\W}} \\ri\\geq (1+\\pi)\\li \\nabla\\Lc(\\W), \\frac{\\Wm}{\\tf{\\Wm}}\\ri.\n\\]",
                        "statement_type": "lemma",
                        "statement_motivation_html": "This result is particularly useful in the context of Support Vector Machines (SVM) optimization. It provides a bound on the gradient of the loss function, ensuring that the optimal solution $\\Wm$ is robust within a certain neighborhood. This can be applied to guarantee the stability and performance of the SVM model under small perturbations, making it a valuable tool for both theoretical analysis and practical implementations of SVMs.",
                        "html_url": "library/lemmas/lemma_37/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "3310d9da-8b18-4af8-8115-c5b6aab27b57",
                            "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\nWe provide the proof in four steps:\\\\\n\\textbf{Step~1: There are no stationary points within $\\conb_{\\mu,R_\\mu}(\\Wm)$ for sufficiently large $R_\\mu$.} \nThis step directly follows from Lemma~\\ref{glocal cond}: for all $\\V,\\W\\in \\Sc_{\\mu}(\\Wm)$ with $\\tf{\\V}=\\tf{\\Wm}$ and $\\tf{\\W}\\geq R_\\mu$, it follows from Lemma~\\ref{glocal cond} that there exists  $R_\\mu$ such that $\\iprod{\\V}{ -\\nabla \\Lc(\\W)}$ is strictly positive.\n\\\\\n\\textbf{Step~2:}  Consider an arbitrary value of  $\\epsilon \\in (0, \\mu/2)$ and let $1/(1+\\pi)=1-\\epsilon$. It follows from Lemma~\\ref{lem:glocal:corr} that, there exists $R_\\epsilon$ such that all  $ \\W \\in \\conb_{\\mu,R_\\epsilon}(\\Wm)$ satisfy\n\\begin{align}\\label{eqn:neg:corr:local:nabla0}\n\\iprod{-\\nabla\\mc{L}(\\W)}      {\\frac{\\Wm}{\\tf{\\Wm}}} \\geq (1-\\epsilon)    \\iprod{-\\nabla \\mc{L}(\\W)}{\\frac{\\W}{\\tf{\\W}}}.\n\\end{align}\n\n\n\\\\\n\\textbf{Step~3: Updates remain inside the cone $\\conb_{\\mu,R_\\mu}(\\Wm)$.}  By leveraging the results from \\textbf{Step 1} and \\textbf{Step 2}, we demonstrate that the gradient iterates remain within this cone. Note that by our assumption and for sufficiently large $\\eta_0$, we get $\\W(1) \\in \\conb_{\\mu,R}(\\Wm)$. Here, $R$ is chosen such that \n\\begin{align}\\label{eqn:bound:R:nabla0}\nR \\geq R_\\mu \\vee R_\\epsilon \\vee 1/2.\n\\end{align}\n We provide the proof by induction. By the above argument, $\\W(1) \\in \\conb_{\\mu,R}(\\Wm)$. Let $\\W(k) \\in \\conb_{\\mu,R}(\\Wm)$. For all $k\\geq 1$, we have that \n\n\n\n\\begin{subequations}\\label{eqn:rho:def:nabla0}\n\\begin{align}\n& \\min_{t\\neq \\op_i,~i\\in[n]} ~~   \\left\\langle  (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top, - \\nabla \\mc{L}(\\W(k)) \\right\\rangle \\geq \\bar{\\rho} , \n\\label{eqn:bar:rho}\n\\\\\n &\\frac{1}{1-\\epsilon} \\iprod{ {\\frac{\\Wm}{\\tf{\\Wm}}}}{-\\nabla\\mc{L}(\\W(k))} \\geq \\rho.\n\\end{align}\n\\end{subequations}\nfor some positive constants $\\bar{\\rho}$ and $\\rho$.  \n\n\nTo proceed, for all $t\\neq \\op_i,~i\\in[n]$, we obtain \n\\begin{equation}\\label{eqn:localgd:1:nabla0}\n    \\begin{split}\n   \\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top,    \\frac{\\W(k+1)}{\\tf{\\W(k)}} \\right\\rangle &=   \\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top,    \\frac{\\W(k)}{\\tf{\\W(k)}} -\\frac{\\eta}{\\tf{\\W(k)}}\\nabla \\mc{L}(\\W(k)) \\right\\rangle\\\\   \n   &\\ge \\mu \\Theta- \\frac{\\eta}{\\tf{\\W(k)}} \\left\\langle   (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top, \\nabla \\mc{L}(\\W(k)) \\right\\rangle  \\\\\n\n      & \\geq \\mu \\Theta +\\frac{\\eta\\bar{\\rho}}{\\tf{\\W(k)}}.\n    \\end{split}\n\\end{equation}\nHere, the first inequality follows from the induction assumption $\\W(k) \\in \\conb_{\\mu,R}(\\Wm)$, and the second inequality uses \\eqref{eqn:bar:rho}.\n\nFrom Lemma~\\ref{glocal cond}, we have $\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle<0$~ which implies that $\\tf{\\W(k+1)} \\geq \\tf{\\W(k)}$. Hence, \n\\begin{align*}\n\\tf{\\W(k+1)}&\\leq\\frac{1}{{2\\tf{\\W(k)}}} \\left(\\tf{\\W(k+1)}^2+\\tf{\\W(k)}^2\\right)\\\\\n& = \\frac{1}{2\\tf{\\W(k)}} \\left(2\\tf{W(t)}^2-2\\eta\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle+\\eta^2\\tf{\\nabla \\Lc(\\W(k))}^2\\right)\\\\\n       &\\leq  \\tf{\\W(k)}- \\frac{\\eta}{\\tf{\\W(k)}}\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle + \\eta^2 \\|\\nabla \\Lc(\\W(k))\\|^2, \n\\end{align*}\nwhere the last inequality follows from $\\tf{\\W(k)} \\geq R \\geq 1/2 $. \n\nThus,\n\\begin{equation}\\label{eqn:localgd:2:nabla0}\n\\begin{split}\n  \\frac{\\tf{\\W(k+1)}}{\\tf{\\W(k)}}& \\leq  1- \\eta\n       \\left\\langle \\nabla \\Lc(\\W(k)),\\frac{\\W(k)}{\\tf{\\W(k)}} \\right\\rangle + \\eta^2 \\frac{\\|\\nabla \\mc{L}(\\W(k))\\|^2}{\\tf{\\W(k)}}\\\\\n& \\leq 1- \\frac{\\eta}{1-\\epsilon}  \\iprod{\\nabla\\mc{L}(\\W(k))}\n     {\\frac{\\Wm}{\\tf{\\Wm}}}+ \\eta^2 \\frac{\\|\\nabla \\mc{L}(\\W(k))\\|^2}{\\tf{\\W(k)}}\\\\\n      & \\leq  1 + \\frac{\\eta \\rho}{\\tf{\\W(k)}} + \\frac{\\eta^2\\|\\nabla \\mc{L}(\\W(k))\\|^2}{\\tf{\\W(k)}}=C_1(\\rho,\\eta).\n\\end{split}\n\\end{equation}\nNow, it follows from \\eqref{eqn:localgd:1:nabla0} and \\eqref{eqn:localgd:2:nabla0} that   \n\\begin{equation}\\label{eqn:localgd:3:nabla0}\n\\begin{split}\n\\min_{t\\neq \\op_i,~i\\in[n]} ~~  \\left\\langle  \\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top, \\frac{\\W(k+1)}{\\tf{\\W(k+1)}}\\right\\rangle   &\\geq \\frac{1}{C_1(\\bar{\\rho},\\eta)} \\left(\\mu \\Theta+\\frac{\\eta \\bar{\\rho}}{\\tf{\\W(k)}}\\right)\\\\\n& \\geq \\mu \\Theta+\\frac{\\eta}{C_1(\\bar{\\rho},\\eta)} \\left(\\frac{\\bar{\\rho}-\\rho\\mu \\Theta}{\\tf{\\W(k)}}\n-   \\eta \\mu \\Theta \\frac{\\|\\nabla \\mc{L}(\\W(k))\\|^2 }{\\tf{\\W(k)}}\\right)\\\\\n& \\geq \\mu \\Theta.\n\\end{split}\n\\end{equation}\nHere, we leverage the fact that the parameter $\\mu$ can be chosen arbitrarily, and then we select the step size $\\eta$ to satisfy the following condition:\n$$\\eta \\leq \\frac{\\bar{\\rho}-\\rho\\mu \\Theta }{\\mu\\Theta} \\frac{1} {\\|\\nabla \\mc{L}(\\W(k))\\|^2}.\n$$\nHence, \\eqref{eqn:localgd:3:nabla0} gives $\\W(k+1) \\in \\conb_{\\mu,R}$.\n\\\\\n\\textbf{Step 4: The correlation of $\\W(k)$ and $\\Wm$ increases over $k$.} \nIt follows from Theorem~\\ref{diverg:norm:w} that \n$\\left\\Vert \\W\\left(k\\right)\\right\\Vert \\rightarrow\\infty$. Hence, we can choose $k_0$ such that for any $k\\ge k_0$, it holds that $\\tf{\\W(k)}>  R$ for some $R \\geq R_\\mu \\vee R_\\epsilon \\vee 1/2$. Now, following similar steps in \\eqref{eqn:decpath:1} and \\eqref{eqn:decpath:22},  for  some constant $C(\\epsilon,\\eta)$, we obtain\n\\begin{align*}\n      \\left\\langle\\frac{\\W(k)}{\\tf{\\W(k)}}, \\frac{\\Wm}{\\tf{\\Wm}} \\right\\rangle \\ge1-\\epsilon+ \\frac{C(\\epsilon,\\eta)}{\\tf{\\W(k)}}.      \n\\end{align*}\nConsequently,\n    \\begin{align*}\n      \\liminf_{t\\to\\infty}\\iprod{\\frac{\\W(k)}{\\tf{\\W(k)}}}{\\frac{\\Wm}{\\tf{\\Wm}}}\\ge1-\\epsilon.\n    \\end{align*}\nSince  $\\epsilon \\in (0, \\mu/2)$  is arbitrary, we get $\\W(k)/\\tf{\\W(k)}\\to  \\Wm/\\tf{\\Wm}$.\n\\end{proof}",
                            "statement_html": "We provide the proof in four steps:<br>\n<b>Step~1: There are no stationary points within $\\conb_{\\mu,R_\\mu}(\\Wm)$ for sufficiently large $R_\\mu$.</b> \nThis step directly follows from <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_36/index.html#glocal+cond\">Lemma 36</a>: for all $\\V,\\W\\in \\Sc_{\\mu}(\\Wm)$ with $\\tf{\\V}=\\tf{\\Wm}$ and $\\tf{\\W}\\geq R_\\mu$, it follows from <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_36/index.html#glocal+cond\">Lemma 36</a> that there exists $R_\\mu$ such that $\\iprod{\\V}{ -\\nabla \\Lc(\\W)}$ is strictly positive.<br>\n<b>Step~2:</b> Consider an arbitrary value of $\\epsilon \\in (0, \\mu/2)$ and let $1/(1+\\pi)=1-\\epsilon$. It follows from <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_37/index.html#lem%3Aglocal%3Acorr\">Lemma 37</a> that, there exists $R_\\epsilon$ such that all $\\W \\in \\conb_{\\mu,R_\\epsilon}(\\Wm)$ satisfy\n\\begin{align}\\label{eqn:neg:corr:local:nabla0}\n\\iprod{-\\nabla\\mc{L}(\\W)}{\\frac{\\Wm}{\\tf{\\Wm}}} \\geq (1-\\epsilon) \\iprod{-\\nabla \\mc{L}(\\W)}{\\frac{\\W}{\\tf{\\W}}}.\n\\end{align}\n\n<b>Step~3: Updates remain inside the cone $\\conb_{\\mu,R_\\mu}(\\Wm)$.</b> By leveraging the results from <b>Step 1</b> and <b>Step 2</b>, we demonstrate that the gradient iterates remain within this cone. Note that by our assumption and for sufficiently large $\\eta_0$, we get $\\W(1) \\in \\conb_{\\mu,R}(\\Wm)$. Here, $R$ is chosen such that \n\\begin{align}\\label{eqn:bound:R:nabla0}\nR \\geq R_\\mu \\vee R_\\epsilon \\vee 1/2.\n\\end{align}\nWe provide the proof by induction. By the above argument, $\\W(1) \\in \\conb_{\\mu,R}(\\Wm)$. Let $\\W(k) \\in \\conb_{\\mu,R}(\\Wm)$. For all $k\\geq 1$, we have that \n\n\\begin{subequations}\\label{eqn:rho:def:nabla0}\n\\begin{align}\n& \\min_{t\\neq \\op_i,~i\\in[n]} ~~ \\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top, - \\nabla \\mc{L}(\\W(k)) \\right\\rangle \\geq \\bar{\\rho} , \n\\label{eqn:bar:rho}\n\\\\\n&\\frac{1}{1-\\epsilon} \\iprod{ {\\frac{\\Wm}{\\tf{\\Wm}}}}{-\\nabla\\mc{L}(\\W(k))} \\geq \\rho.\n\\end{align}\n\\end{subequations}\nfor some positive constants $\\bar{\\rho}$ and $\\rho$.  \n\nTo proceed, for all $t\\neq \\op_i,~i\\in[n]$, we obtain \n\\begin{equation}\\label{eqn:localgd:1:nabla0}\n\\begin{split}\n\\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top, \\frac{\\W(k+1)}{\\tf{\\W(k)}} \\right\\rangle &= \\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top, \\frac{\\W(k)}{\\tf{\\W(k)}} -\\frac{\\eta}{\\tf{\\W(k)}}\\nabla \\mc{L}(\\W(k)) \\right\\rangle\\\\   \n&\\ge \\mu \\Theta- \\frac{\\eta}{\\tf{\\W(k)}} \\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top, \\nabla \\mc{L}(\\W(k)) \\right\\rangle \\\\\n\n& \\geq \\mu \\Theta +\\frac{\\eta\\bar{\\rho}}{\\tf{\\W(k)}}.\n\\end{split}\n\\end{equation}\nHere, the first inequality follows from the induction assumption $\\W(k) \\in \\conb_{\\mu,R}(\\Wm)$, and the second inequality uses \\eqref{eqn:bar:rho}.\n\nFrom <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_36/index.html#glocal+cond\">Lemma 36</a>, we have $\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle<0$ which implies that $\\tf{\\W(k+1)} \\geq \\tf{\\W(k)}$. Hence, \n\\begin{align*}\n\\tf{\\W(k+1)}&\\leq\\frac{1}{{2\\tf{\\W(k)}}} \\left(\\tf{\\W(k+1)}^2+\\tf{\\W(k)}^2\\right)\\\\\n& = \\frac{1}{2\\tf{\\W(k)}} \\left(2\\tf{W(t)}^2-2\\eta\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle+\\eta^2\\tf{\\nabla \\Lc(\\W(k))}^2\\right)\\\\\n&\\leq \\tf{\\W(k)}- \\frac{\\eta}{\\tf{\\W(k)}}\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle + \\eta^2 \\|\\nabla \\Lc(\\W(k))\\|^2, \n\\end{align*}\nwhere the last inequality follows from $\\tf{\\W(k)} \\geq R \\geq 1/2 $. \n\nThus,\n\\begin{equation}\\label{eqn:localgd:2:nabla0}\n\\begin{split}\n\\frac{\\tf{\\W(k+1)}}{\\tf{\\W(k)}}& \\leq 1- \\eta\n\\left\\langle \\nabla \\Lc(\\W(k)),\\frac{\\W(k)}{\\tf{\\W(k)}} \\right\\rangle + \\eta^2 \\frac{\\|\\nabla \\mc{L}(\\W(k))\\|^2}{\\tf{\\W(k)}}\\\\\n& \\leq 1- \\frac{\\eta}{1-\\epsilon} \\iprod{\\nabla\\mc{L}(\\W(k))}\n{\\frac{\\Wm}{\\tf{\\Wm}}}+ \\eta^2 \\frac{\\|\\nabla \\mc{L}(\\W(k))\\|^2}{\\tf{\\W(k)}}\\\\\n& \\leq 1 + \\frac{\\eta \\rho}{\\tf{\\W(k)}} + \\frac{\\eta^2\\|\\nabla \\mc{L}(\\W(k))\\|^2}{\\tf{\\W(k)}}=C_1(\\rho,\\eta).\n\\end{split}\n\\end{equation}\nNow, it follows from \\eqref{eqn:localgd:1:nabla0} and \\eqref{eqn:localgd:2:nabla0} that   \n\\begin{equation}\\label{eqn:localgd:3:nabla0}\n\\begin{split}\n\\min_{t\\neq \\op_i,~i\\in[n]} ~~ \\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top, \\frac{\\W(k+1)}{\\tf{\\W(k+1)}}\\right\\rangle &\\geq \\frac{1}{C_1(\\bar{\\rho},\\eta)} \\left(\\mu \\Theta+\\frac{\\eta \\bar{\\rho}}{\\tf{\\W(k)}}\\right)\\\\\n& \\geq \\mu \\Theta+\\frac{\\eta}{C_1(\\bar{\\rho},\\eta)} \\left(\\frac{\\bar{\\rho}-\\rho\\mu \\Theta}{\\tf{\\W(k)}}\n- \\eta \\mu \\Theta \\frac{\\|\\nabla \\mc{L}(\\W(k))\\|^2 }{\\tf{\\W(k)}}\\right)\\\\\n& \\geq \\mu \\Theta.\n\\end{split}\n\\end{equation}\nHere, we leverage the fact that the parameter $\\mu$ can be chosen arbitrarily, and then we select the step size $\\eta$ to satisfy the following condition:\n$$\\eta \\leq \\frac{\\bar{\\rho}-\\rho\\mu \\Theta }{\\mu\\Theta} \\frac{1} {\\|\\nabla \\mc{L}(\\W(k))\\|^2}.\n$$\nHence, \\eqref{eqn:localgd:3:nabla0} gives $\\W(k+1) \\in \\conb_{\\mu,R}$.<br>\n<b>Step 4: The correlation of $\\W(k)$ and $\\Wm$ increases over $k$.</b> \nIt follows from <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_22/index.html#diverg%3Anorm%3Aw\">Theorem 22</a> that \n$\\left\\Vert \\W\\left(k\\right)\\right\\Vert \\rightarrow\\infty$. Hence, we can choose $k_0$ such that for any $k\\ge k_0$, it holds that $\\tf{\\W(k)}> R$ for some $R \\geq R_\\mu \\vee R_\\epsilon \\vee 1/2$. Now, following similar steps in \\eqref{eqn:decpath:1} and \\eqref{eqn:decpath:22}, for some constant $C(\\epsilon,\\eta)$, we obtain\n\\begin{align*}\n\\left\\langle\\frac{\\W(k)}{\\tf{\\W(k)}}, \\frac{\\Wm}{\\tf{\\Wm}} \\right\\rangle \\ge1-\\epsilon+ \\frac{C(\\epsilon,\\eta)}{\\tf{\\W(k)}}.      \n\\end{align*}\nConsequently,\n\\begin{align*}\n\\liminf_{t\\to\\infty}\\iprod{\\frac{\\W(k)}{\\tf{\\W(k)}}}{\\frac{\\Wm}{\\tf{\\Wm}}}\\ge1-\\epsilon.\n\\end{align*}\nSince $\\epsilon \\in (0, \\mu/2)$ is arbitrary, we get $\\W(k)/\\tf{\\W(k)}\\to \\Wm/\\tf{\\Wm}$.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n\n<br><br>1. <i></i>No Stationary Points in the Cone<i></i>: The first step establishes that there are no stationary points within the cone \\(\\conb_{\\mu,R_\\mu}(\\Wm)\\) for sufficiently large \\(R_\\mu\\). This follows directly from Lemma 36, which states that for all \\(\\V, \\W \\in \\Sc_{\\mu}(\\Wm)\\) with \\(\\tf{\\V} = \\tf{\\Wm}\\) and \\(\\tf{\\W} \\geq R_\\mu\\), there exists \\(R_\\mu\\) such that \\(\\iprod{\\V}{ -\\nabla \\Lc(\\W)}\\) is strictly positive.\n\n<br><br>2. <i></i>Choosing \\(\\epsilon\\) and Establishing Inequality<i></i>: In this step, an arbitrary value of \\(\\epsilon \\in (0, \\mu/2)\\) is considered, and \\(1/(1+\\pi) = 1 - \\epsilon\\) is set. From Lemma 37, it follows that there exists \\(R_\\epsilon\\) such that all \\(\\W \\in \\conb_{\\mu,R_\\epsilon}(\\Wm)\\) satisfy:\n\\[\n\\iprod{-\\nabla\\mc{L}(\\W)}{\\frac{\\Wm}{\\tf{\\Wm}}} \\geq (1-\\epsilon) \\iprod{-\\nabla \\mc{L}(\\W)}{\\frac{\\W}{\\tf{\\W}}}.\n\\]\n\n<br><br>3. <i></i>Updates Remain Inside the Cone<i></i>: This step demonstrates that the gradient iterates remain within the cone \\(\\conb_{\\mu,R_\\mu}(\\Wm)\\). By leveraging the results from Steps 1 and 2, it is shown that for sufficiently large \\(\\eta_0\\), \\(\\W(1) \\in \\conb_{\\mu,R}(\\Wm)\\), where \\(R\\) is chosen such that:\n\\[\nR \\geq R_\\mu \\vee R_\\epsilon \\vee 1/2.\n\\]\nThe proof is provided by induction, showing that if \\(\\W(k) \\in \\conb_{\\mu,R}(\\Wm)\\), then \\(\\W(k+1)\\) also remains in the cone.\n\n<br><br>4. <i></i>Increasing Correlation Over Iterations<i></i>: The final step shows that the correlation between \\(\\W(k)\\) and \\(\\Wm\\) increases over iterations. From Theorem 22, it is known that \\(\\left\\Vert \\W\\left(k\\right)\\right\\Vert \\rightarrow\\infty\\). For any \\(k \\ge k_0\\), it holds that \\(\\tf{\\W(k)} > R\\) for some \\(R \\geq R_\\mu \\vee R_\\epsilon \\vee 1/2\\). Following similar steps as in previous equations, it is shown that:\n\\[\n\\left\\langle\\frac{\\W(k)}{\\tf{\\W(k)}}, \\frac{\\Wm}{\\tf{\\Wm}} \\right\\rangle \\ge1-\\epsilon+ \\frac{C(\\epsilon,\\eta)}{\\tf{\\W(k)}}.\n\\]\nThus, it is concluded that:\n\\[\n\\liminf_{t\\to\\infty}\\iprod{\\frac{\\W(k)}{\\tf{\\W(k)}}}{\\frac{\\Wm}{\\tf{\\Wm}}}\\ge1-\\epsilon.\n\\]\nSince \\(\\epsilon \\in (0, \\mu/2)\\) is arbitrary, it follows that \\(\\W(k)/\\tf{\\W(k)}\\to \\Wm/\\tf{\\Wm}\\)."
                        }
                    },
                    {
                        "statement_id": "db547eda-e029-405a-a5fd-913bb78fbad9",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 38,
                        "library_name": "Lemma 38",
                        "title": "Gradient Escape from Region Lemma",
                        "statement_original_tex": "\\begin{lemma}\\label{lem:out:S}\nSuppose Assumptions~\\ref{assum:loss:prope} and \\ref{assum:token} hold. Assume the gradient at the initialization $(\\Kb(0), \\Qb(0))$ is nonzero and $\\Lc(\\Kb(0), \\Qb(0)) \\leq \\Lc(0, 0)$. Consider Algorithm~\\ref{GD-QK} with a step size $\\eta=1/L(R)$, where $L(R):=RL_{\\W}$. Then for any $R>0$, there exists an iteration index $k$ at which $(\\Kb(k),\\Qb(k)) \\notin \\mc{S} (R)$.\n\\end{lemma}",
                        "statement_html": "Suppose Assumptions A [in <a href=\"https://arxiv.org/pdf/2308.16898#assumption.1\">original paper</a>] and B [in <a href=\"https://arxiv.org/pdf/2308.16898#assumption.2\">original paper</a>] hold. Assume the gradient at the initialization $(\\Kb(0), \\Qb(0))$ is nonzero and $\\Lc(\\Kb(0), \\Qb(0)) \\leq \\Lc(0, 0)$. Consider Algorithm $\\ref{GD-QK}$ with a step size $\\eta=1/L(R)$, where $L(R):=RL_{\\W}$. Then for any $R>0$, there exists an iteration index $k$ at which $(\\Kb(k),\\Qb(k)) \\notin \\mc{S} (R)$.",
                        "statement_type": "lemma",
                        "statement_motivation_html": "This result provides a guarantee for the convergence of the gradient descent algorithm under specific conditions. It is particularly useful in optimization problems where the objective is to minimize a loss function. By ensuring that the gradient at initialization is nonzero and the initial loss is bounded, this theorem helps in selecting an appropriate step size for the algorithm, thereby improving the efficiency and effectiveness of the optimization process.",
                        "html_url": "library/lemmas/lemma_38/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "40df5c64-ebf0-4d5a-9847-1080e62a4047",
                            "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\nLet us select a value for $R$ and fix the step size to $\\eta=1/L(R)$. Assuming that both $(\\Kb({k+1}), \\Qb(k+1))$ and $(\\Kb({k}), \\Qb(k))$ reside within the set $\\mc{S}(R)$, we can establish a descent of objective analogous to Lemma \\ref{lem:grad:descent}. Specifically, we obtain the following result: \n\\begin{align}\\label{eq:descent:obj}\n\\nonumber\n\\mathcal{L}(\\Kb({k+1}), \\Qb({k+1}))-\\mathcal{L}(\\Kb({k}),\\Qb(k)) &\\leq-\\frac{1}{2 L(R)} \\tf{\\nabla \\mathcal{L}(\\Kb({k}), \\Qb(k))}^2 \\\\\n& =-\\frac{\\eta}{2} \\tf{\\nabla \\mathcal{L}(\\Kb({k}), \\Qb(k))}^2.\n\\end{align}\nBy our assumption, $\\tf{\\nabla \\Lc (\\Kb(0), \\Qb(0))} \\neq 0$, which implies that  \n\\begin{equation}\\label{eqn:obj:less0}\n    \\mathcal{L}(\\Kb({1}),\\Qb(1)) \\leq  \\mathcal{L}(\\Kb({0}),\\Qb(0)) - \\frac{1}{ 2 L(R)} \\tf{\\nabla \\mathcal{L}(\\Kb({0}), \\Qb(0))}^2 <    \\mathcal{L}(\\Kb(0),\\Qb(0)) \\leq  \\mathcal{L}(0,0).\n\\end{equation}\nNext, we claim that for any $R>0$, there exists a constant $\\epsilon(R)>0$, such that \n\\begin{align}\\label{eqn:grad:low:eps}\n\\forall k\\ge1\\,:\\,  \\quad (\\Kb (k),\\Qb(k)) \\in \\mathcal{S}, \\qquad \\tf{\\nabla_{\\Qb} \\Lc(\\Kb(k),\\Qb(k))}\\geq \\epsilon(R). \n\\end{align}\nFix an arbitrary $R>0$. If \\eqref{eqn:grad:low:eps} is not true, then for any $\\epsilon>0$, there exists some $k\\ge1$ such that $\\|\\Kb(k)\\|_F\\le R$ and  $\\tf{\\Qb(k)} \\leq R$ while  \\eqref{grad def KQ} gives\n\\begin{align}\\label{eqn:qgrad}\n\\nonumber\n\\left\\|\\nabla_{\\Qb} \\Lc(\\Kb (k),\\Qb(k))\\right\\|_F^2 & =\\left\\|\\frac{1}{n}\\sum_{i=1}^n \\ell' \\left(\\bgam_i^\\top \\sft{\\g_i}\\right) \\cdot \\X_i^\\top  \\sfp{\\g_i}  \\bgam_i  \\z_{i}^\\top \\Kb(k)\\right\\|_F^2 \\\\\n&=  \\tf{\\nabla \\Lc (\\W(k))}^2 \\left\\|\\Kb(k)\\right\\|^2_F \\leq \\epsilon^2,   \n\\end{align}\nwhere $\\g_{i}=\\X_i\\Kb(k) \\Qb(k)^\\top\\z_{i}=\\X_i\\W(k)\\z_{i}$.\n\nIt follows from Lemma~\\ref{global des lem} that $\\li\\nabla\\Lc(\\W (k)),\\Wm/\\|\\Wm\\|_F\\ri \\leq  - c <0$ for some positive constants $c$.  \n\nHence, $\\tf{\\nabla \\Lc (\\W(k))}  \\geq c$ and $\\tf{\\Kb(k)} \\leq \\epsilon/c$.  This together with $\\tf{\\Qb(k)} \\leq R$ implies that   $\\tf{ \\Kb(k) \\Qb(k)^\\top} \\leq  \\epsilon R/ c$.  In other words,  after $k=1$, $\\tf{ \\Kb(k) \\Qb(k)^\\top}$  may be arbitrarily small, which implies $\\Lc(\\Kb(k), \\Qb(k))$ can be arbitrarily close to $ \\mathcal{L}(\\Kb(0),\\Qb(0))$. \nThis is a contradiction to \\eqref{eqn:obj:less0}. Hence, \\eqref{eqn:grad:low:eps} holds.\n\nNow,  it follows from \\eqref{eq:descent:obj} and \\eqref{eqn:grad:low:eps}  that \n\\begin{align*}\n  \\mathcal{L}(\\Kb(0),\\Qb(0))\n         \\ge   \\mathcal{L}(\\Kb(0),\\Qb(0)) -\\mathcal{L}_{\\star} \\geq  \\frac{1}{2 L(R)}\\sum_{k=0}^{\\infty} \\left\\|\\nabla_{\\Qb} \\Lc(\\Kb(k),\\Qb(k)) \\right\\|_{F}^2\n         \n        \n          \\geq \\infty,\n    \\end{align*}\nwhich is a contradiction. Hence,  $(\\Kb(k),\\Qb(k))$ must go out of $\\mc{S} (R)$.\n\\end{proof}",
                            "statement_html": "Let us select a value for $R$ and fix the step size to $\\eta=1/L(R)$. Assuming that both $(\\Kb({k+1}), \\Qb(k+1))$ and $(\\Kb({k}), \\Qb(k))$ reside within the set $\\mc{S}(R)$, we can establish a descent of objective analogous to <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_30/index.html#lem%3Agrad%3Adescent\">Lemma 30</a>. Specifically, we obtain the following result: \n\\begin{align}\\label{eq:descent:obj}\n\\nonumber\n\\mathcal{L}(\\Kb({k+1}), \\Qb({k+1}))-\\mathcal{L}(\\Kb({k}),\\Qb(k)) &\\leq-\\frac{1}{2 L(R)} \\tf{\\nabla \\mathcal{L}(\\Kb({k}), \\Qb(k))}^2 \\\\\n& =-\\frac{\\eta}{2} \\tf{\\nabla \\mathcal{L}(\\Kb({k}), \\Qb(k))}^2.\n\\end{align}\nBy our assumption, $\\tf{\\nabla \\Lc (\\Kb(0), \\Qb(0))} \\neq 0$, which implies that  \n\\begin{equation}\\label{eqn:obj:less0}\n    \\mathcal{L}(\\Kb({1}),\\Qb(1)) \\leq  \\mathcal{L}(\\Kb({0}),\\Qb(0)) - \\frac{1}{ 2 L(R)} \\tf{\\nabla \\mathcal{L}(\\Kb({0}), \\Qb(0))}^2 <    \\mathcal{L}(\\Kb(0),\\Qb(0)) \\leq  \\mathcal{L}(0,0).\n\\end{equation}\nNext, we claim that for any $R>0$, there exists a constant $\\epsilon(R)>0$, such that \n\\begin{align}\\label{eqn:grad:low:eps}\n\\forall k\\ge1\\,:\\,  \\quad (\\Kb (k),\\Qb(k)) \\in \\mathcal{S}, \\qquad \\tf{\\nabla_{\\Qb} \\Lc(\\Kb(k),\\Qb(k))}\\geq \\epsilon(R). \n\\end{align}\nFix an arbitrary $R>0$. If \\eqref{eqn:grad:low:eps} is not true, then for any $\\epsilon>0$, there exists some $k\\ge1$ such that $\\|\\Kb(k)\\|_F\\le R$ and  $\\tf{\\Qb(k)} \\leq R$ while  \\eqref{grad def KQ} gives\n\\begin{align}\\label{eqn:qgrad}\n\\nonumber\n\\left\\|\\nabla_{\\Qb} \\Lc(\\Kb (k),\\Qb(k))\\right\\|_F^2 & =\\left\\|\\frac{1}{n}\\sum_{i=1}^n \\ell' \\left(\\bgam_i^\\top \\sft{\\g_i}\\right) \\cdot \\X_i^\\top  \\sfp{\\g_i}  \\bgam_i  \\z_{i}^\\top \\Kb(k)\\right\\|_F^2 \\\\\n&=  \\tf{\\nabla \\Lc (\\W(k))}^2 \\left\\|\\Kb(k)\\right\\|^2_F \\leq \\epsilon^2,   \n\\end{align}\nwhere $\\g_{i}=\\X_i\\Kb(k) \\Qb(k)^\\top\\z_{i}=\\X_i\\W(k)\\z_{i}$.\n\nIt follows from <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_31/index.html#global+des+lem\">Lemma 31</a> that $\\li\\nabla\\Lc(\\W (k)),\\Wm/\\|\\Wm\\|_F\\ri \\leq  - c <0$ for some positive constants $c$.  \n\nHence, $\\tf{\\nabla \\Lc (\\W(k))}  \\geq c$ and $\\tf{\\Kb(k)} \\leq \\epsilon/c$.  This together with $\\tf{\\Qb(k)} \\leq R$ implies that   $\\tf{ \\Kb(k) \\Qb(k)^\\top} \\leq  \\epsilon R/ c$.  In other words,  after $k=1$, $\\tf{ \\Kb(k) \\Qb(k)^\\top}$  may be arbitrarily small, which implies $\\Lc(\\Kb(k), \\Qb(k))$ can be arbitrarily close to $ \\mathcal{L}(\\Kb(0),\\Qb(0))$. \nThis is a contradiction to \\eqref{eqn:obj:less0}. Hence, \\eqref{eqn:grad:low:eps} holds.\n\nNow,  it follows from \\eqref{eq:descent:obj} and \\eqref{eqn:grad:low:eps}  that \n\\begin{align*}\n  \\mathcal{L}(\\Kb(0),\\Qb(0))\n         \\ge   \\mathcal{L}(\\Kb(0),\\Qb(0)) -\\mathcal{L}_{\\star} \\geq  \\frac{1}{2 L(R)}\\sum_{k=0}^{\\infty} \\left\\|\\nabla_{\\Qb} \\Lc(\\Kb(k),\\Qb(k)) \\right\\|_{F}^2\n         \n        \n          \\geq \\infty,\n    \\end{align*}\nwhich is a contradiction. Hence,  $(\\Kb(k),\\Qb(k))$ must go out of $\\mc{S} (R)$.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n<br>\n<br>1. <i>Initialization and Assumptions</i>: The proof begins by selecting a value for \\(R\\) and fixing the step size to \\(\\eta = 1/L(R)\\). It assumes that both \\((\\Kb({k+1}), \\Qb(k+1))\\) and \\((\\Kb({k}), \\Qb(k))\\) are within the set \\(\\mc{S}(R)\\).\n<br>\n<br>2. <i>Descent of Objective</i>: Using the assumption, a descent of the objective function is established, analogous to Lemma 30. This is expressed as:\n<br>   \\[\n   \\mathcal{L}(\\Kb({k+1}), \\Qb({k+1})) - \\mathcal{L}(\\Kb({k}), \\Qb(k)) \\leq -\\frac{1}{2 L(R)} \\|\\nabla \\mathcal{L}(\\Kb({k}), \\Qb(k))\\|^2 = -\\frac{\\eta}{2} \\|\\nabla \\mathcal{L}(\\Kb({k}), \\Qb(k))\\|^2.\n   \\]\n<br>\n<br>3. <i>Initial Gradient Assumption</i>: It is assumed that \\(\\|\\nabla \\Lc (\\Kb(0), \\Qb(0))\\| \\neq 0\\), leading to:\n<br>   \\[\n   \\mathcal{L}(\\Kb({1}), \\Qb(1)) \\leq \\mathcal{L}(\\Kb({0}), \\Qb(0)) - \\frac{1}{2 L(R)} \\|\\nabla \\mathcal{L}(\\Kb({0}), \\Qb(0))\\|^2 < \\mathcal{L}(\\Kb(0), \\Qb(0)) \\leq \\mathcal{L}(0,0).\n   \\]\n<br>\n<br>4. <i>Gradient Lower Bound Claim</i>: The proof claims that for any \\(R > 0\\), there exists a constant \\(\\epsilon(R) > 0\\) such that:\n<br>   \\[\n   \\forall k \\ge 1\\,:\\, (\\Kb(k), \\Qb(k)) \\in \\mathcal{S}, \\quad \\|\\nabla_{\\Qb} \\Lc(\\Kb(k), \\Qb(k))\\| \\geq \\epsilon(R).\n   \\]\n<br>\n<br>5. <i>Contradiction Argument</i>: If the above claim is not true, then for any \\(\\epsilon > 0\\), there exists some \\(k \\ge 1\\) such that \\(\\|\\Kb(k)\\|_F \\le R\\) and \\(\\|\\Qb(k)\\| \\leq R\\), leading to:\n<br>   \\[\n   \\left\\|\\nabla_{\\Qb} \\Lc(\\Kb(k), \\Qb(k))\\right\\|_F^2 = \\left\\|\\frac{1}{n}\\sum_{i=1}^n \\ell' \\left(\\bgam_i^\\top \\sft{\\g_i}\\right) \\cdot \\X_i^\\top \\sfp{\\g_i} \\bgam_i \\z_{i}^\\top \\Kb(k)\\right\\|_F^2 = \\|\\nabla \\Lc (\\W(k))\\|^2 \\|\\Kb(k)\\|^2_F \\leq \\epsilon^2.\n   \\]\n<br>\n<br>6. <i>Lemma 31 Application</i>: Using Lemma 31, it is shown that \\(\\li\\nabla\\Lc(\\W(k)), \\Wm/\\|\\Wm\\|_F\\ri \\leq -c < 0\\) for some positive constant \\(c\\). Hence, \\(\\|\\nabla \\Lc (\\W(k))\\| \\geq c\\) and \\(\\|\\Kb(k)\\| \\leq \\epsilon/c\\).\n<br>\n<br>7. <i>Contradiction Conclusion</i>: Combining the above results, it is shown that \\(\\|\\Kb(k) \\Qb(k)^\\top\\| \\leq \\epsilon R / c\\), implying \\(\\Lc(\\Kb(k), \\Qb(k))\\) can be arbitrarily close to \\(\\mathcal{L}(\\Kb(0), \\Qb(0))\\), which contradicts the earlier descent result.\n<br>\n<br>Thus, the proof concludes that \\((\\Kb(k), \\Qb(k))\\) must go out of \\(\\mc{S}(R)\\)."
                        }
                    },
                    {
                        "statement_id": "6be9c80c-88f3-4dd3-8f40-9bd1405198ef",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 39,
                        "library_name": "Lemma 39",
                        "title": "Full Row-Rank Concatenation Lemma",
                        "statement_original_tex": "\\begin{lemma}\\label{lem add up}Let $\\A\\in\\R^{n\\times p},\\B\\in\\R^{m\\times p}$. Suppose $n+m\\leq p$ and $\\A$ is full row-rank. Denote the null space of $\\A$ by $S_\\A^\\perp$. Let $P$ be a subspace that is its subset i.e.~$P\\subseteq S_\\A^\\perp$. Let $\\B'$ be the matrix obtained by projecting each of row of $\\B$ on $P$ and suppose $\\B'$ is full rank. Then, the concatenation $\\Cb= [\\A; \\B ]$\n\nis full row-rank.\n\\end{lemma}",
                        "statement_html": "Let $\\A \\in \\R^{n \\times p}, \\B \\in \\R^{m \\times p}$. Suppose $n + m \\leq p$ and $\\A$ is full row-rank. Denote the null space of $\\A$ by $S_\\A^\\perp$. Let $P$ be a subspace that is its subset i.e.~$P \\subseteq S_\\A^\\perp$. Let $\\B'$ be the matrix obtained by projecting each of row of $\\B$ on $P$ and suppose $\\B'$ is full rank. Then, the concatenation $\\Cb = [\\A; \\B]$\n\nis full row-rank.",
                        "statement_type": "lemma",
                        "statement_motivation_html": "This result is useful in linear algebra, particularly in the context of matrix concatenation and rank properties. It provides a condition under which the concatenation of two matrices, $\\A$ and $\\B$, maintains full row-rank. This can be particularly valuable in applications such as solving systems of linear equations, where maintaining the rank of a matrix is crucial for ensuring the existence and uniqueness of solutions.",
                        "html_url": "library/lemmas/lemma_39/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "367e2bd3-f577-4837-8609-40289832036e",
                            "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof} Throughout $\\dm$ denotes either Frobenius norm or nuclear norm. We will prove that $\\wrb{R}$ asymptotically aligns with the set of globally-optimal directions and also $\\td{\\wrb{R}}\\rightarrow \\infty$. $\\Rcm\\subseteq\\R^{d\\times d}$ denote the manifold of rank $\\leq$$m$ matrices.\n\\\\\n\\noindent\\textbf{Step 1:} Let us first prove that $\\wrb{R}$ achieves the optimal risk as $R\\rightarrow\\infty$ -- rather than problem having finite optima. Define $\\xdm=1/\\td{\\Wm}$ and norm-normalized $\\Wsb=\\xdm\\Wm$. Note that $\\Wm$ separates tokens $\\op$ from rest of the tokens for each $i \\in[n]$. Thus, we have that\n\\begin{align}\n\\lim_{R\\rightarrow\\infty}\\Lc(\\wrb{R})\\leq\\lim_{R\\rightarrow\\infty}\\Lc(R\\cdot\\Wsb):=\\Lc_\\star= \\frac{1}{n}\\sum_{i=1}^n \\ell(\\bgam^{\\op}_i).\\label{glob:asymp loss}\n\\end{align}\nOn the other hand, for any $\\W\\in \\Rcm$, define the softmax probabilities $\\s^{(i)}=\\sft{\\X_i\\W\\z_i}$ and attention features $\\x^{\\W}_i=\\sum_{t=1}^T \\s^{(i)}_t\\x_t$. Decompose $\\x^{\\W}_i$ as \n$\n\\x^{\\W}_i=\\s^{(i)}_{\\op_i}\\x_{i\\opt_i}+\\sum_{t\\neq \\op_i}\\s^{(i)}_t\\x_\\itt.\n$ Set $\\bgg_\\itt=\\bgam^{\\op}_i-\\bgam_\\itt=Y_i\\cdot \\vb^\\top(\\x_{i\\opt_i}-\\x_\\itt)>0$, and define\n\\begin{align}\n&B:=\\max_{i\\in[n]}\\max_{t,\\tau\\in[T]}\\tn{\\vb}\\cdot \\tn{\\x_\\itt-\\x_\\ittt}\\geq \\bgg_\\itt.\\label{glob:BB eq}\n\\end{align}\n\n\n\n\n\n\n\nDefine $c_\\op=\\min_{i\\in[n],t\\neq\\op_i}\\bgg_\\itt>0$ and $\\bgam^{\\W}_i=Y_i\\cdot \\vb^\\top\\x^{\\W}_i$. We obtain the following score inequalities\n\\begin{align}\\label{glob:score decomp}\n&\\bgam^{\\W}_i\\leq \\bgam^{\\op}_i-c_\\op (1-\\s^{(i)}_{\\op_i})<\\bgam^{\\op}_i,\\\\\n\n&|\\bgam^{\\W}_i-\\bgam^{\\op}_i|\\leq \\tn{\\vb}\\cdot\\tn{\\x^{\\W}_i-\\xa_i}\\leq \\tn{\\vb} \\sum_{t\\neq \\op_i}\\s^{(i)}_t\\tn{\\x_\\itt-\\xa_i}\\leq B (1-\\s^{(i)}_{\\op_i}).\\nonumber\n\n\n\\end{align}\nWe will use the $\\bgam^{\\W}_i-\\bgam^{\\op}_i$ term in \\eqref{glob:score decomp} to evaluate $\\W$ against the reference loss $\\Lc_\\star$ of \\eqref{glob:asymp loss}. \n\n\n\n\n\nUsing the strictly-decreasing nature of $\\ell$, we conclude with the fact that for all (finite) $\\W\\in \\Rcm$, \n\\[\n\\Lc(\\W)=\\frac{1}{n}\\sum_{i=1}^n \\ell(\\bgam^{\\W}_i)> \\Lc_\\st=\\frac{1}{n}\\sum_{i=1}^n \\ell(\\bgam^{\\op}_i),\n\\]\nwhich implies $\\td{\\wrb{R}}\\rightarrow\\infty$ together with \\eqref{glob:asymp loss}.\n\n\\noindent\\textbf{Step 2:} To proceed, we show that $\\wrb{R}$ converges in direction to $\\Wcs$, which denotes the set of SVM minima. Suppose this is not the case and~convergence fails. We will obtain a contradiction by showing that $\\Wsb_R=R\\cdot\\Wsb$ achieves a strictly superior loss compared to $\\wrb{R}$. Let us introduce the normalized parameters $\\wrt{R}=\\frac{\\wrb{R}}{R\\xdm}$ and $\\W'=\\frac{\\wrb{R}}{\\td{\\wrb{R}}\\xdm}$. Note that $\\wrt{R}$ is obtained by scaling down $\\W'$ since $\\td{\\wrb{R}}\\leq R$ and $\\W'$ obeys $\\td{\\W'}=\\td{\\Wm}$.\nSince $\\wrt{R}$ fails to converge to $\\Wcs$, for some $\\delta>0$, there exists arbitrarily large $R>0$ such that $\\dist{\\wrt{R},\\Wcs}\\geq \\delta$. This translates to the suboptimality in terms of the margin constraints as follows: First, since nuclear norm dominates Frobenius, distance with respect to the $\\dm$-norm obeys $\\distd{\\wrt{R},\\Wcs}\\geq \\delta$. Secondly, using triangle inequality,\n\\[ \n\\text{this implies that either~~~}\\td{\\wrt{R}}\\leq \\td{\\Wm}-\\delta/2\\text{~~~or~~~}\\distd{\\W',\\Wcs}\\geq \\delta/2.\n\\]\nIn either scenario, $\\wrt{R}$ strictly violates one of the margin constraints of \\eqref{seqattnsvm}: If $\\td{\\wrt{R}}\\leq \\td{\\Wm}-\\delta/2$, then, since the optimal SVM objective is $\\td{\\Wm}$, there exists a constraint $i,t\\neq\\op_i$ for which $\\inn{(\\x^\\op_i-\\x_\\itt)\\z_i^\\top,\\wrt{R}}\\leq 1-\\frac{\\delta}{2\\td{\\Wm}}$. If $\\distd{\\W',\\Wcs}\\geq \\delta/2$, then, $\\W'$ has the same SVM objective but it is strictly bounded away from the solution set. Thus, for some $\\eps:=\\eps(\\delta)>0$, $\\W'$ and its scaled down version $\\wrt{R}$ strictly violate an SVM constraint achieving margin $\\leq 1-\\eps$. Without losing generality, suppose $\\wrt{R}$ violates the first constraint $i=1$. Thus, for a properly updated $\\delta>0$ (that is function of the initial $\\delta>0$) and for $i=1$ and some \\nei $\\tau\\in \\Tc_1$,\n\\begin{align}\n\\inn{ (\\x^\\op_1-\\x_{1t})\\z_1^\\top,\\wrt{R}}\\leq 1-\\delta.\\label{margin violate:glob}\n\n\\end{align}\nNow, we will argue that this leads to a contradiction by proving $\\Lc(\\Wsb_R)<\\Lc(\\wrb{R})$ for sufficiently large $R$.\n\nTo obtain the result, we establish a refined softmax probability control as in Step 1 by studying distance to $\\Lc_\\star$. Following \\eqref{glob:score decomp}, denote the score function at $\\wrb{R}$ via $\\bgam^R_i:=\\bgam_i^{\\wrb{R}}$. Similarly, let $\\sir_i=\\sft{\\abr_i}$ with $\\abr_i=\\X_i\\wrb{R}\\z_i$. Set the corresponding notation for the reference parameter $\\Wsb_R$ as $\\bgam^\\st_i,\\s^\\st_i,\\ab^\\st_i$.  Recall that $R\\geq \\td{\\wrb{R}}$ and $\\xdm:=1/\\td{\\Wm}$. We note the following softmax inequalities \n\\begin{align}\n&\\s^\\st_{i\\op_i}\\geq \\frac{1}{1+Te^{-R\\xdm}}\\geq 1-Te^{-R\\xdm}\\quad \\text{for all}\\quad i\\in[n], \\label{glob:salpha bounds}\\\\\n&s^R_{i\\op_i}\\leq \\frac{1}{1+e^{-(1-\\delta)\\td{\\wrb{R}}\\xdm}}\\leq \\frac{1}{1+e^{-(1-\\delta)R\\xdm}}\\quad\\text{for}\\quad i=1.\\nn\n\\end{align}\nThe former inequality is thanks to $\\Wm$ achieving $\\geq$$1$ margins on all tokens $[T]-\\op_i$ and the latter arises from the $\\delta$-margin violation of $\\wrb{R}$ at $i=1$ i.e.~Eq.~\\eqref{margin violate:glob}. Since $\\ell$ is strictly decreasing with Lipschitz derivative and the scores are upper/lower bounded by an absolute constant (as tokens are bounded and fixed), we have that $\\cop\\geq -\\ell'(\\bgam_i^{\\W})\\geq \\cdn$ for some constants $\\cop>\\cdn>0$. Thus, following Eq.~\\eqref{glob:BB eq}, the score decomposition \\eqref{glob:score decomp}, and \\eqref{glob:salpha bounds} we can write\n\\begin{align}\\label{glob:ineq prl}\n\\Lc(\\wrb{R})-\\Lc_\\star&\\geq \\frac{1}{n}[\\ell(\\bgam_1^{\\wrb{R}})-\\ell(\\bgam^{\\op}_1)]\\geq \\frac{\\cdn}{n}(\\bgam^{\\op}_{1}-\\bgam_1^{\\wrb{R}})\\\\\n\\nonumber\n&\\geq \\frac{\\cdn}{n}c_\\op (1-\\s^R_{1\\op_1}).\n\\\\\n\\nonumber \n&\\geq \\frac{\\cdn c_\\op}{n}\\frac{1}{1+e^{(1-\\delta)R\\xdm}}.\n\\end{align}\nConversely, we upper bound the difference between $\\Lc(\\Wsb_R)$ and $\\Lc_\\star$ as follows. Define the worst-case loss difference for $\\wrb{R}$ as $j=\\arg\\max_{i\\in[n]}[\\ell(\\bgam_i^\\st)-\\ell(\\bgam^{\\op}_i)]$. Using \\eqref{glob:score decomp}\\&\\eqref{glob:salpha bounds}, we write\n\n\\begin{equation*}\n\\begin{aligned}\n\\Lc(\\Wsb_R)-\\Lc_\\star&\\leq \\max_{i\\in[n]}[\\ell(\\bgam_i^\\st)-\\ell(\\bgam^{\\op}_i)]\\leq \\cop\\cdot(\\bgam^{\\op}_{j}-\\bgam^\\st_{j})\\\\\n&\\leq \\cop\\cdot(1-\\s^\\st_{j\\op_j})B\\\\\n&\\leq \\cop\\cdot Te^{-R\\xdm}B.\n\\end{aligned}\n\\end{equation*}\nCombining the last inequality and \\eqref{glob:ineq prl}, we conclude that $\\Lc(\\Wsb_R)<\\Lc(\\wrb{R})$ whenever\n\\[\n\\cop T\\cdot e^{-R\\xdm}B<\\frac{\\cdn\\cdot c_\\op }{n}\\frac{1}{1+e^{(1-\\delta)R\\xdm}}\\iff \\frac{e^{R\\xdm}}{1+e^{(1-\\delta)R\\xdm}}> \\frac{\\cop Tn B}{\\cdn c_\\op }.\n\\]\nThe left hand-side inequality holds for all sufficiently large $R$: Specifically, as soon as $R$ obeys $R>\\frac{1}{\\delta\\xdm}\\log(\\frac{2\\cop Tn B}{\\cdn c_\\op})$. This completes the proof of the theorem by contradiction since we obtained $\\Lc(\\wrb{R})>\\Lc(\\Wsb_R)$.\n\\end{proof}",
                            "statement_html": "Throughout $\\dm$ denotes either Frobenius norm or nuclear norm. We will prove that $\\wrb{R}$ asymptotically aligns with the set of globally-optimal directions and also $\\td{\\wrb{R}}\\rightarrow \\infty$. $\\Rcm\\subseteq\\R^{d\\times d}$ denote the manifold of rank $\\leq$$m$ matrices.\n<br>\n<b>Step 1:</b> Let us first prove that $\\wrb{R}$ achieves the optimal risk as $R\\rightarrow\\infty$ -- rather than problem having finite optima. Define $\\xdm=1/\\td{\\Wm}$ and norm-normalized $\\Wsb=\\xdm\\Wm$. Note that $\\Wm$ separates tokens $\\op$ from rest of the tokens for each $i \\in[n]$. Thus, we have that\n\\begin{align}\n\\lim_{R\\rightarrow\\infty}\\Lc(\\wrb{R})\\leq\\lim_{R\\rightarrow\\infty}\\Lc(R\\cdot\\Wsb):=\\Lc_\\star= \\frac{1}{n}\\sum_{i=1}^n \\ell(\\bgam^{\\op}_i).\\label{glob:asymp loss}\n\\end{align}\nOn the other hand, for any $\\W\\in \\Rcm$, define the softmax probabilities $\\s^{(i)}=\\sft{\\X_i\\W\\z_i}$ and attention features $\\x^{\\W}_i=\\sum_{t=1}^T \\s^{(i)}_t\\x_t$. Decompose $\\x^{\\W}_i$ as \n$\n\\x^{\\W}_i=\\s^{(i)}_{\\op_i}\\x_{i\\opt_i}+\\sum_{t\\neq \\op_i}\\s^{(i)}_t\\x_\\itt.\n$ Set $\\bgg_\\itt=\\bgam^{\\op}_i-\\bgam_\\itt=Y_i\\cdot \\vb^\\top(\\x_{i\\opt_i}-\\x_\\itt)>0$, and define\n\\begin{align}\n&B:=\\max_{i\\in[n]}\\max_{t,\\tau\\in[T]}\\tn{\\vb}\\cdot \\tn{\\x_\\itt-\\x_\\ittt}\\geq \\bgg_\\itt.\\label{glob:BB eq}\n\\end{align}\n\nDefine $c_\\op=\\min_{i\\in[n],t\\neq\\op_i}\\bgg_\\itt>0$ and $\\bgam^{\\W}_i=Y_i\\cdot \\vb^\\top\\x^{\\W}_i$. We obtain the following score inequalities\n\\begin{align}\\label{glob:score decomp}\n&\\bgam^{\\W}_i\\leq \\bgam^{\\op}_i-c_\\op (1-\\s^{(i)}_{\\op_i})<\\bgam^{\\op}_i,\\\\\n\n&|\\bgam^{\\W}_i-\\bgam^{\\op}_i|\\leq \\tn{\\vb}\\cdot\\tn{\\x^{\\W}_i-\\xa_i}\\leq \\tn{\\vb} \\sum_{t\\neq \\op_i}\\s^{(i)}_t\\tn{\\x_\\itt-\\xa_i}\\leq B (1-\\s^{(i)}_{\\op_i}).\\nonumber\n\n\\end{align}\nWe will use the $\\bgam^{\\W}_i-\\bgam^{\\op}_i$ term in \\eqref{glob:score decomp} to evaluate $\\W$ against the reference loss $\\Lc_\\star$ of \\eqref{glob:asymp loss}. \n\nUsing the strictly-decreasing nature of $\\ell$, we conclude with the fact that for all (finite) $\\W\\in \\Rcm$, \n\\[\n\\Lc(\\W)=\\frac{1}{n}\\sum_{i=1}^n \\ell(\\bgam^{\\W}_i)> \\Lc_\\st=\\frac{1}{n}\\sum_{i=1}^n \\ell(\\bgam^{\\op}_i),\n\\]\nwhich implies $\\td{\\wrb{R}}\\rightarrow\\infty$ together with \\eqref{glob:asymp loss}.\n\n<b>Step 2:</b> To proceed, we show that $\\wrb{R}$ converges in direction to $\\Wcs$, which denotes the set of SVM minima. Suppose this is not the case and~convergence fails. We will obtain a contradiction by showing that $\\Wsb_R=R\\cdot\\Wsb$ achieves a strictly superior loss compared to $\\wrb{R}$. Let us introduce the normalized parameters $\\wrt{R}=\\frac{\\wrb{R}}{R\\xdm}$ and $\\W'=\\frac{\\wrb{R}}{\\td{\\wrb{R}}\\xdm}$. Note that $\\wrt{R}$ is obtained by scaling down $\\W'$ since $\\td{\\wrb{R}}\\leq R$ and $\\W'$ obeys $\\td{\\W'}=\\td{\\Wm}$.\nSince $\\wrt{R}$ fails to converge to $\\Wcs$, for some $\\delta>0$, there exists arbitrarily large $R>0$ such that $\\dist{\\wrt{R},\\Wcs}\\geq \\delta$. This translates to the suboptimality in terms of the margin constraints as follows: First, since nuclear norm dominates Frobenius, distance with respect to the $\\dm$-norm obeys $\\distd{\\wrt{R},\\Wcs}\\geq \\delta$. Secondly, using triangle inequality,\n\\[ \n\\text{this implies that either~~~}\\td{\\wrt{R}}\\leq \\td{\\Wm}-\\delta/2\\text{~~~or~~~}\\distd{\\W',\\Wcs}\\geq \\delta/2.\n\\]\nIn either scenario, $\\wrt{R}$ strictly violates one of the margin constraints of \\eqref{seqattnsvm}: If $\\td{\\wrt{R}}\\leq \\td{\\Wm}-\\delta/2$, then, since the optimal SVM objective is $\\td{\\Wm}$, there exists a constraint $i,t\\neq\\op_i$ for which $\\inn{(\\x^\\op_i-\\x_\\itt)\\z_i^\\top,\\wrt{R}}\\leq 1-\\frac{\\delta}{2\\td{\\Wm}}$. If $\\distd{\\W',\\Wcs}\\geq \\delta/2$, then, $\\W'$ has the same SVM objective but it is strictly bounded away from the solution set. Thus, for some $\\eps:=\\eps(\\delta)>0$, $\\W'$ and its scaled down version $\\wrt{R}$ strictly violate an SVM constraint achieving margin $\\leq 1-\\eps$. Without losing generality, suppose $\\wrt{R}$ violates the first constraint $i=1$. Thus, for a properly updated $\\delta>0$ (that is function of the initial $\\delta>0$) and for $i=1$ and some \\nei $\\tau\\in \\Tc_1$,\n\\begin{align}\n\\inn{ (\\x^\\op_1-\\x_{1t})\\z_1^\\top,\\wrt{R}}\\leq 1-\\delta.\\label{margin violate:glob}\n\n\\end{align}\nNow, we will argue that this leads to a contradiction by proving $\\Lc(\\Wsb_R)<\\Lc(\\wrb{R})$ for sufficiently large $R$.\n\nTo obtain the result, we establish a refined softmax probability control as in Step 1 by studying distance to $\\Lc_\\star$. Following \\eqref{glob:score decomp}, denote the score function at $\\wrb{R}$ via $\\bgam^R_i:=\\bgam_i^{\\wrb{R}}$. Similarly, let $\\sir_i=\\sft{\\abr_i}$ with $\\abr_i=\\X_i\\wrb{R}\\z_i$. Set the corresponding notation for the reference parameter $\\Wsb_R$ as $\\bgam^\\st_i,\\s^\\st_i,\\ab^\\st_i$.  Recall that $R\\geq \\td{\\wrb{R}}$ and $\\xdm:=1/\\td{\\Wm}$. We note the following softmax inequalities \n\\begin{align}\n&\\s^\\st_{i\\op_i}\\geq \\frac{1}{1+Te^{-R\\xdm}}\\geq 1-Te^{-R\\xdm}\\quad \\text{for all}\\quad i\\in[n], \\label{glob:salpha bounds}\\\\\n&s^R_{i\\op_i}\\leq \\frac{1}{1+e^{-(1-\\delta)\\td{\\wrb{R}}\\xdm}}\\leq \\frac{1}{1+e^{-(1-\\delta)R\\xdm}}\\quad\\text{for}\\quad i=1.\\nn\n\\end{align}\nThe former inequality is thanks to $\\Wm$ achieving $\\geq$$1$ margins on all tokens $[T]-\\op_i$ and the latter arises from the $\\delta$-margin violation of $\\wrb{R}$ at $i=1$ i.e.~Eq.~\\eqref{margin violate:glob}. Since $\\ell$ is strictly decreasing with Lipschitz derivative and the scores are upper/lower bounded by an absolute constant (as tokens are bounded and fixed), we have that $\\cop\\geq -\\ell'(\\bgam_i^{\\W})\\geq \\cdn$ for some constants $\\cop>\\cdn>0$. Thus, following Eq.~\\eqref{glob:BB eq}, the score decomposition \\eqref{glob:score decomp}, and \\eqref{glob:salpha bounds} we can write\n\\begin{align}\\label{glob:ineq prl}\n\\Lc(\\wrb{R})-\\Lc_\\star&\\geq \\frac{1}{n}[\\ell(\\bgam_1^{\\wrb{R}})-\\ell(\\bgam^{\\op}_1)]\\geq \\frac{\\cdn}{n}(\\bgam^{\\op}_{1}-\\bgam_1^{\\wrb{R}})\\\\\n\\nonetheless\n&\\geq \\frac{\\cdn}{n}c_\\op (1-\\s^R_{1\\op_1}).\n\\\\\n\\nonetheless \n&\\geq \\frac{\\cdn c_\\op}{n}\\frac{1}{1+e^{(1-\\delta)R\\xdm}}.\n\\end{align}\nConversely, we upper bound the difference between $\\Lc(\\Wsb_R)$ and $\\Lc_\\star$ as follows. Define the worst-case loss difference for $\\wrb{R}$ as $j=\\arg\\max_{i\\in[n]}[\\ell(\\bgam_i^\\st)-\\ell(\\bgam^{\\op}_i)]$. Using \\eqref{glob:score decomp}\\&\\eqref{glob:salpha bounds}, we write\n\n\\begin{equation*}\n\\begin{aligned}\n\\Lc(\\Wsb_R)-\\Lc_\\star&\\leq \\max_{i\\in[n]}[\\ell(\\bgam_i^\\st)-\\ell(\\bgam^{\\op}_i)]\\leq \\cop\\cdot(\\bgam^{\\op}_{j}-\\bgam^\\st_{j})\\\\\n&\\leq \\cop\\cdot(1-\\s^\\st_{j\\op_j})B\\\\\n&\\leq \\cop\\cdot Te^{-R\\xdm}B.\n\\end{aligned}\n\\end{equation*}\nCombining the last inequality and \\eqref{glob:ineq prl}, we conclude that $\\Lc(\\Wsb_R)<\\Lc(\\wrb{R})$ whenever\n\\[\n\\cop T\\cdot e^{-R\\xdm}B<\\frac{\\cdn\\cdot c_\\op }{n}\\frac{1}{1+e^{(1-\\delta)R\\xdm}}\\iff \\frac{e^{R\\xdm}}{1+e^{(1-\\delta)R\\xdm}}> \\frac{\\cop Tn B}{\\cdn c_\\op }.\n\\]\nThe left hand-side inequality holds for all sufficiently large $R$: Specifically, as soon as $R$ obeys $R>\\frac{1}{\\delta\\xdm}\\log(\\frac{2\\cop Tn B}{\\cdn c_\\op})$. This completes the proof of the theorem by contradiction since we obtained $\\Lc(\\wrb{R})>\\Lc(\\Wsb_R)$.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n<br>\n<br><b>Step 1:</b> <i>Optimal Risk Achievement</i>: The first step is to show that \\(\\wrb{R}\\) achieves the optimal risk as \\(R \\rightarrow \\infty\\). Define \\(\\xdm = 1/\\td{\\Wm}\\) and norm-normalized \\(\\Wsb = \\xdm \\Wm\\). The proof starts by noting that \\(\\Wm\\) separates tokens \\(\\op\\) from the rest for each \\(i \\in [n]\\). This leads to:\n<br>\n\\[\n\\lim_{R \\rightarrow \\infty} \\Lc(\\wrb{R}) \\leq \\lim_{R \\rightarrow \\infty} \\Lc(R \\cdot \\Wsb) := \\Lc_\\star = \\frac{1}{n} \\sum_{i=1}^n \\ell(\\bgam^{\\op}_i).\n\\]\n<br>\n<br>For any \\(\\W \\in \\Rcm\\), define the softmax probabilities \\(\\s^{(i)} = \\sft{\\X_i \\W \\z_i}\\) and attention features \\(\\x^{\\W}_i = \\sum_{t=1}^T \\s^{(i)}_t \\x_t\\). Decompose \\(\\x^{\\W}_i\\) and define:\n<br>\n\\[\nB := \\max_{i \\in [n]} \\max_{t, \\tau \\in [T]} \\tn{\\vb} \\cdot \\tn{\\x_\\itt - \\x_\\ittt} \\geq \\bgg_\\itt.\n\\]\n<br>\n<br>Using the strictly-decreasing nature of \\(\\ell\\), it is concluded that for all (finite) \\(\\W \\in \\Rcm\\):\n<br>\n\\[\n\\Lc(\\W) = \\frac{1}{n} \\sum_{i=1}^n \\ell(\\bgam^{\\W}_i) > \\Lc_\\st = \\frac{1}{n} \\sum_{i=1}^n \\ell(\\bgam^{\\op}_i),\n\\]\n<br>which implies \\(\\td{\\wrb{R}} \\rightarrow \\infty\\).\n<br>\n<br><b>Step 2:</b> <i>Convergence in Direction</i>: The second step is to show that \\(\\wrb{R}\\) converges in direction to \\(\\Wcs\\), the set of SVM minima. Suppose convergence fails. Introduce normalized parameters \\(\\wrt{R} = \\frac{\\wrb{R}}{R \\xdm}\\) and \\(\\W' = \\frac{\\wrb{R}}{\\td{\\wrb{R}} \\xdm}\\). Since \\(\\wrt{R}\\) fails to converge to \\(\\Wcs\\), for some \\(\\delta > 0\\), there exists arbitrarily large \\(R > 0\\) such that \\(\\dist{\\wrt{R}, \\Wcs} \\geq \\delta\\). This implies:\n<br>\n\\[\n\\text{either} \\quad \\td{\\wrt{R}} \\leq \\td{\\Wm} - \\delta/2 \\quad \\text{or} \\quad \\distd{\\W', \\Wcs} \\geq \\delta/2.\n\\]\n<br>\n<br>In either scenario, \\(\\wrt{R}\\) strictly violates one of the margin constraints of \\eqref{seqattnsvm}. Without losing generality, suppose \\(\\wrt{R}\\) violates the first constraint \\(i = 1\\). Thus, for a properly updated \\(\\delta > 0\\):\n<br>\n\\[\n\\inn{ (\\x^\\op_1 - \\x_{1t}) \\z_1^\\top, \\wrt{R}} \\leq 1 - \\delta.\n\\]\n<br>\n<br>Now, we argue that this leads to a contradiction by proving \\(\\Lc(\\Wsb_R) < \\Lc(\\wrb{R})\\) for sufficiently large \\(R\\). Define the worst-case loss difference for \\(\\wrb{R}\\) as \\(j = \\arg\\max_{i \\in [n]} [\\ell(\\bgam_i^\\st) - \\ell(\\bgam^{\\op}_i)]\\). Using the score decomposition and softmax inequalities, we conclude:\n<br>\n\\[\n\\Lc(\\Wsb_R) < \\Lc(\\wrb{R}) \\quad \\text{whenever} \\quad R > \\frac{1}{\\delta \\xdm} \\log \\left( \\frac{2 \\cop Tn B}{\\cdn c_\\op} \\right).\n\\]\n<br>\n<br>This completes the proof by contradiction since we obtained \\(\\Lc(\\wrb{R}) > \\Lc(\\Wsb_R)\\)."
                        }
                    }
                ],
                "theorems": [
                    {
                        "statement_id": "40c46579-becc-423d-9ce9-0e22387b1eda",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 21,
                        "library_name": "Theorem 21",
                        "title": "Regularization Path Bias Theorem",
                        "statement_original_tex": "\\begin{theorem}\\label{thm global reg path}\nSuppose Assumptions \\ref{assum:loss:prope} holds, optimal indices $(\\op_i)_{i=1}^n$ are unique, and \\eqref{eqn:sattnsvm} is feasible. Let $\\Wm$ be the unique solution of \\eqref{eqn:sattnsvm}, and let $\\Wc^\\svm_\\star$ be the solution set of \\eqref{eqn:sattnsvmst} with nuclear norm achieving objective $C_\\st$. Then, Algorithms~\\ref{RP-W} and \\ref{RP-QK}, respectively, satisfy:\n\\begin{itemize}\n\\item $\\W$-parameterization has Frobenius norm bias: $\\underset{R\\rightarrow\\infty}{\\lim} \\frac{\\Wb_R}{R}=\\frac{\\Wm}{\\tf{\\Wm}}$.\n\\item $(\\Kb,\\Qb)$-parameterization has nuclear norm bias: $\\underset{R\\rightarrow\\infty}{\\lim} \\dist{\\frac{\\Kbb_R\\Qbb_R^\\top}{R},\\frac{\\Wc^\\svm_\\star}{C_\\st}}=0$.\n\\begin{itemize}\n\\item Setting $m=d$: \\eqref{eqn:sattnsvmst} is a convex problem without rank constraints.\n\\end{itemize} \n\\end{itemize}\n\\end{theorem}",
                        "statement_html": "Suppose Assumptions A [in <a href=\"https://arxiv.org/pdf/2308.16898#assumption.1\">original paper</a>] holds, optimal indices $(\\op_i)_{i=1}^n$ are unique, and (<a href=\"https://arxiv.org/pdf/2308.16898#equation.2.3\">Att-SVM</a>) is feasible. Let $\\Wm$ be the unique solution of (<a href=\"https://arxiv.org/pdf/2308.16898#equation.2.3\">Att-SVM</a>), and let $\\Wc^\\svm_\\star$ be the solution set of (<a href=\"https://arxiv.org/pdf/2308.16898#equation.2.3\">Att-SVM$_*$</a>) with nuclear norm achieving objective $C_\\st$. Then, Algorithms~$\\ref{RP-W}$ and $\\ref{RP-QK}$, respectively, satisfy:\n<ul>\n<li>$\\W$-parameterization has Frobenius norm bias: $\\underset{R\\rightarrow\\infty}{\\lim} \\frac{\\Wb_R}{R}=\\frac{\\Wm}{\\tf{\\Wm}}$.</li>\n<li>$(\\Kb,\\Qb)$-parameterization has nuclear norm bias: $\\underset{R\\rightarrow\\infty}{\\lim} \\dist{\\frac{\\Kbb_R\\Qbb_R^\\top}{R},\\frac{\\Wc^\\svm_\\star}{C_\\st}}=0$.\n<ul>\n<li>Setting $m=d$: (<a href=\"https://arxiv.org/pdf/2308.16898#equation.2.3\">Att-SVM$_*$</a>) is a convex problem without rank constraints.</li>\n</ul>\n</li>\n</ul>",
                        "statement_type": "theorem",
                        "statement_motivation_html": "Understanding the behavior of the $\\W$-parameterization and $(\\Kb,\\Qb)$-parameterization in the context of optimization problems is crucial for designing efficient algorithms. The Frobenius norm bias and nuclear norm bias results provide insights into how these parameterizations scale with $R$, which can guide the choice of parameterization depending on the desired properties of the solution. This is particularly useful in machine learning and signal processing applications where the structure and regularization of the solution play a significant role in performance.",
                        "html_url": "library/theorems/theorem_21/index.html",
                        "corollary_ids": [],
                        "proof": null
                    },
                    {
                        "statement_id": "9bdda760-d1bb-4706-ad4f-1be6948f697b",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 22,
                        "library_name": "Theorem 22",
                        "title": "Divergence of Weight Norms in Gradient Descent",
                        "statement_original_tex": "\\begin{theorem}\n\\label{diverg:norm:w}\nSuppose Assumption~\\ref{assum:loss:prope} on the loss function $\\ell$ and Assumption \\ref{assum:token} on the tokens hold.  \n\n\\begin{itemize}\n\\item \nThere is no $\\W\\in\\R^{d\\times d}$ satisfying $\\nabla \\Lc(\\W)=0$.\n    \\item  Algorithm~\\ref{GD-W} with the step size $\\eta \\leq 1 /L_{\\W}$ and any starting point $\\W(0)$ satisfies \n\n$\\lim_{k\\rightarrow\\infty} \\tf{\\W(k)}=\\infty$.\n\n\n\n\n\\end{itemize}\n\\end{theorem}",
                        "statement_html": "Suppose Assumption A [in <a href=\"https://arxiv.org/pdf/2308.16898#assumption.1\">original paper</a>] on the loss function $\\ell$ and Assumption B [in <a href=\"https://arxiv.org/pdf/2308.16898#assumption.2\">original paper</a>] on the tokens hold.  \n\n<ul>\n    <li>There is no $\\W\\in\\R^{d\\times d}$ satisfying $\\nabla \\Lc(\\W)=0$.</li>\n    <li>Algorithm  W-GD [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.2.3\">original paper</a>] with the step size $\\eta \\leq 1 /L_{\\W}$ and any starting point $\\W(0)$ satisfies \n\n    $$\\lim_{k\\rightarrow\\infty} \\tf{\\W(k)}=\\infty.$$\n    </li>\n</ul>",
                        "statement_type": "theorem",
                        "statement_motivation_html": "This result is crucial in the context of optimization, particularly when dealing with gradient descent algorithms. It tells us that under certain assumptions on the loss function and tokens, there is no stationary point where the gradient is zero. Additionally, it guarantees that the gradient descent algorithm will diverge to infinity if the step size is appropriately chosen. This insight is valuable for understanding the behavior of optimization algorithms and ensuring that they are applied under the right conditions to avoid divergence.",
                        "html_url": "library/theorems/theorem_22/index.html",
                        "corollary_ids": [],
                        "proof": null
                    },
                    {
                        "statement_id": "5ccd1067-55db-4603-9c88-a7cb4b97d4c1",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 23,
                        "library_name": "Theorem 23",
                        "title": "Global Convergence of Gradient Descent with Initial Gradient Condition",
                        "statement_original_tex": "\\begin{theorem}\\label{conv:gd:w:global:nabla0}\nSuppose Assumption~\\ref{assum:loss:prope} on the loss function $\\ell$ and Assumption \\ref{assum:nabla0} on the initial gradient hold. \n\n\\begin{itemize}\n\n\n\\item  For any $\\mu>0$, there exists  $R>0$ such  that   $\\conb_{\\mu,R}$ does not contain any  stationary points. \n\\item Fix any $\\mu \\in  (0, \\iota/\\tf{\\nabla \\Lc(0)})$. Consider GD iterations with $\\W(0)=0$, $\\W(1)=-R\\nabla\\,\\Lc(0)/\\tf{\\nabla\\Lc(0)}$, and $\\W(k+1)=\\W(k)-\\eta\\nabla\\Lc(\\W(k))$ for $k\\ge 1$, where $\\eta\\le 1/L_{\\W}$ and $R$ sufficiently large. If all iterates remain within $\\conb_{\\mu,R}$, then $\\lim_{k\\rightarrow\\infty} \\tf{\\W(k)}=\\infty$ and $\\lim_{k\\rightarrow\\infty}\\frac{\\W(k)}{\\tf{\\W(k)}}=\\frac{\\Wm}{\\tf{\\Wm}}$.\n\\end{itemize}\n\\end{theorem}",
                        "statement_html": "Suppose Assumption A [in <a href=\"https://arxiv.org/pdf/2308.16898#assumption.1\">original paper</a>] on the loss function $\\ell$ and Assumption $\\ref{assum:nabla0}$ on the initial gradient hold.\n\n<ul>\n    <li>For any $\\mu>0$, there exists $R>0$ such that $\\conb_{\\mu,R}$ does not contain any stationary points.</li>\n    <li>Fix any $\\mu \\in (0, \\iota/\\tf{\\nabla \\Lc(0)})$. Consider GD iterations with $\\W(0)=0$, $\\W(1)=-R\\nabla\\,\\Lc(0)/\\tf{\\nabla\\Lc(0)}$, and $\\W(k+1)=\\W(k)-\\eta\\nabla\\Lc(\\W(k))$ for $k\\ge 1$, where $\\eta\\le 1/L_{\\W}$ and $R$ sufficiently large. If all iterates remain within $\\conb_{\\mu,R}$, then $\\lim_{k\\rightarrow\\infty} \\tf{\\W(k)}=\\infty$ and $\\lim_{k\\rightarrow\\infty}\\frac{\\W(k)}{\\tf{\\W(k)}}=\\frac{\\Wm}{\\tf{\\Wm}}$.</li>\n</ul>",
                        "statement_type": "theorem",
                        "statement_motivation_html": "Understanding the behavior of gradient descent (GD) iterations is crucial for optimizing complex functions. This statement provides conditions under which GD will not converge to stationary points, ensuring continuous progress towards minimizing the loss function. It is particularly useful when analyzing the convergence properties of GD in high-dimensional spaces and ensuring that the algorithm does not get stuck in suboptimal points.",
                        "html_url": "library/theorems/theorem_23/index.html",
                        "corollary_ids": [],
                        "proof": null
                    },
                    {
                        "statement_id": "97b4a9b8-465c-4bc8-ae60-1214494cef10",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 24,
                        "library_name": "Theorem 24",
                        "title": "Local Gradient Descent Convergence Theorem",
                        "statement_original_tex": "\\begin{theorem}\n\\label{thm:local:gd} \nSuppose Assumption~\\ref{assum:loss:prope} on the loss $\\ell$ holds, and let $\\bal=(\\alpha_i)_{i=1}^n$ be locally optimal tokens according to Definition \\ref{def loc opt}. Let $ \\Wma$ denote the SVM solution obtained via \\eqref{eqn:sattnsvm} by  replacing $(\\opt_i)_{i=1}^n$ with $\\boldsymbol{\\alpha} = (\\alpha_i)_{i=1}^n$. \n\n\\begin{itemize}\n    \\item \\label{lem:cond:t1}  There exist parameters $\\mu=\\mu(\\bal) \\in (0,1)$ and  $R>0$ such  that   $ \\Cc_{\\mu,R} (\\Wma)$ does not contain any  stationary points.\n    \\item  Algorithm~\\ref{GD-W} with $\\eta \\leq 1 /L_{\\W}$ and any $\\W(0) \\in \\Cc_{\\mu,R}(\\Wma)$ satisfies $\\lim_{k\\rightarrow\\infty} \\tf{\\W(k)} = \\infty$  and $\\lim_{k\\rightarrow\\infty} \\frac{\\W(k)}{\\tf{\\W(k)}} = \\frac{\\Wma}{\\tf{\\Wma}}$.\n\\end{itemize}\n\\end{theorem}",
                        "statement_html": "Suppose Assumption A [in <a href=\"https://arxiv.org/pdf/2308.16898#assumption.1\">original paper</a>] on the loss $\\ell$ holds, and let $\\bal=(\\alpha_i)_{i=1}^n$ be locally optimal tokens according to <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/definitions/definition_7/index.html#def+loc+opt\">Definition 7</a>. Let $ \\Wma$ denote the SVM solution obtained via (<a href=\"https://arxiv.org/pdf/2308.16898#equation.2.3\">Att-SVM</a>) by replacing $(\\opt_i)_{i=1}^n$ with $\\boldsymbol{\\alpha} = (\\alpha_i)_{i=1}^n$. \n\n<ul>\n    <li>There exist parameters $\\mu=\\mu(\\bal) \\in (0,1)$ and  $R>0$ such  that $\\Cc_{\\mu,R} (\\Wma)$ does not contain any stationary points.</li>\n    <li>Algorithm  W-GD [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.2.3\">original paper</a>] with $\\eta \\leq 1 /L_{\\W}$ and any $\\W(0) \\in \\Cc_{\\mu,R}(\\Wma)$ satisfies $\\lim_{k\\rightarrow\\infty} \\tf{\\W(k)} = \\infty$ and $\\lim_{k\\rightarrow\\infty} \\frac{\\W(k)}{\\tf{\\W(k)}} = \\frac{\\Wma}{\\tf{\\Wma}}$.</li>\n</ul>",
                        "statement_type": "theorem",
                        "statement_motivation_html": "Understanding the behavior of the SVM solution $\\Wma$ in the context of locally optimal tokens $\\bal$ is crucial for ensuring the effectiveness of the algorithm. The given statement provides conditions under which the gradient descent algorithm will converge to a solution that scales proportionally to $\\Wma$. This is particularly useful in machine learning applications where ensuring convergence to a desirable solution is essential for model performance and reliability.",
                        "html_url": "library/theorems/theorem_24/index.html",
                        "corollary_ids": [],
                        "proof": null
                    },
                    {
                        "statement_id": "031d5613-3df2-4503-aada-9d37374107c8",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 25,
                        "library_name": "Theorem 25",
                        "title": "Separation Theorem for Self-Attention",
                        "statement_original_tex": "\\begin{theorem}\\label{thm:separation} Suppose $d\\geq \\max(T-1,n)$. Then, almost all datasets $(Y_i,\\X_i,\\z_i)_{i=1}^n$ -- including the self-attention setting with $\\z_i\\gets\\x_{i1}$ -- obey the following: \n\\eqref{eqn:sattnsvm} is feasible i.e.,~$\\Wm$ separates the desired tokens $\\opt=(\\opt_i)_{i=1}^n$.\n\\end{theorem}",
                        "statement_html": "Suppose $d\\geq \\max(T-1,n)$. Then, almost all datasets $(Y_i,\\X_i,\\z_i)_{i=1}^n$ -- including the self-attention setting with $\\z_i\\gets\\x_{i1}$ -- obey the following: \n(<a href=\"https://arxiv.org/pdf/2308.16898#equation.2.3\">Att-SVM</a>) is feasible i.e.,~$\\Wm$ separates the desired tokens $\\opt=(\\opt_i)_{i=1}^n$.",
                        "statement_type": "theorem",
                        "statement_motivation_html": "This statement is useful in the context of machine learning, particularly in the analysis of self-attention mechanisms within neural networks. It provides a condition under which a support vector machine (SVM) can successfully separate desired tokens in a dataset. This is crucial for ensuring that the model can effectively learn and make accurate predictions. Use this when you need to verify the feasibility of separating tokens in datasets, especially when dealing with high-dimensional data or complex attention mechanisms.",
                        "html_url": "library/theorems/theorem_25/index.html",
                        "corollary_ids": [],
                        "proof": null
                    },
                    {
                        "statement_id": "c5c7d690-f33d-4f4c-b371-ee58d631710e",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 26,
                        "library_name": "Theorem 26",
                        "title": "Separation Feasibility Theorem",
                        "statement_original_tex": "\\begin{theorem}\\label{separation thm} Suppose $d\\geq \\max(T-1,n)$ and $m=d$. Then, almost all datasets\\footnote{Here, \\emph{``almost all datasets''} means that adding i.i.d.~gaussian noise, with arbitrary nonzero variance, to the input features will almost surely result in SVM's feasibility.} $(Y_i,\\X_i,\\z_i)_{i=1}^n$ -- including the self-attention setting with $\\z_i\\gets\\x_{i1}$ -- obey the following: For any choice of indices $\\bal=(\\alpha_i)_{i=1}^n\\subset[T]$, \\eqref{dmattnsvm} is feasible,  i.e.~the attention layer can separate and select indices $\\bal$.\n\\end{theorem}",
                        "statement_html": "Suppose $d\\geq \\max(T-1,n)$ and $m=d$. Then, almost all datasets<sup>1</sup> $(Y_i,\\X_i,\\z_i)_{i=1}^n$ -- including the self-attention setting with $\\z_i\\gets\\x_{i1}$ -- obey the following: For any choice of indices $\\bal=(\\alpha_i)_{i=1}^n\\subset[T]$, (<a href=\"https://arxiv.org/pdf/2308.16898#equation.5.6\">$\\diamond$-SVM</a>) is feasible,  i.e.~the attention layer can separate and select indices $\\bal$.\n\n<sup>1</sup> Here, $\\emph{``almost all datasets''}$ means that adding i.i.d.~gaussian noise, with arbitrary nonzero variance, to the input features will almost surely result in SVM's feasibility.",
                        "statement_type": "theorem",
                        "statement_motivation_html": "This statement is useful in the context of machine learning, particularly in the design and analysis of attention mechanisms within neural networks. It guarantees that, under certain conditions, the attention layer can effectively separate and select specific indices from the input data. This is crucial for tasks that require focusing on particular parts of the input, such as in natural language processing or image recognition. The result also highlights the robustness of the attention mechanism, as it holds true even when the input features are perturbed with Gaussian noise.",
                        "html_url": "library/theorems/theorem_26/index.html",
                        "corollary_ids": [],
                        "proof": null
                    },
                    {
                        "statement_id": "c67e2c58-5978-44cb-af42-e5e3a9a98a33",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 27,
                        "library_name": "Theorem 27",
                        "title": "Convergence of Local Regularization Paths",
                        "statement_original_tex": "\\begin{theorem} [Convergence of Local Regularization Path]\\label{local RP thm1} Suppose Assumption \\ref{assum:loss:prope} holds. Fix locally-optimal token indices $\\bal=(\\al_i)_{i=1}^n$ and $R_0,\\eps>0$. Consider the norm-constrained variation of \\eqref{cone alpha eq1} defined as \n\\[\n\\Ccd:=\\con{\\bal}\\bigcap \\left\\{\\W\\bgl \\td{\\W}\\geq R_0\\right\\}.\n\\]\nDefine local RP as $\\Wb_R=\\min_{\\Ccd,\\td{\\W}\\leq R}\\Lc(\\W)$ where $\\Lc(\\W)$ is given by \\eqref{eqn:erm:w}. Let $\\Wcs$ be the set of minima for \\eqref{dmattnsvm} and $\\xdm>0$ be the associated margin i.e.~$\\xdm=1/\\td{\\Wma}$. For any sufficiently small $\\eps>0$ and sufficiently large $R_0= \\order{1/\\eps}>0$, $\\lim_{R\\rightarrow\\infty} \\dist{\\frac{\\Wb_R}{R\\xdm},\\Wcs}=0$. Additionally, suppose optimal indices $\\op=(\\op_i)_{i=1}^n$ are unique and set $\\bal\\gets\\op$. Then, the same convergence guarantee on regularization path holds by setting $\\Ccd$ as the set of rank-$\\leq$$m$ matrices.\n\\end{theorem}",
                        "statement_html": "Suppose Assumption A [in <a href=\"https://arxiv.org/pdf/2308.16898#assumption.1\">original paper</a>] holds. Fix locally-optimal token indices $\\bal=(\\al_i)_{i=1}^n$ and $R_0,\\eps>0$. Consider the norm-constrained variation of $\\eqref{cone alpha eq1}$ defined as \n\\[\n\\Ccd:=\\con{\\bal}\\bigcap \\left\\{\\W\\bgl \\td{\\W}\\geq R_0\\right\\}.\n\\]\nDefine local RP as $\\Wb_R=\\min_{\\Ccd,\\td{\\W}\\leq R}\\Lc(\\W)$ where $\\Lc(\\W)$ is given by (<a href=\"https://arxiv.org/pdf/2308.16898#figure.3\">W-ERM</a>). Let $\\Wcs$ be the set of minima for (<a href=\"https://arxiv.org/pdf/2308.16898#equation.5.6\">$\\diamond$-SVM</a>) and $\\xdm>0$ be the associated margin i.e.~$\\xdm=1/\\td{\\Wma}$. For any sufficiently small $\\eps>0$ and sufficiently large $R_0= \\order{1/\\eps}>0$, $\\lim_{R\\rightarrow\\infty} \\dist{\\frac{\\Wb_R}{R\\xdm},\\Wcs}=0$. Additionally, suppose optimal indices $\\op=(\\op_i)_{i=1}^n$ are unique and set $\\bal\\gets\\op$. Then, the same convergence guarantee on regularization path holds by setting $\\Ccd$ as the set of rank-$\\leq$$m$ matrices.",
                        "statement_type": "theorem",
                        "statement_motivation_html": "This statement is useful in the context of optimization problems, particularly those involving norm-constrained variations. It provides a convergence guarantee for the regularization path, ensuring that as the constraint parameter $R$ grows, the solution approaches the set of minima for the original problem. This is particularly valuable when dealing with large-scale optimization tasks where ensuring convergence to an optimal solution is critical.",
                        "html_url": "library/theorems/theorem_27/index.html",
                        "corollary_ids": [],
                        "proof": null
                    },
                    {
                        "statement_id": "f0b0e79a-2e19-42da-98be-5ff35441a595",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 28,
                        "library_name": "Theorem 28",
                        "title": "Gradient Initialization in Dataset Models",
                        "statement_original_tex": "\\begin{theorem}\\label{toy data thm} Consider the dataset model $\\Dc_{\\texttt{data}}$ of Def.~\\ref{def data model}. Denote the initial population gradient $\\nabla\\Lc(0):=\\E_{\\Dc_{\\texttt{data}}}[\\nabla\\Lc(0)]$. Let $\\W_1=\\frac{1}{r}\\sum_{j=1}^r\\ab_j\\ab_j^\\top$ and $\\W_\\rho=\\frac{1}{r}\\sum_{j=1}^r\\bb_j\\ab_j^\\top$. We have that\n\\begin{align}\n&\\nabla\\Lc(0)=\\frac{\\ell'(0)}{T}(\\gamma_1\\W_1+\\gamma_\\rho\\W_\\rho)-\\frac{\\ell'(0)}{T^2}(\\gamma_1+\\gamma_\\rho)(\\W_1+\\W_\\rho)\\label{eq nablaL0}\n\\end{align}\nAdditionally, suppose Assumption \\ref{assume sep} holds. Then, $\\x_\\rho$ is the optimal token and Assumption \\ref{assum:nabla0} holds almost surely i.e.\n\n\\[\n\\underset{t\\in[T]}{\\min}\\li(\\x_t-\\x_\\rho)^\\top\\nabla\\Lc(0)\\x_1\\ri>0.\n\\]\n\\end{theorem}",
                        "statement_html": "Consider the dataset model $\\Dc_{\\texttt{data}}$ of Def.~<a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/definitions/definition_10/index.html#def+data+model\">Definition 10</a>. Denote the initial population gradient $\\nabla\\Lc(0):=\\E_{\\Dc_{\\texttt{data}}}[\\nabla\\Lc(0)]$. Let $\\W_1=\\frac{1}{r}\\sum_{j=1}^r\\ab_j\\ab_j^\\top$ and $\\W_\\rho=\\frac{1}{r}\\sum_{j=1}^r\\bb_j\\ab_j^\\top$. We have that\n\\begin{align}\n&\\nabla\\Lc(0)=\\frac{\\ell'(0)}{T}(\\gamma_1\\W_1+\\gamma_\\rho\\W_\\rho)-\\frac{\\ell'(0)}{T^2}(\\gamma_1+\\gamma_\\rho)(\\W_1+\\W_\\rho)\\label{eq nablaL0}\n\\end{align}\nAdditionally, suppose Assumption $\\ref{assume sep}$ holds. Then, $\\x_\\rho$ is the optimal token and Assumption $\\ref{assum:nabla0}$ holds almost surely i.e.\n\n\\[\n\\underset{t\\in[T]}{\\min}\\li(\\x_t-\\x_\\rho)^\\top\\nabla\\Lc(0)\\x_1\\ri>0.\n\\]",
                        "statement_type": "theorem",
                        "statement_motivation_html": "Understanding the initial population gradient $\\nabla\\Lc(0)$ is crucial for analyzing the behavior of learning algorithms, especially in the context of large datasets. This expression helps in determining how the initial gradient is influenced by the data distribution and the model parameters. It is particularly useful when optimizing learning algorithms to ensure they converge efficiently and effectively.",
                        "html_url": "library/theorems/theorem_28/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "c30a23ae-108d-4d8b-9a13-fec5fa3038bc",
                            "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof} We will make use of the simple structure of softmax at $\\W=0$. Let $\\bgam=Y\\cdot\\X\\vb$ and $\\Fb_t=\\x_t\\x_1^\\top$. Using the fact that softmax derivative at $0$ is simply all $1/T$ vector, we have that\n\\begin{align}\n\\nabla\\Lc(0)=\\frac{\\ell'(0)}{T}\\E[\\sum_{t=1}^T \\bgam_t\\Fb_t]-\\frac{\\ell'(0)}{T^2}\\E[\\sum_{t=1}^T \\bgam_t\\sum_{t=1}^T \\Fb_t].\n\\end{align}\n\nLet $\\bSi=\\E[\\x_t\\x_t^\\top]$ for $t\\notin\\{1,\\rho\\}$. To proceed, observe that, for $t\\not\\in\\{1,\\rho\\}$\n\\[\n\\E[\\bgam_t\\Fb_t]=\\E[Y\\vb^\\top\\x_t\\x_t\\x_1^\\top]=\\E[\\vb^\\top\\x_t\\x_t]\\E[Y\\x_1]^\\top=\\bSi\\vb\\E[Y\\x_1]^\\top=0.\n\\]\nSimilarly for $t\\not\\in\\{1,\\rho\\}$, we have $\\E[\\sum_{i=1}^T \\bgam_i\\Fb_t]=0$ by additionally using $\\E[\\x_t]=0$.\n\nWhat remains is $t=1$ and $t=\\rho$. Using fixed score assumption, we find\n\\[\n\\E[\\bgam_1\\Fb_1]=\\E[Y\\vb^\\top\\x_1\\x_1\\x_1^\\top]=\\gamma_1\\W_1,\\quad\\E[\\bgam_\\rho\\Fb_\\rho]=\\E[Y\\vb^\\top\\x_\\rho\\x_\\rho\\x_1^\\top]=\\gamma_\\rho\\W_\\rho.\n\\]\nSimilarly, we obtain\n\\[\n\\E[\\sum_{t=1}^T \\bgam_t\\sum_{t=1}^T \\Fb_t]=\\frac{1}{T^2}(\\gamma_1+\\gamma_\\rho)(\\W_1+\\W_\\rho),\n\\]\nto conclude with \\eqref{eq nablaL0}. To conclude with Assumption \\ref{assum:nabla0}, we use Assumption \\ref{assume sep} and observe that $T\\nabla\\Lc(0)$ is arbitrarily close to $T\\hat{\\nabla}\\Lc(0)$ where $\\hat{\\nabla}\\Lc(0)=\\frac{\\ell'(0)\\gamma_\\rho}{T}\\W_\\rho$. Now, recalling $\\ell'(0)<0$, applying the right hand-side of \\eqref{sep condition} with $\\hat{\\nabla}\\Lc(0)\\propto-\\W_\\rho$, and using the boundedness of tokens and $\\eps>0$ as perturbation buffer, we obtain the desired statement.\n\\end{proof}",
                            "statement_html": "We will make use of the simple structure of softmax at $\\W=0$. Let $\\bgam=Y\\cdot\\X\\vb$ and $\\Fb_t=\\x_t\\x_1^\\top$. Using the fact that softmax derivative at $0$ is simply all $1/T$ vector, we have that\n\\begin{align}\n\\nabla\\Lc(0)=\\frac{\\ell'(0)}{T}\\E[\\sum_{t=1}^T \\bgam_t\\Fb_t]-\\frac{\\ell'(0)}{T^2}\\E[\\sum_{t=1}^T \\bgam_t\\sum_{t=1}^T \\Fb_t].\n\\end{align}\n\nLet $\\bSi=\\E[\\x_t\\x_t^\\top]$ for $t\\notin\\{1,\\rho\\}$. To proceed, observe that, for $t\\not\\in\\{1,\\rho\\}$\n\\[\n\\E[\\bgam_t\\Fb_t]=\\E[Y\\vb^\\top\\x_t\\x_t\\x_1^\\top]=\\E[\\vb^\\top\\x_t\\x_t]\\E[Y\\x_1]^\\top=\\bSi\\vb\\E[Y\\x_1]^\\top=0.\n\\]\nSimilarly for $t\\not\\in\\{1,\\rho\\}$, we have $\\E[\\sum_{i=1}^T \\bgam_i\\Fb_t]=0$ by additionally using $\\E[\\x_t]=0$.\n\nWhat remains is $t=1$ and $t=\\rho$. Using fixed score assumption, we find\n\\[\n\\E[\\bgam_1\\Fb_1]=\\E[Y\\vb^\\top\\x_1\\x_1\\x_1^\\top]=\\gamma_1\\W_1,\\quad\\E[\\bgam_\\rho\\Fb_\\rho]=\\E[Y\\vb^\\top\\x_\\rho\\x_\\rho\\x_1^\\top]=\\gamma_\\rho\\W_\\rho.\n\\]\nSimilarly, we obtain\n\\[\n\\E[\\sum_{t=1}^T \\bgam_t\\sum_{t=1}^T \\Fb_t]=\\frac{1}{T^2}(\\gamma_1+\\gamma_\\rho)(\\W_1+\\W_\\rho),\n\\]\nto conclude with \\eqref{eq nablaL0}. To conclude with Assumption \\ref{assum:nabla0}, we use Assumption \\ref{assume sep} and observe that $T\\nabla\\Lc(0)$ is arbitrarily close to $T\\hat{\\nabla}\\Lc(0)$ where $\\hat{\\nabla}\\Lc(0)=\\frac{\\ell'(0)\\gamma_\\rho}{T}\\W_\\rho$. Now, recalling $\\ell'(0)<0$, applying the right hand-side of \\eqref{sep condition} with $\\hat{\\nabla}\\Lc(0)\\propto-\\W_\\rho$, and using the boundedness of tokens and $\\eps>0$ as perturbation buffer, we obtain the desired statement.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n<br>\n<br>1. <i>Initial Setup</i>: The proof starts by considering the gradient of the loss function \\(\\Lc\\) at \\(\\W=0\\). Using the simple structure of softmax at \\(\\W=0\\), we define \\(\\bgam=Y\\cdot\\X\\vb\\) and \\(\\Fb_t=\\x_t\\x_1^\\top\\). The gradient is expressed as:\n<br>   \\[\n   \\nabla\\Lc(0)=\\frac{\\ell'(0)}{T}\\E[\\sum_{t=1}^T \\bgam_t\\Fb_t]-\\frac{\\ell'(0)}{T^2}\\E[\\sum_{t=1}^T \\bgam_t\\sum_{t=1}^T \\Fb_t].\n   \\]\n<br>\n<br>2. <i>Expectation of \\(\\bgam_t\\Fb_t\\)</i>: Let \\(\\bSi=\\E[\\x_t\\x_t^\\top]\\) for \\(t\\notin\\{1,\\rho\\}\\). For \\(t\\not\\in\\{1,\\rho\\}\\), we observe:\n<br>   \\[\n   \\E[\\bgam_t\\Fb_t]=\\E[Y\\vb^\\top\\x_t\\x_t\\x_1^\\top]=\\E[\\vb^\\top\\x_t\\x_t]\\E[Y\\x_1]^\\top=\\bSi\\vb\\E[Y\\x_1]^\\top=0.\n   \\]\n<br>\n<br>3. <i>Summation of Expectations</i>: Similarly, for \\(t\\not\\in\\{1,\\rho\\}\\), we have:\n<br>   \\[\n   \\E[\\sum_{i=1}^T \\bgam_i\\Fb_t]=0\n   \\]\n   by additionally using \\(\\E[\\x_t]=0\\).\n<br>\n<br>4. <i>Special Cases for \\(t=1\\) and \\(t=\\rho\\)</i>: For \\(t=1\\) and \\(t=\\rho\\), using the fixed score assumption, we find:\n<br>   \\[\n   \\E[\\bgam_1\\Fb_1]=\\E[Y\\vb^\\top\\x_1\\x_1\\x_1^\\top]=\\gamma_1\\W_1,\\quad\\E[\\bgam_\\rho\\Fb_\\rho]=\\E[Y\\vb^\\top\\x_\\rho\\x_\\rho\\x_1^\\top]=\\gamma_\\rho\\W_\\rho.\n   \\]\n<br>\n<br>5. <i>Summation of Products</i>: We then obtain:\n<br>   \\[\n   \\E[\\sum_{t=1}^T \\bgam_t\\sum_{t=1}^T \\Fb_t]=\\frac{1}{T^2}(\\gamma_1+\\gamma_\\rho)(\\W_1+\\W_\\rho),\n   \\]\n   to conclude with \\(\\eqref{eq nablaL0}\\).\n<br>\n<br>6. <i>Final Conclusion</i>: To conclude with Assumption \\(\\ref{assum:nabla0}\\), we use Assumption \\(\\ref{assume sep}\\) and observe that \\(T\\nabla\\Lc(0)\\) is arbitrarily close to \\(T\\hat{\\nabla}\\Lc(0)\\) where \\(\\hat{\\nabla}\\Lc(0)=\\frac{\\ell'(0)\\gamma_\\rho}{T}\\W_\\rho\\). Recalling \\(\\ell'(0)<0\\), applying the right-hand side of \\(\\eqref{sep condition}\\) with \\(\\hat{\\nabla}\\Lc(0)\\propto-\\W_\\rho\\), and using the boundedness of tokens and \\(\\eps>0\\) as a perturbation buffer, we obtain the desired statement."
                        }
                    },
                    {
                        "statement_id": "e2b288d5-6f0b-4ed4-a0f3-4fdb25730c9d",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 29,
                        "library_name": "Theorem 29",
                        "title": "Convergence of Conic Regularization Paths",
                        "statement_original_tex": "\\begin{theorem} [Convergence of Local Regularization Path]\\label{local RP thm} Suppose \\eqref{seqattnsvm} is feasible and $\\bal=(\\al_\\ik)\\ikix$ are locally-optimal token indices. Suppose Assumptions \\ref{assum:loss:prope}\\&\\ref{ass cvx seq} hold. Recall $\\con{\\bal}$ of \\eqref{cone alpha eq} and consider the norm-constrained cone \\[\n\\Ccd:=\\con{\\bal}\\bigcap\\{\\W\\bgl \\td{\\W}\\geq R_0\\}.\n\\]\nDefine the conic regularization path $\\wrb{R}=\\min_{\\Ccd,\\td{\\W}\\leq R}\\Lc(\\W)$. Let $\\Wcs_{\\bal}$ be its set of minima and $\\xdm>0$ be the associated margin i.e.~$\\xdm=1/\\td{\\Wcs_{\\bal}}$. For any sufficiently small $\\eps>0$ and sufficiently large $R_0= \\order{1/\\eps}>0$, $\\lim_{R\\rightarrow\\infty} \\dist{\\frac{\\wrb{R}}{R\\xdm},\\Wcs_{\\bal}}=0$. Additionally, suppose optimal indices $\\op=(\\op_\\ik)\\ikix$ are unique and set $\\bal\\gets\\op$. Then, the same RP convergence guarantee holds with $\\Ccd=\\Rcm$.\n\\end{theorem}",
                        "statement_html": "Suppose \\eqref{seqattnsvm} is feasible and $\\bal=(\\al_\\ik)\\ikix$ are locally-optimal token indices. Suppose Assumptions A [in <a href=\"https://arxiv.org/pdf/2308.16898#assumption.1\">original paper</a>] \\& $\\ref{ass cvx seq}$ hold. Recall $\\con{\\bal}$ of \\eqref{cone alpha eq} and consider the norm-constrained cone \n\\[\n\\Ccd:=\\con{\\bal}\\bigcap\\{\\W\\bgl \\td{\\W}\\geq R_0\\}.\n\\]\nDefine the conic regularization path $\\wrb{R}=\\min_{\\Ccd,\\td{\\W}\\leq R}\\Lc(\\W)$. Let $\\Wcs_{\\bal}$ be its set of minima and $\\xdm>0$ be the associated margin i.e.~$\\xdm=1/\\td{\\Wcs_{\\bal}}$. For any sufficiently small $\\eps>0$ and sufficiently large $R_0= \\order{1/\\eps}>0$, $\\lim_{R\\rightarrow\\infty} \\dist{\\frac{\\wrb{R}}{R\\xdm},\\Wcs_{\\bal}}=0$. Additionally, suppose optimal indices $\\op=(\\op_\\ik)\\ikix$ are unique and set $\\bal\\gets\\op$. Then, the same RP convergence guarantee holds with $\\Ccd=\\Rcm$.",
                        "statement_type": "theorem",
                        "statement_motivation_html": "The given statement provides a convergence guarantee for the conic regularization path in the context of optimization problems involving norm-constrained cones. This result is particularly useful in machine learning and signal processing, where regularization techniques are employed to prevent overfitting and to ensure that solutions generalize well to unseen data. By understanding the behavior of the regularization path as the constraint parameter $R$ grows, practitioners can make informed decisions about the choice of regularization parameters and the expected performance of their models. This theorem is especially relevant when dealing with high-dimensional data and sparse solutions, as it provides insights into the asymptotic properties of the optimization landscape.",
                        "html_url": "library/theorems/theorem_29/index.html",
                        "corollary_ids": [
                            "dc3fce8f-00c5-4fe0-a3dc-df878f0858d7",
                            "7edf41cb-7240-4f9c-9d92-c315b2fccacd"
                        ],
                        "proof": {
                            "statement_id": "63451aba-82ef-4ebb-bca3-370dd94c4c87",
                            "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof} We will prove that $\\wrb{R}$ is the optimal direction and also $\\td{\\wrb{R}}\\rightarrow \\infty$. Define the absolute constant \n\\[\n\\cdm=\\min_{\\td{\\W}=1}\\tf{\\W}.\n\\]\nThis guarantees that for any $\\W$ we have $\\tf{\\W}\\geq \\cdm\\td{\\W}$. Also denote $\\epsd=\\cdm\\eps$. Let us first determine the $\\eps$ parameter: Fix $\\Wma\\in\\Wcs_{\\bal}$. For general $\\bal$, we can choose any $\\eps>0$ that is sufficiently small to guarantee $\\Wma\\in\\con{\\bal}$ based on Lemma \\ref{lemma cone}. For $\\bal=\\op$, our analysis will entirely avoid using $\\eps$, specifically, observe that $\\con{\\bal}=\\Rcm$ based on Lemma \\ref{lemma cone}.\n    \n\\noindent\\textbf{Step 1:} Let us first prove that $\\wrb{R}$ achieves the optimal risk as $R\\rightarrow\\infty$ -- rather than problem having finite optima. Define norm-normalized $\\Wsb=\\xdm\\Wma$. Note that $\\Wma$ separates tokens $\\bal$ from rest of the tokens for each $i,k\\in[n]\\times [K]$. Thus, we have that\n\\begin{align}\n\\lim_{R\\rightarrow\\infty}\\Lc(\\wrb{R})\\leq\\lim_{R\\rightarrow\\infty}\\Lc(R\\cdot\\Wsb):=\\Lc_\\star= \\frac{1}{n}\\sum_{i=1}^n\\sum_{k=1}^K\\ell(\\bga_\\ik).\\label{asymp loss}\n\\end{align}\nOn the other hand, for any choice of $\\W\\in \\con{\\bal}$, set $\\x^{\\W}_\\ik=\\sum_{t=1}^T \\sft{\\X_i\\W\\z_\\ik}_t\\x_t$. Set softmax probabilities $\\sik=\\sft{\\X_i\\W\\z_\\ik}$. Recalling $\\lowi,\\higi$ definitions, we can decompose the attention features as\n\\begin{align}\n\\x^{\\W}_\\ik=\\sik_{\\al_\\ik}\\xa_\\ik+\\sum_{t\\in\\lowi}\\sik_t\\x_\\itt+\\sum_{\\tau\\in\\higi}\\sik_\\tau\\x_{\\ittt}.\n\\end{align}\nWhen $\\bal=\\op$, note that we simply have $\\higi=\\emptyset$. This will be important for setting $R_0=0$ and $\\Ccd=\\Rcm$ in the proof for $\\op$ indices.\n\nSet $\\bgg_\\ikt=\\bgam_\\ikt-\\bga_\\ik=Y_\\ik\\cdot (h_k(\\x_\\itt)-h_k(\\xa_\\ik))$. Building on $L_h$-Lipschitzness of the prediction head $h_k(\\cdot)$, we define\n\\begin{align}\n&B:=\\max_{i\\in[n],k\\in[K]}\\max_{t,\\tau\\in[T]}L_h\\cdot \\tn{\\x_\\itt-\\x_\\ittt}\\geq |\\bgg_\\ikt|.\\label{BB eq}\n\\end{align}\n\n\n\n\n\n\n\nDefine $P^\\ik:=\\sum_{t\\in\\lowi}\\sik_t$, $Q^\\ik:=\\sum_{t\\in\\higi}\\sik_t$, and $\\bgam^{\\W}_\\ik=Y_\\ik\\cdot h_k(\\x^{\\W}_\\ik)$. Also set temporary variables $\\x'=(\\sik_{\\al_\\ik}+Q^\\ik)\\xa_\\ik+\\sum_{t\\in\\lowi}\\sik_t\\x_\\itt$ and $\\bgam'=Y_\\ik\\cdot h_k(\\x^{\\W}_\\ik)$. \nUsing Assumption \\ref{ass cvx seq} on $\\x'$ and noticing $P^\\ik=1-\\sik_{\\al_\\ik}-Q^\\ik$, observe that\n\\[ \n|\\bgam^{\\W}_\\ik-\\bgam'|\\leq BQ^\\ik\\quad\\text{and}\\quad \\bgam'\\leq \\bga_\\ik-c_\\bal P^\\ik. \n\\] \nRecall from \\eqref{c choice} that, when $h_k$ are linear functions, $c_\\bal$ can be chosen as \n\n\\begin{align*}\nc_\\bal:=\\min_{i\\in[n],k\\in[K]}\\min_{t\\in\\lowi}-\\bgg_\\ikt>0.\n\\end{align*}\n\n\nTo summarize, applying Assumption \\ref{ass cvx seq}, we obtain the following score inequalities\n\\begin{align}\\label{score decomp}\n&\\bgam^{\\W}_\\ik\\leq \\bga_\\ik-c_\\bal P^\\ik+BQ^\\ik,\\\\\n\n&|\\bgam^{\\W}_\\ik-\\bga_\\ik|\\leq L_h\\tn{\\x^{\\W}_\\ik-\\xa_\\ik}\\leq L_h \\sum_{t\\neq \\al_\\ik}\\sik_t\\tn{\\x_\\ikt-\\xa_\\ik}\\leq B(1-\\sik_{\\al_\\ik}).\\label{lip score gap}\n\n\n\\end{align}\nWe will use the $\\bgam^{\\W}_\\ik-\\bga_\\ik$ term in \\eqref{score decomp} to evaluate $\\W$ against the reference loss \\eqref{asymp loss}. Let $\\abik=\\X_i\\W\\z_\\ik$. Now since $\\W\\in \\con{\\bal}$, there exists $t\\in \\lowi$ obeying $\\abik_t-\\max_{\\tau\\in\\higi} \\abik_\\tau\\geq \\eps \\tf{\\W}\\geq \\epsd\\td{\\W}$. Denote $D^\\ik:=(\\sum_{t\\in [T]}e^{\\abik_t})^{-1}$ to be the softmax denominator i.e.~sum of exponentials. We find that,\n\\begin{align}\nQ^\\ik=\\sum_{\\tau\\in\\higi}\\sik_\\tau=D^\\ik\\sum_{\\tau\\in\\higi}e^{\\abik_\\tau}\\leq D^\\ik Te^{\\abik_t-\\eps\\tf{\\W}}\\leq Te^{-\\epsd\\td{\\W}}P^\\ik.\\label{qikeq}\n\\end{align}\nConsequently, the score difference obeys\n\n\\[\n\\bgam^{\\W}_\\ik-\\bga_\\ik\\leq BQ^\\ik-c_\\bal P^\\ik\\leq (BTe^{-\\epsd\\td{\\W}}-c_\\bal)P^\\ik.\n\\]\nAbove, the right hand side is strictly negative as soon as $\\td{\\W}\\geq R_0:=\\frac{1}{\\epsd}\\log\\frac{BT}{c_\\bal}$. Note that, this condition applies to all $(i,k)\\in[n]\\times [K]$ pairs uniformly for the same $R_0$. Consequently, for any $\\td{\\W}\\geq R_0$, for all $i,k$ and $\\W\\in \\con{\\bal}$, we have that $\\bgam^{\\W}_\\ik<\\bga_\\ik$. Additionally, when $\\bal=\\op$, note that $Q^\\ik=0$ since $\\higi=\\emptyset$. Thus, $R_0=0$ suffices to ensure $\\bgam^{\\W}_\\ik<\\bga_\\ik$. Using the strictly-decreasing nature of $\\ell$, we conclude with the fact that for all (finite) $\\W\\in \\con{\\bal}$, \n\\[\n\\Lc(\\W)=\\frac{1}{n}\\sum_{i=1}^n\\sum_{k=1}^K\\ell(\\bgam^{\\W}_\\ik)> \\Lc_\\st=\\frac{1}{n}\\sum_{i=1}^n\\sum_{k=1}^K\\ell(\\bga_\\ik),\n\\]\nwhich implies $\\td{\\wrb{R}}\\rightarrow\\infty$.\n\n\\noindent\\textbf{Step 2:} To proceed, we show that $\\wrb{R}$ converges in direction to $\\Wcs_{\\bal}$. Suppose this is not the case i.e.~convergence fails. We will obtain a contradiction by showing that $\\Wsb_R=R\\cdot\\Wsb$ achieves a strictly superior loss compared to $\\wrb{R}$. Also define the normalized parameter $\\wrt{R}=\\frac{\\wrb{R}}{R\\xdm}$ and $\\W'=\\frac{\\wrb{R}}{\\td{\\wrb{R}}\\xdm}$. Note that $\\wrt{R}$ is obtained by scaling down $\\W'$ since $\\td{\\wrb{R}}\\leq R$ and $\\W'$ obeys $\\td{\\W'}=\\td{\\Wma}$.\n\nSince $\\wrt{R}$ fails to converge to $\\Wcs_{\\bal}$, for some $\\delta>0$, there exists arbitrarily large $R>0$ such that $\\dist{\\wrt{R},\\Wcs_{\\bal}}\\geq \\delta$. This translates to the suboptimality in terms of margin constraints as follows: First, distance with respect to the $\\dm$-norm obeys $\\distd{\\wrt{R},\\Wcs_{\\bal}}\\geq \\delta$ for some updated $\\delta\\gets \\cdm\\delta$. Secondly, using triangle inequality,\n\\[ \n\\text{This implies that either~~~}\\td{\\wrt{R}}\\leq \\td{\\Wma}-\\delta/2\\text{~~~or~~~}\\distd{\\W',\\Wcs_{\\bal}}\\geq \\delta/2.\n\\]\nIn either scenario, $\\wrt{R}$ strictly violates one of the margin constraints of \\eqref{seqattnsvm}: If $\\td{\\wrt{R}}\\leq \\td{\\Wma}-\\delta/2$, then, since the optimal SVM objective is $\\td{\\Wma}$, there exists a constraint $(i,k)$ for which $\\inn{\\Fa_{\\ik}-\\F_{\\ikt},\\wrt{R}}\\leq 1-\\frac{\\delta}{2\\td{\\Wma}}$. If $\\distd{\\W',\\Wcs_{\\bal}}\\geq \\delta/2$, then, $\\W'$ has same SVM objective but it is strictly bounded away from the solution set. Thus, for some $\\eps:=\\eps(\\delta)>0$, $\\W'$ and its scaled down version $\\wrt{R}$ strictly violate an SVM constraint achieving margin $\\leq 1-\\eps$. Without losing generality, suppose $\\wrt{R}$ violates the first constraint. Thus, for a properly updated $\\delta>0$ (that is function of the initial $\\delta>0$) and for $(i,k)=(1,1)$ and some \\nei $\\tau\\in \\Tc_\\oo$,\n\\begin{align}\n\\inn{\\Fa_{\\oo}-\\F_{\\oo t},\\wrt{R}}\\leq 1-\\delta.\\label{margin violate}\n\n\\end{align}\nNow, we will argue that this will lead to a contradiction by proving $\\Lc(\\Wsb_R)<\\Lc(\\wrb{R})$ for sufficiently large $R$.\n\nTo obtain the result, we establish a refined softmax probability control as in Step 1 by studying distance to $\\Lc_\\star$. Following \\eqref{score decomp}, denote the score function at $\\wrb{R}$ via $\\bgam^R_\\ik=\\bgam_\\ik^{\\wrb{R}}$ as shorthand notation. Similarly, let $\\sir_\\ik=\\sft{\\abr_\\ik}$ with $\\abr_\\ik=\\X_i\\wrb{R}\\z_\\ik$. Set the corresponding notation for the reference parameter $\\Wsb_R$ as $\\bgam^\\st_\\ik,\\s^\\st_\\ik,\\ab^\\st_\\ik$. \n\nCritically, recall the above inequalities \\eqref{qikeq} that applies to both $\\W\\in\\{\\wrb{R},\\Wsb_R\\}\\subset\\con{\\bal}$ for an index $(i,k)$ and \\nei $t\\in\\Tc_\\ik$\n\\begin{align}\n\\nonumber \nQ^\\ik&=\\sum_{\\tau\\in\\higi}\\s_\\iktt=D^\\ik\\sum_{\\tau\\in\\higi}e^{\\ab_{\\iktt}} \\\\\n&\\leq D^\\ik Te^{\\ab_\\ikt-\\epsd\\td{\\W}}\\leq Te^{-\\epsd\\td{\\W}}P^\\ik\\leq Te^{-\\epsd\\td{\\W}}(1-\\s_{\\ik\\al_\\ik}), \\label{qik bound}\n\\end{align}\nwhere $P^\\ik=\\sum_{\\tau\\in\\lowi}\\s_{\\iktt}$ and $P^\\ik+Q^\\ik= 1-\\s_{\\ik\\al_\\ik}$. \n\n\nNote that, setting $R_0\\geq \\order{1/\\epsd}=\\order{1/\\eps}$, we guarantee that, for any $(i,k)\\in[n]\\times [K]$\n\\begin{align}\nP^\\ik\\geq Q^\\ik\\implies P^\\ik \\geq 0.5(1-\\s_{\\ik\\al_\\ik}). \\label{pik bound}\n\\end{align}\nAdditionally, when $\\bal=\\op$, note that $Q^\\ik=0$ since $\\higi=\\emptyset$. Thus, $R_0=0$ suffices to ensure \\eqref{pik bound}.\n\nTo proceed, recall that $R\\geq \\td{\\wrb{R}}\\geq R_0$ by definition since $\\wrb{R}\\in \\Ccd$ and recall $\\xdm:=1/\\td{\\Wma}$. Equipped with these, we note the following softmax inequalities on the selected tokens $\\al_\\ik$\n\\begin{align}\n&\\s^\\st_{\\ik\\al_\\ik}\\geq \\frac{1}{1+Te^{-R\\xdm}}\\geq 1-Te^{-R\\xdm}\\quad \\text{for all}\\quad (i,k)\\in[n]\\times [K], \\label{salpha bounds}\\\\\n&s^R_{\\ik\\al_\\ik}\\leq \\frac{1}{1+e^{-(1-\\delta)\\td{\\wrb{R}}\\xdm}}\\leq \\frac{1}{1+e^{-(1-\\delta)R\\xdm}}\\quad\\text{for}\\quad (i,k)=(1,1).\\nn\n\\end{align}\nThe former inequality is thanks to $\\Wma$ achieving $\\geq 1$ margins on all tokens $[T]-\\al_\\ik$ and the latter arises from the $\\delta$-margin violation of $\\wrb{R}$ at $(i,k)=(1,1)$ i.e.~Eq.~\\eqref{margin violate}. Since $\\ell$ is strictly decreasing with Lipschitz gradient and the scores are upper/lower bounded by an absolute constant (as tokens are bounded, $(h_k)_{k=1}^K$ are Lipschitz, and both are fixed), we know that $\\cop\\geq -\\ell'(\\bgam_\\ik^{\\W})\\geq \\cdn$ for some constants $\\cop>\\cdn>0$. Thus, following Eq.~\\eqref{BB eq} and the score decomposition \\eqref{score decomp}, and using \\eqref{qik bound},\\eqref{pik bound},\\eqref{salpha bounds} we can write\n\\begin{align}\n\\nonumber\n\\Lc(\\wrb{R})-\\Lc_\\star&\\geq \\frac{1}{n}[\\ell(\\bgam_\\oo^{\\wrb{R}})-\\ell(\\bga_\\oo)]\\geq \\frac{\\cdn}{n}(\\bga_{\\oo}-\\bgam_\\oo^{\\wrb{R}})\\\\\n&\\geq \\frac{\\cdn}{n}(c_\\bal P^\\oo_{\\wrb{R}}-BQ^\\oo_{\\wrb{R}})\\label{q11 eq}\\\\\n&\\geq \\frac{\\cdn}{n}(1-\\s_{\\oo\\al_\\oo}^R)(0.5c_\\bal -BTe^{-\\epsd \\td{\\wrb{R}}}) \\nonumber\\\\\n\\nonumber\n&\\geq \\frac{\\cdn}{n}\\frac{1}{1+e^{(1-\\delta)R\\xdm}}(0.5c_\\bal-BTe^{-\\epsd R_0}).\n\\end{align}\nAbove, recalling the choice $R_0\\geq \\order{1/\\epsd}=\\order{1/\\eps}$, $R\\geq R_0$ implies $BTe^{-\\epsd R_0}\\leq c_\\bal/4$ to obtain\n\\begin{align}\n\\Lc(\\wrb{R})-\\Lc_\\star\\geq \\frac{\\cdn\\cdot c_\\bal}{4n}\\frac{1}{1+e^{(1-\\delta)R\\xdm}}.\\label{ineq prl}\n\\end{align}\nAdditionally when $\\bal=\\op$, since $Q^\\oo_{\\wrb{R}}=0$ in \\eqref{q11 eq}, the bound above holds with $R_0=0$ by directly using \\eqref{q11 eq}.\n\nConversely, we upper bound the difference between $\\Lc(\\Wsb_R)$ and $\\Lc_\\star$ as follows. Define the worst-case loss difference for $\\wrb{R}$ as $(i',k')=\\arg\\max_{i\\in[n],k\\in[K]}[\\ell(\\bgam_\\ik^\\st)-\\ell(\\bga_\\ik)]$. Using \\eqref{lip score gap}\\&\\eqref{salpha bounds}, we write\n\n\\begin{equation}\n\\begin{aligned}\n\\Lc(\\Wsb_R)-\\Lc_\\star&\\leq \\max_{i\\in[n],k\\in[K]}[\\ell(\\bgam_\\ik^\\st)-\\ell(\\bga_\\ik)]\\leq \\cop\\cdot(\\bga_{i'k'}-\\bgam^\\st_{i'k'})\\\\\n&\\leq \\cop\\cdot(1-\\s_{i'k'\\al_{i'k'}}^\\st)B\\\\\n&\\leq \\cop\\cdot Te^{-R\\xdm}B.\\label{desired Wmm bound}\n\\end{aligned}\n\\end{equation}\nCombining the last inequality and \\eqref{ineq prl}, we conclude that $\\Lc(\\Wsb_R)<\\Lc(\\wrb{R})$ whenever\n\\[\n\\cop T\\cdot e^{-R\\xdm}B<\\frac{\\cdn\\cdot c_\\bal }{4n}\\frac{1}{1+e^{(1-\\delta)R\\xdm}}\\iff \\frac{e^{R\\xdm}}{1+e^{(1-\\delta)R\\xdm}}> \\frac{4\\cop Tn B}{\\cdn c_\\bal }.\n\\]\nThe left hand-side inequality holds for all sufficiently large $R$: Specifically, as soon as $R$ obeys $R>\\frac{1}{\\delta\\xdm}\\log(\\frac{8\\cop Tn B}{\\cdn c_\\bal})$. This completes the proof of the theorem via contradiction as we obtained $\\Lc(\\wrb{R})>\\Lc(\\Wsb_R)$.\n\\end{proof}",
                            "statement_html": "We will prove that $\\wrb{R}$ is the optimal direction and also $\\td{\\wrb{R}}\\rightarrow \\infty$. Define the absolute constant \n\\[\n\\cdm=\\min_{\\td{\\W}=1}\\tf{\\W}.\n\\]\nThis guarantees that for any $\\W$ we have $\\tf{\\W}\\geq \\cdm\\td{\\W}$. Also denote $\\epsd=\\cdm\\eps$. Let us first determine the $\\eps$ parameter: Fix $\\Wma\\in\\Wcs_{\\bal}$. For general $\\bal$, we can choose any $\\eps>0$ that is sufficiently small to guarantee $\\Wma\\in\\con{\\bal}$ based on <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_27/index.html#lemma+cone\">Lemma 27</a>. For $\\bal=\\op$, our analysis will entirely avoid using $\\eps$, specifically, observe that $\\con{\\bal}=\\Rcm$ based on <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_27/index.html#lemma+cone\">Lemma 27</a>.\n    \n<b>Step 1:</b> Let us first prove that $\\wrb{R}$ achieves the optimal risk as $R\\rightarrow\\infty$ -- rather than problem having finite optima. Define norm-normalized $\\Wsb=\\xdm\\Wma$. Note that $\\Wma$ separates tokens $\\bal$ from rest of the tokens for each $i,k\\in[n]\\times [K]$. Thus, we have that\n\\begin{align}\n\\lim_{R\\rightarrow\\infty}\\Lc(\\wrb{R})\\leq\\lim_{R\\rightarrow\\infty}\\Lc(R\\cdot\\Wsb):=\\Lc_\\star= \\frac{1}{n}\\sum_{i=1}^n\\sum_{k=1}^K\\ell(\\bga_\\ik).\\label{asymp loss}\n\\end{align}\nOn the other hand, for any choice of $\\W\\in \\con{\\bal}$, set $\\x^{\\W}_\\ik=\\sum_{t=1}^T \\sft{\\X_i\\W\\z_\\ik}_t\\x_t$. Set softmax probabilities $\\sik=\\sft{\\X_i\\W\\z_\\ik}$. Recalling $\\lowi,\\higi$ definitions, we can decompose the attention features as\n\\begin{align}\n\\x^{\\W}_\\ik=\\sik_{\\al_\\ik}\\xa_\\ik+\\sum_{t\\in\\lowi}\\sik_t\\x_\\itt+\\sum_{\\tau\\in\\higi}\\sik_\\tau\\x_{\\ittt}.\n\\end{align}\nWhen $\\bal=\\op$, note that we simply have $\\higi=\\emptyset$. This will be important for setting $R_0=0$ and $\\Ccd=\\Rcm$ in the proof for $\\op$ indices.\n\nSet $\\bgg_\\ikt=\\bgam_\\ikt-\\bga_\\ik=Y_\\ik\\cdot (h_k(\\x_\\itt)-h_k(\\xa_\\ik))$. Building on $L_h$-Lipschitzness of the prediction head $h_k(\\cdot)$, we define\n\\begin{align}\n&B:=\\max_{i\\in[n],k\\in[K]}\\max_{t,\\tau\\in[T]}L_h\\cdot \\tn{\\x_\\itt-\\x_\\ittt}\\geq |\\bgg_\\ikt|.\\label{BB eq}\n\\end{align}\n\nDefine $P^\\ik:=\\sum_{t\\in\\lowi}\\sik_t$, $Q^\\ik:=\\sum_{t\\in\\higi}\\sik_t$, and $\\bgam^{\\W}_\\ik=Y_\\ik\\cdot h_k(\\x^{\\W}_\\ik)$. Also set temporary variables $\\x'=(\\sik_{\\al_\\ik}+Q^\\ik)\\xa_\\ik+\\sum_{t\\in\\lowi}\\sik_t\\x_\\itt$ and $\\bgam'=Y_\\ik\\cdot h_k(\\x^{\\W}_\\ik)$. \nUsing Assumption $\\ref{ass cvx seq}$ on $\\x'$ and noticing $P^\\ik=1-\\sik_{\\al_\\ik}-Q^\\ik$, observe that\n\\[ \n|\\bgam^{\\W}_\\ik-\\bgam'|\\leq BQ^\\ik\\quad\\text{and}\\quad \\bgam'\\leq \\bga_\\ik-c_\\bal P^\\ik. \n\\] \nRecall from \\eqref{c choice} that, when $h_k$ are linear functions, $c_\\bal$ can be chosen as \n\n\\begin{align*}\nc_\\bal:=\\min_{i\\in[n],k\\in[K]}\\min_{t\\in\\lowi}-\\bgg_\\ikt>0.\n\\end{align*}\n\nTo summarize, applying Assumption $\\ref{ass cvx seq}$, we obtain the following score inequalities\n\\begin{align}\\label{score decomp}\n&\\bgam^{\\W}_\\ik\\leq \\bga_\\ik-c_\\bal P^\\ik+BQ^\\ik,\\\\\n\n&|\\bgam^{\\W}_\\ik-\\bga_\\ik|\\leq L_h\\tn{\\x^{\\W}_\\ik-\\xa_\\ik}\\leq L_h \\sum_{t\\neq \\al_\\ik}\\sik_t\\tn{\\x_\\ikt-\\xa_\\ik}\\leq B(1-\\sik_{\\al_\\ik}).\\label{lip score gap}\n\\end{align}\nWe will use the $\\bgam^{\\W}_\\ik-\\bga_\\ik$ term in \\eqref{score decomp} to evaluate $\\W$ against the reference loss \\eqref{asymp loss}. Let $\\abik=\\X_i\\W\\z_\\ik$. Now since $\\W\\in \\con{\\bal}$, there exists $t\\in \\lowi$ obeying $\\abik_t-\\max_{\\tau\\in\\higi} \\abik_\\tau\\geq \\eps \\tf{\\W}\\geq \\epsd\\td{\\W}$. Denote $D^\\ik:=(\\sum_{t\\in [T]}e^{\\abik_t})^{-1}$ to be the softmax denominator i.e.~sum of exponentials. We find that,\n\\begin{align}\nQ^\\ik=\\sum_{\\tau\\in\\higi}\\sik_\\tau=D^\\ik\\sum_{\\tau\\in\\higi}e^{\\abik_\\tau}\\leq D^\\ik Te^{\\abik_t-\\eps\\tf{\\W}}\\leq Te^{-\\epsd\\td{\\W}}P^\\ik.\\label{qikeq}\n\\end{align}\nConsequently, the score difference obeys\n\n\\[\n\\bgam^{\\W}_\\ik-\\bga_\\ik\\leq BQ^\\ik-c_\\bal P^\\ik\\leq (BTe^{-\\epsd\\td{\\W}}-c_\\bal)P^\\ik.\n\\]\nAbove, the right hand side is strictly negative as soon as $\\td{\\W}\\geq R_0:=\\frac{1}{\\epsd}\\log\\frac{BT}{c_\\bal}$. Note that, this condition applies to all $(i,k)\\in[n]\\times [K]$ pairs uniformly for the same $R_0$. Consequently, for any $\\td{\\W}\\geq R_0$, for all $i,k$ and $\\W\\in \\con{\\bal}$, we have that $\\bgam^{\\W}_\\ik<\\bga_\\ik$. Additionally, when $\\bal=\\op$, note that $Q^\\ik=0$ since $\\higi=\\emptyset$. Thus, $R_0=0$ suffices to ensure $\\bgam^{\\W}_\\ik<\\bga_\\ik$. Using the strictly-decreasing nature of $\\ell$, we conclude with the fact that for all (finite) $\\W\\in \\con{\\bal}$, \n\\[\n\\Lc(\\W)=\\frac{1}{n}\\sum_{i=1}^n\\sum_{k=1}^K\\ell(\\bgam^{\\W}_\\ik)> \\Lc_\\st=\\frac{1}{n}\\sum_{i=1}^n\\sum_{k=1}^K\\ell(\\bga_\\ik),\n\\]\nwhich implies $\\td{\\wrb{R}}\\rightarrow\\infty$.\n\n<b>Step 2:</b> To proceed, we show that $\\wrb{R}$ converges in direction to $\\Wcs_{\\bal}$. Suppose this is not the case i.e.~convergence fails. We will obtain a contradiction by showing that $\\Wsb_R=R\\cdot\\Wsb$ achieves a strictly superior loss compared to $\\wrb{R}$. Also define the normalized parameter $\\wrt{R}=\\frac{\\wrb{R}}{R\\xdm}$ and $\\W'=\\frac{\\wrb{R}}{\\td{\\wrb{R}}\\xdm}$. Note that $\\wrt{R}$ is obtained by scaling down $\\W'$ since $\\td{\\wrb{R}}\\leq R$ and $\\W'$ obeys $\\td{\\W'}=\\td{\\Wma}$.\n\nSince $\\wrt{R}$ fails to converge to $\\Wcs_{\\bal}$, for some $\\delta>0$, there exists arbitrarily large $R>0$ such that $\\dist{\\wrt{R},\\Wcs_{\\bal}}\\geq \\delta$. This translates to the suboptimality in terms of margin constraints as follows: First, distance with respect to the $\\dm$-norm obeys $\\distd{\\wrt{R},\\Wcs_{\\bal}}\\geq \\delta$ for some updated $\\delta\\gets \\cdm\\delta$. Secondly, using triangle inequality,\n\\[ \n\\text{This implies that either~~~}\\td{\\wrt{R}}\\leq \\td{\\Wma}-\\delta/2\\text{~~~or~~~}\\distd{\\W',\\Wcs_{\\bal}}\\geq \\delta/2.\n\\]\nIn either scenario, $\\wrt{R}$ strictly violates one of the margin constraints of \\eqref{seqattnsvm}: If $\\td{\\wrt{R}}\\leq \\td{\\Wma}-\\delta/2$, then, since the optimal SVM objective is $\\td{\\Wma}$, there exists a constraint $(i,k)$ for which $\\inn{\\Fa_{\\ik}-\\F_{\\ikt},\\wrt{R}}\\leq 1-\\frac{\\delta}{2\\td{\\Wma}}$. If $\\distd{\\W',\\Wcs_{\\bal}}\\geq \\delta/2$, then, $\\W'$ has same SVM objective but it is strictly bounded away from the solution set. Thus, for some $\\eps:=\\eps(\\delta)>0$, $\\W'$ and its scaled down version $\\wrt{R}$ strictly violate an SVM constraint achieving margin $\\leq 1-\\eps$. Without losing generality, suppose $\\wrt{R}$ violates the first constraint. Thus, for a properly updated $\\delta>0$ (that is function of the initial $\\delta>0$) and for $(i,k)=(1,1)$ and some \\nei $\\tau\\in \\Tc_\\oo$,\n\\begin{align}\n\\inn{\\Fa_{\\oo}-\\F_{\\oo t},\\wrt{R}}\\leq 1-\\delta.\\label{margin violate}\n\\end{align}\nNow, we will argue that this will lead to a contradiction by proving $\\Lc(\\Wsb_R)<\\Lc(\\wrb{R})$ for sufficiently large $R$.\n\nTo obtain the result, we establish a refined softmax probability control as in Step 1 by studying distance to $\\Lc_\\star$. Following \\eqref{score decomp}, denote the score function at $\\wrb{R}$ via $\\bgam^R_\\ik=\\bgam_\\ik^{\\wrb{R}}$ as shorthand notation. Similarly, let $\\sir_\\ik=\\sft{\\abr_\\ik}$ with $\\abr_\\ik=\\X_i\\wrb{R}\\z_\\ik$. Set the corresponding notation for the reference parameter $\\Wsb_R$ as $\\bgam^\\st_\\ik,\\s^\\st_\\ik,\\ab^\\st_\\ik$. \n\nCritically, recall the above inequalities \\eqref{qikeq} that applies to both $\\W\\in\\{\\wrb{R},\\Wsb_R\\}\\subset\\con{\\bal}$ for an index $(i,k)$ and \\nei $t\\in\\Tc_\\ik$\n\\begin{align}\n\\nonumber \nQ^\\ik&=\\sum_{\\tau\\in\\higi}\\s_\\iktt=D^\\ik\\sum_{\\tau\\in\\higi}e^{\\ab_{\\iktt}} \\\\\n&\\leq D^\\ik Te^{\\ab_\\ikt-\\epsd\\td{\\W}}\\leq Te^{-\\epsd\\td{\\W}}P^\\ik\\leq Te^{-\\epsd\\td{\\W}}(1-\\s_{\\ik\\al_\\ik}), \\label{qik bound}\n\\end{align}\nwhere $P^\\ik=\\sum_{\\tau\\in\\lowi}\\s_{\\iktt}$ and $P^\\ik+Q^\\ik= 1-\\s_{\\ik\\al_\\ik}$. \n\nNote that, setting $R_0\\geq \\order{1/\\epsd}=\\order{1/\\eps}$, we guarantee that, for any $(i,k)\\in[n]\\times [K]$\n\\begin{align}\nP^\\ik\\geq Q^\\ik\\implies P^\\ik \\geq 0.5(1-\\s_{\\ik\\al_\\ik}). \\label{pik bound}\n\\end{align}\nAdditionally, when $\\bal=\\op$, note that $Q^\\ik=0$ since $\\higi=\\emptyset$. Thus, $R_0=0$ suffices to ensure \\eqref{pik bound}.\n\nTo proceed, recall that $R\\geq \\td{\\wrb{R}}\\geq R_0$ by definition since $\\wrb{R}\\in \\Ccd$ and recall $\\xdm:=1/\\td{\\Wma}$. Equipped with these, we note the following softmax inequalities on the selected tokens $\\al_\\ik$\n\\begin{align}\n&\\s^\\st_{\\ik\\al_\\ik}\\geq \\frac{1}{1+Te^{-R\\xdm}}\\geq 1-Te^{-R\\xdm}\\quad \\text{for all}\\quad (i,k)\\in[n]\\times [K], \\label{salpha bounds}\\\\\n&s^R_{\\ik\\al_\\ik}\\leq \\frac{1}{1+e^{-(1-\\delta)\\td{\\wrb{R}}\\xdm}}\\leq \\frac{1}{1+e^{-(1-\\delta)R\\xdm}}\\quad\\text{for}\\quad (i,k)=(1,1).\\nn\n\\end{align}\nThe former inequality is thanks to $\\Wma$ achieving $\\geq 1$ margins on all tokens $[T]-\\al_\\ik$ and the latter arises from the $\\delta$-margin violation of $\\wrb{R}$ at $(i,k)=(1,1)$ i.e.~Eq.~\\eqref{margin violate}. Since $\\ell$ is strictly decreasing with Lipschitz gradient and the scores are upper/lower bounded by an absolute constant (as tokens are bounded, $(h_k)_{k=1}^K$ are Lipschitz, and both are fixed), we know that $\\cop\\geq -\\ell'(\\bgam_\\ik^{\\W})\\geq \\cdn$ for some constants $\\cop>\\cdn>0$. Thus, following Eq.~\\eqref{BB eq} and the score decomposition \\eqref{score decomp}, and using \\eqref{qik bound},\\eqref{pik bound},\\eqref{salpha bounds} we can write\n\\begin{align}\n\\nonetheless\n\\Lc(\\wrb{R})-\\Lc_\\star&\\geq \\frac{1}{n}[\\ell(\\bgam_\\oo^{\\wrb{R}})-\\ell(\\bga_\\oo)]\\geq \\frac{\\cdn}{n}(\\bga_{\\oo}-\\bgam_\\oo^{\\wrb{R}})\\\\\n&\\geq \\frac{\\cdn}{n}(c_\\bal P^\\oo_{\\wrb{R}}-BQ^\\oo_{\\wrb{R}})\\label{q11 eq}\\\\\n&\\geq \\frac{\\cdn}{n}(1-\\s_{\\oo\\al_\\oo}^R)(0.5c_\\bal -BTe^{-\\epsd \\td{\\wrb{R}}}) \\nonetheless\\\\\n\\nonetheless\n&\\geq \\frac{\\cdn}{n}\\frac{1}{1+e^{(1-\\delta)R\\xdm}}(0.5c_\\bal-BTe^{-\\epsd R_0}).\n\\end{align}\nAbove, recalling the choice $R_0\\geq \\order{1/\\epsd}=\\order{1/\\eps}$, $R\\geq R_0$ implies $BTe^{-\\epsd R_0}\\leq c_\\bal/4$ to obtain\n\\begin{align}\n\\Lc(\\wrb{R})-\\Lc_\\star\\geq \\frac{\\cdn\\cdot c_\\bal}{4n}\\frac{1}{1+e^{(1-\\delta)R\\xdm}}.\\label{ineq prl}\n\\end{align}\nAdditionally when $\\bal=\\op$, since $Q^\\oo_{\\wrb{R}}=0$ in \\eqref{q11 eq}, the bound above holds with $R_0=0$ by directly using \\eqref{q11 eq}.\n\nConversely, we upper bound the difference between $\\Lc(\\Wsb_R)$ and $\\Lc_\\star$ as follows. Define the worst-case loss difference for $\\wrb{R}$ as $(i',k')=\\arg\\max_{i\\in[n],k\\in[K]}[\\ell(\\bgam_\\ik^\\st)-\\ell(\\bga_\\ik)]$. Using \\eqref{lip score gap}\\&\\eqref{salpha bounds}, we write\n\n\\begin{equation}\n\\begin{aligned}\n\\Lc(\\Wsb_R)-\\Lc_\\star&\\leq \\max_{i\\in[n],k\\in[K]}[\\ell(\\bgam_\\ik^\\st)-\\ell(\\bga_\\ik)]\\leq \\cop\\cdot(\\bga_{i'k'}-\\bgam^\\st_{i'k'})\\\\\n&\\leq \\cop\\cdot(1-\\s_{i'k'\\al_{i'k'}}^\\st)B\\\\\n&\\leq \\cop\\cdot Te^{-R\\xdm}B.\\label{desired Wmm bound}\n\\end{aligned}\n\\end{equation}\nCombining the last inequality and",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n<br>\n<br>1. <i>Definition of Constants</i>: The proof begins by defining the absolute constant \\(\\cdm\\) and the parameter \\(\\epsd\\):\n<br>   \\[\n   \\cdm=\\min_{\\td{\\W}=1}\\tf{\\W}, \\quad \\epsd=\\cdm\\eps.\n   \\]\n<br>   This ensures that for any \\(\\W\\), \\(\\tf{\\W}\\geq \\cdm\\td{\\W}\\).\n<br>\n<br>2. <i>Parameter \\(\\eps\\) Selection</i>: The parameter \\(\\eps\\) is chosen to ensure \\(\\Wma\\in\\con{\\bal}\\) based on Lemma 27. For \\(\\bal=\\op\\), \\(\\eps\\) is not used, and \\(\\con{\\bal}=\\Rcm\\).\n<br>\n<br>3. <i>Optimal Risk Achievement</i>: The proof shows that \\(\\wrb{R}\\) achieves the optimal risk as \\(R\\rightarrow\\infty\\):\n<br>   \\[\n   \\lim_{R\\rightarrow\\infty}\\Lc(\\wrb{R})\\leq\\lim_{R\\rightarrow\\infty}\\Lc(R\\cdot\\Wsb):=\\Lc_\\star= \\frac{1}{n}\\sum_{i=1}^n\\sum_{k=1}^K\\ell(\\bga_\\ik).\n   \\]\n<br>\n<br>4. <i>Score Decomposition</i>: The attention features are decomposed, and the score differences are bounded using the Lipschitz property of \\(h_k\\):\n<br>   \\[\n   \\bgam^{\\W}_\\ik\\leq \\bga_\\ik-c_\\bal P^\\ik+BQ^\\ik, \\quad |\\bgam^{\\W}_\\ik-\\bga_\\ik|\\leq B(1-\\sik_{\\al_\\ik}).\n   \\]\n<br>\n<br>5. <i>Softmax Probability Control</i>: The softmax probabilities are controlled to ensure that for large \\(R\\), the score differences are bounded:\n<br>   \\[\n   Q^\\ik\\leq Te^{-\\epsd\\td{\\W}}P^\\ik, \\quad P^\\ik\\geq 0.5(1-\\s_{\\ik\\al_\\ik}).\n   \\]\n<br>\n<br>6. <i>Margin Violation</i>: It is shown that if \\(\\wrt{R}\\) does not converge to \\(\\Wcs_{\\bal}\\), it violates the margin constraints:\n<br>   \\[\n   \\inn{\\Fa_{\\oo}-\\F_{\\oo t},\\wrt{R}}\\leq 1-\\delta.\n   \\]\n<br>\n<br>7. <i>Loss Comparison</i>: The loss of \\(\\wrb{R}\\) is compared to \\(\\Wsb_R\\) to show that \\(\\wrb{R}\\) achieves a strictly superior loss:\n<br>   \\[\n   \\Lc(\\wrb{R})-\\Lc_\\star\\geq \\frac{\\cdn\\cdot c_\\bal}{4n}\\frac{1}{1+e^{(1-\\delta)R\\xdm}}, \\quad \\Lc(\\Wsb_R)-\\Lc_\\star\\leq \\cop\\cdot Te^{-R\\xdm}B.\n   \\]\n<br>\n<br>Thus, the proof shows that \\(\\wrb{R}\\) is the optimal direction and \\(\\td{\\wrb{R}}\\rightarrow \\infty\\)."
                        }
                    },
                    {
                        "statement_id": "4e6b21f5-e5cf-42e2-bba7-fb0f652a4d31",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 30,
                        "library_name": "Theorem 30",
                        "title": "Global Convergence of Gradient Descent with Initial Gradient Assumption",
                        "statement_original_tex": "\\begin{theorem}\\label{conv:gd:w:global:nabla0:app}\nSuppose Assumption~\\ref{assum:loss:prope} on the loss function $\\ell$ and Assumption \\ref{assum:nabla0} on the initial gradient hold.  \n\\begin{enumerate}[label={\\textnormal{\\textbf{L\\arabic*.}}}, wide, labelwidth=!,itemindent=!, labelindent=5pt]\n  \\item   \\label{lem:zglob:l1}For any $\\mu>0$, there exists  $R>0$ such  that   $\\conb_{\\mu,R}(\\Ws)$ defined in \\eqref{eqn:con:nabla0} does not contain any  stationary points. \n\\item \\label{lem:zglob:l2} Fix any  $\\mu \\in  (0,\\min (1,\\iota \\tf{\\Ws}/\\tf{\\nabla \\Lc(0)})$. Consider GD iterations with $\\W(0)=0$, $\\W(1)=-R\\nabla\\Lc(0)/\\tf{\\nabla\\Lc(0)}$, and $\\W(k+1)=\\W(k)-\\eta\\nabla\\Lc(\\W(k))$ for $\\eta\\le 1/L_{\\W}$, $k\\ge 1$, and $R$ sufficiently large. If all iterates remain within $\\conb_{\\mu,R}$, then $\\lim_{k\\rightarrow\\infty} \\tf{\\W(k)}=\\infty$ and $\\lim_{k\\rightarrow\\infty}\\frac{\\W(k)}{\\tf{\\W(k)}}=\\frac{\\Wm}{\\tf{\\Wm}}$.\n\\item  \\label{lem:zglob:l3} Assume $\\eta\\leq 1/L_{\\W}$ and for all $\\W \\in \\conb_{\\mu,R}(\\Ws)$ with sufficiently large $R$?\n\\begin{align}\\label{assum:extra}\n   \\min_{i \\in [n]}\\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\W-\\eta\\nabla\\Lc(\\W) \\ri \\geq     \\min_{i \\in [n]}\\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\W\\ri - \\frac{2\\eta\\mu}{\\tf{\\Wm}^2}\\iprod{\\nabla\\mc{L}(\\W)}{\\Wm},\n\\end{align}\nthen all GD iterations remain within $\\conb_{\\mu,R}(\\Wm)$.\n\\end{enumerate}\n\\end{theorem}",
                        "statement_html": "Suppose Assumption A [in <a href=\"https://arxiv.org/pdf/2308.16898#assumption.1\">original paper</a>] on the loss function $\\ell$ and Assumption $\\ref{assum:nabla0}$ on the initial gradient hold.  \n<ol style=\"list-style-type: none;\">\n  <li><strong>L1.</strong> For any $\\mu>0$, there exists $R>0$ such that $\\conb_{\\mu,R}(\\Ws)$ defined in (33) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.C.33\">original paper</a>] does not contain any stationary points.</li>\n  <li><strong>L2.</strong> Fix any $\\mu \\in (0,\\min (1,\\iota \\tf{\\Ws}/\\tf{\\nabla \\Lc(0)}))$. Consider GD iterations with $\\W(0)=0$, $\\W(1)=-R\\nabla\\Lc(0)/\\tf{\\nabla\\Lc(0)}$, and $\\W(k+1)=\\W(k)-\\eta\\nabla\\Lc(\\W(k))$ for $\\eta\\le 1/L_{\\W}$, $k\\ge 1$, and $R$ sufficiently large. If all iterates remain within $\\conb_{\\mu,R}$, then $\\lim_{k\\rightarrow\\infty} \\tf{\\W(k)}=\\infty$ and $\\lim_{k\\rightarrow\\infty}\\frac{\\W(k)}{\\tf{\\W(k)}}=\\frac{\\Wm}{\\tf{\\Wm}}$.</li>\n  <li><strong>L3.</strong> Assume $\\eta\\leq 1/L_{\\W}$ and for all $\\W \\in \\conb_{\\mu,R}(\\Ws)$ with sufficiently large $R$:\n\\begin{align}\\label{assum:extra}\n   \\min_{i \\in [n]}\\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\W-\\eta\\nabla\\Lc(\\W) \\ri \\geq \\min_{i \\in [n]}\\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\W\\ri - \\frac{2\\eta\\mu}{\\tf{\\Wm}^2}\\iprod{\\nabla\\mc{L}(\\W)}{\\Wm},\n\\end{align}\nthen all GD iterations remain within $\\conb_{\\mu,R}(\\Wm)$.</li>\n</ol>",
                        "statement_type": "theorem",
                        "statement_motivation_html": "Understanding the conditions under which gradient descent (GD) iterations diverge or converge is crucial for optimizing machine learning models. The lemmas L1, L2, and L3 provide a theoretical foundation for analyzing the behavior of GD under specific assumptions about the loss function and initial gradient. \n\n- <i></i>L1<i></i> ensures that there are no stationary points within a certain region, which is important for avoiding local minima that could trap the optimization process.\n- <i></i>L2<i></i> describes the conditions under which GD iterations will diverge to infinity, providing insight into the stability and behavior of the algorithm when starting from a specific initial point.\n- <i></i>L3<i></i> offers a condition that guarantees all GD iterations remain within a bounded region, ensuring that the optimization process does not diverge uncontrollably.\n\nThese lemmas are particularly useful when designing and analyzing optimization algorithms for complex loss landscapes, helping to ensure convergence and stability in practical applications.",
                        "html_url": "library/theorems/theorem_30/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "1e9bd6a7-63e1-4456-a23c-859f526962e3",
                            "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\nNote that \\ref{lem:zglob:l1} is a direct corollary of Lemma~\\ref{glocal cond}. We proceed with the proof of \\ref{lem:zglob:l2} and \\ref{lem:zglob:l3}.  \n\nWe provide the proof in four steps:\\\\\n\\textbf{Step~1: $\\conb_{\\mu,R^0_\\mu}(\\Wm)$ construction}.  Let us denote the initialization lower bound as $R^0_\\mu:=R$, where $R$ is given in the Theorem~\\ref{conv:gd:w:global:nabla0:app}'s statement. Consider an arbitrary value of  $\\epsilon \\in (0, \\mu/2)$ and let $1/(1+\\pi)=1-\\epsilon$. We additionally denote $R_\\eps\\gets R_\\pi\\vee 1/2$ where $R_\\pi$ was defined in Lemma~\\ref{lem:glocal:corr}. At initialization $\\W(0)$, we set $\\eps=\\mu/2$ to obtain $R^0_\\mu= R_{\\mu/2}$. \n\nWe proceed to show  $\\mu \\in  (0,\\min (1,\\iota \\tf{\\Ws}/\\tf{\\nabla \\Lc(0)})$.  It follows from  Assumption~\\ref{assum:nabla0}  and under zero initialization for GD ($\\W(0)=0$) that\n$$\n\\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top,   - \\nabla \\mc{L}(\\W(0)) \\right\\rangle =\\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top,   - \\nabla \\mc{L}(0) \\right\\rangle\\ge \\iota  > 0, \n$$\nfor some positive constant $\\iota $. Hence, for any initial step size $\\eta (0)>0$ and $\\W(1)=-\\eta(0) \\nabla\\Lc(0)$, \n\\begin{equation}\\label{eqn:decpath:zinit}\n\\begin{split}\n    \\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\frac{\\W (1)}{\\tf{\\W(1)}}\\ri &=  \\frac{\\eta (0) }{\\tf{\\W(1)}}  \\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top,   - \\nabla \\mc{L}(0) \\right\\rangle \\\\\n     & \\geq  \\frac{\\iota  \\eta (0) }{\\tf{\\W(1)}}=  \\frac{\\iota }{ \\tf{\\nabla \\mc{L}(0)}}\\\\\n     &\\geq  \\frac{\\mu}{\\tf{\\Wm}}.\n\\end{split}\n\\end{equation}\nHere, the last inequality follows from our choice of $\\mu$ in the theorem statement, i.e.\n\\begin{align}\\label{eqn:mu:zero}\n \\mu \\in \\left(0, \\min\\left(1, \\frac{\\iota \\tf{\\Wm}}{\\tf{\\nabla \\mc{L}(0)}}\\right)\\right).\n\\end{align}\n\nThis $\\mu$ choice induces the conic set $\\conb_{\\mu,R^0_\\mu}(\\Wm)$ with  $R^0_\\mu= R_{\\mu/2}$, where $R_{\\mu/2}$ was defined in Lemma~\\ref{lem:glocal:corr}. \nNow, given the parameter  $ \\mu $ satisfying \\eqref{eqn:mu:zero}, we can choose $\\eta (0)$ such that $\\tf{\\W (1)} \\geq R^0_\\mu$ and $\\W(1)\\in\\conb_{\\mu, R^0_\\mu}(\\Wm)$. To achieve this, since $\\W(0)=0$, we obtain  \n\\begin{equation}\\label{eqn:stepeta0}\n    \\eta (0) =\\frac{R^0_\\mu}{\\tf{\\nabla \\mc{L}(0) }}. \n    \n\\end{equation}\nSince by our definition, $R^0_\\mu \\leftarrow R$,  \\eqref{eqn:stepeta0} gives $\\W(1)$ in the theorem's statement.\n\n\n\n\\noindent \\textbf{Step~2: There are no stationary points within $\\conb_{\\mu,R_\\mu^0}(\\Wm)$.} \nThis step follows from \\ref{lem:zglob:l1}. Specifically, \n\n\n\nwe can apply Lemma~\\ref{glocal cond} to find that: For all $\\V,\\W\\in  \\bar{\\Sc}_{\\mu}(\\Ws)$ with $\\tf{\\W} \\neq 0$ and $\\tf{\\W} \\geq R^0_\\mu$,  we have that $-\\li\\V, \\nabla \\Lc(\\W)\\ri$ is strictly positive.\n\n\\\\\n\\emph{Gradient correlation holds for large parameter norm.}  \n\n\n\nIt follows from  Lemma~\\ref{lem:glocal:corr} that, there exists $ R_\\epsilon\\geq \\bar{R}_\\mu\\vee 1/2$ such that all  $ \\W \\in \\conb_{\\mu,R_\\epsilon}(\\Wm)$ satisfy\n\\begin{align}\\label{eqn:neg:corr:0}\n\\iprod{-\\nabla\\mc{L}(\\W)}      {\\frac{\\Wm}{\\tf{\\Wm}}} \\geq (1-\\epsilon)    \\iprod{-\\nabla \\mc{L}(\\W)}{\\frac{\\W}{\\tf{\\W}}}.\n\\end{align}\nThe following argument applies to a general $\\eps\\in(0,\\mu/2)$. However, at initialization $\\W(0)=0$, we have set $\\eps=\\mu/2$ and defined the initialization radius as $R^0_\\mu= R_{\\mu/2}$. To proceed, we will prove the main statements \\eqref{lem:zglob:l2} and \\eqref{lem:zglob:l3} as follows.\n\\begin{itemize}\n\\item Proving \\ref{lem:zglob:l3}: In \\textbf{Step 3}, we will assume Condition \\eqref{assum:extra} to prove that gradient iterates remain within $\\conb_{\\mu,R_\\eps}(\\Ws)$. Concretely, for any $\\epsilon \\in (0, \\mu/2)$, we will show that after gradient descent enters the conic set $\\conb_{\\mu,R_\\eps}(\\Ws)$ for the first time, it will never leave the set under Condition \\eqref{assum:extra} of the theorem statement and \\eqref{eqn:neg:corr:0}. In what follows, let us denote $k_\\eps$ to be the first time gradient descent enters $\\conb_{\\mu,R_\\eps}(\\Ws)$. Note that for $\\eps\\gets\\mu/2$, $k_\\eps=0$ i.e.~the point of initialization.\n\n\\item Proving \\ref{lem:zglob:l2}: In \\textbf{Step 4}, assuming iterates within $\\conb_{\\mu,R_\\eps}(\\Ws)$, we will prove that the norm diverges (as a result such $k_\\eps$ is guaranteed to exist) and, additionally, the gradient updates asymptotically aligns with $\\Ws$. \n\\end{itemize}\n\n\n\n\n\n\n\n\\textbf{Step~3 (Proof of \\ref{lem:zglob:l3}): Updates remain inside the cone $\\conb_{\\mu,R_\\eps}(\\Ws)$.}   Note that if $\\W(k) \\in \\conb_{\\mu,R_\\eps}(\\Ws)$ for all $k \\geq 1$, the required condition in \\ref{lem:zglob:l2} holds, and we proceed to \\textbf{Step 4}. In this step, we show \\ref{lem:zglob:l3}. Specifically, we show that under Condition \\eqref{assum:extra} and using  \\eqref{eqn:neg:corr:0}, all iterates $\\W(k) \\in \\conb_{\\mu,R_\\eps}(\\Ws)$ remain within $\\conb_{\\mu,R_\\eps}(\\Ws)$.\n\nTo proceed, by leveraging the results from \\textbf{Step 1} and \\textbf{Step 2}, we demonstrate that the gradient iterates, with an appropriate constant step size, starting from $\\W(k_\\eps) \\in \\conb_{\\mu,R_\\eps}(\\Ws)$, remain within this set. We proceed by induction.  Suppose that the claim holds up to iteration $k \\geq k_\\eps$. This implies that $ \\W(k) \\in \\conb_{\\mu,R_\\eps}(\\Ws)$. Hence, recalling $\\conb_{\\mu,R_\\eps}(\\Ws)$ defined in \\eqref{eqn:con:nabla0}, there exists scalar $\\mu=\\mu(\\bal) \\in (0,1)$  and $R_\\eps$ such that  $\\tf{\\W(k)}\\geq R_\\eps$, and\n\n\n\n\n\n\\begin{equation*}\n\\begin{split}\n\\left\\langle (\\x_{i\\op_i}-\\x_{it})\\z_i^\\top,\\frac{\\W(k)}{\\tf{\\W(k)}} \\right\\rangle  \\geq \\mu\\Theta,\n\\end{split}\n\\end{equation*}\n\n\n\nwhere $\\Theta=1/\\tf{\\Wm}$. \n\nLet \n\\begin{subequations}\\label{eqn:rho:def:nabla0}\n\\begin{align}\n\\frac{1}{1-\\epsilon} \\iprod{ {\\frac{\\Wm}{\\tf{\\Wm}}}}{-\\nabla\\mc{L}(\\W(k))} =:\\rho(k)>0.\n\\end{align}\n\\end{subequations}\nUsing \\eqref{assum:extra}, we have \n\\begin{equation}\\label{eqn:localgd:1:nabla0}\n    \\begin{split}\n   \\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top,    \\frac{\\W(k+1)}{\\tf{\\W(k)}} \\right\\rangle &=   \\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top,    \\frac{\\W(k)}{\\tf{\\W(k)}} -\\frac{\\eta}{\\tf{\\W(k)}}\\nabla \\mc{L}(\\W(k)) \\right\\rangle\\\\   \n\n      & \\geq \\mu \\Theta +\\frac{ 2\\eta  (1-\\epsilon)\\mu \\Theta \\rho(k) }{\\tf{\\W(k)}}.\n    \\end{split}\n\\end{equation}\n\n\nFrom Lemma~\\ref{glocal cond},   we have $\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle<0$~ which implies that $\\tf{\\W(k+1)} \\geq \\tf{\\W(k)}$.  This together with  $R_\\eps$ definition and $\\tf{\\W(k)}\\geq 1/2$ implies that  \n\\begin{align*}\n\\tf{\\W(k+1)}&\\leq\\frac{1}{{2\\tf{\\W(k)}}} \\left(\\tf{\\W(k+1)}^2+\\tf{\\W(k)}^2\\right)\\\\\n& = \\frac{1}{2\\tf{\\W(k)}} \\left(2\\tf{\\W(k)}^2-2\\eta\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle+\\eta^2\\tf{\\nabla \\Lc(\\W(k))}^2\\right)\\\\\n       &\\leq  \\tf{\\W(k)}- \\frac{\\eta}{\\tf{\\W(k)}}\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle + \\eta^2 \\|\\nabla \\Lc(\\W(k))\\|_F^2.\n\\end{align*}\n\nThus,\n\\begin{equation}\\label{eqn:localgd:2:nabla0}\n\\begin{split}\n  \\frac{\\tf{\\W(k+1)}}{\\tf{\\W(k)}}& \\leq  1- \\frac{\\eta}{\\tf{\\W(k)}}\n       \\left\\langle \\nabla \\Lc(\\W(k)),\\frac{\\W(k)}{\\tf{\\W(k)}} \\right\\rangle + \\eta^2 \\frac{\\|\\nabla \\mc{L}(\\W(k))\\|_F^2}{\\tf{\\W(k)}}\\\\\n& \\leq 1- \\frac{\\eta}{(1-\\epsilon)\\tf{\\W(k)}}  \\iprod{\\nabla\\mc{L}(\\W(k))}\n     {\\frac{\\Wm}{\\tf{\\Wm}}}+ \\eta^2 \\frac{\\|\\nabla \\mc{L}(\\W(k))\\|_F^2}{\\tf{\\W(k)}}\\\\\n      & \\leq  1 + \\frac{\\eta \\rho(k)}{\\tf{\\W(k)}} + \\frac{\\eta^2\\|\\nabla \\mc{L}(\\W(k))\\|_F^2}{\\tf{\\W(k)}}=:C_1(\\rho(k),\\eta).\n\\end{split}\n\\end{equation}\nHere, the second inequality uses \\eqref{eqn:neg:corr:0}. \n\nNow, it follows from \\eqref{eqn:localgd:1:nabla0} and \\eqref{eqn:localgd:2:nabla0} that \n\\begin{equation}\\label{eqn:localgd:3:nabla0}\n\\begin{split}\n\\min_{t\\neq \\op_i,~i\\in[n]} ~~  \\left\\langle  (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top, \\frac{\\W(k+1)}{\\tf{\\W(k+1)}}\\right\\rangle   &\\geq \\frac{1}{C_1({\\rho}(k),\\eta)} \\left(\\mu \\Theta+\\frac{2\\eta (1-\\epsilon)\\mu \\Theta  {\\rho}(k)}{\\tf{\\W(k)}}\\right)\\\\\n& = \\mu \\Theta+\\frac{\\eta\\mu \\Theta}{C_1({\\rho}(k),\\eta)} \\left(\\frac{ \\big(2(1-\\epsilon) -1 \\big)  \\rho(k)}{\\tf{\\W(k)}}\n-   \\eta \\frac{\\tf{\\nabla \\mc{L}(\\W(k))}^2 }{\\tf{\\W(k)}}\\right)\\\\\n& = \\mu \\Theta+\\frac{\\eta\\mu \\Theta}{C_1({\\rho}(k),\\eta)} \\left(\\frac{ (1-2\\epsilon)   \\rho(k)}{\\tf{\\W(k)}}\n-   \\eta  \\frac{\\tf{\\nabla \\mc{L}(\\W(k))}^2 }{\\tf{\\W(k)}}\\right)\\\\\n\n\n& \\geq \\mu \\Theta,\n\\end{split}\n\\end{equation}\n where the last inequality uses our choice of stepsize $\\eta\\leq 1/L_W$ in Theorem~\\ref{conv:gd:w:global:nabla0}'s statement. Specifically, we need $\\eta$ to be small to ensure the last inequality. We will guarantee this by choosing a proper $R_\\eps$ in Lemma \\ref{lem:glocal:corr}. Specifically, Lemma \\ref{lem:glocal:corr} leaves the choice of $D_0$ in $R_\\eps$ lower bound of \\eqref{Rpi choice2} open (it can always be chosen larger). Here, by choosing $D_0\\gtrsim 1/L_{\\W}$ will ensure $\\eta\\leq 1/L_{\\W}$ works well.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{equation}\\label{eqn:zeta:mu:0}\n\\begin{split}\n    \\eta &\\leq    \\big( 1-\\mu\\big)\\mu\n \\frac{c}{C}  \\frac{\\Theta}{\\bar{A}}   \\frac{1}{\\bar{A}C T}    e^{R_\\mu^0\\Theta/2}\\\\\n \n \n &\\leq      \\frac{1-2\\epsilon  }{1-\\epsilon}   \\frac{c \\mu}{C}   \\frac{\\Theta}{\\bar{A}}    \\frac{1}{\\bar{A}C T}  e^{R_\\mu^0\\Theta/2} \\\\\n &\\leq   \\big(1-2\\epsilon \\big)   \\frac{\\rho(k) } { \\|\\nabla \\mc{L}(\\W(k))\\|^2_F}.\n\\end{split}    \n\\end{equation}\n\n\n\n\n\n\n\n\n\nHere, the first inequality follows since $\\epsilon \\in (0, \\mu/2)$ (as seen in \\textbf{Step 2}). Also,  $\\mu < 1$ implies that $1-\\mu > 0$, we obtain $\\eta > 0$. The last inequality is obtained from Lemma~\\ref{glocal cond}:\n\\begin{align*}\n    \\frac{\\rho(k) } { \\tf{\\nabla \\mc{L}(\\W(k))}} &= - \\frac{1}{1-\\epsilon} \\iprod{ \\frac{\\nabla\\mc{L}(\\W(k))}{\\tf{\\nabla \\mc{L}(\\W(k))}}}\n     {\\frac{\\Wm}{\\tf{\\Wm}}}  \\geq \\frac{1}{1-\\epsilon} \\cdot \\frac{c \\mu}{C} \\cdot \\frac{\\Theta}{\\bar{A}},\\\\\n         \\frac{1} { \\tf{\\nabla \\mc{L}(\\W(k))}} &{\\geq \\frac{1}{\\bar{A}C \\cdot \\frac{1}{n} \\sum_{i=1}^n  \\left(1-\\s_{i\\op_i}\\right)} \\geq     \\frac{1}{ \\bar{A} C T e^{-R_\\mu^{0}\\Theta/2}} }\n\\end{align*}\nfor some data dependent constrants $c$, $C$, $\\bar{A}=\\max_{i\\in[n],t,\\tau\\in[T]}\\tn{(\\x_{it}- \\x_{i\\tau})}~\\tn{\\z_i}$, and $\\Theta=1/\\tf{\\Ws}$.\n\nThe remainder of the proof of this step is identical to \\eqref{eqn:pitoC0}--\\eqref{eqn:pitoC02}, with the replacement of $C_0$ by $D_0$ and the tracking of changes. Specifically, Lemma \\ref{lem:glocal:corr} leaves the choice of $D_0$ in $R_\\eps$ lower bound of \\eqref{Rpi choice2} open (it can always be chosen larger). Hence,  for sufficiently large $D_0$, we have\n\\begin{align}\n\\eta \\leq \\frac{1}{L_{\\W}}\\leq   \\big( 1-\\mu\\big)\\mu\n \\frac{c}{C}  \\frac{\\Theta}{\\bar{A}}   \\frac{1}{\\bar{A}C T}    e^{R_\\mu^0\\Theta/2}.\n\\end{align}\n\n\nThis implies \\eqref{eqn:localgd:3:nabla0} and  $\\W(k+1) \\in\\conb_{\\mu,R_\\eps}(\\Ws)$. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\noindent\\textbf{Step 4 (Proof of \\ref{lem:zglob:l2}): $\\W(k)$ and $\\Wm$ perfectly align over time.} \nBy theorem statement (alternatively via \\textbf{Step 3}), we have that all iterates remain within the initial conic set i.e.~$\\W(k)\\in\\conb_{\\mu,R^0_\\mu}(\\Ws)$ for all $k\\geq 0$. Note that it follows from Lemma~\\ref{glocal cond}  that  $\\li\\nabla\\Lc(\\W), \\Ws/\\tf{\\Ws}\\ri<0$, for any finite $\\W \\in \\conb_{\\mu,R^0_\\mu}(\\Ws)$. Hence, there are no finite critical points $\\W \\in \\conb_{\\mu,R^0_\\mu}(\\Ws)$, for which $\\nabla \\mc{L} (\\W)=0$. Now, based on Lemma~\\ref{lem:grad:descent}, which guarantees that $\\nabla\\Lc(\\W(k))\\rightarrow 0$, this\nimplies that $\\left\\Vert \\W\\left(k\\right)\\right\\Vert \\rightarrow\\infty$. Consequently, for any choice of $\\eps\\in (0,\\mu/2)$ there is an iteration $k_\\eps$ such that, for all $k\\geq k_\\eps$, $\\W(k)\\in\\conb_{\\mu,R_\\eps}(\\Ws)$. Once within $\\conb_{\\mu,R_\\eps}(\\Ws)$,  multiplying both sides of \\eqref{eqn:neg:corr:0} by the stepsize $\\eta$ and using the gradient descent update, we get\n\\begin{equation*}\n\\begin{split}\n     \\left\\langle \\W(k+1)-\\W(k),\\frac{ \\Wm}{\\tf{\\Wm}} \\right\\rangle &\\geq  (1-\\epsilon) \\left\\langle \\W(k+1)-\\W(k), \\frac{\\W(k)}{\\tf{\\W(k)}}\\right\\rangle\\\\\n     &= \\frac{(1-\\epsilon)}{2\\tf{\\W(k)}}\\left(\\tf{\\W(k+1)}^2- \\tf{\\W(k)}^2-\\tf{\\W(k+1)-\\W(k)}^2\\right) \\\\\n     & \\geq (1-\\epsilon)\\left( \\frac{1}{2\\tf{\\W(k)}} \\left(\\tf{\\W(k+1)}^2- \\tf{\\W(k)}^2\\right)-\\tf{\\W(k+1)-\\W(k)}^2\\right) \\\\\n     & \\geq (1-\\epsilon)\\left(\\tf{\\W(k+1)}- \\tf{\\W(k)}-\\tf{\\W(k+1)-\\W(k)}^2\\right) \\\\\n          & \\geq (1-\\epsilon)\\Big(\\tf{\\W(k+1)}- \\tf{\\W(k)}- 2\\eta  \\left(\\mc{L}(\\W(k))-\\mc{L}(\\W(k+1))\\right) \\Big).\n\\end{split}\n\\end{equation*}\n\nHere, the second inequality is obtained from  $\\tf{\\W(k)}\\geq 1/2$; the third inequality follows since  for any $a, b >0$, we have $  (a^2-b^2)/(2b) -  (a-b) \\geq 0$; and the last inequality  uses Lemma~\\ref{lem:grad:descent}.\n\n\n\nSumming the above inequality over $k\\geq k_\\eps$ gives \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{align*}\n      \\left\\langle\\frac{\\W(k)}{\\tf{\\W(k)}}, \\frac{\\Wm}{\\tf{\\Wm}} \\right\\rangle \\ge1-\\epsilon+ \\frac{C(\\epsilon,\\eta)}{\\tf{\\W(k)}}, \\qquad \\W(k)\\in\\conb_{\\mu,R_\\eps}(\\Ws),   \n\\end{align*}\nwhere $\\mathcal{L}_{\\star}\\leq\\mathcal{L}\\left(\\W\\left(k\\right)\\right)$ for all $k\\geq k_\\eps$, and \n\\begin{equation*}\nC(\\epsilon,\\eta)= \\left\\langle \\W(k_\\eps), \\frac{ \\Wm}{\\tf{\\Wm}}\\right\\rangle-(1-\\epsilon)\\tf{\\W(k_\\eps)} -2\\eta (1-\\epsilon) (\\mc{L}(\\W(k_\\eps))-\\mathcal{L}_{\\star}).\n\\end{equation*}\nConsequently,\n    \\begin{align*}\n      \\liminf_{k\\to\\infty}\\iprod{\\frac{\\W(k)}{\\tf{\\W(k)}}}{\\frac{\\Wm}{\\tf{\\Wm}}}\\ge1-\\epsilon, \\qquad \\W(k)\\in\\conb_{\\mu,R_\\eps}(\\Ws).  \n    \\end{align*}\nSince  $\\epsilon \\in (0, \\mu/2)$  is arbitrary, this implies $\\W(k)/\\tf{\\W(k)}\\to  \\Wm/\\tf{\\Wm}$.\n\\end{proof}",
                            "statement_html": "Note that <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_30/index.html#lem%3Azglob%3Al1\">Theorem 30</a> is a direct corollary of <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_36/index.html#glocal+cond\">Lemma 36</a>. We proceed with the proof of <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_30/index.html#lem%3Azglob%3Al2\">Theorem 30</a> and <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_30/index.html#lem%3Azglob%3Al3\">Theorem 30</a>.\n\nWe provide the proof in four steps:\n<br>\n<b>Step~1: $\\conb_{\\mu,R^0_\\mu}(\\Wm)$ construction</b>. Let us denote the initialization lower bound as $R^0_\\mu:=R$, where $R$ is given in the <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_30/index.html#conv%3Agd%3Aw%3Aglobal%3Anabla0%3Aapp\">Theorem 30</a>'s statement. Consider an arbitrary value of $\\epsilon \\in (0, \\mu/2)$ and let $1/(1+\\pi)=1-\\epsilon$. We additionally denote $R_\\eps\\gets R_\\pi\\vee 1/2$ where $R_\\pi$ was defined in <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_37/index.html#lem%3Aglocal%3Acorr\">Lemma 37</a>. At initialization $\\W(0)$, we set $\\eps=\\mu/2$ to obtain $R^0_\\mu= R_{\\mu/2}$.\n\nWe proceed to show $\\mu \\in (0,\\min (1,\\iota \\tf{\\Ws}/\\tf{\\nabla \\Lc(0)})$. It follows from Assumption $\\ref{assum:nabla0}$ and under zero initialization for GD ($\\W(0)=0$) that\n$$\n\\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top, - \\nabla \\mc{L}(\\W(0)) \\right\\rangle =\\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top, - \\nabla \\mc{L}(0) \\right\\rangle\\ge \\iota > 0,\n$$\nfor some positive constant $\\iota$. Hence, for any initial step size $\\eta (0)>0$ and $\\W(1)=-\\eta(0) \\nabla\\Lc(0)$,\n\\begin{equation}\\label{eqn:decpath:zinit}\n\\begin{split}\n    \\li(\\x_{i\\op_i}-\\x_\\itt)\\z_i^\\top, \\frac{\\W (1)}{\\tf{\\W(1)}}\\ri &= \\frac{\\eta (0) }{\\tf{\\W(1)}} \\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top, - \\nabla \\mc{L}(0) \\right\\rangle \\\\\n     & \\geq \\frac{\\iota \\eta (0) }{\\tf{\\W(1)}}= \\frac{\\iota }{ \\tf{\\nabla \\mc{L}(0)}}\\\\\n     &\\geq \\frac{\\mu}{\\tf{\\Wm}}.\n\\end{split}\n\\end{equation}\nHere, the last inequality follows from our choice of $\\mu$ in the theorem statement, i.e.\n\\begin{align}\\label{eqn:mu:zero}\n \\mu \\in \\left(0, \\min\\left(1, \\frac{\\iota \\tf{\\Wm}}{\\tf{\\nabla \\mc{L}(0)}}\\right)\\right).\n\\end{align}\n\nThis $\\mu$ choice induces the conic set $\\conb_{\\mu,R^0_\\mu}(\\Wm)$ with $R^0_\\mu= R_{\\mu/2}$, where $R_{\\mu/2}$ was defined in <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_37/index.html#lem%3Aglocal%3Acorr\">Lemma 37</a>. Now, given the parameter $\\mu$ satisfying \\eqref{eqn:mu:zero}, we can choose $\\eta (0)$ such that $\\tf{\\W (1)} \\geq R^0_\\mu$ and $\\W(1)\\in\\conb_{\\mu, R^0_\\mu}(\\Wm)$. To achieve this, since $\\W(0)=0$, we obtain\n\\begin{equation}\\label{eqn:stepeta0}\n    \\eta (0) =\\frac{R^0_\\mu}{\\tf{\\nabla \\mc{L}(0) }}.\n\\end{equation}\nSince by our definition, $R^0_\\mu \\leftarrow R$, \\eqref{eqn:stepeta0} gives $\\W(1)$ in the theorem's statement.\n\n<br>\n<b>Step~2: There are no stationary points within $\\conb_{\\mu,R_\\mu^0}(\\Wm)$.</b>\nThis step follows from <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_30/index.html#lem%3Azglob%3Al1\">Theorem 30</a>. Specifically,\n\nwe can apply <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_36/index.html#glocal+cond\">Lemma 36</a> to find that: For all $\\V,\\W\\in \\bar{\\Sc}_{\\mu}(\\Ws)$ with $\\tf{\\W} \\neq 0$ and $\\tf{\\W} \\geq R^0_\\mu$, we have that $-\\li\\V, \\nabla \\Lc(\\W)\\ri$ is strictly positive.\n\n<br>\n$\\emph{Gradient correlation holds for large parameter norm.}$\n\nIt follows from <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_37/index.html#lem%3Aglocal%3Acorr\">Lemma 37</a> that, there exists $R_\\epsilon\\geq \\bar{R}_\\mu\\vee 1/2$ such that all $\\W \\in \\conb_{\\mu,R_\\epsilon}(\\Wm)$ satisfy\n\\begin{align}\\label{eqn:neg:corr:0}\n\\iprod{-\\nabla\\mc{L}(\\W)} {\\frac{\\Wm}{\\tf{\\Wm}}} \\geq (1-\\epsilon) \\iprod{-\\nabla \\mc{L}(\\W)}{\\frac{\\W}{\\tf{\\W}}}.\n\\end{align}\nThe following argument applies to a general $\\eps\\in(0,\\mu/2)$. However, at initialization $\\W(0)=0$, we have set $\\eps=\\mu/2$ and defined the initialization radius as $R^0_\\mu= R_{\\mu/2}$. To proceed, we will prove the main statements <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_30/index.html#lem%3Azglob%3Al2\">Theorem 30</a> and <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_30/index.html#lem%3Azglob%3Al3\">Theorem 30</a> as follows.\n<ul>\n<li>Proving <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_30/index.html#lem%3Azglob%3Al3\">Theorem 30</a>: In <b>Step 3</b>, we will assume Condition \\eqref{assum:extra} to prove that gradient iterates remain within $\\conb_{\\mu,R_\\eps}(\\Ws)$. Concretely, for any $\\epsilon \\in (0, \\mu/2)$, we will show that after gradient descent enters the conic set $\\conb_{\\mu,R_\\eps}(\\Ws)$ for the first time, it will never leave the set under Condition \\eqref{assum:extra} of the theorem statement and \\eqref{eqn:neg:corr:0}. In what follows, let us denote $k_\\eps$ to be the first time gradient descent enters $\\conb_{\\mu,R_\\eps}(\\Ws)$. Note that for $\\eps\\gets\\mu/2$, $k_\\eps=0$ i.e.~the point of initialization.</li>\n\n<li>Proving <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_30/index.html#lem%3Azglob%3Al2\">Theorem 30</a>: In <b>Step 4</b>, assuming iterates within $\\conb_{\\mu,R_\\eps}(\\Ws)$, we will prove that the norm diverges (as a result such $k_\\eps$ is guaranteed to exist) and, additionally, the gradient updates asymptotically aligns with $\\Ws$.</li>\n</ul>\n\n<br>\n<b>Step~3 (Proof of <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_30/index.html#lem%3Azglob%3Al3\">Theorem 30</a>): Updates remain inside the cone $\\conb_{\\mu,R_\\eps}(\\Ws)$.</b>\nNote that if $\\W(k) \\in \\conb_{\\mu,R_\\eps}(\\Ws)$ for all $k \\geq 1$, the required condition in <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_30/index.html#lem%3Azglob%3Al2\">Theorem 30</a> holds, and we proceed to <b>Step 4</b>. In this step, we show <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_30/index.html#lem%3Azglob%3Al3\">Theorem 30</a>. Specifically, we show that under Condition \\eqref{assum:extra} and using \\eqref{eqn:neg:corr:0}, all iterates $\\W(k) \\in \\conb_{\\mu,R_\\eps}(\\Ws)$ remain within $\\conb_{\\mu,R_\\eps}(\\Ws)$.\n\nTo proceed, by leveraging the results from <b>Step 1</b> and <b>Step 2</b>, we demonstrate that the gradient iterates, with an appropriate constant step size, starting from $\\W(k_\\eps) \\in \\conb_{\\mu,R_\\eps}(\\Ws)$, remain within this set. We proceed by induction. Suppose that the claim holds up to iteration $k \\geq k_\\eps$. This implies that $\\W(k) \\in \\conb_{\\mu,R_\\eps}(\\Ws)$. Hence, recalling $\\conb_{\\mu,R_\\eps}(\\Ws)$ defined in (33) [in <a href=\"https://arxiv.org/pdf/2308.16898#equation.C.33\">original paper</a>], there exists scalar $\\mu=\\mu(\\bal) \\in (0,1)$ and $R_\\eps$ such that $\\tf{\\W(k)}\\geq R_\\eps$, and\n\n\\begin{equation*}\n\\begin{split}\n\\left\\langle (\\x_{i\\op_i}-\\x_{it})\\z_i^\\top,\\frac{\\W(k)}{\\tf{\\W(k)}} \\right\\rangle \\geq \\mu\\Theta,\n\\end{split}\n\\end{equation*}\n\nwhere $\\Theta=1/\\tf{\\Wm}$.\n\nLet\n\\begin{subequations}\\label{eqn:rho:def:nabla0}\n\\begin{align}\n\\frac{1}{1-\\epsilon} \\iprod{ {\\frac{\\Wm}{\\tf{\\Wm}}}}{-\\nabla\\mc{L}(\\W(k))} =:\\rho(k)>0.\n\\end{align}\n\\end{subequations}\nUsing \\eqref{assum:extra}, we have\n\\begin{equation}\\label{eqn:localgd:1:nabla0}\n    <br>\n    \\begin{split}\n   \\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top, \\frac{\\W(k+1)}{\\tf{\\W(k)}} \\right\\rangle &= \\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top, \\frac{\\W(k)}{\\tf{\\W(k)}} -\\frac{\\eta}{\\tf{\\W(k)}}\\nabla \\mc{L}(\\W(k)) \\right\\rangle\\\\\n      & \\geq \\mu \\Theta +\\frac{ 2\\eta (1-\\epsilon)\\mu \\Theta \\rho(k) }{\\tf{\\W(k)}}.\n    </split>\n\\end{equation}\n\nFrom <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_36/index.html#glocal+cond\">Lemma 36</a>, we have $\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle<0$ which implies that $\\tf{\\W(k+1)} \\geq \\tf{\\W(k)}$. This together with $R_\\eps$ definition and $\\tf{\\W(k)}\\geq 1/2$ implies that\n<pre>\n\\tf{\\W(k+1)}&\\leq\\frac{1}{{2\\tf{\\W(k)}}} \\left(\\tf{\\W(k+1)}^2+\\tf{\\W(k)}^2\\right)\\\\\n& = \\frac{1}{2\\tf{\\W(k)}} \\left(2\\tf{\\W(k)}^2-2\\eta\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle+\\eta^2\\tf{\\nabla \\Lc(\\W(k))}^2\\right)\\\\\n       &\\leq \\tf{\\W(k)}- \\frac{\\eta}{\\tf{\\W(k)}}\\left\\langle \\nabla \\Lc(\\W(k)),\\W(k)\\right\\rangle + \\eta^2 \\frac{\\|\\nabla \\Lc(\\W(k))\\|_F^2}{\\tf{\\W(k)}}.\n</pre>\nThus,\n<pre>\n  \\frac{\\tf{\\W(k+1)}}{\\tf{\\W(k)}}& \\leq 1- \\frac{\\eta}{\\tf{\\W(k)}}\n       \\left\\langle \\nabla \\Lc(\\W(k)),\\frac{\\W(k)}{\\tf{\\W(k)}} \\right\\rangle + \\eta^2 \\frac{\\|\\nabla \\mc{L}(\\W(k))\\|_F^2}{\\tf{\\W(k)}}\\\\\n& \\leq 1- \\frac{\\eta}{(1-\\epsilon)\\tf{\\W(k)}} \\iprod{\\nabla\\mc{L}(\\W(k))}\n     {\\frac{\\Wm}{\\tf{\\Wm}}}+ \\eta^2 \\frac{\\|\\nabla \\mc{L}(\\W(k))\\|_F^2}{\\tf{\\W(k)}}\\\\\n      & \\leq 1 + \\frac{\\eta \\rho(k)}{\\tf{\\W(k)}} + \\frac{\\eta^2\\|\\nabla \\mc{L}(\\W(k))\\|_F^2}{\\tf{\\W(k)}}=:C_1(\\rho(k),\\eta).\n</pre>\nHere, the second inequality uses \\eqref{eqn:neg:corr:0}.\n\nNow, it follows from \\eqref{eqn:localgd:1:nabla0} and \\eqref{eqn:localgd:2:nabla0} that\n<pre>\n\\min_{t\\neq \\op_i,~i\\in[n]} ~~ \\left\\langle (\\x_{i\\op_i}-\\x_\\itt) \\z_i^\\top, \\frac{\\W(k+1)}{\\tf{\\W(k+1)}}\\right\\rangle &\\geq \\frac{1}{C_1({\\rho}(k),\\eta)} \\left(\\mu \\Theta+\\frac{2\\eta (1-\\epsilon)\\mu \\Theta {\\rho}(k)}{\\tf{\\W(k)}}\\right)\\\\\n& = \\mu \\Theta+\\frac{\\eta\\mu \\Theta}{C_1({\\rho}(k),\\eta)} \\left(\\frac{ \\big(2(1-\\epsilon) -1 \\big) \\rho(k)}{\\tf{\\W(k)}}\n- \\eta \\frac{\\tf{\\nabla \\mc{L}(\\W(k))}^2 }{\\tf{\\W(k)}}\\right)\\\\\n& = \\mu \\Theta+\\frac{\\eta\\mu \\Theta}{C_1({\\rho}(k),\\eta)} \\left(\\frac{ (1-2\\epsilon) \\rho(k)}{\\tf{\\W(k)}}\n- \\eta \\frac{\\tf{\\nabla \\mc{L}(\\W(k))}^2 }{\\tf{\\W(k)}}\\right)\\\\\n& \\geq \\mu \\Theta,\n</pre>\nwhere the last inequality uses our choice of stepsize $\\eta\\leq 1/L_W$ in <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_23/index.html#conv%3Agd%3Aw%3Aglobal%3Anabla0\">Theorem 23</a>'s statement. Specifically, we need $\\eta$ to be small to ensure the last inequality. We will guarantee this by choosing a proper $R_\\eps$ in <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_37/index.html#lem%3Aglocal%3Acorr\">Lemma 37</a>. Specifically, <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_37/index.html#lem%3Aglocal%3Acorr\">Lemma 37</a> leaves the choice of $D_0$ in $R_\\eps$ lower bound of \\eqref{Rpi choice2} open (it can always be chosen larger). Here, by choosing $D_0\\gtrsim 1/L_{\\W}$ will ensure $\\eta\\leq 1/L_{\\W}$ works well.\n\n<pre>\n\\begin{equation}\\label{eqn:zeta:mu:0}\n\\begin{split}\n    \\eta &\\leq \\big( 1-\\mu\\big)\\mu\n \\frac{c}{C} \\frac{\\Theta}{\\bar{A}} \\frac{1}{\\bar{A}C T} e^{R_\\mu^0\\Theta/2}\\\\\n &\\leq \\frac{1-2\\epsilon }{1-\\epsilon} \\frac{c \\mu}{C} \\frac{\\Theta}{\\bar{A}} \\frac{1}{\\bar{A}C T} e^{R_\\mu^0\\Theta/2} \\\\\n &\\leq \\big(1-2\\epsilon \\big) \\frac{\\rho(k) } { \\|\\nabla \\mc{L}(\\W(k))\\|^2_F}.\n\\end{split}\n\\end{equation}\n</pre>\n\nHere, the first inequality follows since $\\epsilon \\in (0, \\mu/2)$ (as seen in <b>Step 2</b>). Also, $\\mu < 1$ implies that $1-\\mu > 0$, we obtain $\\eta > 0$. The last inequality is obtained from <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_36/index.html#glocal+cond\">Lemma 36</a>:\n<pre>\n    \\frac{\\rho(k) } { \\tf{\\nabla \\mc{L}(\\W(k))}} &= - \\frac{1}{1-\\epsilon} \\iprod{ \\frac{\\nabla\\mc{L}(\\W(k))}{\\tf{\\nabla \\mc{L}(\\W(k))}}}\n     {\\frac{\\Wm}{\\tf{\\Wm}}} \\geq \\frac{1}{1-\\epsilon} \\cdot \\frac{c \\mu}{C} \\cdot \\frac{\\Theta}{\\bar{A}},\\\\\n         \\frac{1} { \\tf{\\nabla \\mc{L}(\\W(k))}} &{\\geq \\frac{1}{\\bar{A}C \\cdot \\frac{1}{n} \\sum_{i=1}^n \\left(1-\\s_{i\\op_i}\\right)} \\geq \\frac{1}{ \\bar{A} C T e^{-R_\\mu^{0}\\Theta/2}} }\n</pre>\nfor some data dependent constrants $c$, $C$, $\\bar{A}=\\max_{i\\in[n],t,\\tau\\in[T]}\\tn{(\\x_{it}- \\x_{i\\tau})}~\\tn{\\z_i}$, and $\\Theta=1/\\tf{\\Ws}$.\n\nThe remainder of the proof of this step is identical to \\eqref{eqn:pitoC0}--\\eqref{eqn:pitoC02}, with the replacement of $C_0$ by $D_0$ and the tracking of changes. Specifically, <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_37/index.html#lem%3Aglocal%3Acorr\">Lemma 37</a> leaves the choice of $D_0$ in $",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n<br>\n<br>1. <b>Initialization and Setup</b>: The proof begins by defining the initialization lower bound \\( R^0_\\mu \\) and setting up the parameters \\(\\epsilon\\) and \\(R_\\epsilon\\). The initial step size \\(\\eta(0)\\) is chosen to ensure that the initial weight \\(\\W(1)\\) lies within the conic set \\(\\conb_{\\mu, R^0_\\mu}(\\Wm)\\).\n<br>\n<br>2. <b>No Stationary Points</b>: This step leverages Theorem 30 and Lemma 36 to show that there are no stationary points within the conic set \\(\\conb_{\\mu, R^0_\\mu}(\\Wm)\\). It uses the gradient correlation property to establish that the inner product of the gradient and the weight vector is strictly positive.\n<br>\n<br>3. <b>Updates Remain Inside the Cone</b>: The proof proceeds by induction to show that if the weight vector \\(\\W(k)\\) is within the conic set \\(\\conb_{\\mu, R_\\epsilon}(\\Wm)\\) at some iteration \\(k\\), it will remain within this set for all subsequent iterations. This is achieved by carefully choosing the step size \\(\\eta\\) and using the properties of the gradient and the conic set.\n<br>\n<br>4. <b>Gradient Descent Convergence</b>: Finally, the proof shows that under the given conditions, the norm of the weight vector diverges, and the gradient updates asymptotically align with the target vector \\(\\Ws\\). This ensures that the gradient descent algorithm converges to the desired solution.\n\nThus, the proof demonstrates that the gradient descent algorithm, with appropriately chosen parameters, will converge to the desired solution while maintaining the weight vector within the specified conic set."
                        }
                    },
                    {
                        "statement_id": "cbebd782-59d1-44bd-b1b0-cb2b0cadebbf",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 31,
                        "library_name": "Theorem 31",
                        "title": "Divergence of Norms in Gradient Descent",
                        "statement_original_tex": "\\begin{theorem}\\label{diverg:norm:qk}\nSuppose Assumption~\\ref{assum:loss:prope} on the loss function $\\ell$ and Assumption \\ref{assum:token} on the tokens hold. Assume the initialization $(\\Kb(0), \\Qb(0))$ satisfies $\\nabla \\Lc (\\Kb(0),\\Qb(0)) \\neq 0$. Let $\\eta_k=\\min\\{1/L(R_k),1\\}$, where $R_k$ is chosen such that $(\\Kb(k),\\Qb(k))\\in \\mc{S}(R_k-1)$, and if $(\\Kb(k+1),\\Qb(k+1))\\in \\mc{S} (R_k-1)$, then $R_{k+1}=R_k$.  Then, the following statements hold:\n\\begin{itemize}\n\\item \nThere is no $\\Kb,\\Qb\\in\\R^{d\\times m}$ satisfying $\\nabla \\Lc(\\Kb,\\Qb)=0$.\n\\item Algorithm~\\ref{GD-QK} with the step size $\\eta_k$  satisfies  $\\lim_{k \\rightarrow \\infty} \\tf{\\nabla \\Lc_\\Kb(\\Kb(k), \\Qb(k))}\\vee\\tf{\\nabla\\Lc_\\Qb(\\Kb(k),\\Qb(k))}=0$, and  $\\lim_{k\\rightarrow\\infty} \\tf{\\Kb(k)}\\wedge\\tf{\\Qb(k)}=\\infty$.\n\\end{itemize}\n\\end{theorem}",
                        "statement_html": "Suppose Assumption A [in <a href=\"https://arxiv.org/pdf/2308.16898#assumption.1\">original paper</a>] on the loss function $\\ell$ and Assumption B [in <a href=\"https://arxiv.org/pdf/2308.16898#assumption.2\">original paper</a>] on the tokens hold. Assume the initialization $(\\Kb(0), \\Qb(0))$ satisfies $\\nabla \\Lc (\\Kb(0),\\Qb(0)) \\neq 0$. Let $\\eta_k=\\min\\{1/L(R_k),1\\}$, where $R_k$ is chosen such that $(\\Kb(k),\\Qb(k))\\in \\mc{S}(R_k-1)$, and if $(\\Kb(k+1),\\Qb(k+1))\\in \\mc{S} (R_k-1)$, then $R_{k+1}=R_k$. Then, the following statements hold:\n<ul>\n<li>There is no $\\Kb,\\Qb\\in\\R^{d\\times m}$ satisfying $\\nabla \\Lc(\\Kb,\\Qb)=0$.</li>\n<li>Algorithm $\\ref{GD-QK}$ with the step size $\\eta_k$  satisfies  $\\lim_{k \\rightarrow \\infty} \\tf{\\nabla \\Lc_\\Kb(\\Kb(k), \\Qb(k))}\\vee\\tf{\\nabla\\Lc_\\Qb(\\Kb(k),\\Qb(k))}=0$, and  $\\lim_{k\\rightarrow\\infty} \\tf{\\Kb(k)}\\wedge\\tf{\\Qb(k)}=\\infty$.</li>\n</ul>",
                        "statement_type": "theorem",
                        "statement_motivation_html": "This result is crucial in the context of optimization algorithms, particularly gradient descent. It provides conditions under which the gradient descent algorithm will not converge to a stationary point, ensuring that the algorithm continues to make progress. This is particularly useful when dealing with non-convex loss functions, as it guarantees that the algorithm will not get stuck in suboptimal points. Additionally, the result gives a specific step size $\\eta_k$ that ensures the gradients diminish over time, leading to the desired behavior of the parameters $\\Kb$ and $\\Qb$. This can be applied in machine learning and deep learning to ensure effective training of models.",
                        "html_url": "library/theorems/theorem_31/index.html",
                        "corollary_ids": [],
                        "proof": {
                            "statement_id": "1e2083a8-aa91-4ec7-aa40-e6039b242828",
                            "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof}\nSince $(\\Kb(k),\\Qb(k))\\in \\mc{S}(R_k)$, \nwe have \n    \\begin{align}\n        \\|\\Qb(k+1)\\|_F \\le\\|\\Qb(k)\\|_F+\\eta_k\\tf{ \\nabla_{\\Qb} \\Lc(\\Kb(k),\\Qb(k))} \\nonumber    & \\le\\|\\Qb(k)\\|_F+\\frac{1}{L(R_k)}\\tf{\\nabla_{\\Qb} \\Lc(\\Kb(k),\\Qb(k))} \\nonumber \\\\\n\n         & \\le\\|\\Qb(k)\\|_F+1. \\label{eq:gd_inc}\n    \\end{align}\n\n    \nSince $R_k\\to\\infty$ by Lemma~\\ref{lem:out:S} and $R_{k+1}=R_k$ as long as $ (\\Kb(k+1),\\Qb(k+1))\\in \\mc{S}(R_k-1)$, we obtain\n\\begin{equation}\\label{eqn:c1}\n \\max\\{ \\|\\Qb(k)\\|_F,  \\|\\Kb(k)\\|_F \\} \\quad    \\textnormal{is unbounded}. \\tag{C1}\n\\end{equation}\n\n    \nIt then follows that for any $k$, by Cauchy-Schwarz,\n\\begin{align*}\n\\left(\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}\\right) \\left(\\sum_{\\tau=0}^{k-1}\\eta_{\\tau} \\|\\nabla \\Lc(\\Kb(\\tau),\\Qb(\\tau))\\|^2_F\\right) & \n\\ge \\left(\\sum_{\\tau=0}^{k-1}\\eta_{\\tau} \\|\\nabla \\Lc(\\Kb(\\tau),\\Qb(\\tau))\\|^2_F\\right)^2\\to\\infty.\n\\end{align*}\nSince by  \\eqref{eq:descent:obj},\n\\begin{align*}\n        \\sum_{\\tau=0}^{k-1}\\eta_{\\tau} \\left\\|\\nabla \\Lc(\\Kb(\\tau),\\Qb(\\tau))\\right\\|^2_F\\le 2\\Lc(\\Kb(0),\\Qb(0))-2\\Lc(\\Kb(k),\\Qb(k) )\\le2 \\Lc(\\Kb(0),\\Qb(0)),\n\\end{align*}\nwe have $\\sum_{t=0}^{\\infty}\\eta_k=\\infty$.\n\nSince gradient descent never increases the risk,  for $(\\Kb(k),\\Qb(k))\\in \\mc{S}(R)$, $\\|\\partial\\Lc/\\partial \\Qb(k)\\|_F\\ge\\epsilon(R)$ for some constant $\\epsilon(R)>0$.  Following similar steps  as the proof of \\eqref{eqn:grad:low:eps},   we get that $\\sum_{k:(\\Kb(k),\\Qb(k))\\in \\mc{S}(R)}^{}\\eta_k<\\infty$.  \n\n\nFor simplicity, we  replace $\\Qb^\\top$ with $\\Qb$ in \\eqref{eqn:erm:kq}. \n\n\n\n For gradient descent iterates \\ref{GD-QK}, summing from $0$ to $k-1$, we get\n\\begin{align}\\label{eqn:qk:recur}\n& \\quad\\Qb(k)^\\top\\Qb(k)-\\Qb(0)^\\top\\Qb(0)+\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\nabla_{\\Qb} \\Lc(\\Kb(\\tau),\\Qb(\\tau))^\\top \\nabla_{\\Qb} \\Lc(\\Kb(\\tau),\\Qb(\\tau)) \\nonumber \\\\\n= & \\quad \\Kb(k)\\Kb^{\\top}(k)-\\Kb(0)\\Kb^{\\top}(0)+\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2  \\nabla_{\\Kb} \\Lc(\\Kb(\\tau),\\Qb(\\tau)) \\nabla_{\\Kb} \\Lc(\\Kb(\\tau),\\Qb(\\tau))^\\top.\n\\end{align}\nLet\n\\begin{subequations}\n    \\begin{align*}\n       P_{\\Qb}&=\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\nabla_{\\Qb} \\Lc(\\Kb,\\Qb) \\nabla_{\\Qb} \\Lc(\\Kb(\\tau), \\Qb (\\tau)))^\\top,\\\\\n   P_{\\Kb}&:=\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\nabla_{\\Kb} \\Lc(\\Kb,\\Qb)  \\nabla_{\\Qb} \\Lc(\\Kb(\\tau), \\Kb (\\tau)))^\\top,\n    \\end{align*}\n    and\n\\begin{align*}\nS_{\\Qb}(k)=\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\nabla_{\\Qb} \\Lc(\\Kb(\\tau), \\Qb (\\tau)))^\\top \\nabla_{\\Qb} \\Lc(\\Kb(\\tau), \\Qb (\\tau))),\\\\\nS_{\\Kb}(k)=\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\nabla_{\\Kb} \\Lc(\\Kb(\\tau), \\Qb (\\tau)))^\\top \\nabla_{\\Kb} \\Lc(\\Kb(\\tau), \\Qb (\\tau))) .\n    \\end{align*}\n\\end{subequations}\nWe obtain\n\\begin{subequations}\n    \\begin{align}\\label{eq:qk:tr:b1}\n           \\tr(P_{\\Kb}(k))+ \\tr(P_{\\Qb}(k))  & =\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\|\\nabla \\Lc(\\Kb(\\tau), \\Qb (\\tau))\\|^2_F \\nonumber \\\\\n         & \\le \\sum_{\\tau=0}^{k-1}\\eta_{\\tau} \\|\\nabla \\Lc(\\Kb(\\tau), \\Qb (\\tau))\\|^2_F  \\nonumber \\\\\n         &  \\leq 2 \\Lc(\\W(0)) -2 \\Lc(\\W(k))  \\nonumber\\\\\n            &  \\leq 2 \\Lc(\\W(0)) -2 \\Lc_\\star.\n    \\end{align}\nIt follows from \\eqref{eqn:qk:recur} that\n\\begin{align}\\label{eq:qk:tr:b2}\n\\|\\Kb(k)\\|_F^2=\\|\\Qb(k)\\|_F^2+\\|\\Kb(0)\\|_F^2-\\|\\Qb(0)\\|_F^2-\\tr(P_{\\Kb}(k))+\\tr(S_{\\Qb}(k)).\n\\end{align}    \n\\end{subequations}\nIn other words, the difference between the squares of Frobenius norms of $(\\Kb,\\Qb)$ is still bounded.\n\n\nNow, combining \\eqref{eq:qk:tr:b1} and \\eqref{eq:qk:tr:b2}, we get\n \\begin{align*}\n   2 \\Lc(\\W(0))\n           \\ge\\sum_{t=0}^{\\infty} \\eta_k \\left\\|\\nabla_{\\Qb} \\Lc(\\Kb (k),\\Qb(k))\\right\\|_F^2 \n          \n          \n           \\geq \\infty,\n     \\end{align*}\n     which is a contradiction. This implies $\\tf{\\Kb(k)}\\to\\infty$, since $\\Lc(\\Kb,\\Qb)$ has no finite optimum.\n\\end{proof}",
                            "statement_html": "Since $(\\Kb(k),\\Qb(k))\\in \\mc{S}(R_k)$, we have \n\\begin{align}\n    \\|\\Qb(k+1)\\|_F \\le\\|\\Qb(k)\\|_F+\\eta_k\\tf{ \\nabla_{\\Qb} \\Lc(\\Kb(k),\\Qb(k))} \\nonumber    & \\le\\|\\Qb(k)\\|_F+\\frac{1}{L(R_k)}\\tf{\\nabla_{\\Qb} \\Lc(\\Kb(k),\\Qb(k))} \\nonumber \\\\\n    & \\le\\|\\Qb(k)\\|_F+1. \\label{eq:gd_inc}\n\\end{align}\n\nSince $R_k\\to\\infty$ by <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_38/index.html#lem%3Aout%3AS\">Lemma 38</a> and $R_{k+1}=R_k$ as long as $(\\Kb(k+1),\\Qb(k+1))\\in \\mc{S}(R_k-1)$, we obtain\n\\begin{equation}\\label{eqn:c1}\n \\max\\{ \\|\\Qb(k)\\|_F,  \\|\\Kb(k)\\|_F \\} \\quad    \\textnormal{is unbounded}. \\tag{C1}\n\\end{equation}\n\nIt then follows that for any $k$, by Cauchy-Schwarz,\n\\begin{align*}\n\\left(\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}\\right) \\left(\\sum_{\\tau=0}^{k-1}\\eta_{\\tau} \\|\\nabla \\Lc(\\Kb(\\tau),\\Qb(\\tau))\\|^2_F\\right) & \n\\ge \\left(\\sum_{\\tau=0}^{k-1}\\eta_{\\tau} \\|\\nabla \\Lc(\\Kb(\\tau),\\Qb(\\tau))\\|^2_F\\right)^2\\to\\infty.\n\\end{align*}\nSince by  \\eqref{eq:descent:obj},\n\\begin{align*}\n    \\sum_{\\tau=0}^{k-1}\\eta_{\\tau} \\left\\|\\nabla \\Lc(\\Kb(\\tau),\\Qb(\\tau))\\right\\|^2_F\\le 2\\Lc(\\Kb(0),\\Qb(0))-2\\Lc(\\Kb(k),\\Qb(k) )\\le2 \\Lc(\\Kb(0),\\Qb(0)),\n\\end{align*}\nwe have $\\sum_{t=0}^{\\infty}\\eta_k=\\infty$.\n\nSince gradient descent never increases the risk, for $(\\Kb(k),\\Qb(k))\\in \\mc{S}(R)$, $\\|\\partial\\Lc/\\partial \\Qb(k)\\|_F\\ge\\epsilon(R)$ for some constant $\\epsilon(R)>0$. Following similar steps as the proof of \\eqref{eqn:grad:low:eps}, we get that $\\sum_{k:(\\Kb(k),\\Qb(k))\\in \\mc{S}(R)}^{}\\eta_k<\\infty$.  \n\nFor simplicity, we replace $\\Qb^\\top$ with $\\Qb$ in \\eqref{eqn:erm:kq}. \n\nFor gradient descent iterates \\ref{GD-QK}, summing from $0$ to $k-1$, we get\n\\begin{align}\\label{eqn:qk:recur}\n& \\quad\\Qb(k)^\\top\\Qb(k)-\\Qb(0)^\\top\\Qb(0)+\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\nabla_{\\Qb} \\Lc(\\Kb(\\tau),\\Qb(\\tau))^\\top \\nabla_{\\Qb} \\Lc(\\Kb(\\tau),\\Qb(\\tau)) \\nonumber \\\\\n= & \\quad \\Kb(k)\\Kb^{\\top}(k)-\\Kb(0)\\Kb^{\\top}(0)+\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2  \\nabla_{\\Kb} \\Lc(\\Kb(\\tau),\\Qb(\\tau)) \\nabla_{\\Kb} \\Lc(\\Kb(\\tau),\\Qb(\\tau))^\\top.\n\\end{align}\nLet\n\\begin{subequations}\n    <div>\n    \\begin{align*}\n       P_{\\Qb}&=\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\nabla_{\\Qb} \\Lc(\\Kb,\\Qb) \\nabla_{\\Qb} \\Lc(\\Kb(\\tau), \\Qb (\\tau)))^\\top,\\\\\n       P_{\\Kb}&:=\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\nabla_{\\Kb} \\Lc(\\Kb,\\Qb)  \\nabla_{\\Qb} \\Lc(\\Kb(\\tau), \\Kb (\\tau)))^\\top,\n    </div>\n    </begin{align*}\n    and\n    <div>\n    \\begin{align*>\n        S_{\\Qb}(k)=\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\nabla_{\\Qb} \\Lc(\\Kb(\\tau), \\Qb (\\tau)))^\\top \\nabla_{\\Qb} \\Lc(\\Kb(\\tau), \\Qb (\\tau))),\\\\\n        S_{\\Kb}(k)=\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\nabla_{\\Kb} \\Lc(\\Kb(\\tau), \\Qb (\\tau)))^\\top \\nabla_{\\Kb} \\Lc(\\Kb(\\tau), \\Qb (\\tau))) .\n    </div>\n    </begin{align*>\n\\end{subequations}\nWe obtain\n\\begin{subequations>\n    <div>\n    <begin{align}\\label{eq:qk:tr:b1}\n           \\tr(P_{\\Kb}(k))+ \\tr(P_{\\Qb}(k))  & =\\sum_{\\tau=0}^{k-1}\\eta_{\\tau}^2 \\|\\nabla \\Lc(\\Kb(\\tau), \\Qb (\\tau))\\|^2_F \\nonumber \\\\\n         & \\le \\sum_{\\tau=0}^{k-1}\\eta_{\\tau} \\|\\nabla \\Lc(\\Kb(\\tau), \\Qb (\\tau))\\|^2_F  \\nonumber \\\\\n         &  \\leq 2 \\Lc(\\W(0)) -2 \\Lc(\\W(k))  \\nonumber\\\\\n            &  \\leq 2 \\Lc(\\W(0)) -2 \\Lc_\\star.\n    </div>\n    </begin{align}\nIt follows from \\eqref{eqn:qk:recur} that\n<div>\n<begin{align}\\label{eq:qk:tr:b2}\n\\|\\Kb(k)\\|_F^2=\\|\\Qb(k)\\|_F^2+\\|\\Kb(0)\\|_F^2-\\|\\Qb(0)\\|_F^2-\\tr(P_{\\Kb}(k))+\\tr(S_{\\Qb}(k)).\n</div>\n</begin{align>    \n\\end{subequations}\nIn other words, the difference between the squares of Frobenius norms of $(\\Kb,\\Qb)$ is still bounded.\n\nNow, combining \\eqref{eq:qk:tr:b1} and \\eqref{eq:qk:tr:b2}, we get\n<div>\n<begin{align*>\n   2 \\Lc(\\W(0))\n           \\ge\\sum_{t=0}^{\\infty} \\eta_k \\left\\|\\nabla_{\\Qb} \\Lc(\\Kb (k),\\Qb(k))\\right\\|_F^2 \n          \n          \n           \\geq \\infty,\n</div>\n</begin{align*>\nwhich is a contradiction. This implies $\\tf{\\Kb(k)}\\to\\infty$, since $\\Lc(\\Kb,\\Qb)$ has no finite optimum.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n<br>\n<br>1. <i>Initial Bound on \\(\\|\\Qb(k+1)\\|_F\\)</i>: The proof starts by establishing an upper bound on the Frobenius norm of \\(\\Qb(k+1)\\):\n<br>   \\[\n   \\|\\Qb(k+1)\\|_F \\le \\|\\Qb(k)\\|_F + \\eta_k \\|\\nabla_{\\Qb} \\Lc(\\Kb(k), \\Qb(k))\\|_F\n   \\]\n<br>   Using the Lipschitz constant \\(L(R_k)\\), this is further bounded by:\n<br>   \\[\n   \\|\\Qb(k)\\|_F + \\frac{1}{L(R_k)} \\|\\nabla_{\\Qb} \\Lc(\\Kb(k), \\Qb(k))\\|_F \\le \\|\\Qb(k)\\|_F + 1\n   \\]\n<br>\n<br>2. <i>Unboundedness of \\(\\|\\Qb(k)\\|_F\\) and \\(\\|\\Kb(k)\\|_F\\)</i>: By Lemma 38, \\(R_k \\to \\infty\\) and \\(R_{k+1} = R_k\\) as long as \\((\\Kb(k+1), \\Qb(k+1)) \\in \\mc{S}(R_k-1)\\). This implies:\n<br>   \\[\n   \\max\\{ \\|\\Qb(k)\\|_F, \\|\\Kb(k)\\|_F \\} \\quad \\text{is unbounded}. \\tag{C1}\n   \\]\n<br>\n<br>3. <i>Cauchy-Schwarz Inequality Application</i>: For any \\(k\\), applying the Cauchy-Schwarz inequality gives:\n<br>   \\[\n   \\left(\\sum_{\\tau=0}^{k-1} \\eta_{\\tau}\\right) \\left(\\sum_{\\tau=0}^{k-1} \\eta_{\\tau} \\|\\nabla \\Lc(\\Kb(\\tau), \\Qb(\\tau))\\|^2_F\\right) \\ge \\left(\\sum_{\\tau=0}^{k-1} \\eta_{\\tau} \\|\\nabla \\Lc(\\Kb(\\tau), \\Qb(\\tau))\\|^2_F\\right)^2 \\to \\infty\n   \\]\n<br>\n<br>4. <i>Bound on Gradient Norm Sum</i>: Using the descent property of the objective function:\n<br>   \\[\n   \\sum_{\\tau=0}^{k-1} \\eta_{\\tau} \\left\\|\\nabla \\Lc(\\Kb(\\tau), \\Qb(\\tau))\\right\\|^2_F \\le 2 \\Lc(\\Kb(0), \\Qb(0)) - 2 \\Lc(\\Kb(k), \\Qb(k)) \\le 2 \\Lc(\\Kb(0), \\Qb(0))\n   \\]\n<br>   This implies:\n<br>   \\[\n   \\sum_{t=0}^{\\infty} \\eta_k = \\infty\n   \\]\n<br>\n<br>5. <i>Gradient Descent and Risk</i>: Since gradient descent never increases the risk, for \\((\\Kb(k), \\Qb(k)) \\in \\mc{S}(R)\\), we have:\n<br>   \\[\n   \\|\\partial \\Lc / \\partial \\Qb(k)\\|_F \\ge \\epsilon(R)\n   \\]\n<br>   for some constant \\(\\epsilon(R) > 0\\). Following similar steps as the proof of \\eqref{eqn:grad:low:eps}, we get:\n<br>   \\[\n   \\sum_{k:(\\Kb(k), \\Qb(k)) \\in \\mc{S}(R)} \\eta_k < \\infty\n   \\]\n<br>\n<br>6. <i>Recurrent Relation for Gradient Descent Iterates</i>: Summing from \\(0\\) to \\(k-1\\) for gradient descent iterates, we get:\n<br>   \\[\n   \\Qb(k)^\\top \\Qb(k) - \\Qb(0)^\\top \\Qb(0) + \\sum_{\\tau=0}^{k-1} \\eta_{\\tau}^2 \\nabla_{\\Qb} \\Lc(\\Kb(\\tau), \\Qb(\\tau))^\\top \\nabla_{\\Qb} \\Lc(\\Kb(\\tau), \\Qb(\\tau))\n   \\]\n<br>   \\[\n   = \\Kb(k) \\Kb^{\\top}(k) - \\Kb(0) \\Kb^{\\top}(0) + \\sum_{\\tau=0}^{k-1} \\eta_{\\tau}^2 \\nabla_{\\Kb} \\Lc(\\Kb(\\tau), \\Qb(\\tau)) \\nabla_{\\Kb} \\Lc(\\Kb(\\tau), \\Qb(\\tau))^\\top\n   \\]\n<br>\n<br>7. <i>Trace and Frobenius Norms</i>: Defining \\(P_{\\Qb}\\) and \\(P_{\\Kb}\\) and their respective sums, we obtain:\n<br>   \\[\n   \\tr(P_{\\Kb}(k)) + \\tr(P_{\\Qb}(k)) = \\sum_{\\tau=0}^{k-1} \\eta_{\\tau}^2 \\|\\nabla \\Lc(\\Kb(\\tau), \\Qb(\\tau))\\|^2_F \\le 2 \\Lc(\\W(0)) - 2 \\Lc_\\star\n   \\]\n<br>   From the recurrent relation, we get:\n<br>   \\[\n   \\|\\Kb(k)\\|_F^2 = \\|\\Qb(k)\\|_F^2 + \\|\\Kb(0)\\|_F^2 - \\|\\Qb(0)\\|_F^2 - \\tr(P_{\\Kb}(k)) + \\tr(S_{\\Qb}(k))\n   \\]\n<br>   This shows that the difference between the squares of Frobenius norms of \\(\\Kb\\) and \\(\\Qb\\) is still bounded.\n<br>\n<br>8. <i>Contradiction and Conclusion</i>: Combining the above results, we get:\n<br>   \\[\n   2 \\Lc(\\W(0)) \\ge \\sum_{t=0}^{\\infty} \\eta_k \\left\\|\\nabla_{\\Qb} \\Lc(\\Kb(k), \\Qb(k))\\right\\|_F^2 \\ge \\infty\n   \\]\n<br>   This leads to a contradiction, implying that \\(\\|\\Kb(k)\\|_F \\to \\infty\\), since \\(\\Lc(\\Kb, \\Qb)\\) has no finite optimum."
                        }
                    }
                ],
                "corollaries": [
                    {
                        "statement_id": "dc3fce8f-00c5-4fe0-a3dc-df878f0858d7",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 2,
                        "library_name": "Corollary 2",
                        "title": "Global Convergence of Regularization Path",
                        "statement_original_tex": "\\begin{corollary}[Global Convergence of Regularization Path]\\label{cor gm} Suppose Assumptions \\ref{assum:loss:prope}\\&\\ref{ass cvx seq} hold and the optimal indices $\\op_\\ik=\\arg\\max_{t\\in[T]}\\bgam_\\ikt$ are unique. Consider the global regularization path $\\Wb_{\\dm,R}=\\min_{\\W\\in\\Rcm,\\td{\\W}\\leq R}\\Lc(\\W)$. Let $\\Wcb^\\svm_\\dm$ be the non-empty solution set of \\eqref{seqattnsvm} with $\\bal\\gets\\op$ normalized to have unit $\\dm$-norm. Then\n\\[\n\\lim_{R\\rightarrow\\infty}\\dist{\\frac{\\Wb_{\\dm,R}}{R},\\Wcb^\\svm_\\dm}\n\\]\n\\end{corollary}",
                        "statement_html": "Suppose Assumptions A [in <a href=\"https://arxiv.org/pdf/2308.16898#assumption.1\">original paper</a>] and $\\ref{ass cvx seq}$ hold and the optimal indices $\\op_\\ik=\\arg\\max_{t\\in[T]}\\bgam_\\ikt$ are unique. Consider the global regularization path $\\Wb_{\\dm,R}=\\min_{\\W\\in\\Rcm,\\td{\\W}\\leq R}\\Lc(\\W)$. Let $\\Wcb^\\svm_\\dm$ be the non-empty solution set of $\\eqref{seqattnsvm}$ with $\\bal\\gets\\op$ normalized to have unit $\\dm$-norm. Then\n\\[\n\\lim_{R\\rightarrow\\infty}\\dist{\\frac{\\Wb_{\\dm,R}}{R},\\Wcb^\\svm_\\dm}\n\\]",
                        "statement_type": "corollary",
                        "statement_motivation_html": "Understanding the behavior of the global regularization path $\\Wb_{\\dm,R}$ as $R$ approaches infinity is crucial in optimization and machine learning. This result shows that the normalized solution of the regularization path converges to the solution set of a specific support vector machine (SVM) problem. This is particularly useful when dealing with large-scale optimization problems, as it provides insights into the asymptotic properties of the solutions and helps in understanding the impact of regularization on the model.",
                        "html_url": "library/corollaries/corollary_2/index.html",
                        "proof": null,
                        "parent_id": "e2b288d5-6f0b-4ed4-a0f3-4fdb25730c9d"
                    },
                    {
                        "statement_id": "7edf41cb-7240-4f9c-9d92-c315b2fccacd",
                        "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                        "library_nr": 3,
                        "library_name": "Corollary 3",
                        "title": "Asymptotic Regularization Path Convergence",
                        "statement_original_tex": "\\begin{corollary}\\label{cor global reg path} Suppose Assumptions \\ref{assum:loss:prope}\\&\\ref{ass cvx seq} hold and the optimal indices $\\op_\\ik=\\arg\\max_{t\\in[T]}\\bgam_\\ikt$ are unique. Consider the regularization paths associated to \\eqref{serm-w} and \\eqref{serm-kq}:\n\\begin{align}\n&\\Wb_R=\\underset{\\W\\in\\Rcm,\\tf{\\W}\\leq R}{\\arg\\min}\\Lc(\\W)\\quad\\text{and}\\quad \\Kbb_R,\\Qbb_R=\\underset{\\tf{\\Kb}^2+\\tf{\\Qb}^2\\leq 2R}{\\arg\\min}\\Lc(\\Kb,\\Qb)\n\\end{align}\nSuppose \\eqref{seqattnsvm} is feasible for $\\bal\\gets\\op$. Let $\\Ws$ be the unique solution of \\eqref{seqattnsvm} with Frobenius norm and $\\Wc^{\\svm}_\\star$ be the solution set of \\eqref{seqattnsvm} with nuclear norm and cost function $\\tnuc{\\Wc^{\\svm}_\\star}$. We have that\n\n\\[\n\\lim_{R\\rightarrow\\infty} \\frac{\\Wb_R}{R}=\\frac{\\Ws}{\\tf{\\Ws}},\\quad\\lim_{R\\rightarrow\\infty} \\dist{\\frac{\\Qbb_R\\Kbb_R^\\top}{R},\\frac{\\Wcs_\\star}{\\tnuc{\\Wc^{\\svm}_\\star}}}=0.\n\\]\n\\end{corollary}",
                        "statement_html": "Suppose Assumptions A [in <a href=\"https://arxiv.org/pdf/2308.16898#assumption.1\">original paper</a>] and $\\ref{ass cvx seq}$ hold and the optimal indices $\\op_\\ik=\\arg\\max_{t\\in[T]}\\bgam_\\ikt$ are unique. Consider the regularization paths associated to (<a href=\"https://arxiv.org/pdf/2308.16898#appendix.E\">SERM-W</a>) and (<a href=\"https://arxiv.org/pdf/2308.16898#appendix.E\">SERM-KQ</a>):\n\\begin{align}\n&\\Wb_R=\\underset{\\W\\in\\Rcm,\\tf{\\W}\\leq R}{\\arg\\min}\\Lc(\\W)\\quad\\text{and}\\quad \\Kbb_R,\\Qbb_R=\\underset{\\tf{\\Kb}^2+\\tf{\\Qb}^2\\leq 2R}{\\arg\\min}\\Lc(\\Kb,\\Qb)\n\\end{align}\nSuppose $\\eqref{seqattnsvm}$ is feasible for $\\bal\\gets\\op$. Let $\\Ws$ be the unique solution of $\\eqref{seqattnsvm}$ with Frobenius norm and $\\Wc^{\\svm}_\\star$ be the solution set of $\\eqref{seqattnsvm}$ with nuclear norm and cost function $\\tnuc{\\Wc^{\\svm}_\\star}$. We have that\n\n\\[\n\\lim_{R\\rightarrow\\infty} \\frac{\\Wb_R}{R}=\\frac{\\Ws}{\\tf{\\Ws}},\\quad\\lim_{R\\rightarrow\\infty} \\dist{\\frac{\\Qbb_R\\Kbb_R^\\top}{R},\\frac{\\Wcs_\\star}{\\tnuc{\\Wc^{\\svm}_\\star}}}=0.\n\\]",
                        "statement_type": "corollary",
                        "statement_motivation_html": "Understanding the behavior of regularization paths is crucial in optimization and machine learning. This result shows that as the regularization parameter $R$ grows, the solutions $\\Wb_R$ and $\\Qbb_R\\Kbb_R^\\top$ become proportional to the solutions of the corresponding optimization problems with Frobenius and nuclear norms, respectively. This insight can be used to analyze the asymptotic properties of regularized models and to understand how different regularization techniques influence the solutions.",
                        "html_url": "library/corollaries/corollary_3/index.html",
                        "proof": {
                            "statement_id": "5c0765f5-8b01-4c32-9395-1dfc68137306",
                            "paper_id": "34602a94-5e5f-4f29-bf44-5ee44fc0d2e1",
                            "library_nr": null,
                            "library_name": null,
                            "title": null,
                            "statement_original_tex": "\\begin{proof} We directly apply Corollary \\ref{cor gm} with $\\dm=F$ and $\\dm=\\star$ respectively. To obtain the result on $\\Wb_R$, we note that $\\Ws$ is unique because Frobenius norm-squared is strongly convex. To obtain the result on $(\\Qbb_R,\\Kbb_R)$, we use Lemma \\ref{kqw mapping} and observe that\n\\[\n\\Wb_{\\st,R}:=\\Qbb_R\\Kbb_R^\\top\\in\\underset{\\W\\in\\Rcm,\\tnuc{\\W}\\leq R}{\\arg\\min}\\Lc(\\W).\n\\]\nWe then apply Corollary \\ref{cor gm} with $\\dm=\\star$ to conclude with the convergence of the path $ \\Wb_{\\st,R}$.\n\\end{proof}",
                            "statement_html": "We directly apply Corollary <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/corollaries/corollary_2/index.html#cor+gm\">Corollary 2</a> with $\\dm=F$ and $\\dm=\\star$ respectively. To obtain the result on $\\Wb_R$, we note that $\\Ws$ is unique because Frobenius norm-squared is strongly convex. To obtain the result on $(\\Qbb_R,\\Kbb_R)$, we use <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_28/index.html#kqw+mapping\">Lemma 28</a> and observe that\n\\[\n\\Wb_{\\st,R}:=\\Qbb_R\\Kbb_R^\\top\\in\\underset{\\W\\in\\Rcm,\\tnuc{\\W}\\leq R}{\\arg\\min}\\Lc(\\W).\n\\]\nWe then apply Corollary <a href=\"/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/corollaries/corollary_2/index.html#cor+gm\">Corollary 2</a> with $\\dm=\\star$ to conclude with the convergence of the path $ \\Wb_{\\st,R}$.",
                            "statement_type": "proof",
                            "statement_motivation_html": null,
                            "html_url": null,
                            "proof_explaination_html": "To understand the proof, let's break it down into several steps:\n<br>\n<br>1. <i>Application of Corollary 2</i>: The proof begins by directly applying Corollary 2 with \\(\\dm=F\\) and \\(\\dm=\\star\\) respectively.\n<br>\n<br>2. <i>Uniqueness of \\(\\Ws\\)</i>: To obtain the result on \\(\\Wb_R\\), it is noted that \\(\\Ws\\) is unique because the Frobenius norm-squared is strongly convex.\n<br>\n<br>3. <i>Result on \\((\\Qbb_R, \\Kbb_R)\\)</i>: To obtain the result on \\((\\Qbb_R, \\Kbb_R)\\), Lemma 28 is used. It is observed that:\n<br>   \\[\n   \\Wb_{\\st,R}:=\\Qbb_R\\Kbb_R^\\top\\in\\underset{\\W\\in\\Rcm,\\tnuc{\\W}\\leq R}{\\arg\\min}\\Lc(\\W).\n   \\]\n<br>\n<br>4. <i>Convergence of the Path</i>: Finally, Corollary 2 is applied again with \\(\\dm=\\star\\) to conclude with the convergence of the path \\(\\Wb_{\\st,R}\\)."
                        },
                        "parent_id": "e2b288d5-6f0b-4ed4-a0f3-4fdb25730c9d"
                    }
                ]
            },
            "mathjax_macros": [
                "emph: [\"\\\\textit{#1}\", 1]",
                "bm: [\"\\\\boldsymbol{\\\\mathbf{#1}}\", 1]",
                "mathds: [\"\\\\mathbf{#1}\", 1]",
                "textsl: [\"\\\\textit{#1}\", 1]",
                "vspace: [\"\", 1]",
                "xspace: \"\"",
                "RSIS: \"RSI$+$Smooth\"",
                "qed: \"{\\\\unskip\\\\nobreak\\\\hfil\\\\penalty50\\\\hskip2em\\\\vadjust{}           \\\\nobreak\\\\hfil$\\\\Box$\\\\parfillskip=0pt\\\\finalhyphendemerits=0\\\\par}\"",
                "LM: \"\\\\texttt{LMM}\\\\xspace\"",
                "GM: \"\\\\texttt{GMM}\\\\xspace\"",
                "oo: \"{11}\"",
                "red: \"\\\\textcolor{red}\"",
                "blue: \"\\\\textcolor{blue}\"",
                "grn: \"\\\\textcolor{darkgreen}\"",
                "xat: \"\\\\ob\"",
                "rfn: \"\\\\texttt{SVMeq}\"",
                "ont: \"\\\\texttt{1token}\"",
                "xast: \"\\\\xat^\\\\st\"",
                "Wf: \"{\\\\mtx{W}^\\\\tsc{fin}}\"",
                "noi: \"\\\\noindent\"",
                "new: \"\\\\text{new}\"",
                "prm: \"\\\\text{prompt}\"",
                "TF: \"{\\\\texttt{TF}}\"",
                "eps: \"\\\\epsilon\"",
                "epsd: \"\\\\varepsilon_\\\\dm\"",
                "dv: \"\\\\rho\"",
                "beps: \"\\\\boldsymbol{\\\\eps}\"",
                "bota: \"\\\\boldsymbol{\\\\iota}\"",
                "ept: \"\\\\eps_{\\\\TF}\"",
                "cz: \"c_0\"",
                "kz: \"\\\\nu\"",
                "bxi: \"\\\\boldsymbol{\\\\xi}\"",
                "vsp: \"\\\\vspace\"",
                "fnn: \"f_{\\\\text{nn}}\"",
                "ff: \"f_{\\\\text{nn}}\"",
                "wi: \"k_\\\\st\"",
                "hbm: \"\\\\vct{\\\\bar{h}}\"",
                "vc: \"\\\\text{vec}\"",
                "flin: \"f_{\\\\text{lin}}\"",
                "bhbg: \"{\\\\bar h}_{\\\\tsc{gap}}\"",
                "rest: \"\\\\text{rest}\"",
                "hp: \"\\\\tilde{\\\\mtx{h}}\"",
                "Mb: \"\\\\bar{\\\\M}\"",
                "Nb: \"\\\\bar{N}\"",
                "epsr: \"\\\\varepsilon_R\"",
                "mur: \"\\\\mu_R\"",
                "bp: \"\\\\bar{p}\"",
                "zig: \"\\\\nu\"",
                "bh: \"\\\\bar{h}\"",
                "hb: \"\\\\vct{h}\"",
                "al: \"\\\\alpha\"",
                "als: \"\\\\alpha\"",
                "fs: \"f^{\\\\Dc}\"",
                "Ncov: \"\\\\mathcal{N}_\\\\eps(\\\\Bal)\"",
                "ft: \"h\"",
                "ftv: \"f^{\\\\Tc\\\\cup\\\\Vc}\"",
                "deff: \"h_{\\\\text{eff}}\"",
                "defz: \"\\\\bar{h}_{\\\\text{eff}}\"",
                "deft: \"\\\\tilde{h}_{\\\\text{eff}}\"",
                "defg: \"\\\\bar{h}^{\\\\nabla}_{\\\\text{eff}}\"",
                "defF: \"\\\\bar{h}^{\\\\FB}_{\\\\text{eff}}\"",
                "bC: \"\\\\bar{C}\"",
                "st: \"\\\\star\"",
                "dm: \"{\\\\diamond}\"",
                "nt: \"n_\\\\mathcal{T}\"",
                "h: \"h\"",
                "nv: \"n_\\\\mathcal{V}\"",
                "distas: \"\\\\overset{\\\\text{i.i.d.}}{\\\\sim}\"",
                "pleq: \"\\\\overset{{P}}{\\\\leq}\"",
                "np: \"{n\\\\wedge p}\"",
                "KB: \"\\\\bar{K}\"",
                "bG: \"\\\\boldsymbol{\\\\Gamma}\"",
                "rP: \"\\\\stackrel{{P}}{\\\\longrightarrow}\"",
                "xv: \"\\\\x_{\\\\vb}\"",
                "xa: \"\\\\x^\\\\bal\"",
                "xw: \"\\\\x'_{\\\\vb}\"",
                "xu: \"\\\\x_{\\\\ub}\"",
                "xs: \"\\\\x_{\\\\ws}\"",
                "yst: \"\\\\hat{\\\\y}\"",
                "yb: \"\\\\bar{\\\\y}\"",
                "lnk: \"\\\\psi\"",
                "eig: \"\\\\text{eigvec}\"",
                "hu: \"h\"",
                "ru: \"r\"",
                "pbar: \"{{\\\\bar{p}}}\"",
                "pbd: \"p\"",
                "lan: \"\\\\vct{k}\"",
                "cmp: \"P_{\\\\text{nn}}\"",
                "dcp: \"\\\\vct{d}^{\\\\text{cnn}}\"",
                "fcp: \"\\\\vct{g}^{\\\\text{CNN}}\"",
                "beq: \"\\\\begin{equation}\"",
                "ba: \"\\\\begin{align}\"",
                "ea: \"\\\\end{align}\"",
                "nrest: \"\\\\bar{n}\"",
                "eeq: \"\\\\end{equation}\"",
                "prox: \"{{\\\\text{\\\\bf{prox}}}}\"",
                "cov: \"{{\\\\text{\\\\bf{cov}}}}\"",
                "ex: \"{{\\\\text{\\\\bf{ex}}}}\"",
                "modu: \"{{\\\\text{{mod}}}}\"",
                "map: \"{{\\\\text{\\\\bf{map}}}}\"",
                "vrn: \"\\\\sigma\"",
                "Rcm: \"\\\\Rcc_m\"",
                "lowo: \"\\\\texttt{low}_\\\\oo^{\\\\alpha}\"",
                "tsig: \"\\\\tilde{\\\\sigma}\"",
                "dev: \"{{\\\\text{\\\\bf{dev}}}}\"",
                "meas: \"{{\\\\text{\\\\bf{mean}}(\\\\sigma)}}\"",
                "nn: \"\\\\nonumber\"",
                "la: \"\\\\lambda\"",
                "laz: \"\\\\la_0\"",
                "blaz: \"\\\\bar{\\\\la}_0\"",
                "caz: \"\\\\la_C\"",
                "saz: \"L_\\\\sigma\"",
                "smax: \"\\\\bar{\\\\sigma}_{\\\\max}\"",
                "Lat: \"\\\\tilde{\\\\Lambda}\"",
                "K: \"\\\\mtx{K}\"",
                "A: \"{\\\\mtx{A}}\"",
                "Aa: \"{\\\\mtx{A}}\"",
                "Bb: \"{\\\\mtx{B}}\"",
                "N: \"{\\\\mtx{N}}\"",
                "amp: \"\\\\alpha_{\\\\text{cnn}}\"",
                "btm: \"\\\\bigotimes\"",
                "bd: \"\\\\bigodot\"",
                "kapp: \"s_{\\\\max}^{\\\\nu}\"",
                "kal: \"\\\\prod_{i=1}^{D-1}\\\\tn{\\\\lay{i}}\"",
                "Ub: \"{\\\\mtx{U}}\"",
                "Ubb: \"{\\\\mtx{U}}\"",
                "kron: \"\\\\otimes\"",
                "M: \"{\\\\mtx{M}}\"",
                "dimondf: \"\\\\W^\\\\svm\"",
                "name: \"Att-SVM\"",
                "ten: \"(\\\\{\\\\vb_\\\\ell\\\\}_{\\\\ell=1}^D)\"",
                "uten: \"(\\\\{\\\\ub_\\\\ell\\\\}_{\\\\ell=1}^D)\"",
                "ken: \"(\\\\{\\\\lay{i}\\\\}_{i=1}^D)\"",
                "B: \"{{\\\\mtx{B}}}\"",
                "alx: \"{{\\\\alpha_\\\\X}}\"",
                "Ib: \"{{\\\\mtx{I}}}\"",
                "fp: \"F\"",
                "Xbin: \"{{\\\\mtx{B}}}\"",
                "Gbin: \"{{\\\\mtx{S}}}\"",
                "xbin: \"{{\\\\mtx{b}}}\"",
                "Sb: \"{{\\\\mtx{S}}}\"",
                "Sbt: \"{\\\\mathbb{S}}^t\"",
                "Sbtt: \"{\\\\mathbb{S}}'^{t}\"",
                "Gb: \"{\\\\mtx{G}}\"",
                "ymean: \"{\\\\bar{\\\\y}(\\\\w;\\\\text{avg})}\"",
                "yavg: \"{{y}_{\\\\text{avg}}}\"",
                "fmean: \"{\\\\funcw{\\\\w;\\\\text{avg}}}\"",
                "out: \"\\\\text{out}\"",
                "linest: \"\\\\text{lin}\"",
                "inp: \"\\\\text{in}\"",
                "inh: \"\\\\hat{\\\\text{in}}\"",
                "sexp: \"subexponential \"",
                "Lc: \"{\\\\cal{L}}\"",
                "Hc: \"{\\\\cal{H}}\"",
                "Lcz: \"{\\\\cal{L}}^{0-1}\"",
                "Lczh: \"{\\\\hat{\\\\cal{L}}}^{0-1}\"",
                "Lch: \"{\\\\widehat{\\\\cal{L}}}\"",
                "Lct: \"{\\\\tilde{\\\\cal{L}}}\"",
                "Lcb: \"{\\\\bar{\\\\cal{L}}}\"",
                "Nc: \"{\\\\cal{N}}\"",
                "xm: \"\\\\x^{\\\\tsc{mix}}\"",
                "Jc: \"{\\\\cal{J}}\"",
                "Dc: \"{\\\\cal{D}}\"",
                "Dci: \"{\\\\cal{D}}_{\\\\text{init}}\"",
                "Ro: \"{\\\\cal{RO}}\"",
                "PC: \"{\\\\cal{PC}}\"",
                "Pb: \"{\\\\mtx{P}}\"",
                "Tb: \"{\\\\mtx{T}}\"",
                "Tn: \"{T_0}\"",
                "Kba: \"{\\\\vct{k}}_{\\\\text{all}}\"",
                "Qb: \"{\\\\mtx{Q}}\"",
                "QK: \"{\\\\mtx{Q} \\\\mtx{K}^\\\\top}\"",
                "rng: \"\\\\gamma\"",
                "ta: \"\\\\tau\"",
                "gmax: \"\\\\gamma^\\\\star\"",
                "ggap: \"\\\\bar{\\\\gamma}_{\\\\tsc{gap}}\"",
                "Cb: \"{\\\\mtx{C}}\"",
                "BC: \"{\\\\bar{C}}\"",
                "Eb: \"{\\\\mtx{E}}\"",
                "Hb: \"{\\\\mtx{H}}\"",
                "Gc: \"{\\\\cal{G}}\"",
                "Zc: \"{\\\\cal{Z}}\"",
                "F: \"{\\\\mtx{F}}\"",
                "Fa: \"{\\\\mtx{F}}^\\\\bal\"",
                "diff: \"{\\\\text{diff}}\"",
                "La: \"{\\\\boldsymbol{{\\\\Lambda}}}\"",
                "sigmap: \"\\\\phi'\"",
                "sigmaa: \"\\\\sigma'_{\\\\text{all}}\"",
                "bSi: \"{\\\\boldsymbol{{\\\\Sigma}}}\"",
                "bSii: \"{\\\\boldsymbol{{\\\\Sigma}}}_{i,i}\"",
                "bSib: \"\\\\bar{{\\\\boldsymbol{{\\\\Sigma}}}}\"",
                "bSit: \"{\\\\boldsymbol{{\\\\tilde{\\\\Sigma}}}}\"",
                "bSih: \"{\\\\boldsymbol{{\\\\hat{\\\\Sigma}}}}\"",
                "bmu: \"{\\\\boldsymbol{{\\\\mu}}}\"",
                "Db: \"{\\\\mtx{D}}\"",
                "bB: \"{\\\\bar{B}}\"",
                "tB: \"{\\\\tilde{B}}\"",
                "db: \"{\\\\vct{d}}\"",
                "oneb: \"{\\\\mathbb{1}}\"",
                "onebb: \"{\\\\mathbf{1}}\"",
                "Iden: \"{\\\\mtx{I}}\"",
                "gm: \"\\\\gamma_m\"",
                "OR: \"\\\\text{OR}\"",
                "z: \"{\\\\vct{z}}\"",
                "zt: \"{\\\\tilde{\\\\vct{z}}}\"",
                "fab: \"f^\\\\bal_\\\\bt\"",
                "zb: \"{\\\\bar{\\\\z}}\"",
                "el: \"{\\\\ell}\"",
                "sftx: \"\\\\mathbb{S}\"",
                "bad: \"{\\\\bar{d}}\"",
                "Lcg: \"\\\\tilde{\\\\Lc}\"",
                "Dp: \"{D^+}\"",
                "upp: \"{\\\\cal{B}}_{\\\\alpha,\\\\Gamma}\"",
                "upz: \"{\\\\cal{B}}_{\\\\alpha_0,\\\\Gamma}\"",
                "dpz: \"{\\\\cal{D}}_{\\\\alpha_0,\\\\Gamma}\"",
                "dpp: \"{\\\\cal{D}}_{\\\\alpha,\\\\Gamma}\"",
                "mpp: \"M_{\\\\alpha,\\\\Gamma}\"",
                "paf: \"\\\\partial f(\\\\x)\"",
                "Cc: \"\\\\mathcal{C}\"",
                "Rcc: \"\\\\mathcal{R}\"",
                "Qcc: \"\\\\mathcal{Q}\"",
                "Kcc: \"\\\\mathcal{K}\"",
                "Ac: \"\\\\mathcal{A}\"",
                "Acg: \"\\\\Ac_g\"",
                "At: \"\\\\mathcal{A}\"",
                "Acr: \"\\\\mathcal{A_\\\\text{ridge}}\"",
                "Bal: \"{\\\\boldsymbol{\\\\Delta}}\"",
                "pbb: \"\\\\vct{\\\\bar{p}}\"",
                "pbs: \"\\\\tilde{\\\\pb}^\\\\svm\"",
                "del: \"\\\\delta\"",
                "GG: \"\\\\texttt{GG}\"",
                "BB: \"\\\\texttt{BB}\"",
                "BG: \"\\\\texttt{BG}\"",
                "bGam: \"\\\\bar{\\\\Gamma}_y\"",
                "GB: \"\\\\texttt{GB}\"",
                "delw: \"\\\\delta_w\"",
                "delq: \"\\\\delta_q\"",
                "bdel: \"\\\\boldsymbol{\\\\delta}\"",
                "Ccb: \"\\\\bar{\\\\mathcal{C}}\"",
                "Rc: \"\\\\mathcal{O}\"",
                "Rcp: \"\\\\mathcal{\\\\bar{O}}'\"",
                "btrue: \"\\\\bbeta_{true}\"",
                "bt: \"{\\\\boldsymbol{\\\\theta}}\"",
                "bet: \"{\\\\boldsymbol{\\\\beta}}\"",
                "bT: \"\\\\Theta\"",
                "bts: \"{\\\\boldsymbol{\\\\beta}_\\\\st}\"",
                "btb: \"\\\\bar{\\\\boldsymbol{\\\\theta}}\"",
                "btid: \"\\\\boldsymbol{\\\\theta}^\\\\text{ideal}\"",
                "btt: \"\\\\tilde{\\\\boldsymbol{\\\\theta}}\"",
                "bal: \"{\\\\boldsymbol{\\\\alpha}}\"",
                "bgam: \"\\\\boldsymbol{\\\\gamma}\"",
                "bga: \"\\\\boldsymbol{\\\\gamma}^\\\\bal\"",
                "gamb: \"\\\\bar{\\\\gamma}\"",
                "bab: \"{\\\\boldsymbol{\\\\bar{\\\\alpha}}}\"",
                "bah: \"{\\\\widehat{\\\\bal}}\"",
                "berm: \"\\\\bal^{\\\\texttt{ERM}}\"",
                "bas: \"{\\\\boldsymbol{\\\\alpha}_\\\\st}\"",
                "bth: \"{\\\\boldsymbol{\\\\hat{\\\\beta}}}\"",
                "bPhi: \"{\\\\boldsymbol{\\\\Phi}}\"",
                "bbteta: \"\\\\widetilde{\\\\boldsymbol{\\\\theta}}\"",
                "bbeta: \"{\\\\boldsymbol{\\\\beta}}\"",
                "ddelta: \"{\\\\boldsymbol{\\\\delta}}\"",
                "DD: \"{D}\"",
                "babeta: \"{\\\\bar{\\\\beta}}\"",
                "balpha: \"{\\\\bar{\\\\alpha}}\"",
                "bgamma: \"\\\\gamma^\\\\star\"",
                "agam: \"{\\\\bar{\\\\gamma}}_t\"",
                "No: \"N\"",
                "Bc: \"\\\\mathcal{B}\"",
                "Sc: \"\\\\mathcal{S}\"",
                "Scc: \"\\\\bar{\\\\mathcal{S}}\"",
                "Sca: \"\\\\mathcal{S}_{\\\\text{all}}\"",
                "Dca: \"\\\\mathcal{D}_{\\\\text{all}}\"",
                "Scn: \"\\\\mathcal{S}_{\\\\new}\"",
                "Dcn: \"\\\\mathcal{D}_{new}\"",
                "Scb: \"\\\\bar{\\\\mathcal{S}}\"",
                "Sci: \"\\\\mathcal{S}_{\\\\text{in}}\"",
                "Sco: \"\\\\mathcal{S}_{\\\\text{out}}\"",
                "Ect: \"\\\\mathcal{S}_{\\\\text{top}}\"",
                "Mc: \"\\\\mathcal{M}\"",
                "pa: \"{\\\\partial}\"",
                "Nn: \"\\\\mathcal{N}\"",
                "pol: \"^\\\\circ\"",
                "vb: \"\\\\vct{v}\"",
                "Jb: \"\\\\mtx{J}\"",
                "Jt: \"\\\\mtx{\\\\tilde{J}}\"",
                "vbl: \"\\\\vct{v}^{\\\\text{lin}}\"",
                "fb: \"\\\\vct{f}\"",
                "Fb: \"\\\\vct{F}\"",
                "FB: \"\\\\mathbb{F}\"",
                "Ft: \"\\\\tilde{\\\\vct{F}}\"",
                "fa: \"\\\\tilde{\\\\vct{f}}\"",
                "ib: \"{\\\\bf{i}}\"",
                "hib: \"{\\\\bf{\\\\hat{i}}}\"",
                "Ic: \"{\\\\mathcal{I}}\"",
                "all: \"{\\\\text{all}}\"",
                "vh: \"\\\\vct{\\\\hat{v}}\"",
                "vbb: \"\\\\vct{\\\\bar{v}}\"",
                "Xb: \"\\\\mtx{\\\\bar{X}}\"",
                "xb: \"\\\\vct{\\\\bar{x}}\"",
                "abb: \"\\\\mtx{\\\\bar{a}}\"",
                "ap: \"\\\\mtx{a}'\"",
                "cb: \"\\\\mtx{c}\"",
                "cbb: \"\\\\mtx{\\\\bar{c}}\"",
                "kbb: \"\\\\mtx{\\\\bar{k}}\"",
                "bbb: \"\\\\mtx{\\\\bar{b}}\"",
                "nei: \"\\\\text{support index}\\\\xspace\"",
                "neis: \"\\\\text{support indices}\\\\xspace\"",
                "Nei: \"\\\\text{Support index}\\\\xspace\"",
                "Neis: \"\\\\text{Support indices}\\\\xspace\"",
                "NEIS: \"\\\\text{Support Indices}\\\\xspace\"",
                "w: \"\\\\vct{w}\"",
                "ww: \"\\\\vct{V}\"",
                "lgt: \"\\\\texttt{lgt}'\"",
                "ist: \"i_\\\\st\"",
                "cdm: \"c_\\\\dm\"",
                "cop: \"c_\\\\texttt{up}\"",
                "cdn: \"c_\\\\texttt{dn}\"",
                "Wp: \"\\\\mtx{W}^\\\\dagger\"",
                "tilW: \"\\\\widetilde{\\\\mtx{W}}\"",
                "tilw: \"\\\\widetilde{\\\\vct{w}}\"",
                "ob: \"\\\\mtx{o}\"",
                "obo: \"\\\\mtx{o}_1(t)\"",
                "obt: \"\\\\mtx{o}_2(t)\"",
                "obh: \"\\\\mtx{\\\\hat{o}}\"",
                "wh: \"{\\\\hat{\\\\mtx{w}}}\"",
                "li: \"\\\\left<\"",
                "xdm: \"\\\\Xi_\\\\dm\"",
                "ri: \"\\\\right>\"",
                "s: \"\\\\vct{s}\"",
                "sik: \"\\\\s^{(\\\\ik)}\"",
                "sir: \"\\\\s^R\"",
                "abik: \"\\\\ab^{(\\\\ik)}\"",
                "abr: \"\\\\ab^R\"",
                "ab: \"{\\\\vct{a}}\"",
                "abm: \"\\\\vct{\\\\bar{a}}\"",
                "abg: \"\\\\vct{a}_{\\\\tsc{gap}}\"",
                "bgag: \"{\\\\gamma}_{\\\\tsc{gap}}\"",
                "bgg: \"\\\\gamma^{\\\\tsc{gap}}\"",
                "bggm: \"\\\\gamma^{\\\\tsc{gap}}_{\\\\min}\"",
                "bgm: \"\\\\bar{\\\\gamma}^{\\\\tsc{gap}}\"",
                "abp: \"\\\\vct{a}^\\\\pb\"",
                "bb: \"\\\\vct{b}\"",
                "ub: \"{\\\\vct{u}}\"",
                "ubb: \"\\\\bar{\\\\vct{u}}\"",
                "hh: \"{\\\\vct{h}}\"",
                "ii: \"{\\\\vct{i}}\"",
                "dd: \"{\\\\vct{d}}\"",
                "ddt: \"{\\\\vct{d}}_\\\\tau\"",
                "Zb: \"\\\\mathbb{Z}\"",
                "hf: \"\\\\hat{f}\"",
                "corrCA: \"\\\\rho_\\\\Cc(\\\\M)\"",
                "Tc: \"\\\\mathcal{T}\"",
                "Tcb: \"\\\\bar{\\\\mathcal{T}}\"",
                "TVc: \"\\\\Tc\\\\cup\\\\Vc\"",
                "Ttc: \"\\\\mathcal{T}_{\\\\text{test}}\"",
                "Fc: \"\\\\mathcal{F}\"",
                "Fcl: \"\\\\mathcal{F}^{\\\\text{lin}}\"",
                "Xc: \"\\\\mathcal{X}\"",
                "Yc: \"\\\\mathcal{Y}\"",
                "rel: \"optimal\\\\xspace\"",
                "iopt: \"non-optimal\\\\xspace\"",
                "irel: \"non-optimal\\\\xspace\"",
                "ihere: \"{\\\\textcolor{red}{I AM HERE! }}\"",
                "gb: \"\\\\bar{\\\\g}\"",
                "bs: \"\\\\bar{s}\"",
                "cgain: \"\\\\alpha_{\\\\text{CNN}}\"",
                "cgainp: \"\\\\tn{\\\\E_{\\\\x\\\\sim\\\\Nn(0,\\\\Iden)}[\\\\gcnn{}]}\"",
                "ccorr: \"\\\\rho_{\\\\text{cnn}}\"",
                "kb: \"\\\\vct{k}\"",
                "kbo: \"\\\\vct{k}^\\\\op\"",
                "xbo: \"\\\\vct{x}^\\\\op\"",
                "xh: \"\\\\hat{\\\\x}\"",
                "cone: \"\\\\Sc\"",
                "conb: \"\\\\bar{\\\\Cc}\"",
                "xbr: \"\\\\bar{\\\\h}\"",
                "low: \"\\\\texttt{low}^{\\\\alpha}(\\\\X)\"",
                "high: \"\\\\texttt{high}^{\\\\alpha}(\\\\X)\"",
                "lowi: \"\\\\texttt{low}_\\\\ik^{\\\\alpha}\"",
                "higi: \"\\\\texttt{high}_\\\\ik^{\\\\alpha}\"",
                "xdr: \"\\\\tilde{\\\\x}\"",
                "ybt: \"\\\\tilde{\\\\y}\"",
                "Xt: \"\\\\tilde{\\\\X}\"",
                "gh: \"\\\\hat{\\\\g}\"",
                "gt: \"\\\\tilde{\\\\g}\"",
                "ir: \"q\"",
                "sbs: \"\\\\vct{s}^{\\\\texttt{ref}}\"",
                "bbg: \"\\\\bgam^{\\\\tsc{gap}}\"",
                "bbs: \"\\\\bar{\\\\s}\"",
                "gmb: \"\\\\bar{\\\\gamma}\"",
                "irm: \"q^\\\\pb_{\\\\max}\"",
                "ira: \"q^{\\\\pb'}_{\\\\max}\"",
                "vbs: \"\\\\tilde{\\\\vb}^\\\\svm\"",
                "vs: \"\\\\vb^\\\\svm\"",
                "ps: \"\\\\W^\\\\svm\"",
                "Ws: \"\\\\W^\\\\svm\"",
                "Wma: \"\\\\W^\\\\svm_\\\\bal\"",
                "Wsf: \"\\\\W^\\\\svm\"",
                "Wsb: \"\\\\bar{\\\\W}^\\\\svm\"",
                "Wcs: \"\\\\Wc^\\\\svm\"",
                "ik: \"{ik}\"",
                "itt: \"{it}\"",
                "ittt: \"{i\\\\tau}\"",
                "ikt: \"{ikt}\"",
                "iktt: \"{ik\\\\tau}\"",
                "ikix: \"_{ik=(1,1)}^{(n,k)}\"",
                "Ccd: \"\\\\Cc_{\\\\eps,R_0}^\\\\dm\"",
                "aik: \"\\\\alpha_{ik}\"",
                "prr: \"\\\\pb^\\\\tsc{relax}\"",
                "pre: \"\\\\pb^\\\\tsc{$\\\\eps$-rlx}\"",
                "pbr: \"\\\\tilde{\\\\pb}^\\\\tsc{$\\\\eps$-rlx}\"",
                "qbr: \"\\\\tilde{\\\\qb}^\\\\tsc{relax}\"",
                "pseb: \"\\\\pbb^\\\\svm_\\\\eps\"",
                "pset: \"\\\\tilde{\\\\pb}^\\\\svm_\\\\eps\"",
                "psdb: \"\\\\pbb^\\\\svm_\\\\delta\"",
                "pse: \"\\\\pb^\\\\svm_\\\\eps\"",
                "psd: \"\\\\pb^\\\\svm_\\\\delta\"",
                "pst: \"\\\\pb^\\\\star\"",
                "wst: \"\\\\W^\\\\star\"",
                "gamp: \"\\\\Gamma_\\\\eps\"",
                "gamt: \"\\\\Gamma^{\\\\geq 2}_\\\\eps\"",
                "damp: \"\\\\Gamma_\\\\delta\"",
                "pt: \"\\\\tilde{\\\\pb}^\\\\svm\"",
                "RR: \"\\\\bar{R}\"",
                "MM: \"\\\\bar{M}\"",
                "psb: \"\\\\bar{\\\\pb}^\\\\svm\"",
                "psp: \"\\\\pb^\\\\beta\"",
                "pso: \"\\\\pb^{\\\\svm\\\\star}\"",
                "Wso: \"\\\\W^{\\\\svm\\\\star}\"",
                "mipp: \"L_{\\\\max}\"",
                "mupp: \"\\\\bar{\\\\mu}_{\\\\max}\"",
                "mapp: \"\\\\mu\"",
                "tcnn: \"{\\\\mtx{T}}_{\\\\text{cnn}}\"",
                "lcnn: \"{\\\\mtx{L}}_{\\\\text{CNN}}\"",
                "smo: \"S\"",
                "SM: \"{{\\\\bf{S}}}_{L,\\\\kb}\"",
                "SMB: \"{{\\\\bf{\\\\bar{S}}}}_{L,\\\\kb}\"",
                "ws: \"{\\\\W^\\\\star}\"",
                "wss: \"{\\\\w^\\\\star}\"",
                "lab: \"\\\\bar{\\\\la}\"",
                "mult: \"B^D\\\\bar{M}N\"",
                "liptwo: \"20R^3B\\\\nt^2\\\\laz^{-2}(B\\\\nt+1)\"",
                "lipp: \"5R^2\\\\sqrt{B^3\\\\nt^3\\\\h}\\\\laz^{-2}\\\\tn{\\\\yT}\"",
                "lips: \"6R^3B^2\\\\sqrt{\\\\nt^3\\\\h}\\\\laz^{-2}\\\\tn{\\\\yT}\"",
                "lipl: \"5(RB\\\\nt\\\\sqrt{\\\\h}+1)RB\\\\nt\\\\laz^{-2}\"",
                "lip: \"\\\\frac{5B^2\\\\tn{\\\\yT}}{\\\\laz^2}\"",
                "lipt: \"6R^3B^2\\\\Gamma\\\\sqrt{\\\\nt^3\\\\h}\\\\laz^{-2} \\\\tn{\\\\yT}\"",
                "lipf: \"{20R^4B^2\\\\laz^{-2}\\\\Gamma\\\\nt^2\\\\tn{\\\\yT}}\"",
                "lipsum: \"30R^4B^2\\\\laz^{-2}\\\\Gamma(\\\\nt^2+\\\\nv^2) \\\\tn{\\\\yT}\"",
                "lipnn: \"120B^4\\\\blaz^{-2}\\\\Gamma(\\\\nt^2+\\\\nv^2) \\\\tn{\\\\yT}\"",
                "bL: \"\\\\bar{L}\"",
                "scl: \"M\"",
                "cA: \"\\\\mathcal{A}\"",
                "x: \"\\\\vct{x}\"",
                "rb: \"\\\\vct{r}\"",
                "rbb: \"\\\\vct{\\\\widetilde{r}}\"",
                "y: \"\\\\vct{y}\"",
                "yT: \"\\\\vct{y}\"",
                "yh: \"\\\\hat{y}\"",
                "ybh: \"\\\\vct{\\\\hat{y}}\"",
                "W: \"\\\\mtx{W}\"",
                "Wt: \"\\\\tilde{\\\\mtx{W}}\"",
                "Wc: \"{\\\\cal{W}}\"",
                "Wcb: \"{\\\\cal{W}}\"",
                "Vc: \"{\\\\cal{V}}\"",
                "bgl: \"{~\\\\big |~}\"",
                "p: \"{\\\\vct{p}}\"",
                "Kb: \"{\\\\mtx{K}}\"",
                "Kbb: \"{\\\\mtx{\\\\bar{K}}}\"",
                "Qbb: \"{\\\\mtx{\\\\bar{Q}}}\"",
                "Wb: \"{\\\\mtx{\\\\bar{W}}}\"",
                "Wpro: \"\\\\mtx{\\\\W}_{\\\\textnormal{prod}}\"",
                "Kbh: \"{\\\\widehat{\\\\mtx{K}}}\"",
                "somelog: \"16\\\\Kb\\\\log (\\\\Kb)\"",
                "somelg: \"\\\\Kb\\\\log (\\\\Kb)\"",
                "pb: \"{\\\\vct{p}}\"",
                "dpb: \"{\\\\vct{\\\\dot p}}\"",
                "drb: \"{\\\\vct{\\\\dot r}}\"",
                "qb: \"{\\\\vct{q}}\"",
                "qstar: \"\\\\vct{q}_\\\\star\"",
                "qtt: \"{\\\\vct{\\\\tilde{q}}}\"",
                "wstar: \"\\\\vct{v}_\\\\star\"",
                "vstar: \"\\\\vct{v}_\\\\star\"",
                "wstab: \"\\\\bar{\\\\vb}_\\\\star\"",
                "vstab: \"\\\\bar{\\\\vb}_\\\\star\"",
                "qstab: \"\\\\bar{\\\\qb}_\\\\star\"",
                "qbb: \"\\\\bar{\\\\qb}\"",
                "wbb: \"\\\\bar{\\\\w}\"",
                "wtt: \"{\\\\vct{\\\\tilde{w}}}\"",
                "R: \"\\\\mathbb{R}\"",
                "Pro: \"\\\\mathbb{P}\"",
                "C: \"\\\\mathbb{C}\"",
                "Z: \"\\\\mtx{Z}\"",
                "V: \"\\\\mtx{V}\"",
                "Za: \"\\\\mtx{Z}^\\\\bal\"",
                "Var: \"\\\\textrm{Var}\"",
                "E: \"\\\\operatorname{\\\\mathbb{E}}\"",
                "Eh: \"\\\\operatorname{\\\\mathbb{\\\\hat{E}}}\"",
                "e: \"\\\\mathrm{e}\"",
                "eb: \"\\\\vct{e}\"",
                "vba: \"{\\\\bf{\\\\emph{v}}}\"",
                "pba: \"{\\\\bf{\\\\emph{p}}}\"",
                "supp: \"\\\\Sc\"",
                "Id: \"\\\\text{\\\\em I}\"",
                "OpId: \"\\\\mathcal{I}\"",
                "Real: \"\\\\operatorname{Re}\"",
                "Imag: \"\\\\operatorname{Im}\"",
                "piyp: \"\\\\pi'_1(\\\\vct{y})\"",
                "piy: \"\\\\pi_1(\\\\vct{y})\"",
                "piar: \"\\\\pi_1(\\\\vct{a}_r)\"",
                "piarp: \"\\\\pi'_1(\\\\vct{a}_r)\"",
                "set: \"{\\\\cal{F}}\"",
                "des: \"{\\\\x_0}\"",
                "Pc: \"{\\\\cal{P}}\"",
                "X: \"{\\\\mtx{X}}\"",
                "Y: \"{\\\\mtx{Y}}\"",
                "Vb: \"{\\\\mtx{V}}\"",
                "Vh: \"\\\\hat{{\\\\mtx{V}}}\"",
                "Vbd: \"{\\\\mtx{V}^\\\\dagger}\"",
                "Rb: \"{\\\\mtx{R}}\"",
                "bR: \"{\\\\bar{R}}\"",
                "calF: \"\\\\mathcal{I}\"",
                "calS: \"\\\\mathcal{N}\"",
                "gi: \"\\\\ab_{\\\\gamma_i}\"",
                "ang: \"\\\\text{ang}\"",
                "mc: \"\\\\mathcal\"",
                "g: \"\\\\vct{g}\"",
                "tsc: \"\\\\textsl\"",
                "Prml: \"\\\\text{Primal}\\\\xspace\"",
                "cls: \"\\\\texttt{[CLS]}\\\\xspace\"",
                "prml: \"\\\\text{primal}\\\\xspace\"",
                "fat: \"f_{\\\\tsc{sa}}\"",
                "fatt: \"f_{\\\\tsc{cls}}\"",
                "faptt: \"f^\\\\top_{\\\\tsc{cls}}\"",
                "thetab: \"\\\\boldsymbol{\\\\theta}\"",
                "acc: \"\\\\text{acc}\"",
                "svm: \"\\\\tsc{mm}\"",
                "Wm: \"\\\\W^\\\\svm\"",
                "Wmt: \"\\\\tilde\\\\W^\\\\svm\"",
                "Wu: \"\\\\W^{\\\\texttt{uni}}\"",
                "Wbi: \"\\\\W^{\\\\texttt{bi}}\"",
                "Wub: \"\\\\bar{\\\\W}^{\\\\texttt{uni}}\"",
                "Wbb: \"\\\\bar{\\\\W}^{\\\\texttt{bi}}\"",
                "op: \"\\\\texttt{opt}\"",
                "opt: \"\\\\texttt{opt}\"",
                "xop: \"\\\\x^\\\\texttt{opt}\"",
                "reg: \"\\\\tsc{reg}\"",
                "vstap: \"\\\\vstar'\"",
                "Cbp: \"\\\\Cb'\"",
                "Rcb: \"\\\\bar{\\\\Rc}\"",
                "err: \"\\\\texttt{err}\"",
                "ones: \"\\\\onebb\"",
                "onet: \"\\\\bar{\\\\onebb}\"",
                "taub: \"{\\\\bar{\\\\tau}}\"",
                "taut: \"\\\\tau\"",
                "Qc: \"{\\\\cal{Q}}\"",
                "tr: \"{{\\\\operatorname{trace}}}\"",
                "goodbox: [\"\\\\begin{center} \\\\begin{tcolorbox}[boxsep=1pt,left=7pt,right=7pt,top=0pt,bottom=2pt,enhanced, width=14cm,colframe=white!3!black,colback=black!2!white,colbacktitle=orange!5!yellow!10!white,fonttitle=\\\\bfseries,coltitle=black,attach boxed title to top center={yshift=-0.25mm-\\\\tcboxedtitleheight/2,yshifttext=2mm-\\\\tcboxedtitleheight/2},boxed title style={boxrule=0.2mm,frame code={ \\\\path[tcb fill frame] ([xshift=-4mm]frame.west)-- (frame.north west) -- (frame.north east) -- ([xshift=4mm]frame.east)-- (frame.south east) -- (frame.south west) -- cycle; },interior code={ \\\\path[tcb fill interior] ([xshift=-2mm]interior.west)-- (interior.north west) -- (interior.north east)-- ([xshift=2mm]interior.east) -- (interior.south east) -- (interior.south west)-- cycle;} }] #1 \\\\end{tcolorbox}\\\\end{center}\", 1]",
                "nicebox: [\"\\\\begin{center} \\\\begin{tcolorbox}[boxsep=1pt,left=7pt,right=7pt,top=0pt,bottom=2pt,enhanced, width=14cm,colframe=green!3!black,colback=green!3!white,colbacktitle=orange!5!yellow!10!white,fonttitle=\\\\bfseries,coltitle=black,attach boxed title to top center={yshift=-0.25mm-\\\\tcboxedtitleheight/2,yshifttext=2mm-\\\\tcboxedtitleheight/2},boxed title style={boxrule=0.2mm,frame code={ \\\\path[tcb fill frame] ([xshift=-4mm]frame.west)-- (frame.north west) -- (frame.north east) -- ([xshift=4mm]frame.east)-- (frame.south east) -- (frame.south west) -- cycle; },interior code={ \\\\path[tcb fill interior] ([xshift=-2mm]interior.west)-- (interior.north west) -- (interior.north east)-- ([xshift=2mm]interior.east) -- (interior.south east) -- (interior.south west)-- cycle;} }] #1 \\\\end{tcolorbox}\\\\end{center} \", 1]",
                "redp: [\"\\\\textcolor{red}{[#1]}\", 1]",
                "so: [\"\\\\textcolor{darkblue}{#1}\", 1]",
                "SO: [\"\\\\textcolor{red}{[SO: #1]\\\\\\\\}\", 1]",
                "dat: [\"\\\\textcolor{darkgreen}{#1}\", 1]",
                "DAT: [\"\\\\textcolor{green}{[DAT: #1]}\", 1]",
                "yl: [\"\\\\textcolor{orange}{#1}\", 1]",
                "YL: [\"\\\\textcolor{orange}{[YL: #1]}\", 1]",
                "ct: [\"\\\\textcolor{magenta}{[CT: #1]}\", 1]",
                "todo: [\"\\\\textcolor{darkred}{TODO: #1}\", 1]",
                "clr: [\"\\\\red{#1}\", 1]",
                "cln: [\"\\\\red{}\", 1]",
                "som: [\"\\\\marginpar{\\\\color{darkblue}\\\\tiny\\\\ttfamily SO: #1}\", 1]",
                "outr: [\"\\\\text{outer\\\\_update}(#1)\", 1]",
                "cc: [\"\\\\Cc(#1)\", 1]",
                "fF: [\"f_{\\\\text{nn},#1}\", 1]",
                "fln: [\"f_{\\\\text{lin},#1}\", 1]",
                "exc: [\"\\\\mathcal{R}_{\\\\tsc{gap}}(#1)\", 1]",
                "xvt: [\"\\\\x_{\\\\vb_{#1}}\", 1]",
                "xvtt: [\"\\\\x_{\\\\vb,#1}\", 1]",
                "xwt: [\"\\\\x_{#1,\\\\w}\", 1]",
                "xws: [\"\\\\x_{#1,\\\\ws}\", 1]",
                "link: [\"\\\\lnk(#1)\", 1]",
                "lay: [\"{\\\\vct{k}}^{(#1)}\", 1]",
                "lah: [\"{\\\\vct{\\\\hat{k}}}^{(#1)}\", 1]",
                "bea: [\"\\\\begin{align}#1\\\\end{align}\", 1]",
                "var: [\"{{\\\\text{\\\\bf{var}}}}[#1]\", 1]",
                "sgt: [\"\\\\tilde{\\\\sigma}^{(#1)}\", 1]",
                "func: [\"{f_{\\\\text{CNN}}}(#1)\", 1]",
                "funh: [\"{{\\\\hat{f}}_{\\\\text{CNN}}}(#1)\", 1]",
                "funcp: [\"{f'_{\\\\text{cnn}}}(#1)\", 1]",
                "funcw: [\"{{{f}}'_{\\\\text{cnn}}}(#1)\", 1]",
                "robt: [\"\\\\bigotimes_{\\\\ell=1}^D{#1}_{\\\\ell}\", 1]",
                "robtu: [\"\\\\bigotimes_{\\\\ell=1}^D{#1}^{(\\\\ell)}\", 1]",
                "kall: [\"1_{\\\\lan #1}\", 1]",
                "lall: [\"{\\\\order{\\\\lip_{\\\\lan #1}}}\", 1]",
                "lell: [\"{\\\\lip}_{\\\\lan #1}\", 1]",
                "bell: [\"{{{\\\\beta}}}_{\\\\lan #1}\", 1]",
                "aly: [\"{{\\\\alpha_{\\\\X_{#1}}}}\", 1]",
                "ravg: [\"{{r}_{{\\\\text{avg}}}(#1)}\", 1]",
                "diag: [\"\\\\text{diag}(#1)\", 1]",
                "Lcv: [\"{\\\\cal{L}}^{#1}_{\\\\text{up}}\", 1]",
                "noresamp: [\"{\\\\textcolor{red}{#1}}\", 1]",
                "relu: [\"\\\\phi(#1)\", 1]",
                "one: [\"{\\\\bm{1}}(#1)\", 1]",
                "sigmal: [\"\\\\sigma^{(#1)}\", 1]",
                "sigmalp: [\"\\\\sigma'^{(#1)}\", 1]",
                "sigmai: [\"\\\\sigma'_{\\\\text{all},#1}\", 1]",
                "order: [\"{\\\\cal{O}}(#1)\", 1]",
                "ordet: [\"{\\\\widetilde{\\\\cal{O}}}(#1)\", 1]",
                "rmax: [\"{\\\\bf{r}_{\\\\max}(#1)}\", 1]",
                "rbmax: [\"{\\\\bf{\\\\bar{r}}_{\\\\max}(#1)}\", 1]",
                "rmin: [\"{\\\\bf{r}_{\\\\min}(#1)}\", 1]",
                "gmmin: [\"{\\\\gamma_{\\\\min}(#1)}\", 1]",
                "gmmax: [\"{\\\\gamma_{\\\\max}(#1)}\", 1]",
                "smn: [\"{s_{\\\\min}(#1)}\", 1]",
                "smx: [\"{s_{\\\\max}(#1)}\", 1]",
                "isnr: [\"\\\\texttt{ISNR}(#1)\", 1]",
                "sft: [\"\\\\mathbb{S}(#1)\", 1]",
                "sftk: [\"\\\\mathbb{S}_k(#1)\", 1]",
                "sfp: [\"\\\\mathbb{S}'(#1)\", 1]",
                "distd: [\"\\\\texttt{dist}_\\\\dm\\\\left(#1\\\\right)\", 1]",
                "tn: [\"\\\\|{#1}\\\\|\", 1]",
                "td: [\"\\\\|{#1}\\\\|_\\\\dm\", 1]",
                "tl: [\"\\\\|{#1}\\\\|_{L_2}\", 1]",
                "ts: [\"\\\\|{#1}\\\\|_{\\\\Sc}\", 1]",
                "ti: [\"\\\\|{#1}\\\\|_{\\\\infty}\", 1]",
                "nrm: [\"\\\\|{#1}\\\\|\", 1]",
                "inr: [\"\\\\left<#1\\\\right>\", 1]",
                "tone: [\"\\\\|{#1}\\\\|_{\\\\ell_1}\", 1]",
                "lix: [\"\\\\|{#1}\\\\|_{\\\\Xc}\", 1]",
                "lif: [\"\\\\text{dist}_{\\\\FB}({#1})\", 1]",
                "lit: [\"\\\\text{max}(#1)\", 1]",
                "lia: [\"\\\\text{avg}(#1)\", 1]",
                "lin: [\"\\\\|{#1}\\\\|_{L_\\\\infty}\", 1]",
                "tff: [\"\\\\|{#1}\\\\|_{\\\\ell_4}\", 1]",
                "tin: [\"\\\\|{#1}\\\\|_{\\\\ell_\\\\infty}\", 1]",
                "trow: [\"\\\\|{#1}\\\\|_{2,\\\\infty}\", 1]",
                "tf: [\"\\\\|{#1}\\\\|_{F}\", 1]",
                "tnuc: [\"\\\\|{#1}\\\\|_{\\\\star}\", 1]",
                "te: [\"\\\\|{#1}\\\\|_{\\\\psi_1}\", 1]",
                "tsub: [\"\\\\|{#1}\\\\|_{\\\\psi_2}\", 1]",
                "tsut: [\"\\\\|{#1}\\\\|_{\\\\psi_{2/3}}\", 1]",
                "tsup: [\"\\\\|{#1}\\\\|_{\\\\psi_a}\", 1]",
                "dist: [\"\\\\texttt{dist}\\\\left(#1\\\\right)\", 1]",
                "Al: [\"\\\\Ac(#1)\", 1]",
                "Alg: [\"\\\\Ac_g(#1)\", 1]",
                "sbl: [\"\\\\sigma_{\\\\boldsymbol{\\\\alpha^{(#1)}}}\", 1]",
                "bl: [\"{\\\\boldsymbol{\\\\alpha}}^{(#1)}\", 1]",
                "blb: [\"\\\\bar{\\\\boldsymbol{\\\\alpha}}^{(#1)}\", 1]",
                "zm: [\"{\\\\texttt{{{zm}}}[#1]}\", 1]",
                "corr: [\"{\\\\texttt{corr\\\\_coef}}(#1)\", 1]",
                "qqq: [\"{\\\\textcolor{red}{?{#1}?}}\", 1]",
                "mat: [\"{\\\\text{mat}{#1}}\", 1]",
                "gcnn: [\"{\\\\vct{g}}_{\\\\text{cnn}#1}\", 1]",
                "con: [\"\\\\texttt{cone}_{\\\\eps}(#1)\", 1]",
                "xp: [\"\\\\x^{(#1)}_\\\\prm\", 1]",
                "Sn: [\"\\\\Sc^{(#1)}\", 1]",
                "Dn: [\"\\\\Dc^{(#1)}\", 1]",
                "xt: [\"\\\\x^{(#1)}\", 1]",
                "yt: [\"y^{(#1)}\", 1]",
                "inn: [\"\\\\left<#1\\\\right>\", 1]",
                "yp: [\"\\\\textcolor{red}{ #1}\", 1]",
                "zeronorm: [\"\\\\left\\\\|#1 \\\\right\\\\|_0\", 1]",
                "unorm: [\"\\\\left\\\\|#1 \\\\right\\\\|_u\", 1]",
                "ynorm: [\"\\\\left\\\\|#1 \\\\right\\\\|_{\\\\bar{y}}\", 1]",
                "onetwonorm: [\"\\\\left\\\\|#1\\\\right\\\\|_{1,2}\", 1]",
                "opnorm: [\"\\\\left\\\\|#1\\\\right\\\\|\", 1]",
                "fronorm: [\"\\\\left\\\\|#1\\\\right\\\\|_{F}\", 1]",
                "onenorm: [\"\\\\left\\\\|#1\\\\right\\\\|_{\\\\ell_1}\", 1]",
                "twonorm: [\"\\\\left\\\\|#1\\\\right\\\\|_{\\\\ell_2}\", 1]",
                "Dnorm: [\"\\\\left\\\\|#1\\\\right\\\\|_{D}\", 1]",
                "oneinfnorm: [\"\\\\left\\\\|#1\\\\right\\\\|_{1,\\\\infty}\", 1]",
                "infnorm: [\"\\\\left\\\\|#1\\\\right\\\\|_{\\\\ell_\\\\infty}\", 1]",
                "nucnorm: [\"\\\\left\\\\|#1\\\\right\\\\|_*\", 1]",
                "abs: [\"\\\\left|#1\\\\right|\", 1]",
                "avg: [\"\\\\left< #1 \\\\right>\", 1]",
                "xx: [\"\\\\vct{x}^{(#1)}\", 1]",
                "Ww: [\"\\\\mtx{W}^{(#1)}\", 1]",
                "ejc: [\"\\\\textcolor{emmanuel}{EJC: #1}\", 1]",
                "pp: [\"{\\\\vct{p}}(#1)\", 1]",
                "pr: [\"{\\\\vct{\\\\bar{p}}}(#1)\", 1]",
                "prl: [\"{\\\\vct{\\\\tilde{p}}}(#1)\", 1]",
                "prb: [\"{\\\\vct{\\\\bar{p}}}(#1)\", 1]",
                "wrb: [\"{\\\\vct{\\\\bar{W}}}(#1)\", 1]",
                "wrt: [\"{\\\\vct{\\\\bar{W}}}_0(#1)\", 1]",
                "sgn: [\"\\\\textrm{sgn}(#1)\", 1]",
                "grad: [\"{\\\\nabla\\\\Lc(#1)}\", 1]",
                "gradf: [\"{\\\\nabla f(#1)}\", 1]",
                "hessf: [\"{\\\\nabla^2f(#1)}\", 1]",
                "gradw: [\"{\\\\nabla{\\\\Lc}(#1)}\", 1]",
                "vct: [\"\\\\bm{#1}\", 1]",
                "mtx: [\"\\\\bm{#1}\", 1]",
                "rank: [\"\\\\texttt{rank}(#1)\", 1]",
                "restrict: [\"\\\\big\\\\vert_{#1}\", 1]",
                "note: [\"{\\\\bf [{\\\\em Note:} #1]}\", 1]",
                "m: [\"{\\\\bf{#1}}\", 1]",
                "mb: [\"{\\\\mathbb{#1}}\", 1]",
                "fapt: [\"f_{\\\\tsc{cls}}(#1\\\\bT)\", 1]",
                "fapn: [\"f_{\\\\lnk}(#1\\\\bT)\", 1]",
                "zX: [\"\\\\z\\\\{#1\\\\}\", 1]",
                "aX: [\"\\\\ab\\\\{#1\\\\}\", 1]",
                "wnorm: [\"\\\\left\\\\|#1\\\\right\\\\|_{#2}\", 2]",
                "grd: [\"{\\\\nabla\\\\Lc_{#1}(#2)}\", 2]",
                "iprod: [\"\\\\left\\\\langle #1 , #2 \\\\right\\\\rangle\", 2]",
                "ham: [\"{\\\\|#1,#2\\\\|_H}\", 2]",
                "endprf: \"\\\\hfill {\\\\vrule height6pt width6pt depth0pt}\\\\medskip\""
            ],
            "mathjax_environments": [
                "subequations: [\"{\", \"}\"]"
            ],
            "label2statementid": {
                "def loc opt": "8b4381d3-4f77-44ff-b28b-1cb33455102f",
                "score def": "7c093de6-8552-483f-90e0-5b6f7ea177b5",
                "HL cone def main": "66d850ce-ac5b-4e5d-aff3-19ee0ab7bb5d",
                "cone alpha eq1": "66d850ce-ac5b-4e5d-aff3-19ee0ab7bb5d",
                "def data model": "e95f020a-7716-4370-a0de-0d4bbaeedc65",
                "HL cone def": "38a2ca6a-b47e-4ac8-bbcf-c37cee0c13ef",
                "cone alpha eq": "38a2ca6a-b47e-4ac8-bbcf-c37cee0c13ef",
                "seq loc opt": "2c42c01e-0346-4000-96e8-2ab03bddf31b",
                "lem min risk": "a1634479-9af0-472b-ab5b-3ffdccc1a613",
                "lem:lip": "3f18c03f-226b-4ca9-9165-8868c28c9907",
                "eqn:lip:cons:erm": "3f18c03f-226b-4ca9-9165-8868c28c9907",
                "lem:rank": "3c2b211c-19ba-4dbe-a6dd-feefddb48c30",
                "lemma cone main": "06fd87b2-4fe5-4edc-b881-dc9bce163025",
                "example dataset": "72962460-6103-4d8c-b1de-88a92a770852",
                "tau description": "72962460-6103-4d8c-b1de-88a92a770852",
                "direct opt": "72962460-6103-4d8c-b1de-88a92a770852",
                "lemma cone": "a165a686-200b-4f67-8bc3-4961e665fdf3",
                "kqw mapping": "e69fcfe5-c221-4494-a431-7c607dd91ae5",
                "Wpath": "e69fcfe5-c221-4494-a431-7c607dd91ae5",
                "KQpath": "e69fcfe5-c221-4494-a431-7c607dd91ae5",
                "lem:q_reduce": "8fd6e58d-c3bf-49fd-bfcf-e6af7b7bf276",
                "grad def step3": "8fd6e58d-c3bf-49fd-bfcf-e6af7b7bf276",
                "lem:grad:descent": "24f524c6-9779-4b5c-9456-f6210fef911c",
                "eq:descent:obj new": "24f524c6-9779-4b5c-9456-f6210fef911c",
                "global des lem": "d47ee3c7-8de8-4689-9c13-c19a0b937d1e",
                "grad def new": "d47ee3c7-8de8-4689-9c13-c19a0b937d1e",
                "eqn:grad:prod:p": "d47ee3c7-8de8-4689-9c13-c19a0b937d1e",
                "grad def2": "d47ee3c7-8de8-4689-9c13-c19a0b937d1e",
                "eqn:al:lem": "d47ee3c7-8de8-4689-9c13-c19a0b937d1e",
                "eqn:lower": "d47ee3c7-8de8-4689-9c13-c19a0b937d1e",
                "eqn:al:lem:2": "d47ee3c7-8de8-4689-9c13-c19a0b937d1e",
                "eqn:bound:lprim": "d47ee3c7-8de8-4689-9c13-c19a0b937d1e",
                "eqn:grad:prod:p:fin": "d47ee3c7-8de8-4689-9c13-c19a0b937d1e",
                "glocal cond": "a632ebee-b8f8-4fec-8945-2c8e11909cdd",
                "lem:gcond:l1": "a632ebee-b8f8-4fec-8945-2c8e11909cdd",
                "lem:gcond:l2": "a632ebee-b8f8-4fec-8945-2c8e11909cdd",
                "zero:g:lbound": "88506101-38eb-4532-8e56-86f786b8f886",
                "zero1:g:bound": "88506101-38eb-4532-8e56-86f786b8f886",
                "zero2:g:bound": "88506101-38eb-4532-8e56-86f786b8f886",
                "zero3:g:bound": "88506101-38eb-4532-8e56-86f786b8f886",
                "mu choice2": "a632ebee-b8f8-4fec-8945-2c8e11909cdd",
                "cone-A-eq": "a632ebee-b8f8-4fec-8945-2c8e11909cdd",
                "grad def32": "a632ebee-b8f8-4fec-8945-2c8e11909cdd",
                "soft prob bound2": "a632ebee-b8f8-4fec-8945-2c8e11909cdd",
                "aggregate2": "a632ebee-b8f8-4fec-8945-2c8e11909cdd",
                "wishfor2": "a632ebee-b8f8-4fec-8945-2c8e11909cdd",
                "R bound2": "a632ebee-b8f8-4fec-8945-2c8e11909cdd",
                "pbb corr2": "a632ebee-b8f8-4fec-8945-2c8e11909cdd",
                "lem:glocal:corr": "446ace68-6d7b-476b-8a21-a4bb0e9c9009",
                "main local cond2": "694f89ee-59e8-458c-881a-23ca7d9a82a8",
                "Rpi choice2": "694f89ee-59e8-458c-881a-23ca7d9a82a8",
                "desired comp2": "694f89ee-59e8-458c-881a-23ca7d9a82a8",
                "eqn:grad:difff0": "694f89ee-59e8-458c-881a-23ca7d9a82a8",
                "R bound pi": "694f89ee-59e8-458c-881a-23ca7d9a82a8",
                "eqn:grad:difff1": "694f89ee-59e8-458c-881a-23ca7d9a82a8",
                "local cond": "d108ee94-cb8a-43bf-9ea6-1760f23ab71c",
                "lem:cond:l1": "d108ee94-cb8a-43bf-9ea6-1760f23ab71c",
                "lem:cond:l2": "d108ee94-cb8a-43bf-9ea6-1760f23ab71c",
                "local:g:lbound": "d108ee94-cb8a-43bf-9ea6-1760f23ab71c",
                "local1:g:bound": "d108ee94-cb8a-43bf-9ea6-1760f23ab71c",
                "local2:g:bound": "d108ee94-cb8a-43bf-9ea6-1760f23ab71c",
                "local3:g:bound": "d108ee94-cb8a-43bf-9ea6-1760f23ab71c",
                "mu choice": "d108ee94-cb8a-43bf-9ea6-1760f23ab71c",
                "cone-non-nei": "d108ee94-cb8a-43bf-9ea6-1760f23ab71c",
                "grad def3": "d108ee94-cb8a-43bf-9ea6-1760f23ab71c",
                "soft prob bound": "d108ee94-cb8a-43bf-9ea6-1760f23ab71c",
                "aggregate": "d108ee94-cb8a-43bf-9ea6-1760f23ab71c",
                "wishwish2": "d108ee94-cb8a-43bf-9ea6-1760f23ab71c",
                "wishfor": "d108ee94-cb8a-43bf-9ea6-1760f23ab71c",
                "s bound": "d108ee94-cb8a-43bf-9ea6-1760f23ab71c",
                "R bound": "d108ee94-cb8a-43bf-9ea6-1760f23ab71c",
                "pbb corr": "d108ee94-cb8a-43bf-9ea6-1760f23ab71c",
                "local lamma general upper": "d108ee94-cb8a-43bf-9ea6-1760f23ab71c",
                "lem:local:corr": "bd49c7be-7b04-4027-a0a4-c0ebb4ce989e",
                "main local cond": "bd49c7be-7b04-4027-a0a4-c0ebb4ce989e",
                "R boundC0": "bd49c7be-7b04-4027-a0a4-c0ebb4ce989e",
                "desired comp": "bd49c7be-7b04-4027-a0a4-c0ebb4ce989e",
                "eqn:grad:difff2": "bd49c7be-7b04-4027-a0a4-c0ebb4ce989e",
                "R bound pi 1": "bd49c7be-7b04-4027-a0a4-c0ebb4ce989e",
                "eqn:grad:difff3": "bd49c7be-7b04-4027-a0a4-c0ebb4ce989e",
                "eqn:con:nabla0": "a632ebee-b8f8-4fec-8945-2c8e11909cdd",
                "eqn:neg:corr:local:nabla0": "446ace68-6d7b-476b-8a21-a4bb0e9c9009",
                "eqn:bound:R:nabla0": "446ace68-6d7b-476b-8a21-a4bb0e9c9009",
                "eqn:rho:def:nabla0": "4e6b21f5-e5cf-42e2-bba7-fb0f652a4d31",
                "eqn:bar:rho": "446ace68-6d7b-476b-8a21-a4bb0e9c9009",
                "eqn:localgd:1:nabla0": "4e6b21f5-e5cf-42e2-bba7-fb0f652a4d31",
                "eqn:localgd:2:nabla0": "4e6b21f5-e5cf-42e2-bba7-fb0f652a4d31",
                "eqn:localgd:3:nabla0": "4e6b21f5-e5cf-42e2-bba7-fb0f652a4d31",
                "lem:out:S": "db547eda-e029-405a-a5fd-913bb78fbad9",
                "eq:descent:obj": "db547eda-e029-405a-a5fd-913bb78fbad9",
                "eqn:obj:less0": "db547eda-e029-405a-a5fd-913bb78fbad9",
                "eqn:grad:low:eps": "db547eda-e029-405a-a5fd-913bb78fbad9",
                "eqn:qgrad": "db547eda-e029-405a-a5fd-913bb78fbad9",
                "lem add up": "6be9c80c-88f3-4dd3-8f40-9bd1405198ef",
                "glob:asymp loss": "6be9c80c-88f3-4dd3-8f40-9bd1405198ef",
                "glob:BB eq": "6be9c80c-88f3-4dd3-8f40-9bd1405198ef",
                "glob:score decomp": "6be9c80c-88f3-4dd3-8f40-9bd1405198ef",
                "margin violate:glob": "6be9c80c-88f3-4dd3-8f40-9bd1405198ef",
                "glob:salpha bounds": "6be9c80c-88f3-4dd3-8f40-9bd1405198ef",
                "glob:ineq prl": "6be9c80c-88f3-4dd3-8f40-9bd1405198ef",
                "thm global reg path": "40c46579-becc-423d-9ce9-0e22387b1eda",
                "diverg:norm:w": "9bdda760-d1bb-4706-ad4f-1be6948f697b",
                "conv:gd:w:global:nabla0": "5ccd1067-55db-4603-9c88-a7cb4b97d4c1",
                "thm:local:gd": "97b4a9b8-465c-4bc8-ae60-1214494cef10",
                "lem:cond:t1": "97b4a9b8-465c-4bc8-ae60-1214494cef10",
                "thm:separation": "031d5613-3df2-4503-aada-9d37374107c8",
                "separation thm": "c5c7d690-f33d-4f4c-b371-ee58d631710e",
                "local RP thm1": "c67e2c58-5978-44cb-af42-e5e3a9a98a33",
                "toy data thm": "f0b0e79a-2e19-42da-98be-5ff35441a595",
                "eq nablaL0": "f0b0e79a-2e19-42da-98be-5ff35441a595",
                "local RP thm": "e2b288d5-6f0b-4ed4-a0f3-4fdb25730c9d",
                "asymp loss": "e2b288d5-6f0b-4ed4-a0f3-4fdb25730c9d",
                "BB eq": "e2b288d5-6f0b-4ed4-a0f3-4fdb25730c9d",
                "score decomp": "e2b288d5-6f0b-4ed4-a0f3-4fdb25730c9d",
                "lip score gap": "e2b288d5-6f0b-4ed4-a0f3-4fdb25730c9d",
                "qikeq": "e2b288d5-6f0b-4ed4-a0f3-4fdb25730c9d",
                "margin violate": "e2b288d5-6f0b-4ed4-a0f3-4fdb25730c9d",
                "qik bound": "e2b288d5-6f0b-4ed4-a0f3-4fdb25730c9d",
                "pik bound": "e2b288d5-6f0b-4ed4-a0f3-4fdb25730c9d",
                "salpha bounds": "e2b288d5-6f0b-4ed4-a0f3-4fdb25730c9d",
                "q11 eq": "e2b288d5-6f0b-4ed4-a0f3-4fdb25730c9d",
                "ineq prl": "e2b288d5-6f0b-4ed4-a0f3-4fdb25730c9d",
                "desired Wmm bound": "e2b288d5-6f0b-4ed4-a0f3-4fdb25730c9d",
                "conv:gd:w:global:nabla0:app": "4e6b21f5-e5cf-42e2-bba7-fb0f652a4d31",
                "lem:zglob:l1": "4e6b21f5-e5cf-42e2-bba7-fb0f652a4d31",
                "lem:zglob:l2": "4e6b21f5-e5cf-42e2-bba7-fb0f652a4d31",
                "lem:zglob:l3": "4e6b21f5-e5cf-42e2-bba7-fb0f652a4d31",
                "assum:extra": "4e6b21f5-e5cf-42e2-bba7-fb0f652a4d31",
                "eqn:decpath:zinit": "4e6b21f5-e5cf-42e2-bba7-fb0f652a4d31",
                "eqn:mu:zero": "4e6b21f5-e5cf-42e2-bba7-fb0f652a4d31",
                "eqn:stepeta0": "4e6b21f5-e5cf-42e2-bba7-fb0f652a4d31",
                "eqn:neg:corr:0": "4e6b21f5-e5cf-42e2-bba7-fb0f652a4d31",
                "eqn:zeta:mu:0": "4e6b21f5-e5cf-42e2-bba7-fb0f652a4d31",
                "diverg:norm:qk": "cbebd782-59d1-44bd-b1b0-cb2b0cadebbf",
                "eq:gd_inc": "cbebd782-59d1-44bd-b1b0-cb2b0cadebbf",
                "eqn:c1": "cbebd782-59d1-44bd-b1b0-cb2b0cadebbf",
                "eqn:qk:recur": "cbebd782-59d1-44bd-b1b0-cb2b0cadebbf",
                "eq:qk:tr:b1": "cbebd782-59d1-44bd-b1b0-cb2b0cadebbf",
                "eq:qk:tr:b2": "cbebd782-59d1-44bd-b1b0-cb2b0cadebbf",
                "cor gm": "dc3fce8f-00c5-4fe0-a3dc-df878f0858d7",
                "cor global reg path": "7edf41cb-7240-4f9c-9d92-c315b2fccacd"
            }
        }
    ],
    "extraction_dir": "/tmp/paper_extraction",
    "pages_root": "/"
}
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Papers - LM Theory</title>
    <link rel="shortcut icon" href="/assets/images/favicon.png">
    <link rel="stylesheet" href="/assets/css/styles.css">
    
</head>
<body>
    <header>
        <h1>Theoretical Results on Transformers and Language Models</h1>
        <nav>
            <a href="/">Home</a>
            <a href="/examples">Examples</a>
            <a href="/contact">Contact</a>
            <div class="hover-container">
                <span class="hover-word"><a href="/contribute">Contribute</a></span>
                <div class="dropdown-content">
                    <a href="/contribute/add_statement">Add statement</a>
                    <a href="/contribute/add_paper">Add paper</a>
                </div>
            </div>
            <!-- <a href="/proof_assistant">Proof Assistant</a> -->
            <div class="hover-container">
                <span class="hover-word"><a href="/library">Index</a></span>
                <div class="dropdown-content">
                    <a href="/library">Full Index</a>
                    <a href="/library/papers">Papers</a>
                    <a href="/library/definitions">Definitions</a>
                    <a href="/library/axioms">Axioms</a>
                    <a href="/library/lemmas">Lemmas</a>
                    <a href="/library/theorems">Theorems</a>
                    <a href="/library/corollaries">Corollaries</a>
                </div>
            </div>
        </nav>
    </header>
    <main>
        
<section id="index">
    <h2>Papers - Index</h2>
    <p>This page lists all the papers available in our repository.</p>
    <section id="papers">
        <h3>Papers</h3>
        <ul>
        
            <li><a href="/library/papers/the_lipschitz_constant_of_self-attention/index.html">The Lipschitz Constant of Self-Attention</a></li>
        
            <li><a href="/library/papers/transformer_normalisation_layers_and_the_independence_of_semantic_subspaces/index.html">Transformer Normalisation Layers and the Independence of Semantic Subspaces</a></li>
        
            <li><a href="/library/papers/the_geometry_of_categorical_and_hierarchical_concepts_in_large_language_models/index.html">The Geometry of Categorical and Hierarchical Concepts in Large Language Models</a></li>
        
            <li><a href="/library/papers/signal_propagation_in_transformers_theoretical_perspectives_and_the_role_of_rank_collapse/index.html">Signal Propagation in Transformers: Theoretical Perspectives and the Role of Rank Collapse</a></li>
        
            <li><a href="/library/papers/transformers_as_support_vector_machines/index.html">Transformers as Support Vector Machines</a></li>
        
        </ul>
    </section>
</section>

    </main>
    <footer>
        <p>&copy; 2024 Theoretical Results on Transformers and Language Models</p>
    </footer>
</body>
</html>
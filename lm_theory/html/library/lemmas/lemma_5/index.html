<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lemma 5 - LM Theory</title>
    <link rel="shortcut icon" href="/assets/images/favicon.png">
    <link rel="stylesheet" href="/assets/css/styles.css">
    
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>
    window.MathJax = {
        loader: {load: ['[tex]/mathtools']},
        tex: {
            tags: 'all',  // Automatically number all display equations
            // loader: {load: ['[tex]/autoload']},               // TODO: remove
            // packages: {'[+]': ['autoload', 'mathtools']},     // TODO: remove
            packages: {'[+]': ['mathtools']},
            autoload: {
                coloneqq: ['mathtools']
            },
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            macros: {
                emph: ["\\textit{#1}", 1],
bm: ["\\boldsymbol{\\mathbf{#1}}", 1],
mathds: ["\\mathbf{#1}", 1],
textsl: ["\\textit{#1}", 1],
vspace: ["", 1],
xspace: "",
Tr: "\\operatorname{Tr}",
softmaxOp: "\\operatorname{softmax}",
lip: "\\operatorname{Lip}",
diag: "\\operatorname{diag}",
spaces: "\\hspace{2mm}",
unif: "\\pazocal{U}",
softmax: ["\\softmaxOp\\left(#1\\right)", 1],
norm: ["\\left\\lVert#1\\right\\rVert", 1],
andriy: ["\\textcolor{blue}{[AM: #1]}", 1],
hyunjik: ["\\textcolor{red}{[HK: #1]}", 1],
george: ["\\textcolor{magenta}{[George: #1]}", 1]

            },
            environments: {
                subequations: ["{", "}"]

            },
        },
        options: {
            renderActions: {
                addMenu: []
            },
            ignoreHtmlClass: "tex2jax_ignore",
            skipHtmlTags: ["script", "noscript", "style", "textarea", "pre", "code"]
        }
    };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <header>
        <h1>Theoretical Results on Transformers and Language Models</h1>
        <nav>
            <a href="/">Home</a>
            <a href="/examples">Examples</a>
            <a href="/contact">Contact</a>
            <div class="hover-container">
                <span class="hover-word"><a href="/contribute">Contribute</a></span>
                <div class="dropdown-content">
                    <a href="/contribute/add_statement">Add statement</a>
                    <a href="/contribute/add_paper">Add paper</a>
                </div>
            </div>
            <!-- <a href="/proof_assistant">Proof Assistant</a> -->
            <div class="hover-container">
                <span class="hover-word"><a href="/library">Index</a></span>
                <div class="dropdown-content">
                    <a href="/library">Full Index</a>
                    <a href="/library/papers">Papers</a>
                    <a href="/library/definitions">Definitions</a>
                    <a href="/library/axioms">Axioms</a>
                    <a href="/library/lemmas">Lemmas</a>
                    <a href="/library/theorems">Theorems</a>
                    <a href="/library/corollaries">Corollaries</a>
                </div>
            </div>
        </nav>
    </header>
    <main>
        
<section id="Low-Rank Approximation Bound">
    <h2>Lemma 5: Low-Rank Approximation Bound</h2>
    <section id="Statement">
        <h3>Statement</h3>
        <div>
            $\sum_j \|P_{ij}(\mathbf{y}_j - \sum_k P_{ik} \mathbf{y}_k)(\mathbf{y}_i - \mathbf{y}_j)^\top\|_\infty \leq  \phi^{-1}(N-1) \sqrt{D/H}$.
        </div>
    </section>
    <section id="motivation">
        <h3>Motivation of Statement (AI-Generated)</h3>
        <div>
            This inequality is useful in the context of analyzing the stability and convergence of certain iterative algorithms, particularly in high-dimensional spaces. It provides a bound on the infinity norm of a sum involving projection matrices and vectors, which can be critical for ensuring that the algorithm behaves as expected. This type of result is often used in optimization and machine learning to guarantee that updates to parameters do not lead to instability or divergence.
        </div>
    </section>
        
<section id="OriginalProof">
<h3>Original Proof</h3>
Note $\|\mathbf{u}\mathbf{v}^\top\|_{\infty} = \|\mathbf{u}\|_{\infty} \|\mathbf{v}\|_1$ for real vectors $\mathbf{u},\mathbf{v}$. Hence
\begin{align*}
    \sum_j \|P_{ij}(\mathbf{y}_j - \sum_k P_{ik} \mathbf{y}_k)(\mathbf{y}_i - \mathbf{y}_j)^\top\|_\infty & = \sum_j P_{ij} \|\mathbf{y}_j - \sum_k P_{ik} \mathbf{y}_k\|_\infty \|\mathbf{y}_i - \mathbf{y}_j\|_1 \\
    & = \mathbf{a}^\top \mathbf{b} \leq \|\mathbf{a}\|_2 \|\mathbf{b}\|_2,
\end{align*}
where $a_j = \sqrt{P_{ij}} \|\mathbf{y}_j - \sum_k P_{ik} \mathbf{y}_k\|_\infty$, $b_j = \sqrt{P_{ij}} \|\mathbf{y}_i - \mathbf{y}_j\|_1$.

Note $a_j \leq c_j \coloneqq  \sqrt{P_{ij}} \|\mathbf{y}_j - \sum_k P_{ik} \mathbf{y}_k\|_2$ since $\|\mathbf{u}\|_\infty \leq \|\mathbf{u}\|_2$ for vector $\mathbf{u}$. Hence $\|\mathbf{a}\|_2 \leq \|\mathbf{c}\|_2$.

Also $b_j \leq \sqrt{\frac{D}{H}} d_j \coloneqq  \sqrt{\frac{D}{H}} \sqrt{P_{ij}} \|\mathbf{y}_i - \mathbf{y}_j\|_2$ since $\|\mathbf{u}\|_1 \leq \sqrt{\frac{D}{H}}\|\mathbf{u}\|_2$ for $\mathbf{u} \in \mathbb{R}^{D/H}$ (e.g.~by the Cauchy--Schwartz inequality on $[|\mathbf{u}_1|, \ldots, |\mathbf{u}_{D/H}|]$ and $\mathbf{1}$). Hence $\|b\|_2 \leq \sqrt{\frac{D}{H}}\|d\|_2$.

Note $\|c\|_2^2 = \sum_j P_{ij} \|y_j - \sum_k P_{ik} y_k\|_2^2 = \Tr(\mathrm{Cov}(\mathbb{Y})) \leq \phi^{-1}(N-1)$ from <a href="/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_3/index.html#lemma%3Akey">Lemma 3</a>,
and $\|d\|_2^2 =  \sum_j P_{ij} \|y_i - y_j\|_2^2 \leq  \phi^{-1}(N-1)$ also from <a href="/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_3/index.html#lemma%3Akey">Lemma 3</a>.
Hence $\|a\|_2 \|b\|_2 \leq \sqrt{\frac{D}{H}} \|c\|_2 \|d\|_2 \leq \sqrt{\frac{D}{H}} \phi^{-1}(N-1)$.
</section>

    <section id="proofExplanation">
        <h4>Explanation of Proof (AI-Generated)</h4>
        <div>
            To understand the proof, let's break down the steps involved:

1. <i></i>Initial Norm Equality<i></i>:
   - The proof starts by noting the equality $\|\mathbf{u}\mathbf{v}^\top\|_{\infty} = \|\mathbf{u}\|_{\infty} \|\mathbf{v}\|_1$ for real vectors $\mathbf{u}$ and $\mathbf{v}$.

2. <i></i>Summation and Norms<i></i>:
   - The expression $\sum_j \|P_{ij}(\mathbf{y}_j - \sum_k P_{ik} \mathbf{y}_k)(\mathbf{y}_i - \mathbf{y}_j)^\top\|_\infty$ is rewritten using the initial norm equality:
     \[
     \sum_j P_{ij} \|\mathbf{y}_j - \sum_k P_{ik} \mathbf{y}_k\|_\infty \|\mathbf{y}_i - \mathbf{y}_j\|_1
     \]
   - This is then expressed as a dot product $\mathbf{a}^\top \mathbf{b}$, where:
     \[
     a_j = \sqrt{P_{ij}} \|\mathbf{y}_j - \sum_k P_{ik} \mathbf{y}_k\|_\infty, \quad b_j = \sqrt{P_{ij}} \|\mathbf{y}_i - \mathbf{y}_j\|_1
     \]

3. <i></i>Cauchy-Schwarz Inequality<i></i>:
   - The dot product $\mathbf{a}^\top \mathbf{b}$ is bounded by the Cauchy-Schwarz inequality:
     \[
     \mathbf{a}^\top \mathbf{b} \leq \|\mathbf{a}\|_2 \|\mathbf{b}\|_2
     \]

4. <i></i>Bounding $\mathbf{a}$<i></i>:
   - The vector $\mathbf{a}$ is bounded by another vector $\mathbf{c}$:
     \[
     a_j \leq c_j \coloneqq \sqrt{P_{ij}} \|\mathbf{y}_j - \sum_k P_{ik} \mathbf{y}_k\|_2
     \]
   - This follows from the inequality $\|\mathbf{u}\|_\infty \leq \|\mathbf{u}\|_2$ for any vector $\mathbf{u}$.
   - Consequently, $\|\mathbf{a}\|_2 \leq \|\mathbf{c}\|_2$.

5. <i></i>Bounding $\mathbf{b}$<i></i>:
   - The vector $\mathbf{b}$ is bounded by another vector $\mathbf{d}$:
     \[
     b_j \leq \sqrt{\frac{D}{H}} d_j \coloneqq \sqrt{\frac{D}{H}} \sqrt{P_{ij}} \|\mathbf{y}_i - \mathbf{y}_j\|_2
     \]
   - This follows from the inequality $\|\mathbf{u}\|_1 \leq \sqrt{\frac{D}{H}}\|\mathbf{u}\|_2$ for $\mathbf{u} \in \mathbb{R}^{D/H}$, which can be derived using the Cauchy-Schwarz inequality.
   - Consequently, $\|\mathbf{b}\|_2 \leq \sqrt{\frac{D}{H}}\|\mathbf{d}\|_2$.

6. <i></i>Norms of $\mathbf{c}$ and $\mathbf{d}$<i></i>:
   - The norms $\|\mathbf{c}\|_2^2$ and $\|\mathbf{d}\|_2^2$ are evaluated using <a href="/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/lemmas/lemma_3/index.html#lemma%3Akey">Lemma 3</a>:
     \[
     \|c\|_2^2 = \sum_j P_{ij} \|y_j - \sum_k P_{ik} y_k\|_2^2 = \Tr(\mathrm{Cov}(\mathbb{Y})) \leq \phi^{-1}(N-1)
     \]
     \[
     \|d\|_2^2 = \sum_j P_{ij} \|y_i - y_j\|_2^2 \leq \phi^{-1}(N-1)
     \]

7. <i></i>Final Bound<i></i>:
   - Combining the bounds, we get:
     \[
     \|\mathbf{a}\|_2 \|\mathbf{b}\|_2 \leq \sqrt{\frac{D}{H}} \|\mathbf{c}\|_2 \|\mathbf{d}\|_2 \leq \sqrt{\frac{D}{H}} \phi^{-1}(N-1)
     \]

This completes the proof by showing the desired inequality.
        </div>
    </section>




    <section id="Reference">
        <h3>Reference</h3>
            <p>Title: <a href="/library/papers/the_lipschitz_constant_of_self-attention/index.html">The Lipschitz Constant of Self-Attention</a></p>
            <p>Authors: Hyunjik Kim, George Papamakarios, and Andriy Mnih</p>
            <p>URL: <a href="https://arxiv.org/abs/2006.04710">https://arxiv.org/abs/2006.04710</a></p>
    </section>
    <section id="Resources">
        <!-- Bibtex Modal -->
        <div id="bibtexModal" class="modal">
        <div class="modal-content">
            <span class="close" onclick="closeBibtexModal()">&times;</span>
            <p>@misc{kim2021lipschitzconstantselfattention,<br>&emsp;title={The Lipschitz Constant of Self-Attention},<br>&emsp;author={Hyunjik Kim and George Papamakarios and Andriy Mnih},<br>&emsp;year={2021},<br>&emsp;eprint={2006.04710},<br>&emsp;archivePrefix={arXiv},<br>&emsp;primaryClass={stat.ML},<br>&emsp;url={https://arxiv.org/abs/2006.04710}<br>}</p>
            <button class="modal-button" onclick="copyBibtex()">Copy</button>
        </div>
        </div>

        <!-- Latex Modal -->
        <div id="latexModal" class="modal">
        <div class="modal-content">
            <span class="close" onclick="closeLatexModal()">&times;</span>
            <p class="tex2jax_ignore"><code>\begin{lemma} \label{lemma:low_rank}
$\sum_j \|P_{ij}(\mathbf{y}_j - \sum_k P_{ik} \mathbf{y}_k)(\mathbf{y}_i - \mathbf{y}_j)^\top\|_\infty \leq  \phi^{-1}(N-1) \sqrt{D/H}$.
\end{lemma}</code></p>
            <button class="modal-button" onclick="copyLatex()">Copy</button>
        </div>
        </div>

        <h3>Resources</h3>    
            <p>
                <b>BibTex</b>:
                    <button onclick="copyBibtex()">Copy</button>
                    <button onclick="openBibtexModel()">Show</button>
            </p>
            <p>
                <b>LaTeX</b>:
                    <button onclick="copyLatex()">Copy</button>
                    <button onclick="openLatexModal()">Show</button>
            </p>
        <script>
            function copyBibtex() {
                var textToCopy = `@misc{kim2021lipschitzconstantselfattention,
  title={The Lipschitz Constant of Self-Attention},
  author={Hyunjik Kim and George Papamakarios and Andriy Mnih},
  year={2021},
  eprint={2006.04710},
  archivePrefix={arXiv},
  primaryClass={stat.ML},
  url={https://arxiv.org/abs/2006.04710}
}`;
                navigator.clipboard.writeText(textToCopy).then(function() {
                    //alert('Text copied to clipboard: ' + textToCopy);
                }, function(err) {
                    console.error('Failed to copy text: ', err);
                });
            }
            
            function copyLatex() {
                var textToCopy = `\\begin{lemma} \\label{lemma:low_rank}
$\\sum_j \\|P_{ij}(\\mathbf{y}_j - \\sum_k P_{ik} \\mathbf{y}_k)(\\mathbf{y}_i - \\mathbf{y}_j)^\\top\\|_\\infty \\leq  \\phi^{-1}(N-1) \\sqrt{D/H}$.
\\end{lemma}`;
                navigator.clipboard.writeText(textToCopy).then(function() {
                    //alert('Text copied to clipboard: ' + textToCopy);
                }, function(err) {
                    console.error('Failed to copy text: ', err);
                });
            }

            // Bibtex
            function openBibtexModel() {
                document.getElementById('bibtexModal').style.display = 'block';
            }
            function closeBibtexModal() {
                document.getElementById('bibtexModal').style.display = 'none';
            }

            // Latex
            function openLatexModal() {
                document.getElementById('latexModal').style.display = 'block';
            }
            function closeLatexModal() {
                document.getElementById('latexModal').style.display = 'none';
            }
        </script>
    </section>
</section>

    </main>
    <footer>
        <p>&copy; 2024 Theoretical Results on Transformers and Language Models</p>
    </footer>
</body>
</html>
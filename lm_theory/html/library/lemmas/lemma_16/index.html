<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lemma 16 - LM Theory</title>
    <link rel="stylesheet" href="/assets/css/styles.css">
    
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>
    window.MathJax = {
        loader: {load: ['[tex]/mathtools']},
        tex: {
            tags: 'all',  // Automatically number all display equations
            // loader: {load: ['[tex]/autoload']},               // TODO: remove
            // packages: {'[+]': ['autoload', 'mathtools']},     // TODO: remove
            packages: {'[+]': ['mathtools']},
            autoload: {
                coloneqq: ['mathtools']
            },
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            macros: {
                emph: ["\\textit{#1}", 1],
bm: ["\\boldsymbol{\\mathbf{#1}}", 1],
mathds: ["\\mathbf{#1}", 1],
textsl: ["\\textit{#1}", 1],
vspace: ["", 1],
xspace: "",
soft: "\\operatorname{softmax}",
blockdiag: "\\operatorname{blockdiag}",
diag: "\\operatorname{diag}",
vect: "\\operatorname{vec}",
tr: "\\operatorname{tr}",
rank: "\\operatorname{rank}",
SA: "\\operatorname{SA}",
mb: "\\mathbf",
kro: "  \\mathbin{\\mathop{\\otimes}}",
wQ: "\\mb W^{Q}",
wQT: "\\mb W^{Q \\top}",
wK: "\\mb W^{K}",
wKT: "\\mb W^{K \\top}",
wV: "\\mb W^{V}",
wVT: "\\mb W^{V \\top}",
p: "\\bm p",
Exp: "\\mathbb{E}",
R: "\\mathbb{R}",
wM: ["\\mb W^{#1}", 1],
wMt: ["\\mb W^{#1^\\top}", 1],
norm: ["\\left\\lVert#1\\right\\rVert", 1],
extra: ["{ #1}", 1],
luca: ["{\\color{red} Luca: ``#1''}", 1],
antonio: ["{\\color{magenta} Antonio: ``#1''}", 1],
lorenzo: ["{\\color{blue} Lorenzo: ``#1''}", 1],
sotiris: ["{\\color{green} Sotiris: ``#1''}", 1],
sidak: ["{\\color{orange} Sidak: ``#1''}", 1],
aurelien: ["{\\color{ForestGreen} Aurelien: ``#1''}", 1],
Im: "{\\bf I}",
Km: "{\\bf K}",
Am: "{\\bf A}",
Em: "{\\boldsymbol \\epsilon}",
Lm: "{\\bf L}",
Bm: "{\\bf B}",
Cm: "{\\bf C}",
Dm: "{\\bf D}",
Sm: "{\\bf S}",
Xm: "{\\bf X}",
Ym: "{\\bf Y}",
Nm: "{\\bf N}",
Mm: "{\\bf M}",
Tm: "{\\bf T}",
Wm: "{\\bf W}",
Zm: "{\\bf Z}",
Sm: "{\\bf S}"

            },
            environments: {
                subequations: ["{", "}"]

            },
        },
        options: {
            renderActions: {
                addMenu: []
            },
            ignoreHtmlClass: "tex2jax_ignore",
            skipHtmlTags: ["script", "noscript", "style", "textarea", "pre", "code"]
        }
    };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <header>
        <h1>Theoretical Results on Transformers and Language Models</h1>
        <nav>
            <a href="/">Home</a>
            <a href="/examples">Examples</a>
            <a href="/contact">Contact</a>
            <a href="/contribute">Contribute</a>
            <div class="hover-container">
                <span class="hover-word"><a href="/library">Index</a></span>
                <div class="dropdown-content">
                    <a href="/library">Full Index</a>
                    <a href="/library/papers">Papers</a>
                    <a href="/library/definitions">Definitions</a>
                    <a href="/library/axioms">Axioms</a>
                    <a href="/library/lemmas">Lemmas</a>
                    <a href="/library/theorems">Theorems</a>
                    <a href="/library/corollaries">Corollaries</a>
                </div>
            </div>
        </nav>
    </header>
    <main>
        
<section id="Expectation of Softmax Products">
    <h2>Lemma 16: Expectation of Softmax Products</h2>
    <section id="Statement">
        <h3>Statement</h3>
        <div>
            Under the uniform-attention assumption:
\begin{equation*}
    \Exp[\Sm_{kj}\Sm_{k'j}] = \frac{1}{d_vn^2}\Exp C(\Xm).
\end{equation*}
        </div>
    </section>
    <section id="motivation">
        <h3>Motivation of Statement (AI-Generated)</h3>
        <div>
            The given statement is useful in the context of analyzing the behavior of attention mechanisms in neural networks, particularly under the uniform-attention assumption. It provides a way to compute the expected value of the product of attention scores, which can be crucial for understanding how information is distributed and aggregated across different layers of the network. This can be particularly helpful when designing or optimizing attention-based models, such as transformers, to ensure they are effectively capturing and utilizing the input data.
        </div>
    </section>
        
<section id="OriginalProof">
<h3>Original Proof</h3>
Note that under the uniform-attention assumption:
\begin{equation*}
    \Sm_{kj} = \frac{1}{n}\left(\mathbf{1}_{n\times n}\Xm \Wm^V\right)_{kj} = \frac{1}{n}\sum_{zi}\Xm_{zi}\Wm^V_{ij}.
\end{equation*}
Hence, using the fact that the weights are i.i.d with variance $\sigma_v^2=\frac{1}{d_v}$:
\begin{align*}
    \Exp[\Sm_{kj}\Sm_{k'j}] = \frac{\sigma_{v}^2}{n^2}\sum_{z,z'}\sum_i\Exp[\Xm_{zi} \Xm_{z'i}] = \frac{1}{d_vn^2}\sum_{k,k'}\langle\Xm_z, \Xm_{z'}\rangle = \frac{1}{d_vn^2}\Exp C(\Xm).
\end{align*}
</section>

    <section id="proofExplanation">
        <h4>Explanation of Proof (AI-Generated)</h4>
        <div>
            Here is a short explanation of the different steps in the proof:
<br>
<br>1. <i></i>Uniform-Attention Assumption<i></i>:
<br>   Under the uniform-attention assumption, the matrix \(\Sm\) is defined as:
<br>   \[
   \Sm_{kj} = \frac{1}{n}\left(\mathbf{1}_{n\times n}\Xm \Wm^V\right)_{kj} = \frac{1}{n}\sum_{zi}\Xm_{zi}\Wm^V_{ij}.
   \]
<br>   This equation expresses \(\Sm_{kj}\) as the average of the product of elements from \(\Xm\) and \(\Wm^V\).
<br>
<br>2. <i></i>Expectation of Product of \(\Sm\) Elements<i></i>:
<br>   Using the fact that the weights \(\Wm^V_{ij}\) are independent and identically distributed (i.i.d) with variance \(\sigma_v^2 = \frac{1}{d_v}\), we compute the expectation of the product \(\Sm_{kj}\Sm_{k'j}\):
<br>   \[
   \Exp[\Sm_{kj}\Sm_{k'j}] = \frac{\sigma_{v}^2}{n^2}\sum_{z,z'}\sum_i\Exp[\Xm_{zi} \Xm_{z'i}].
   \]
<br>   This step involves summing over all possible indices \(z\) and \(z'\) and taking the expectation of the product of corresponding elements in \(\Xm\).
<br>
<br>3. <i></i>Inner Product and Expectation<i></i>:
<br>   The expectation of the product \(\Xm_{zi} \Xm_{z'i}\) can be expressed in terms of the inner product of vectors \(\Xm_z\) and \(\Xm_{z'}\):
<br>   \[
   \frac{1}{d_vn^2}\sum_{k,k'}\langle\Xm_z, \Xm_{z'}\rangle.
   \]
<br>   Here, \(\langle\Xm_z, \Xm_{z'}\rangle\) denotes the inner product of the vectors \(\Xm_z\) and \(\Xm_{z'}\).
<br>
<br>4. <i></i>Final Expression<i></i>:
<br>   Finally, the expectation is expressed in terms of the expected value of a function \(C(\Xm)\):
<br>   \[
   \frac{1}{d_vn^2}\Exp C(\Xm).
   \]
<br>   This step consolidates the previous results into a compact form involving the expected value of \(C(\Xm)\).
<br>
<br>This proof demonstrates how the uniform-attention assumption and the properties of i.i.d weights lead to the final expression for the expectation of the product of elements in \(\Sm\).
        </div>
    </section>




    <section id="Reference">
        <h3>Reference</h3>
            <p>Title: <a href="/library/papers/signal_propagation_in_transformers_theoretical_perspectives_and_the_role_of_rank_collapse/index.html">Signal Propagation in Transformers: Theoretical Perspectives and the Role of Rank Collapse</a></p>
            <p>Authors: Lorenzo Noci, Sotiris Anagnostidis, Luca Biggio, Antonio Orvieto, Sidak Pal Singh, and Aurelien Lucchi</p>
            <p>URL: <a href="https://arxiv.org/abs/2206.03126">https://arxiv.org/abs/2206.03126</a></p>
    </section>
    <section id="Resources">
        <!-- Bibtex Modal -->
        <div id="bibtexModal" class="modal">
        <div class="modal-content">
            <span class="close" onclick="closeBibtexModal()">&times;</span>
            <p>@misc{noci2022signalpropagationtransformerstheoretical,<br>&emsp;title={Signal Propagation in Transformers: Theoretical Perspectives and the Role of Rank Collapse}, <br>&emsp;author={Lorenzo Noci and Sotiris Anagnostidis and Luca Biggio and Antonio Orvieto and Sidak Pal Singh and Aurelien Lucchi},<br>&emsp;year={2022},<br>&emsp;eprint={2206.03126},<br>&emsp;archivePrefix={arXiv},<br>&emsp;primaryClass={cs.LG},<br>&emsp;url={https://arxiv.org/abs/2206.03126}}</p>
            <button class="modal-button" onclick="copyBibtex()">Copy</button>
        </div>
        </div>

        <!-- Latex Modal -->
        <div id="latexModal" class="modal">
        <div class="modal-content">
            <span class="close" onclick="closeLatexModal()">&times;</span>
            <p class="tex2jax_ignore"><code>\begin{lemma}[Expectation of Attention Layers]
\label{lemma:exp_softmax}
Under the uniform-attention assumption:
\begin{equation*}
    \Exp[\Sm_{kj}\Sm_{k'j}] = \frac{1}{d_vn^2}\Exp C(\Xm).
\end{equation*}
\end{lemma}</code></p>
            <button class="modal-button" onclick="copyLatex()">Copy</button>
        </div>
        </div>

        <h3>Resources</h3>    
            <p>
                <b>BibTex</b>:
                    <button onclick="copyBibtex()">Copy</button>
                    <button onclick="openBibtexModel()">Show</button>
            </p>
            <p>
                <b>LaTeX</b>:
                    <button onclick="copyLatex()">Copy</button>
                    <button onclick="openLatexModal()">Show</button>
            </p>
        <script>
            function copyBibtex() {
                var textToCopy = `@misc{noci2022signalpropagationtransformerstheoretical,
  title={Signal Propagation in Transformers: Theoretical Perspectives and the Role of Rank Collapse}, 
  author={Lorenzo Noci and Sotiris Anagnostidis and Luca Biggio and Antonio Orvieto and Sidak Pal Singh and Aurelien Lucchi},
  year={2022},
  eprint={2206.03126},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2206.03126}}`;
                navigator.clipboard.writeText(textToCopy).then(function() {
                    //alert('Text copied to clipboard: ' + textToCopy);
                }, function(err) {
                    console.error('Failed to copy text: ', err);
                });
            }
            
            function copyLatex() {
                var textToCopy = `\\begin{lemma}[Expectation of Attention Layers]
\\label{lemma:exp_softmax}
Under the uniform-attention assumption:
\\begin{equation*}
    \\Exp[\\Sm_{kj}\\Sm_{k'j}] = \\frac{1}{d_vn^2}\\Exp C(\\Xm).
\\end{equation*}
\\end{lemma}`;
                navigator.clipboard.writeText(textToCopy).then(function() {
                    //alert('Text copied to clipboard: ' + textToCopy);
                }, function(err) {
                    console.error('Failed to copy text: ', err);
                });
            }

            // Bibtex
            function openBibtexModel() {
                document.getElementById('bibtexModal').style.display = 'block';
            }
            function closeBibtexModal() {
                document.getElementById('bibtexModal').style.display = 'none';
            }

            // Latex
            function openLatexModal() {
                document.getElementById('latexModal').style.display = 'block';
            }
            function closeLatexModal() {
                document.getElementById('latexModal').style.display = 'none';
            }
        </script>
    </section>
</section>

    </main>
    <footer>
        <p>&copy; 2024 Theoretical Results on Transformers and Language Models</p>
    </footer>
</body>
</html>
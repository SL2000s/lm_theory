<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Index - LM Theory</title>
    <link rel="stylesheet" href="/assets/css/styles.css">
    
</head>
<body>
    <header>
        <h1>Theoretical Results on Transformers and Language Models</h1>
        <nav>
            <a href="/">Home</a>
            <a href="/examples">Examples</a>
            <a href="/contact">Contact</a>
            <a href="/contribute">Contribute</a>
            <div class="hover-container">
                <span class="hover-word"><a href="/library">Index</a></span>
                <div class="dropdown-content">
                    <a href="/library">Full Index</a>
                    <a href="/library/papers">Papers</a>
                    <a href="/library/definitions">Definitions</a>
                    <a href="/library/axioms">Axioms</a>
                    <a href="/library/lemmas">Lemmas</a>
                    <a href="/library/theorems">Theorems</a>
                    <a href="/library/corollaries">Corollaries</a>
                </div>
            </div>
        </nav>
    </header>
    <main>
        
<section id="index">
    <h2>Full Index</h2>
    <p>This page lists all the definitions, axioms, lemmas, theorems, and corollaries available in our repository. The papers used can be found <a href="/library/papers/index.html">here</a>.</p>
    
    <section id="definition">
        <h3><a href="/library/definitions/index.html">Definitions</a></h3>
        <ul>
        
            <li><a href="/library/definitions/definition_1/index.html">Definition 1: Lipschitz Continuity</a></li>
        
            <li><a href="/library/definitions/definition_2/index.html">Definition 2: Linear Concept Separability</a></li>
        
            <li><a href="/library/definitions/definition_3/index.html">Definition 3: Subordination Hierarchy</a></li>
        
            <li><a href="/library/definitions/definition_4/index.html">Definition 4: Linear Concept Representation</a></li>
        
            <li><a href="/library/definitions/definition_5/index.html">Definition 5: Vector Magnitude Maximization</a></li>
        
            <li><a href="/library/definitions/definition_6/index.html">Definition 6: Polytope Representation of Categorical Concepts</a></li>
        
            <li><a href="/library/definitions/definition_7/index.html">Definition 7: Locally-Optimal Indices and Direction</a></li>
        
            <li><a href="/library/definitions/definition_8/index.html">Definition 8: Token Scoring and Optimality Criterion</a></li>
        
            <li><a href="/library/definitions/definition_9/index.html">Definition 9: Low and High Score Token Separation Criterion</a></li>
        
            <li><a href="/library/definitions/definition_10/index.html">Definition 10: Toy Distribution for Self-Attention</a></li>
        
            <li><a href="/library/definitions/definition_11/index.html">Definition 11: Low and High Score Token Separation</a></li>
        
            <li><a href="/library/definitions/definition_12/index.html">Definition 12: Locally-Optimal Token Selection</a></li>
        
        </ul>
    </section>
    
    <section id="axiom">
        <h3><a href="/library/axioms/index.html">Axioms</a></h3>
        <ul>
        
        </ul>
    </section>
    
    <section id="lemma">
        <h3><a href="/library/lemmas/index.html">Lemmas</a></h3>
        <ul>
        
            <li><a href="/library/lemmas/lemma_1/index.html">Lemma 1: Lipschitz Composition Lemma</a></li>
        
            <li><a href="/library/lemmas/lemma_2/index.html">Lemma 2: Non-Lipschitz Condition for Distinct Full Rank Matrices</a></li>
        
            <li><a href="/library/lemmas/lemma_3/index.html">Lemma 3: Covariance Trace Bound</a></li>
        
            <li><a href="/library/lemmas/lemma_4/index.html">Lemma 4: Matrix Norm Bound Lemma</a></li>
        
            <li><a href="/library/lemmas/lemma_5/index.html">Lemma 5: Low-Rank Approximation Bound</a></li>
        
            <li><a href="/library/lemmas/lemma_6/index.html">Lemma 6: Block Row Norm Inequality</a></li>
        
            <li><a href="/library/lemmas/lemma_7/index.html">Lemma 7: Matrix Norm Bound</a></li>
        
            <li><a href="/library/lemmas/lemma_8/index.html">Lemma 8: Bounding Sum of Projected Differences</a></li>
        
            <li><a href="/library/lemmas/lemma_9/index.html">Lemma 9: Block Column Norm Inequality</a></li>
        
            <li><a href="/library/lemmas/lemma_10/index.html">Lemma 10: Properties of the Kronecker Product</a></li>
        
            <li><a href="/library/lemmas/lemma_11/index.html">Lemma 11: Gradients of Self Attention</a></li>
        
            <li><a href="/library/lemmas/lemma_12/index.html">Lemma 12: Gradients of Self Attention with respect to Embedding Matrix</a></li>
        
            <li><a href="/library/lemmas/lemma_13/index.html">Lemma 13: Gradients of Uniform-Attention Representations</a></li>
        
            <li><a href="/library/lemmas/lemma_14/index.html">Lemma 14: Expectation of Product of Linear Transformations</a></li>
        
            <li><a href="/library/lemmas/lemma_15/index.html">Lemma 15: Linearity of Expectation with Skip Connection</a></li>
        
            <li><a href="/library/lemmas/lemma_16/index.html">Lemma 16: Expectation of Softmax Products</a></li>
        
            <li><a href="/library/lemmas/lemma_17/index.html">Lemma 17: Exponential Growth of Inner Products</a></li>
        
            <li><a href="/library/lemmas/lemma_18/index.html">Lemma 18: Exponential Norm Growth in Deep Networks</a></li>
        
            <li><a href="/library/lemmas/lemma_19/index.html">Lemma 19: Vanishing Attention Lemma</a></li>
        
            <li><a href="/library/lemmas/lemma_20/index.html">Lemma 20: Borel-Cantelli Lemma</a></li>
        
            <li><a href="/library/lemmas/lemma_21/index.html">Lemma 21: Optimal Token Separation Lemma</a></li>
        
            <li><a href="/library/lemmas/lemma_22/index.html">Lemma 22: Equivalence of Solution Sets in Constrained Optimization</a></li>
        
            <li><a href="/library/lemmas/lemma_23/index.html">Lemma 23: Lipschitz Continuity of Gradients Lemma</a></li>
        
            <li><a href="/library/lemmas/lemma_24/index.html">Lemma 24: Rank Bound Lemma</a></li>
        
            <li><a href="/library/lemmas/lemma_25/index.html">Lemma 25: Cone Membership Lemma</a></li>
        
            <li><a href="/library/lemmas/lemma_26/index.html">Lemma 26: Sparse Softmax Optimization Lemma</a></li>
        
            <li><a href="/library/lemmas/lemma_27/index.html">Lemma 27: Cone Membership Lemma</a></li>
        
            <li><a href="/library/lemmas/lemma_28/index.html">Lemma 28: Equivalence of Regularization Paths</a></li>
        
            <li><a href="/library/lemmas/lemma_29/index.html">Lemma 29: 'Quadratic Reduction Lemma'</a></li>
        
            <li><a href="/library/lemmas/lemma_30/index.html">Lemma 30: Gradient Descent Convergence Lemma</a></li>
        
            <li><a href="/library/lemmas/lemma_31/index.html">Lemma 31: Global Descent Lemma</a></li>
        
            <li><a href="/library/lemmas/lemma_32/index.html">Lemma 32: Global Optimality Conditions</a></li>
        
            <li><a href="/library/lemmas/lemma_33/index.html">Lemma 33: Cone Gradient Inequality Lemma</a></li>
        
            <li><a href="/library/lemmas/lemma_34/index.html">Lemma 34: Local Optimality Conditions for SVM Solutions</a></li>
        
            <li><a href="/library/lemmas/lemma_35/index.html">Lemma 35: Local Correlation Inequality</a></li>
        
            <li><a href="/library/lemmas/lemma_36/index.html">Lemma 36: Gradient Cone Lemma</a></li>
        
            <li><a href="/library/lemmas/lemma_37/index.html">Lemma 37: Full Row-Rank Concatenation Lemma</a></li>
        
        </ul>
    </section>
    
    <section id="theorem">
        <h3><a href="/library/theorems/index.html">Theorems</a></h3>
        <ul>
        
            <li><a href="/library/theorems/theorem_1/index.html">Theorem 1: Jacobian Lipschitz Norm Theorem</a></li>
        
            <li><a href="/library/theorems/theorem_2/index.html">Theorem 2: Non-Lipschitzness of DP-MHA</a></li>
        
            <li><a href="/library/theorems/theorem_3/index.html">Theorem 3: Lipschitz Bound for L2-MHA</a></li>
        
            <li><a href="/library/theorems/theorem_4/index.html">Theorem 4: Orthogonal Attention Subspaces</a></li>
        
            <li><a href="/library/theorems/theorem_5/index.html">Theorem 5: Orthogonal Spheres Theorem</a></li>
        
            <li><a href="/library/theorems/theorem_6/index.html">Theorem 6: Semantic Orthogonality Theorem</a></li>
        
            <li><a href="/library/theorems/theorem_7/index.html">Theorem 7: Propagation of Infinitesimal Perturbations in Attention Mechanisms</a></li>
        
            <li><a href="/library/theorems/theorem_8/index.html">Theorem 8: Stability of Sparse Attention</a></li>
        
            <li><a href="/library/theorems/theorem_9/index.html">Theorem 9: Stability of Isotropic Attention</a></li>
        
            <li><a href="/library/theorems/theorem_10/index.html">Theorem 10: Sensitivity of Sparse Attention to Multiplicative Perturbations</a></li>
        
            <li><a href="/library/theorems/theorem_11/index.html">Theorem 11: Multiplicative Stability in Isotropic Attention</a></li>
        
            <li><a href="/library/theorems/theorem_12/index.html">Theorem 12: Shift Invariance of Attention</a></li>
        
            <li><a href="/library/theorems/theorem_13/index.html">Theorem 13: Attention Scaling Theorem</a></li>
        
            <li><a href="/library/theorems/theorem_14/index.html">Theorem 14: Inverse-Temperature Projection Theorem</a></li>
        
            <li><a href="/library/theorems/theorem_15/index.html">Theorem 15: Bias Nullification Theorem</a></li>
        
            <li><a href="/library/theorems/theorem_16/index.html">Theorem 16: Magnitude Consistency Theorem</a></li>
        
            <li><a href="/library/theorems/theorem_17/index.html">Theorem 17: Hierarchical Orthogonality Theorem</a></li>
        
            <li><a href="/library/theorems/theorem_18/index.html">Theorem 18: Simplex Representation Theorem</a></li>
        
            <li><a href="/library/theorems/theorem_19/index.html">Theorem 19: Isserlis' theorem</a></li>
        
            <li><a href="/library/theorems/theorem_20/index.html">Theorem 20: Asymptotic Uniformity of Softmax Attention</a></li>
        
            <li><a href="/library/theorems/theorem_21/index.html">Theorem 21: Regularization Path Bias Theorem</a></li>
        
            <li><a href="/library/theorems/theorem_22/index.html">Theorem 22: Divergence of Weight Norms in Gradient Descent</a></li>
        
            <li><a href="/library/theorems/theorem_23/index.html">Theorem 23: Global Convergence of Gradient Descent with Initial Gradient Condition</a></li>
        
            <li><a href="/library/theorems/theorem_24/index.html">Theorem 24: Local Gradient Descent Convergence Theorem</a></li>
        
            <li><a href="/library/theorems/theorem_25/index.html">Theorem 25: Separation Theorem for Self-Attention</a></li>
        
            <li><a href="/library/theorems/theorem_26/index.html">Theorem 26: Separation Feasibility Theorem</a></li>
        
            <li><a href="/library/theorems/theorem_27/index.html">Theorem 27: Convergence of Local Regularization Paths</a></li>
        
            <li><a href="/library/theorems/theorem_28/index.html">Theorem 28: Gradient Initialization in Dataset Models</a></li>
        
            <li><a href="/library/theorems/theorem_29/index.html">Theorem 29: Convergence of Conic Regularization Paths</a></li>
        
            <li><a href="/library/theorems/theorem_30/index.html">Theorem 30: Global Convergence of Gradient Descent with Initial Gradient Assumption</a></li>
        
            <li><a href="/library/theorems/theorem_31/index.html">Theorem 31: Divergence of Norms in Gradient Descent</a></li>
        
        </ul>
    </section>
    
    <section id="corollary">
        <h3><a href="/library/corollaries/index.html">Corollaries</a></h3>
        <ul>
        
            <li><a href="/library/corollaries/corollary_1/index.html">Corollary 1: Lipschitz Bound for Neural Networks</a></li>
        
            <li><a href="/library/corollaries/corollary_2/index.html">Corollary 2: Global Convergence of Regularization Path</a></li>
        
            <li><a href="/library/corollaries/corollary_3/index.html">Corollary 3: Asymptotic Regularization Path Convergence</a></li>
        
        </ul>
    </section>
    
</section>

    </main>
    <footer>
        <p>&copy; 2024 Theoretical Results on Transformers and Language Models</p>
    </footer>
</body>
</html>
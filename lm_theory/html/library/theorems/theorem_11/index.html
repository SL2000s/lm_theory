<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Theorem 11 - LM Theory</title>
    <link rel="stylesheet" href="/assets/css/styles.css">
    
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>
    window.MathJax = {
        loader: {load: ['[tex]/mathtools']},
        tex: {
            tags: 'all',  // Automatically number all display equations
            // loader: {load: ['[tex]/autoload']},               // TODO: remove
            // packages: {'[+]': ['autoload', 'mathtools']},     // TODO: remove
            packages: {'[+]': ['mathtools']},
            autoload: {
                coloneqq: ['mathtools']
            },
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            macros: {
                emph: ["\\textit{#1}", 1],
mathds: ["\\mathbf{#1}", 1],
bm: ["\\boldsymbol{\\mathbf{#1}}", 1],
data: ["\\textcolor{Maroon}{\\texttt{{#1}}}", 1]

            },
        },
        options: {
            renderActions: {
                addMenu: []
            },
            ignoreHtmlClass: "tex2jax_ignore",
            skipHtmlTags: ["script", "noscript", "style", "textarea", "pre", "code"]
        }
    };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <header>
        <h1>Theoretical Results on Transformers and Language Models</h1>
        <nav>
            <a href="/">Home</a>
            <a href="/examples">Examples</a>
            <a href="/contact">Contact</a>
            <a href="/contribute">Contribute</a>
            <div class="hover-container">
                <span class="hover-word"><a href="/library">Index</a></span>
                <div class="dropdown-content">
                    <a href="/library">Full Index</a>
                    <a href="/library/papers">Papers</a>
                    <a href="/library/definitions">Definitions</a>
                    <a href="/library/axioms">Axioms</a>
                    <a href="/library/lemmas">Lemmas</a>
                    <a href="/library/theorems">Theorems</a>
                    <a href="/library/corollaries">Corollaries</a>
                </div>
            </div>
        </nav>
    </header>
    <main>
        
<section id="Multiplicative Stability in Isotropic Attention">
    <h2>Theorem 11: Multiplicative Stability in Isotropic Attention</h2>
    <section id="Statement">
        <h3>Statement</h3>
        <div>
            Sensitivity of isotropic attention to multiplicative perturbations. Say $\epsilon^k = \kappa^k_t k_t$ with $\kappa^k_t\ll1$ where $\{\kappa_t\}$ have comparable amplitudes. Then
\begin{equation}
    \epsilon^{\Delta x(k)} 
    ~\approx~
    \begin{cases}
    0 ~&~ \text{if $\kappa_t$ independent of ${\tilde m}_t$, by symmetry} \\
    0 ~&~ \text{if $\kappa_t\equiv\kappa$ for constant $\kappa$} \\
    0 ~&~ \text{if $q=0$} \\
    w \langle {\tilde m}_t \kappa^k_t \rangle_t  ~&~ \text{otherwise}
    \end{cases}
\end{equation}
        </div>
    </section>
    <section id="motivation">
        <h3>Motivation of Statement (AI-Generated)</h3>
        <div>
            This statement is useful in the context of analyzing the sensitivity of isotropic attention mechanisms to small multiplicative perturbations. It provides conditions under which the perturbations have negligible effects, which can be crucial for understanding the stability and robustness of such systems. Specifically, it tells us that if the perturbations are independent of the variable ${\tilde m}_t$, constant, or if $q=0$, the effect is approximately zero. Otherwise, the effect is proportional to the weighted average of the product of ${\tilde m}_t$ and the perturbation $\kappa^k_t$. This can be particularly useful in designing and evaluating attention mechanisms in machine learning models.
        </div>
    </section>
        
<section id="OriginalProof">
<h3>Original Proof</h3>
<em>Proof.</em> We begin with the following result from <a href="/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_9/index.html#theorem%3A+stability%3A+isotropic">Theorem 9</a>:
\begin{equation}
\epsilon^{\Delta x(k)} ~\xrightarrow[\epsilon^k_t\rightarrow0]{\mathrm{~~perturb~k~~}}~ \langle {\tilde m}_t {\epsilon^k_t}^T \rangle_t ~q
\end{equation}
Substituting $\epsilon^k = \kappa^k_t k_t$ and taking $q$ inside the brackets gives
\begin{equation}
\langle{{\tilde m}_t  \epsilon^k_t}^T \rangle_t ~q ~=~ 
\langle {\tilde m}_t \kappa_t {k_t}^T \rangle_t q ~=~
 ~ \langle {\tilde m}_t \kappa_t w_t \rangle_t
\end{equation}
We then notice that isotropic attention requires that $w_t$ is a constant, which we call $w$. Then
\begin{equation}
\epsilon^{\Delta x(k)} ~\approx~ w \langle {\tilde m}_t \kappa_t \rangle_t
\end{equation}
is our general result. We then note three special cases, each resulting in $\epsilon^{\Delta x(k)}=0$:
<ol>
    <li>If $\kappa_t \perp {\tilde m}_t$ then $\langle {\tilde m}_t \kappa_t \rangle_t = \langle m_t \kappa_t \rangle_t - \langle m_t \rangle_t \langle \kappa_t \rangle_t = Cov(m_t,\kappa_t) = 0$. This is case when interference $\kappa_t^k$ on the keys is not dominated by the same semantic subspace as the message $m_t$.</li>
    <li>If all keys are perturbed by the same factor $\kappa_t\equiv\kappa$, then $\langle {\tilde m}_t \kappa_t \rangle_t = \kappa \langle {\tilde m}_t \rangle_t =0$ because $\langle {\tilde m}_t \rangle_t=0$.</li>
    <li>Isotropic attention can be achieved by either $q=0$ or $k_t=const$. If the case is $q=0$ then this implies $w=0$ also.</li>
</ol>
</section>

    <section id="proofExplanation">
        <h4>Explanation of Proof (AI-Generated)</h4>
        <div>
            

We start by referencing a result from <a href="/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_9/index.html#theorem%3A+stability%3A+isotropic">Theorem 9</a>:

\[
\epsilon^{\Delta x(k)} ~\xrightarrow[\epsilon^k_t\rightarrow0]{\mathrm{~~perturb~k~~}}~ \langle {\tilde m}_t {\epsilon^k_t}^T \rangle_t ~q
\]

Next, we substitute \(\epsilon^k = \kappa^k_t k_t\) and bring \(q\) inside the brackets:

\[
\langle{{\tilde m}_t  \epsilon^k_t}^T \rangle_t ~q ~=~ 
\langle {\tilde m}_t \kappa_t {k_t}^T \rangle_t q ~=~
 ~ \langle {\tilde m}_t \kappa_t w_t \rangle_t
\]

We then observe that isotropic attention requires \(w_t\) to be a constant, denoted as \(w\). Thus,

\[
\epsilon^{\Delta x(k)} ~\approx~ w \langle {\tilde m}_t \kappa_t \rangle_t
\]

This is our general result. We then consider three special cases, each leading to \(\epsilon^{\Delta x(k)}=0\):
<br><br>1. <i></i>Orthogonality Condition<i></i>: If \(\kappa_t \perp {\tilde m}_t\), then \(\langle {\tilde m}_t \kappa_t \rangle_t = \langle m_t \kappa_t \rangle_t - \langle m_t \rangle_t \langle \kappa_t \rangle_t = \text{Cov}(m_t,\kappa_t) = 0\). This occurs when the interference \(\kappa_t^k\) on the keys is not dominated by the same semantic subspace as the message \(m_t\).
<br><br>2. <i></i>Uniform Perturbation<i></i>: If all keys are perturbed by the same factor \(\kappa_t \equiv \kappa\), then \(\langle {\tilde m}_t \kappa_t \rangle_t = \kappa \langle {\tilde m}_t \rangle_t = 0\) because \(\langle {\tilde m}_t \rangle_t = 0\).
<br><br>3. <i></i>Isotropic Attention<i></i>: This can be achieved by either \(q = 0\) or \(k_t = \text{const}\). If \(q = 0\), it implies \(w = 0\) as well.
        </div>
    </section>




    <section id="Reference">
        <h3>Reference</h3>
            <p>Title: <a href="/library/papers/transformer_normalisation_layers_and_the_independence_of_semantic_subspaces/index.html">Transformer Normalisation Layers and the Independence of Semantic Subspaces</a></p>
            <p>Authors: Stephen Menary, Samuel Kaski, and Andre Freitas</p>
            <p>URL: <a href="https://arxiv.org/abs/2406.17837">https://arxiv.org/abs/2406.17837</a></p>
    </section>
    <section id="Resources">
        <!-- Bibtex Modal -->
        <div id="bibtexModal" class="modal">
        <div class="modal-content">
            <span class="close" onclick="closeBibtexModal()">&times;</span>
            <p>@misc{menary2024transformernormalisationlayersindependence,<br>&emsp;title={Transformer Normalisation Layers and the Independence of Semantic Subspaces}, <br>&emsp;author={Stephen Menary and Samuel Kaski and Andre Freitas},<br>&emsp;year={2024},<br>&emsp;eprint={2406.17837},<br>&emsp;archivePrefix={arXiv},<br>&emsp;primaryClass={cs.LG},<br>&emsp;url={https://arxiv.org/abs/2406.17837}<br>}</p>
            <button class="modal-button" onclick="copyBibtex()">Copy</button>
        </div>
        </div>

        <!-- Latex Modal -->
        <div id="latexModal" class="modal">
        <div class="modal-content">
            <span class="close" onclick="closeLatexModal()">&times;</span>
            <p class="tex2jax_ignore"><code>\begin{theorem}
    Sensitivity of isotropic attention to multiplicative perturbations. Say $\epsilon^k = \kappa^k_t k_t$ with $\kappa^k_t\ll1$ where $\{\kappa_t\}$ have comparable amplitudes. Then
    \begin{equation}
        \epsilon^{\Delta x(k)} 
        ~\approx~
        \begin{cases}
        0 ~&~ \text{if~$\kappa_t$~independent~of~${\tilde m}_t$,~by~symmetry} \\
        0 ~&~ \text{if~$\kappa_t\equiv\kappa$~for~constant~$\kappa$} \\
        0 ~&~ \text{if~$q=0$} \\
        w \langle {\tilde m}_t \kappa^k_t \rangle_t  ~&~ \text{otherwise}
        \end{cases}
    \end{equation}
\label{theorem: multiplicative stability: isotropic}
\end{theorem}</code></p>
            <button class="modal-button" onclick="copyLatex()">Copy</button>
        </div>
        </div>

        <h3>Resources</h3>    
            <p>
                <b>BibTex</b>:
                    <button onclick="copyBibtex()">Copy</button>
                    <button onclick="openBibtexModel()">Show</button>
            </p>
            <p>
                <b>LaTeX</b>:
                    <button onclick="copyLatex()">Copy</button>
                    <button onclick="openLatexModal()">Show</button>
            </p>
        <script>
            function copyBibtex() {
                var textToCopy = `@misc{menary2024transformernormalisationlayersindependence,
  title={Transformer Normalisation Layers and the Independence of Semantic Subspaces}, 
  author={Stephen Menary and Samuel Kaski and Andre Freitas},
  year={2024},
  eprint={2406.17837},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2406.17837}
}`;
                navigator.clipboard.writeText(textToCopy).then(function() {
                    //alert('Text copied to clipboard: ' + textToCopy);
                }, function(err) {
                    console.error('Failed to copy text: ', err);
                });
            }
            
            function copyLatex() {
                var textToCopy = `\\begin{theorem}
    Sensitivity of isotropic attention to multiplicative perturbations. Say $\\epsilon^k = \\kappa^k_t k_t$ with $\\kappa^k_t\\ll1$ where $\\{\\kappa_t\\}$ have comparable amplitudes. Then
    \\begin{equation}
        \\epsilon^{\\Delta x(k)} 
        ~\\approx~
        \\begin{cases}
        0 ~&~ \\text{if~$\\kappa_t$~independent~of~${\\tilde m}_t$,~by~symmetry} \\\\
        0 ~&~ \\text{if~$\\kappa_t\\equiv\\kappa$~for~constant~$\\kappa$} \\\\
        0 ~&~ \\text{if~$q=0$} \\\\
        w \\langle {\\tilde m}_t \\kappa^k_t \\rangle_t  ~&~ \\text{otherwise}
        \\end{cases}
    \\end{equation}
\\label{theorem: multiplicative stability: isotropic}
\\end{theorem}`;
                navigator.clipboard.writeText(textToCopy).then(function() {
                    //alert('Text copied to clipboard: ' + textToCopy);
                }, function(err) {
                    console.error('Failed to copy text: ', err);
                });
            }

            // Bibtex
            function openBibtexModel() {
                document.getElementById('bibtexModal').style.display = 'block';
            }
            function closeBibtexModal() {
                document.getElementById('bibtexModal').style.display = 'none';
            }

            // Latex
            function openLatexModal() {
                document.getElementById('latexModal').style.display = 'block';
            }
            function closeLatexModal() {
                document.getElementById('latexModal').style.display = 'none';
            }
        </script>
    </section>
</section>

    </main>
    <footer>
        <p>&copy; 2024 Theoretical Results on Transformers and Language Models</p>
    </footer>
</body>
</html>
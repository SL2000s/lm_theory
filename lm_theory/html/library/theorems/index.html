<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Theorems - LM Theory</title>
    <link rel="stylesheet" href="/assets/css/styles.css">
    
</head>
<body>
    <header>
        <h1>Theoretical Results on Transformers and Language Models</h1>
        <nav>
            <a href="/">Home</a>
            <a href="/examples">Examples</a>
            <a href="/contact">Contact</a>
            <a href="/contribute">Contribute</a>
            <div class="hover-container">
                <span class="hover-word"><a href="/library">Index</a></span>
                <div class="dropdown-content">
                    <a href="/library">Full Index</a>
                    <a href="/library/papers">Papers</a>
                    <a href="/library/definitions">Definitions</a>
                    <a href="/library/axioms">Axioms</a>
                    <a href="/library/lemmas">Lemmas</a>
                    <a href="/library/theorems">Theorems</a>
                    <a href="/library/corollaries">Corollaries</a>
                </div>
            </div>
        </nav>
    </header>
    <main>
        
<section id="theorems">
    <h2>Theorems - Index</h2>
    <p>This page lists all the theorems available in our repository.</p>
    <section id="">
        <h3><a href="/library/theorems/index.html">Theorems</a></h3>
        <ul>
        
            <li><a href="/library/theorems/theorem_1/index.html">Theorem 1: Jacobian Lipschitz Norm Theorem</a></li>
        
            <li><a href="/library/theorems/theorem_2/index.html">Theorem 2: Non-Lipschitzness of DP-MHA</a></li>
        
            <li><a href="/library/theorems/theorem_3/index.html">Theorem 3: Lipschitz Bound for L2-MHA</a></li>
        
            <li><a href="/library/theorems/theorem_4/index.html">Theorem 4: Orthogonal Attention Subspaces</a></li>
        
            <li><a href="/library/theorems/theorem_5/index.html">Theorem 5: Orthogonal Spheres Theorem</a></li>
        
            <li><a href="/library/theorems/theorem_6/index.html">Theorem 6: Semantic Orthogonality Theorem</a></li>
        
            <li><a href="/library/theorems/theorem_7/index.html">Theorem 7: Propagation of Infinitesimal Perturbations in Attention Mechanisms</a></li>
        
            <li><a href="/library/theorems/theorem_8/index.html">Theorem 8: Stability of Sparse Attention</a></li>
        
            <li><a href="/library/theorems/theorem_9/index.html">Theorem 9: Stability of Isotropic Attention</a></li>
        
            <li><a href="/library/theorems/theorem_10/index.html">Theorem 10: Sensitivity of Sparse Attention to Multiplicative Perturbations</a></li>
        
            <li><a href="/library/theorems/theorem_11/index.html">Theorem 11: Multiplicative Stability in Isotropic Attention</a></li>
        
            <li><a href="/library/theorems/theorem_12/index.html">Theorem 12: Shift Invariance of Attention</a></li>
        
            <li><a href="/library/theorems/theorem_13/index.html">Theorem 13: Attention Scaling Theorem</a></li>
        
            <li><a href="/library/theorems/theorem_14/index.html">Theorem 14: Inverse-Temperature Projection Theorem</a></li>
        
            <li><a href="/library/theorems/theorem_15/index.html">Theorem 15: Bias Nullification Theorem</a></li>
        
            <li><a href="/library/theorems/theorem_16/index.html">Theorem 16: Magnitude Consistency Theorem</a></li>
        
            <li><a href="/library/theorems/theorem_17/index.html">Theorem 17: Hierarchical Orthogonality Theorem</a></li>
        
            <li><a href="/library/theorems/theorem_18/index.html">Theorem 18: Simplex Representation Theorem</a></li>
        
            <li><a href="/library/theorems/theorem_19/index.html">Theorem 19: Isserlis' theorem</a></li>
        
            <li><a href="/library/theorems/theorem_20/index.html">Theorem 20: Asymptotic Uniformity of Softmax Attention</a></li>
        
            <li><a href="/library/theorems/theorem_21/index.html">Theorem 21: Regularization Path Bias Theorem</a></li>
        
            <li><a href="/library/theorems/theorem_22/index.html">Theorem 22: Divergence of Weight Norms in Gradient Descent</a></li>
        
            <li><a href="/library/theorems/theorem_23/index.html">Theorem 23: Global Convergence of Gradient Descent with Initial Gradient Condition</a></li>
        
            <li><a href="/library/theorems/theorem_24/index.html">Theorem 24: Local Gradient Descent Convergence Theorem</a></li>
        
            <li><a href="/library/theorems/theorem_25/index.html">Theorem 25: Separation Theorem for Self-Attention</a></li>
        
            <li><a href="/library/theorems/theorem_26/index.html">Theorem 26: Separation Feasibility Theorem</a></li>
        
            <li><a href="/library/theorems/theorem_27/index.html">Theorem 27: Convergence of Local Regularization Paths</a></li>
        
            <li><a href="/library/theorems/theorem_28/index.html">Theorem 28: Gradient Initialization in Dataset Models</a></li>
        
            <li><a href="/library/theorems/theorem_29/index.html">Theorem 29: Convergence of Conic Regularization Paths</a></li>
        
            <li><a href="/library/theorems/theorem_30/index.html">Theorem 30: Global Convergence of Gradient Descent with Initial Gradient Assumption</a></li>
        
            <li><a href="/library/theorems/theorem_31/index.html">Theorem 31: Divergence of Norms in Gradient Descent</a></li>
        
        </ul>
    </section>
</section>

    </main>
    <footer>
        <p>&copy; 2024 Theoretical Results on Transformers and Language Models</p>
    </footer>
</body>
</html>
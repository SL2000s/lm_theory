<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Theorem 5 - LM Theory</title>
    <link rel="stylesheet" href="/assets/css/styles.css">
    
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>
    window.MathJax = {
        loader: {load: ['[tex]/mathtools']},
        tex: {
            tags: 'all',  // Automatically number all display equations
            // loader: {load: ['[tex]/autoload']},               // TODO: remove
            // packages: {'[+]': ['autoload', 'mathtools']},     // TODO: remove
            packages: {'[+]': ['mathtools']},
            autoload: {
                coloneqq: ['mathtools']
            },
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            macros: {
                emph: ["\\textit{#1}", 1],
bm: ["\\boldsymbol{\\mathbf{#1}}", 1],
mathds: ["\\mathbf{#1}", 1],
textsl: ["\\textit{#1}", 1],
vspace: ["", 1],
xspace: "",
data: ["\\textcolor{Maroon}{\\texttt{{#1}}}", 1]

            },
            environments: {
                subequations: ["{", "}"]

            },
        },
        options: {
            renderActions: {
                addMenu: []
            },
            ignoreHtmlClass: "tex2jax_ignore",
            skipHtmlTags: ["script", "noscript", "style", "textarea", "pre", "code"]
        }
    };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <header>
        <h1>Theoretical Results on Transformers and Language Models</h1>
        <nav>
            <a href="/">Home</a>
            <a href="/examples">Examples</a>
            <a href="/contact">Contact</a>
            <a href="/contribute">Contribute</a>
            <div class="hover-container">
                <span class="hover-word"><a href="/library">Index</a></span>
                <div class="dropdown-content">
                    <a href="/library">Full Index</a>
                    <a href="/library/papers">Papers</a>
                    <a href="/library/definitions">Definitions</a>
                    <a href="/library/axioms">Axioms</a>
                    <a href="/library/lemmas">Lemmas</a>
                    <a href="/library/theorems">Theorems</a>
                    <a href="/library/corollaries">Corollaries</a>
                </div>
            </div>
        </nav>
    </header>
    <main>
        
<section id="Orthogonal Spheres Theorem">
    <h2>Theorem 5: Orthogonal Spheres Theorem</h2>
    <section id="Statement">
        <h3>Statement</h3>
        <div>
            $\texttt{Pre-Norm}$: Semantic subspaces must be represented as orthogonal spheres $\mathbb{S}^{N_\alpha}\equiv\mathcal{S}^{N_\alpha-1}$ defined using the $L_2$-norm. Corollary: if either orthogonality or constant-norm are violated, semantic subspaces interfere through a multiplicative factor on $w_t$.
        </div>
    </section>
    <section id="motivation">
        <h3>Motivation of Statement (AI-Generated)</h3>
        <div>
            The concept of $\texttt{Pre-Norm}$ is crucial in the context of semantic subspaces, particularly in ensuring that these subspaces are represented as orthogonal spheres. This orthogonality and constant-norm condition, defined using the $L_2$-norm, helps in maintaining the integrity of the subspaces. The corollary highlights the importance of these conditions by stating that any violation leads to interference in the semantic subspaces, affecting the multiplicative factor on $w_t$. This is particularly useful in fields like machine learning and data representation, where maintaining orthogonal and normalized subspaces can significantly enhance the performance and accuracy of models.
        </div>
    </section>
        
<section id="OriginalProof">
<h3>Original Proof</h3>
Write 
\begin{equation}
x(\theta_A,\theta_B,\phi) ~=~ x_A(\theta_A) ~+~ x_B(\theta_B) ~+~ x_{AB}(\theta_A,\theta_B) ~+~ x_{other}(\theta_A,\theta_B,\phi)
\end{equation}
Then for head A we have
\begin{equation}
    w_t^{(A)}(\theta_A) ~=~ \frac{1}{\left|y_t\right|\left|x(\theta_A,\theta_B,\phi) \right|} {w^*_t}^{(A)}(\theta_A)
\end{equation}
where $w^*_t$ are the attention scores from the $\texttt{No-Norm}$ case, which requires $x_A(\theta_A)$ and $x_B(\theta_B)$ to be linearly independent. Now we additionally require $\left|x(\theta_A,\theta_B,\phi) \right| \perp \theta_B,\phi$, with
\begin{equation}
|x| ~=~ \sqrt{|x_A|^2 ~+~ |x_B ~+~ x_{AB} ~+~ x_{other}|^2 ~+~ 2 x_A^T \left(x_B ~+~ x_{AB} ~+~ x_{other}\right)}
\end{equation}
where we suppress parameter dependence for readability. Since $\sqrt{\cdot}$ is a monotonic function, this can only be satisfied if
\begin{equation}
|x_A|^2 ~+~ |x_B ~+~ x_{AB} ~+~ x_{other}|^2 ~+~ 2 x_A^T \left(x_B ~+~ x_{AB} ~+~ x_{other}\right) ~\perp~ \theta_B,\phi
\end{equation}
Repeating this process for head B gives
\begin{equation}
|x_B|^2 ~+~ |x_A ~+~ x_{AB} ~+~ x_{other}|^2 ~+~ 2 x_B^T \left(x_A ~+~ x_{AB} ~+~ x_{other}\right) ~\perp~ \theta_A,\phi
\end{equation}
Combining and collecting dependencies, we then have
\begin{align}
    |x_A|^2 ~=~ const ~~~&\forall~~~ \theta_A \\
    |x_B|^2 ~=~ const ~~~&\forall~~~ \theta_B \\
    
    \left( x_{AB} ~+~ 2x_A ~+~ 2x_B \right)^T x_{AB} ~+~ 2x_A^Tx_B ~=~ const ~~~&\forall~~~ \theta_A,\theta_B \\
    \left(x_{other} + 2x_A + 2x_B + 2x_{AB}\right)^T x_{other} ~=~ const ~~~&\forall~~~ \theta_A,\theta_B,\phi
\end{align}
We can go one step further, noticing that each individual term carries a different functional dependence, and so must independently be constant<sup>N.B. If $|x_{AB}|^2 \propto x_A^Tx_B$ then $|x_{AB}|^2=const$ reduces to $x_A^Tx_B=const$, which is already required.</sup>. We then have $\forall~~\mu,\nu\in\{A,B,AB,other\}$
\begin{equation}
    |x_\mu|=const   ~~~~~~\mathrm{and}~~~~~~  x_\mu^Tx_\nu=const 
\end{equation}
The requirements $|x_A(\theta_A)|=const ~\forall~\theta_A$ and $|x_B(\theta_B)|=const ~\forall~\theta_B$ mean that the semantic subspaces have a spherical structure defined by the $L_2$-norm $|\cdot|$.

Now consider the requirement $x_A(\theta_A)^Tx_B(\theta_B)=const$. Say that $\theta_A$ and $\theta_B$ have $N_A$ and $N_B$ degrees of freedom, meaning that $x_A$ and $x_B$ have $N_A-1$ and $N_B-1$ respectively, since they each lose one by confinement to the sphere. Say that the constant is nonzero such that $x_A^Tx_B \neq 0$. This means that there must be some direction $i$ for which $x_{Ai}x_{Bi} \neq0$. If we know all $N_A-1$ coordinates of $x_A$, and all $N_B - 2$ coordinates of $x_B$ except for direction $i$, then we also know the value of $x_{Bi}$, because it is fixed by the constant. However, this would mean that $x_A$ and $x_B$ are not independent, violating the condition $\theta_A \perp \theta_B$. The only way to satisfy independence is if $x_{Ai}x_{Bi}=0~\forall~i$, ensuring that degrees of freedom on $x_A$ and $x_B$ never become entangled. Therefore, to satisfy semantic independence, we must have $x_A(\theta_A)^Tx_B(\theta_B)=0 ~\forall~\theta_A,\theta_B$. This means that the subspaces are not just linearly independent, but orthogonal.

We have shown the proof for semantic subspaces of $x$. As for <a href="/remote/idiap.svm/temp.rea01/sljungbeck/lm_theory_library/lm_theory_library/generated_pages/library/theorems/theorem_4/index.html#theorem%3A+structure%3A+no-norm">Theorem 4</a>, the same structure must be true for $y_t$ by symmetry.
</section>

    <section id="proofExplanation">
        <h4>Explanation of Proof (AI-Generated)</h4>
        <div>
            The proof involves several steps to demonstrate the orthogonality and independence of semantic subspaces. Here is a breakdown of the steps:
<br>
<br>1. <i></i>Expression of \( x \)<i></i>:
<br>   The function \( x \) is decomposed into four components:
<br>   \[
   x(\theta_A,\theta_B,\phi) = x_A(\theta_A) + x_B(\theta_B) + x_{AB}(\theta_A,\theta_B) + x_{other}(\theta_A,\theta_B,\phi)
   \]
<br>
<br>2. <i></i>Weight for Head A<i></i>:
<br>   The weight \( w_t^{(A)} \) for head A is defined as:
<br>   \[
   w_t^{(A)}(\theta_A) = \frac{1}{\left|y_t\right|\left|x(\theta_A,\theta_B,\phi) \right|} {w^*_t}^{(A)}(\theta_A)
   \]
<br>   Here, \( w^*_t \) are the attention scores from the \(\texttt{No-Norm}\) case, requiring \( x_A(\theta_A) \) and \( x_B(\theta_B) \) to be linearly independent.
<br>
<br>3. <i></i>Norm Condition<i></i>:
<br>   The norm \( \left|x(\theta_A,\theta_B,\phi) \right| \) must be independent of \( \theta_B \) and \( \phi \):
<br>   \[
   |x| = \sqrt{|x_A|^2 + |x_B + x_{AB} + x_{other}|^2 + 2 x_A^T (x_B + x_{AB} + x_{other})}
   \]
<br>
<br>4. <i></i>Orthogonality Condition for Head A<i></i>:
<br>   This condition can only be satisfied if:
<br>   \[
   |x_A|^2 + |x_B + x_{AB} + x_{other}|^2 + 2 x_A^T (x_B + x_{AB} + x_{other}) \perp \theta_B,\phi
   \]
<br>
<br>5. <i></i>Repeating for Head B<i></i>:
<br>   Similarly, for head B:
<br>   \[
   |x_B|^2 + |x_A + x_{AB} + x_{other}|^2 + 2 x_B^T (x_A + x_{AB} + x_{other}) \perp \theta_A,\phi
   \]
<br>
<br>6. <i></i>Combining Dependencies<i></i>:
<br>   Combining the conditions, we get:
<br>   \[
   |x_A|^2 = const \quad \forall \theta_A
   \]
<br>   \[
   |x_B|^2 = const \quad \forall \theta_B
   \]
<br>   \[
   (x_{AB} + 2x_A + 2x_B)^T x_{AB} + 2x_A^T x_B = const \quad \forall \theta_A,\theta_B
   \]
<br>   \[
   (x_{other} + 2x_A + 2x_B + 2x_{AB})^T x_{other} = const \quad \forall \theta_A,\theta_B,\phi
   \]
<br>
<br>7. <i></i>Independence of Terms<i></i>:
<br>   Each term must independently be constant:
<br>   \[
   |x_\mu| = const \quad \text{and} \quad x_\mu^T x_\nu = const \quad \forall \mu,\nu \in \{A,B,AB,other\}
   \]
<br>
<br>8. <i></i>Spherical Structure<i></i>:
<br>   The requirements \( |x_A(\theta_A)| = const \forall \theta_A \) and \( |x_B(\theta_B)| = const \forall \theta_B \) imply that the semantic subspaces have a spherical structure defined by the \( L_2 \)-norm.
<br>
<br>9. <i></i>Orthogonality of Subspaces<i></i>:
<br>   To ensure semantic independence, \( x_A(\theta_A)^T x_B(\theta_B) = const \) must be zero:
<br>   \[
   x_A(\theta_A)^T x_B(\theta_B) = 0 \quad \forall \theta_A,\theta_B
   \]
<br>   This means the subspaces are orthogonal.
<br>
<br>10. <i></i>Symmetry for \( y_t \)<i></i>:
<br>    By symmetry, the same structure must hold for \( y_t \) as stated in Theorem 4.
<br>
<br>This proof demonstrates that the semantic subspaces are not only linearly independent but also orthogonal, ensuring the required independence and structure.
        </div>
    </section>




    <section id="Reference">
        <h3>Reference</h3>
            <p>Title: <a href="/library/papers/transformer_normalisation_layers_and_the_independence_of_semantic_subspaces/index.html">Transformer Normalisation Layers and the Independence of Semantic Subspaces</a></p>
            <p>Authors: Stephen Menary, Samuel Kaski, and Andre Freitas</p>
            <p>URL: <a href="https://arxiv.org/abs/2406.17837">https://arxiv.org/abs/2406.17837</a></p>
    </section>
    <section id="Resources">
        <!-- Bibtex Modal -->
        <div id="bibtexModal" class="modal">
        <div class="modal-content">
            <span class="close" onclick="closeBibtexModal()">&times;</span>
            <p>@misc{menary2024transformernormalisationlayersindependence,<br>&emsp;title={Transformer Normalisation Layers and the Independence of Semantic Subspaces}, <br>&emsp;author={Stephen Menary and Samuel Kaski and Andre Freitas},<br>&emsp;year={2024},<br>&emsp;eprint={2406.17837},<br>&emsp;archivePrefix={arXiv},<br>&emsp;primaryClass={cs.LG},<br>&emsp;url={https://arxiv.org/abs/2406.17837}<br>}</p>
            <button class="modal-button" onclick="copyBibtex()">Copy</button>
        </div>
        </div>

        <!-- Latex Modal -->
        <div id="latexModal" class="modal">
        <div class="modal-content">
            <span class="close" onclick="closeLatexModal()">&times;</span>
            <p class="tex2jax_ignore"><code>\begin{theorem}
    \texttt{Pre-Norm}: Semantic subspaces must be represented as orthogonal spheres $\mathbb{S}^{N_\alpha}\equiv\mathcal{S}^{N_\alpha-1}$ defined using the $L_2$-norm. Corollary: if either orthogonality or constant-norm are violated, semantic subspaces interfere through a multiplicative factor on $w_t$.
\label{theorem: structure: pre-norm}
\end{theorem}</code></p>
            <button class="modal-button" onclick="copyLatex()">Copy</button>
        </div>
        </div>

        <h3>Resources</h3>    
            <p>
                <b>BibTex</b>:
                    <button onclick="copyBibtex()">Copy</button>
                    <button onclick="openBibtexModel()">Show</button>
            </p>
            <p>
                <b>LaTeX</b>:
                    <button onclick="copyLatex()">Copy</button>
                    <button onclick="openLatexModal()">Show</button>
            </p>
        <script>
            function copyBibtex() {
                var textToCopy = `@misc{menary2024transformernormalisationlayersindependence,
  title={Transformer Normalisation Layers and the Independence of Semantic Subspaces}, 
  author={Stephen Menary and Samuel Kaski and Andre Freitas},
  year={2024},
  eprint={2406.17837},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2406.17837}
}`;
                navigator.clipboard.writeText(textToCopy).then(function() {
                    //alert('Text copied to clipboard: ' + textToCopy);
                }, function(err) {
                    console.error('Failed to copy text: ', err);
                });
            }
            
            function copyLatex() {
                var textToCopy = `\\begin{theorem}
    \\texttt{Pre-Norm}: Semantic subspaces must be represented as orthogonal spheres $\\mathbb{S}^{N_\\alpha}\\equiv\\mathcal{S}^{N_\\alpha-1}$ defined using the $L_2$-norm. Corollary: if either orthogonality or constant-norm are violated, semantic subspaces interfere through a multiplicative factor on $w_t$.
\\label{theorem: structure: pre-norm}
\\end{theorem}`;
                navigator.clipboard.writeText(textToCopy).then(function() {
                    //alert('Text copied to clipboard: ' + textToCopy);
                }, function(err) {
                    console.error('Failed to copy text: ', err);
                });
            }

            // Bibtex
            function openBibtexModel() {
                document.getElementById('bibtexModal').style.display = 'block';
            }
            function closeBibtexModal() {
                document.getElementById('bibtexModal').style.display = 'none';
            }

            // Latex
            function openLatexModal() {
                document.getElementById('latexModal').style.display = 'block';
            }
            function closeLatexModal() {
                document.getElementById('latexModal').style.display = 'none';
            }
        </script>
    </section>
</section>

    </main>
    <footer>
        <p>&copy; 2024 Theoretical Results on Transformers and Language Models</p>
    </footer>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Theorem 3 - LM Theory</title>
    <link rel="stylesheet" href="assets/css/styles.css">
    
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>
    window.MathJax = {
        loader: {load: ['[tex]/mathtools']},
        tex: {
            tags: 'all',  // Automatically number all display equations
            // loader: {load: ['[tex]/autoload']},               // TODO: remove
            // packages: {'[+]': ['autoload', 'mathtools']},     // TODO: remove
            packages: {'[+]': ['mathtools']},
            autoload: {
                coloneqq: ['mathtools']
            },
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            macros: {
                emph: ["\\textit{#1}", 1],
mathds: ["\\mathbf{#1}", 1],
bm: ["\\boldsymbol{\\mathbf{#1}}", 1],
Tr: "\\operatorname{Tr}",
softmaxOp: "\\operatorname{softmax}",
lip: "\\operatorname{Lip}",
diag: "\\operatorname{diag}",
spaces: "\\hspace{2mm}",
unif: "\\pazocal{U}",
softmax: ["\\softmaxOp\\left(#1\\right)", 1],
norm: ["\\left\\lVert#1\\right\\rVert", 1],
andriy: ["\\textcolor{blue}{[AM: #1]}", 1],
hyunjik: ["\\textcolor{red}{[HK: #1]}", 1],
george: ["\\textcolor{magenta}{[George: #1]}", 1]

            },
        },
        options: {
            renderActions: {
                addMenu: []
            },
            ignoreHtmlClass: "tex2jax_ignore",
            skipHtmlTags: ["script", "noscript", "style", "textarea", "pre", "code"]
        }
    };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <header>
        <h1>Theoretical Results on Transformers and Language Models</h1>
        <nav>
            <a href="/index.html">Home</a>
            <a href="/examples.html">Examples</a>
            <a href="/contact.html">Contact</a>
            <a href="/contribute.html">Contribute</a>
            <div class="hover-container">
                <span class="hover-word"><a href="/library/index.html">Index</a></span>
                <div class="dropdown-content">
                    <a href="/library/index.html">Full Index</a>
                    <a href="/library/papers/index.html">Papers</a>
                    <a href="/library/definitions/index.html">Definitions</a>
                    <a href="/library/axioms/index.html">Axioms</a>
                    <a href="/library/lemmas/index.html">Lemmas</a>
                    <a href="/library/theorems/index.html">Theorems</a>
                    <a href="/library/corollaries/index.html">Corollaries</a>
                </div>
            </div>
        </nav>
    </header>
    <main>
        
<section id="Lipschitz Bound for L2-MHA">
    <h2>Theorem 3: Lipschitz Bound for L2-MHA</h2>
    <section id="Statement">
        <h3>Statement</h3>
        <div>
            $\verb!L2-MHA!$ is Lipschitz, with the following bound on $\lip_{\infty}(F)$:
\begin{align*}
    \lip_{\infty}(F)  \leq &\left(4 \phi^{-1}(N-1) + \frac{1}{\sqrt{D/H}}\right) \|W^{O^\top}\|_{\infty} \\
    &\max_h \|W^{Q,h}\|_{\infty} \|W^{{Q,h}^\top}\|_{\infty} \max_h \|W^{{V,h}^\top}\|_{\infty} 
\end{align*}
and the following bound on $\lip_{2}(F)$:
\begin{align*}
    \lip_2(F) \leq & \frac{\sqrt{N}}{\sqrt{D/H}}
    \left(4 \phi^{-1}(N-1) + 1 \right) \\ 
    & \left(\sqrt{\textstyle\sum_h \|W^{Q,h}\|_2^2\, \|W^{V,h}\|_2^2}\right) \|W^O\|_2 
\end{align*}
where $\phi(x) \coloneqq x\exp(x+1)$ is an invertible univariate function on $x > 0$, and $N$ is the input sequence length.

Specifically, $\phi^{-1}(N-1) = W_0(\frac{N}{e})$ where $W_0$ is the Lambert $W$-function, which grows sub-logarithmically as $O(\log N - \log \log N)$ \citep{corless1996lambertw}. Hence the above bounds can be simplified to $O(\log N)$ for $p=\infty$ and $O(\sqrt{N} \log N)$ for $p=2$.
        </div>
    </section>
    <section id="motivation">
        <h3>Motivation of Statement (AI-Generated)</h3>
        <div>
            Understanding the Lipschitz properties of $\verb!L2-MHA!$ is crucial for analyzing the stability and robustness of neural networks, particularly in the context of multi-head attention mechanisms. These bounds provide insights into how the output of the network changes in response to small perturbations in the input, which is essential for ensuring reliable performance in various applications, such as natural language processing and computer vision. The bounds also help in designing networks that are less sensitive to noise and adversarial attacks, making them more robust and trustworthy.
        </div>
    </section>
        
<section id="OriginalProof">
<h3>Original Proof</h3>
The mapping $f$ can be written as
\vspace{-5mm}
\begin{equation}
f(X) = PX = \softmax{X A^\top X^\top} X = \begin{bmatrix}
    f_1(X)^\top \\
    \vdots \\
    f_N(X)^\top
\end{bmatrix} \in \mathbb{R}^{N \times D},
\end{equation}
where $A = W^K W^{Q^\top} / \sqrt{D/H} \in \mathbb{R}^{D \times D}$ and
$f_i(X) = \sum_{j=1}^N P_{ij}\mathbf{x}_j$ with $P_{i:}^\top = \softmax{XA\mathbf{x}_i}$.
Hence $f$ can be interpreted as a map of each $\mathbf{x}_i$ to a point in the convex hull of ${\mathbf{x}_1,...,\mathbf{x}_N}$.
Since $f$ is a map from $\mathbb{R}^{N \times D}$ to $\mathbb{R}^{N \times D}$, its Jacobian is
\begin{equation}
    J_f = \begin{bmatrix}
    J_{11} & \dots & J_{1N} \\
    \vdots & \ddots & \vdots \\
    J_{N1} & \dots & J_{NN} \\
    \end{bmatrix}\in \mathbb{R}^{ND \times ND},
\end{equation}
where $J_{ij} = \frac{\partial f_i(X)}{\partial \mathbf{x}_j} \in \mathbb{R}^{D \times D}$. 
By taking partial derivatives we can show that $J_{ij} = X^\top P^{(i)} \left[E_{ji}XA^\top + XA\delta_{ij}\right] + P_{ij}I$
where $E_{ij} \in \mathbb{R}^{N \times N}$ is a binary matrix with zeros everywhere except the $(i,j)$th entry, $\delta_{ij}$ is the Kronecker delta, and $P^{(i)} \coloneqq \diag(P_{i:}) - P_{i:}^\top P_{i:}$.
So for $i=j$:
\begin{align}
J_{ii} &=X^\top P^{(i)}E_{ii}XA^\top + X^\top P^{(i)}XA + P_{ii}I \nonumber \\
&= P_{ii}\left(\mathbf{x}_i - \textstyle\sum_k P_{ik} \mathbf{x}_k\right)\mathbf{x}_i^\top A^\top + X^\top P^{(i)}XA + P_{ii}I. \label{eq:jac_dot_general}
\end{align}
For the last equality, note $E_{ii}X$ has all rows equal to zero except for the $i$th row given by $\mathbf{x}_i^\top$. We can then verify that $X^\top P^{(i)}E_{ii}X$ simplifies to $P_{ii}(\mathbf{x}_i - \sum_k P_{ik} \mathbf{x}_k)\mathbf{x}_i^\top$.

For vector $p$-norms, $\|J_f\|_p$ is bounded if and only if its entries are bounded, by definition of the operator norm. 
The entries of $X^\top P^{(i)}XA$ are bounded for arbitrary $A$ only if the entries of $X^\top P^{(i)}X$ are bounded.
So let us investigate the entries of this $D\times D$ matrix. 
Writing out each term of the matrix, we observe that it is in fact a covariance matrix of a discrete distribution. Specifically:
\begin{equation} \label{eq:cov_general}
    [X^\top P^{(i)}X]_{lm}  = \textstyle\sum_k P_{ik} x_{kl} x_{km} - \left(\textstyle\sum_k P_{ik}  x_{kl}\right)\left(\textstyle\sum_k P_{ik} x_{km}\right) = \mathrm{Cov}(\mathbb{X}_l,\mathbb{X}_m),
\end{equation}
where $\mathbb{X}$ is a discrete distribution with support at the inputs $\{\mathbf{x}_1,\ldots,\mathbf{x}_N \}$ and probability mass function given by their softmax probabilities $\mathbb{P}(\mathbb{X}=\mathbf{x}_j)=P_{ij}$. 
A consequence of this interpretation is that $P^{(i)}$ is \textit{positive semi-definite} (PSD) since for $D=1$, Equation \eqref{eq:cov_general} becomes $X^\top P^{(i)} X = \mathrm{Var}(\mathbb{X}) \geq 0$, with equality if and only if the $\mathbf{x}_j$ are all equal.

We use this observation to show that the terms of $J_{ii}$ are unbounded, and so $\verb!DP-MHA!$ is $\emph{not}$ Lipschitz.
Consider the case $\mathbf{x}_i=0$. Then $P_{i:}^\top = \softmax{XA\mathbf{x}_i} = \frac{1}{N} \mathds{1}$, i.e.\ we have uniform attention regardless of $\mathbf{x}_{ \neq i}$. 
The first term of $J_{ii}$ in Equation \eqref{eq:jac_dot_general} disappears since $\mathbf{x}_i=\mathbf{0}$, and the last term becomes $\frac{1}{N} I$. For the second term, the entries $[X^\top P^{(i)}X]_{ll} = \mathrm{Var}(\mathbb{X}_l)$ are unbounded since the latter is equal to the sample variance of ${x_{1l},\ldots,x_{Nl}}$, which can be arbitrarily large.

Note that we have shown that single head dot-product self-atttention ($H=1$) is not Lipschitz, but it is clear that this implies multihead self-attention $\verb!DP-MHA!$ is also not Lipschitz, since the output of multihead attention is a linear combination of the outputs of each head.
</section>

    <section id="proofExplanation">
        <h4>Explanation of Proof (AI-Generated)</h4>
        <div>
            To understand the proof, let's break it down into several steps:
<br>
<br>1. <i>Mapping Definition</i>: The proof starts by defining the mapping \( f \) as:
<br>   \[
   f(X) = PX = \softmax{X A^\top X^\top} X = \begin{bmatrix}
       f_1(X)^\top \\
       \vdots \\
       f_N(X)^\top
   \end{bmatrix} \in \mathbb{R}^{N \times D},
   \]
<br>   where \( A = W^K W^{Q^\top} / \sqrt{D/H} \in \mathbb{R}^{D \times D} \) and \( f_i(X) = \sum_{j=1}^N P_{ij}\mathbf{x}_j \) with \( P_{i:}^\top = \softmax{XA\mathbf{x}_i} \).
<br>
<br>2. <i>Interpretation of \( f \)</i>: It is noted that \( f \) maps each \( \mathbf{x}_i \) to a point in the convex hull of \( \{\mathbf{x}_1, \ldots, \mathbf{x}_N\} \).
<br>
<br>3. <i>Jacobian Definition</i>: The Jacobian of \( f \) is defined as:
<br>   \[
   J_f = \begin{bmatrix}
       J_{11} & \dots & J_{1N} \\
       \vdots & \ddots & \vdots \\
       J_{N1} & \dots & J_{NN} \\
   \end{bmatrix} \in \mathbb{R}^{ND \times ND},
   \]
<br>   where \( J_{ij} = \frac{\partial f_i(X)}{\partial \mathbf{x}_j} \in \mathbb{R}^{D \times D} \).
<br>
<br>4. <i>Partial Derivatives</i>: By taking partial derivatives, it is shown that:
<br>   \[
   J_{ij} = X^\top P^{(i)} \left[E_{ji}XA^\top + XA\delta_{ij}\right] + P_{ij}I,
   \]
<br>   where \( E_{ij} \) is a binary matrix, \( \delta_{ij} \) is the Kronecker delta, and \( P^{(i)} \coloneqq \diag(P_{i:}) - P_{i:}^\top P_{i:} \).
<br>
<br>5. <i>Case \( i = j \)</i>: For \( i = j \), the expression simplifies to:
<br>   \[
   J_{ii} = P_{ii}\left(\mathbf{x}_i - \textstyle\sum_k P_{ik} \mathbf{x}_k\right)\mathbf{x}_i^\top A^\top + X^\top P^{(i)}XA + P_{ii}I.
   \]
<br>   It is noted that \( E_{ii}X \) has all rows equal to zero except for the \( i \)th row given by \( \mathbf{x}_i^\top \).
<br>
<br>6. <i>Boundedness of Entries</i>: For vector \( p \)-norms, \( \|J_f\|_p \) is bounded if and only if its entries are bounded. The entries of \( X^\top P^{(i)}XA \) are bounded for arbitrary \( A \) only if the entries of \( X^\top P^{(i)}X \) are bounded.
<br>
<br>7. <i>Covariance Matrix</i>: The entries of \( X^\top P^{(i)}X \) are shown to form a covariance matrix:
<br>   \[
   [X^\top P^{(i)}X]_{lm} = \textstyle\sum_k P_{ik} x_{kl} x_{km} - \left(\textstyle\sum_k P_{ik} x_{kl}\right)\left(\textstyle\sum_k P_{ik} x_{km}\right) = \mathrm{Cov}(\mathbb{X}_l, \mathbb{X}_m),
   \]
<br>   where \( \mathbb{X} \) is a discrete distribution with support at the inputs \( \{\mathbf{x}_1, \ldots, \mathbf{x}_N \} \) and probability mass function given by their softmax probabilities.
<br>
<br>8. <i>Positive Semi-Definiteness</i>: It is observed that \( P^{(i)} \) is positive semi-definite (PSD) since for \( D=1 \), \( X^\top P^{(i)} X = \mathrm{Var}(\mathbb{X}) \geq 0 \).
<br>
<br>9. <i>Unbounded Terms</i>: The terms of \( J_{ii} \) are shown to be unbounded, indicating that \( \verb!DP-MHA! \) is not Lipschitz. Specifically, for \( \mathbf{x}_i = 0 \), the entries \( [X^\top P^{(i)}X]_{ll} = \mathrm{Var}(\mathbb{X}_l) \) are unbounded.
<br>
<br>10. <i>Multihead Attention</i>: It is concluded that single head dot-product self-attention (\( H=1 \)) is not Lipschitz, implying that multihead self-attention (\( \verb!DP-MHA! \)) is also not Lipschitz, as the output of multihead attention is a linear combination of the outputs of each head.
        </div>
    </section>




    <section id="Reference">
        <h3>Reference</h3>
            <p>Title: <a href="/library/papers/895bbef7-305d-4454-850a-7279bb832d9d/index.html"></a></p>
            <p>Authors: []</p>
            <p>URL: <a href="https://arxiv.org/abs/2006.04710">https://arxiv.org/abs/2006.04710</a></p>
    </section>
    <section id="Resources">
        <!-- Bibtex Modal -->
        <div id="bibtexModal" class="modal">
        <div class="modal-content">
            <span class="close" onclick="closeBibtexModal()">&times;</span>
            <p></p>
            <button class="modal-button" onclick="copyBibtex()">Copy</button>
        </div>
        </div>

        <!-- Latex Modal -->
        <div id="latexModal" class="modal">
        <div class="modal-content">
            <span class="close" onclick="closeLatexModal()">&times;</span>
            <p class="tex2jax_ignore"><code>\begin{theorem} \label{thm:main}
\verb!L2-MHA! is Lipschitz, with the following bound on $\lip_{\infty}(F)$:
\begin{align*}
    \lip_{\infty}(F)  \leq &\left(4 \phi^{-1}(N-1) + \frac{1}{\sqrt{D/H}}\right) \|W^{O^\top}\|_{\infty} \\
    &\max_h \|W^{Q,h}\|_{\infty} \|W^{{Q,h}^\top}\|_{\infty} \max_h \|W^{{V,h}^\top}\|_{\infty} 
\end{align*}
and the following bound on $\lip_{2}(F)$:
\begin{align*}
    \lip_2(F) \leq & \frac{\sqrt{N}}{\sqrt{D/H}}
    \left(4 \phi^{-1}(N-1) + 1 \right) \\ 
    & \left(\sqrt{\textstyle\sum_h \|W^{Q,h}\|_2^2\, \|W^{V,h}\|_2^2}\right) \|W^O\|_2 
\end{align*}
where $\phi(x) \coloneqq x\exp(x+1)$ is an invertible univariate function on $x > 0$, and $N$ is the input sequence length.

Specifically, $\phi^{-1}(N-1) = W_0(\frac{N}{e})$ where $W_0$ is the Lambert $W$-function, which grows sub-logarithmically as $O(\log N - \log \log N)$ \citep{corless1996lambertw}. Hence the above bounds can be simplified to $O(\log N)$ for $p=\infty$ and $O(\sqrt{N} \log N)$ for $p=2$.
\end{theorem}</code></p>
            <button class="modal-button" onclick="copyLatex()">Copy</button>
        </div>
        </div>

        <h3>Resources</h3>    
            <p>
                <b>BibTex</b>:
                    <button onclick="copyBibtex()">Copy</button>
                    <button onclick="openBibtexModel()">Show</button>
            </p>
            <p>
                <b>LaTeX</b>:
                    <button onclick="copyLatex()">Copy</button>
                    <button onclick="openLatexModal()">Show</button>
            </p>
        <script>
            function copyBibtex() {
                var textToCopy = ``;
                navigator.clipboard.writeText(textToCopy).then(function() {
                    //alert('Text copied to clipboard: ' + textToCopy);
                }, function(err) {
                    console.error('Failed to copy text: ', err);
                });
            }
            
            function copyLatex() {
                var textToCopy = `\\begin{theorem} \\label{thm:main}
\\verb!L2-MHA! is Lipschitz, with the following bound on $\\lip_{\\infty}(F)$:
\\begin{align*}
    \\lip_{\\infty}(F)  \\leq &\\left(4 \\phi^{-1}(N-1) + \\frac{1}{\\sqrt{D/H}}\\right) \\|W^{O^\\top}\\|_{\\infty} \\\\
    &\\max_h \\|W^{Q,h}\\|_{\\infty} \\|W^{{Q,h}^\\top}\\|_{\\infty} \\max_h \\|W^{{V,h}^\\top}\\|_{\\infty} 
\\end{align*}
and the following bound on $\\lip_{2}(F)$:
\\begin{align*}
    \\lip_2(F) \\leq & \\frac{\\sqrt{N}}{\\sqrt{D/H}}
    \\left(4 \\phi^{-1}(N-1) + 1 \\right) \\\\ 
    & \\left(\\sqrt{\\textstyle\\sum_h \\|W^{Q,h}\\|_2^2\\, \\|W^{V,h}\\|_2^2}\\right) \\|W^O\\|_2 
\\end{align*}
where $\\phi(x) \\coloneqq x\\exp(x+1)$ is an invertible univariate function on $x > 0$, and $N$ is the input sequence length.

Specifically, $\\phi^{-1}(N-1) = W_0(\\frac{N}{e})$ where $W_0$ is the Lambert $W$-function, which grows sub-logarithmically as $O(\\log N - \\log \\log N)$ \\citep{corless1996lambertw}. Hence the above bounds can be simplified to $O(\\log N)$ for $p=\\infty$ and $O(\\sqrt{N} \\log N)$ for $p=2$.
\\end{theorem}`;
                navigator.clipboard.writeText(textToCopy).then(function() {
                    //alert('Text copied to clipboard: ' + textToCopy);
                }, function(err) {
                    console.error('Failed to copy text: ', err);
                });
            }

            // Bibtex
            function openBibtexModel() {
                document.getElementById('bibtexModal').style.display = 'block';
            }
            function closeBibtexModal() {
                document.getElementById('bibtexModal').style.display = 'none';
            }

            // Latex
            function openLatexModal() {
                document.getElementById('latexModal').style.display = 'block';
            }
            function closeLatexModal() {
                document.getElementById('latexModal').style.display = 'none';
            }
        </script>
    </section>
</section>

    </main>
    <footer>
        <p>&copy; 2024 Theoretical Results on Transformers and Language Models</p>
    </footer>
</body>
</html>
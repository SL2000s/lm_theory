<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Theorem 14 - LM Theory</title>
    <link rel="shortcut icon" href="/assets/images/favicon.png">
    <link rel="stylesheet" href="/assets/css/styles.css">
    
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>
    window.MathJax = {
        loader: {load: ['[tex]/mathtools']},
        tex: {
            tags: 'all',  // Automatically number all display equations
            // loader: {load: ['[tex]/autoload']},               // TODO: remove
            // packages: {'[+]': ['autoload', 'mathtools']},     // TODO: remove
            packages: {'[+]': ['mathtools']},
            autoload: {
                coloneqq: ['mathtools']
            },
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            macros: {
                emph: ["\\textit{#1}", 1],
bm: ["\\boldsymbol{\\mathbf{#1}}", 1],
mathds: ["\\mathbf{#1}", 1],
textsl: ["\\textit{#1}", 1],
vspace: ["", 1],
xspace: "",
data: ["\\textcolor{Maroon}{\\texttt{{#1}}}", 1]

            },
            environments: {
                subequations: ["{", "}"]

            },
        },
        options: {
            renderActions: {
                addMenu: []
            },
            ignoreHtmlClass: "tex2jax_ignore",
            skipHtmlTags: ["script", "noscript", "style", "textarea", "pre", "code"]
        }
    };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <header>
        <h1>Theoretical Results on Transformers and Language Models</h1>
        <nav>
            <a href="/">Home</a>
            <a href="/examples">Examples</a>
            <a href="/contact">Contact</a>
            <div class="hover-container">
                <span class="hover-word"><a href="/contribute">Contribute</a></span>
                <div class="dropdown-content">
                    <a href="/contribute/add_statement">Add statement</a>
                    <a href="/contribute/add_paper">Add paper</a>
                </div>
            </div>
            <!-- <a href="/proof_assistant">Proof Assistant</a> -->
            <div class="hover-container">
                <span class="hover-word"><a href="/library">Index</a></span>
                <div class="dropdown-content">
                    <a href="/library">Full Index</a>
                    <a href="/library/papers">Papers</a>
                    <a href="/library/definitions">Definitions</a>
                    <a href="/library/axioms">Axioms</a>
                    <a href="/library/lemmas">Lemmas</a>
                    <a href="/library/theorems">Theorems</a>
                    <a href="/library/corollaries">Corollaries</a>
                </div>
            </div>
        </nav>
    </header>
    <main>
        
<section id="Inverse-Temperature Projection Theorem">
    <h2>Theorem 14: Inverse-Temperature Projection Theorem</h2>
    <section id="Statement">
        <h3>Statement</h3>
        <div>
            In the $\texttt{No-Norm}$ case, the attention distribution $a_t$ is defined by the projection of $y_t$ onto a fixed vector $y_x$ for a given $x$. The length of $y_x$ is an inverse-temperature parameter.
        </div>
    </section>
    <section id="motivation">
        <h3>Motivation of Statement (AI-Generated)</h3>
        <div>
            The $\texttt{No-Norm}$ case in attention mechanisms is useful for simplifying the computation of attention distributions. By projecting $y_t$ onto a fixed vector $y_x$, we can efficiently determine the relevance of $y_t$ with respect to $y_x$. This approach is particularly beneficial in scenarios where computational efficiency is crucial, such as in real-time applications or when dealing with large datasets. The inverse-temperature parameter, represented by the length of $y_x$, allows for fine-tuning the sharpness of the attention distribution, providing flexibility in how attention is allocated.
        </div>
    </section>
        
<section id="OriginalProof">
<h3>Original Proof</h3>
Write $w_t = x^TW_{QK}y_t = (W_{QK}^Tx)^T y_t \equiv y_x^T y_t$ where $y_x \triangleq W_{QK}^T x\in\mathbb{R}^{N_y}$, which is the dot-product between $y_t$ and a fixed vector $y_x$ on the row space of $W_{QK}$. Then, re-writing in terms of the vector lengths and the enclosing angle $\theta_{y_t} = y_x\wedge y_t$, we have $w = |y_x||y_t|\cos\theta_{y_t}$. The factor $|y_x|$ is identical for all $t$, making it an inverse-temperature.
</section>

    <section id="proofExplanation">
        <h4>Explanation of Proof (AI-Generated)</h4>
        <div>
            To understand the proof, we can break it down into the following steps:
<br>
<br>1. <i></i>Initial Expression<i></i>:
<br>   - We start with the expression \( w_t = x^T W_{QK} y_t \).
<br>   - This can be rewritten as \( w_t = (W_{QK}^T x)^T y_t \).
<br>
<br>2. <i></i>Definition of \( y_x \)<i></i>:
<br>   - Define \( y_x \triangleq W_{QK}^T x \in \mathbb{R}^{N_y} \).
<br>   - This allows us to express \( w_t \) as \( w_t \equiv y_x^T y_t \).
<br>
<br>3. <i></i>Dot-Product Interpretation<i></i>:
<br>   - The expression \( y_x^T y_t \) represents the dot-product between \( y_t \) and a fixed vector \( y_x \) in the row space of \( W_{QK} \).
<br>
<br>4. <i></i>Rewriting in Terms of Vector Lengths and Angle<i></i>:
<br>   - We rewrite the dot-product in terms of the magnitudes (lengths) of the vectors and the cosine of the angle between them.
<br>   - This gives us \( w = |y_x||y_t|\cos\theta_{y_t} \), where \( \theta_{y_t} \) is the angle between \( y_x \) and \( y_t \).
<br>
<br>5. <i></i>Inverse-Temperature Factor<i></i>:
<br>   - The magnitude \( |y_x| \) is constant for all \( t \), making it an inverse-temperature factor.
<br>
<br>This step-by-step breakdown helps in understanding how the initial expression is transformed and interpreted in terms of vector magnitudes and angles.
        </div>
    </section>




    <section id="Reference">
        <h3>Reference</h3>
            <p>Title: <a href="/library/papers/transformer_normalisation_layers_and_the_independence_of_semantic_subspaces/index.html">Transformer Normalisation Layers and the Independence of Semantic Subspaces</a></p>
            <p>Authors: Stephen Menary, Samuel Kaski, and Andre Freitas</p>
            <p>URL: <a href="https://arxiv.org/abs/2406.17837">https://arxiv.org/abs/2406.17837</a></p>
    </section>
    <section id="Resources">
        <!-- Bibtex Modal -->
        <div id="bibtexModal" class="modal">
        <div class="modal-content">
            <span class="close" onclick="closeBibtexModal()">&times;</span>
            <p>@misc{menary2024transformernormalisationlayersindependence,<br>&emsp;title={Transformer Normalisation Layers and the Independence of Semantic Subspaces}, <br>&emsp;author={Stephen Menary and Samuel Kaski and Andre Freitas},<br>&emsp;year={2024},<br>&emsp;eprint={2406.17837},<br>&emsp;archivePrefix={arXiv},<br>&emsp;primaryClass={cs.LG},<br>&emsp;url={https://arxiv.org/abs/2406.17837}<br>}</p>
            <button class="modal-button" onclick="copyBibtex()">Copy</button>
        </div>
        </div>

        <!-- Latex Modal -->
        <div id="latexModal" class="modal">
        <div class="modal-content">
            <span class="close" onclick="closeLatexModal()">&times;</span>
            <p class="tex2jax_ignore"><code>\begin{theorem}
    In the \texttt{No-Norm} case, the attention distribution $a_t$ is defined by the projection of $y_t$ onto a fixed vector $y_x$ for a given $x$. The length of $y_x$ is an inverse-temperature parameter.
\end{theorem}</code></p>
            <button class="modal-button" onclick="copyLatex()">Copy</button>
        </div>
        </div>

        <h3>Resources</h3>    
            <p>
                <b>BibTex</b>:
                    <button onclick="copyBibtex()">Copy</button>
                    <button onclick="openBibtexModel()">Show</button>
            </p>
            <p>
                <b>LaTeX</b>:
                    <button onclick="copyLatex()">Copy</button>
                    <button onclick="openLatexModal()">Show</button>
            </p>
        <script>
            function copyBibtex() {
                var textToCopy = `@misc{menary2024transformernormalisationlayersindependence,
  title={Transformer Normalisation Layers and the Independence of Semantic Subspaces}, 
  author={Stephen Menary and Samuel Kaski and Andre Freitas},
  year={2024},
  eprint={2406.17837},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2406.17837}
}`;
                navigator.clipboard.writeText(textToCopy).then(function() {
                    //alert('Text copied to clipboard: ' + textToCopy);
                }, function(err) {
                    console.error('Failed to copy text: ', err);
                });
            }
            
            function copyLatex() {
                var textToCopy = `\\begin{theorem}
    In the \\texttt{No-Norm} case, the attention distribution $a_t$ is defined by the projection of $y_t$ onto a fixed vector $y_x$ for a given $x$. The length of $y_x$ is an inverse-temperature parameter.
\\end{theorem}`;
                navigator.clipboard.writeText(textToCopy).then(function() {
                    //alert('Text copied to clipboard: ' + textToCopy);
                }, function(err) {
                    console.error('Failed to copy text: ', err);
                });
            }

            // Bibtex
            function openBibtexModel() {
                document.getElementById('bibtexModal').style.display = 'block';
            }
            function closeBibtexModal() {
                document.getElementById('bibtexModal').style.display = 'none';
            }

            // Latex
            function openLatexModal() {
                document.getElementById('latexModal').style.display = 'block';
            }
            function closeLatexModal() {
                document.getElementById('latexModal').style.display = 'none';
            }
        </script>
    </section>
</section>

    </main>
    <footer>
        <p>&copy; 2024 Theoretical Results on Transformers and Language Models</p>
    </footer>
</body>
</html>
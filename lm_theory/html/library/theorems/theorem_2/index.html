<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Theorem 2 - LM Theory</title>
    <link rel="stylesheet" href="/assets/css/styles.css">
    
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>
    window.MathJax = {
        loader: {load: ['[tex]/mathtools']},
        tex: {
            tags: 'all',  // Automatically number all display equations
            // loader: {load: ['[tex]/autoload']},               // TODO: remove
            // packages: {'[+]': ['autoload', 'mathtools']},     // TODO: remove
            packages: {'[+]': ['mathtools']},
            autoload: {
                coloneqq: ['mathtools']
            },
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            macros: {
                emph: ["\\textit{#1}", 1],
mathds: ["\\mathbf{#1}", 1],
bm: ["\\boldsymbol{\\mathbf{#1}}", 1],
Tr: "\\operatorname{Tr}",
softmaxOp: "\\operatorname{softmax}",
lip: "\\operatorname{Lip}",
diag: "\\operatorname{diag}",
spaces: "\\hspace{2mm}",
unif: "\\pazocal{U}",
softmax: ["\\softmaxOp\\left(#1\\right)", 1],
norm: ["\\left\\lVert#1\\right\\rVert", 1],
andriy: ["\\textcolor{blue}{[AM: #1]}", 1],
hyunjik: ["\\textcolor{red}{[HK: #1]}", 1],
george: ["\\textcolor{magenta}{[George: #1]}", 1]

            },
        },
        options: {
            renderActions: {
                addMenu: []
            },
            ignoreHtmlClass: "tex2jax_ignore",
            skipHtmlTags: ["script", "noscript", "style", "textarea", "pre", "code"]
        }
    };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <header>
        <h1>Theoretical Results on Transformers and Language Models</h1>
        <nav>
            <a href="/">Home</a>
            <a href="/examples">Examples</a>
            <a href="/contact">Contact</a>
            <a href="/contribute">Contribute</a>
            <div class="hover-container">
                <span class="hover-word"><a href="/library">Index</a></span>
                <div class="dropdown-content">
                    <a href="/library">Full Index</a>
                    <a href="/library/papers">Papers</a>
                    <a href="/library/definitions">Definitions</a>
                    <a href="/library/axioms">Axioms</a>
                    <a href="/library/lemmas">Lemmas</a>
                    <a href="/library/theorems">Theorems</a>
                    <a href="/library/corollaries">Corollaries</a>
                </div>
            </div>
        </nav>
    </header>
    <main>
        
<section id="Non-Lipschitz Property of DP-MHA">
    <h2>Theorem 2: Non-Lipschitz Property of DP-MHA</h2>
    <section id="Statement">
        <h3>Statement</h3>
        <div>
            $\verb!DP-MHA!$ is not Lipschitz for any vector $p$-norm $\|\cdot\|_p$ with $p \in [1, \infty]$.
        </div>
    </section>
    <section id="motivation">
        <h3>Motivation of Statement (AI-Generated)</h3>
        <div>
            The fact that $\verb!DP-MHA!$ is not Lipschitz for any vector $p$-norm $\|\cdot\|_p$ with $p \in [1, \infty]$ is crucial in understanding the limitations of certain algorithms in machine learning and optimization. This result highlights that the stability and robustness of $\verb!DP-MHA!$ cannot be guaranteed under these norms, which is important when analyzing the performance and convergence of algorithms that rely on Lipschitz continuity. This insight is particularly useful when designing or choosing algorithms for applications where stability is a critical factor.
        </div>
    </section>
        
<section id="OriginalProof">
<h3>Original Proof</h3>
We show the proof for the case $D=H=1$ (i.e.~$X \in \mathbb{R}^{N \times 1}$, a column vector, and $x_i \in \mathbb{R}$) for readability. See Appendix $\ref{apd:general_d}$ for the general case, which follows the same logic. 

The mapping $f$ can be written as
\begin{align*}
f(X) = PX = & \softmax{a X X^\top} X = \begin{bmatrix}
    f_1(X) \\
    \vdots \\
    f_N(X)
\end{bmatrix} \in \mathbb{R}^{N \times 1}, \\
\text{where} \quad & f_i(X) = \sum_{j=1}^N P_{ij}x_j \in \mathbb{R}
\end{align*}
and $a = W^K W^Q \in \mathbb{R}$ (we assume $a \neq 0$ such that self-attention is non-trivial).
Hence $f$ can be interpreted as a map of each $x_i$ to a point in the convex hull of ${x_1,...,x_N}$.
Since $f$ is a map from $\mathbb{R}^{N \times 1}$ to $\mathbb{R}^{N \times 1}$, its Jacobian is
\begin{equation}
    J_f = \begin{bmatrix}
    J_{11} & \dots & J_{1N} \\
    \vdots & \ddots & \vdots \\
    J_{N1} & \dots & J_{NN} \\
    \end{bmatrix}\in \mathbb{R}^{N \times N},
\end{equation}
where $\smash{J_{ij} = \frac{\partial f_i(X)}{\partial x_j} \in \mathbb{R}}$. 
By taking partial derivatives we can show that
\begin{equation*}
    J_{ij} = a X^\top P^{(i)} \left[E_{ji}X + \delta_{ij}X \right] + P_{ij}I
\end{equation*}
where 
<ul>
    <li>$E_{ij} \in \mathbb{R}^{N \times N}$ is a binary matrix with zeros everywhere except the $(i,j)$th entry</li>
    <li>$\delta_{ij} \in \{0,1\}$ is the Kronecker delta</li>
    <li>$P^{(i)} \coloneqq \diag(P_{i:}) - P_{i:}^\top P_{i:} \in \mathbb{R}^{N \times N}$.</li>
</ul>
See Appendix $\ref{apd:identities}$ for useful identities in deriving the above Jacobian.

So for $i=j$:
\begin{align}
J_{ii} =
 a X^\top P^{(i)} e_{ii} X + a X^\top P^{(i)} X + P_{ii} \label{eq:jac_dot}
\end{align}

Let us investigate the scalar $X^\top P^{(i)}X$. We observe that it is in fact a variance of a discrete distribution. Specifically:
\begin{equation} \label{eq:cov}
    X^\top P^{(i)}X  = \textstyle\sum_k P_{ik} x_k^2 - \left(\textstyle\sum_k P_{ik}  x_k\right)^2 = \mathrm{Var}(\mathbb{X}),
\end{equation}
where $\mathbb{X}$ is a discrete distribution with support at the inputs $\{x_1,\ldots,x_N \}$ and probability mass function given by their softmax probabilities $\mathbb{P}(\mathbb{X}=x_j)=P_{ij}$. 
A consequence of this interpretation is that $P^{(i)}$ is \textit{positive semi-definite} (PSD) since $X^\top P^{(i)} X = \mathrm{Var}(\mathbb{X}) \geq 0$, with equality if and only if the $x_j$ are all equal.

We use this observation to show that $J_{ii}$ is unbounded, and so $\|J_f\|_p$ is unbounded, hence $\verb!DP-MHA!$ is $\emph{not}$ Lipschitz.
Consider the case $x_i=0$. Then 
\begin{equation*}
    P_{i:}^\top = \softmax{XAx_i} = \frac{1}{N} \mathds{1},
\end{equation*}
i.e.\ we have uniform attention regardless of $x_{ \neq i}$. 
The first term of $J_{ii}$ in Equation $\eqref{eq:jac_dot}$ disappears since $e_{ii} X = [0, \ldots, x_i, \ldots, 0] = \mathbf{0}$, and the last term becomes $\frac{1}{N} I$. Now consider the second term $a X^\top P^{(i)}X = a \mathrm{Var}(\mathbb{X}_l)$. Note $\mathbb{X}$ is uniformly distributed, since $\mathbb{P}(\mathbb{X}=x_j)=P_{ij}= 1/N$. Hence the second term is equal to $a$ times the sample variance of ${x_1,\ldots,x_N}$, which can be arbitrarily large. Hence $J_{ii}$ can become arbitrarily large, so the full Jacobian $J_f$ is unbounded.
</section>

    <section id="proofExplanation">
        <h4>Explanation of Proof (AI-Generated)</h4>
        <div>
            To understand the proof, let's break it down into several steps:
<br>
<br>1. <i>Setup and Assumptions</i>: The proof begins by considering the case where \(D=H=1\), meaning \(X\) is a column vector in \(\mathbb{R}^{N \times 1}\) and each \(x_i \in \mathbb{R}\). The general case is discussed in the appendix.
<br>
<br>2. <i>Mapping Definition</i>: The mapping \(f\) is defined as:
<br>   \[
   f(X) = PX = \softmax{a X X^\top} X = \begin{bmatrix}
       f_1(X) \\
       \vdots \\
       f_N(X)
   \end{bmatrix} \in \mathbb{R}^{N \times 1},
   \]
<br>   where \(f_i(X) = \sum_{j=1}^N P_{ij}x_j \in \mathbb{R}\) and \(a = W^K W^Q \in \mathbb{R}\).
<br>
<br>3. <i>Jacobian Matrix</i>: The Jacobian \(J_f\) of the mapping \(f\) is given by:
<br>   \[
   J_f = \begin{bmatrix}
       J_{11} & \dots & J_{1N} \\
       \vdots & \ddots & \vdots \\
       J_{N1} & \dots & J_{NN} \\
   \end{bmatrix}\in \mathbb{R}^{N \times N},
   \]
<br>   where \(J_{ij} = \frac{\partial f_i(X)}{\partial x_j} \in \mathbb{R}\).
<br>
<br>4. <i>Partial Derivatives</i>: By taking partial derivatives, it is shown that:
<br>   \[
   J_{ij} = a X^\top P^{(i)} \left[E_{ji}X + \delta_{ij}X \right] + P_{ij}I,
   \]
<br>   where:
<ul>
    <li>\(E_{ij} \in \mathbb{R}^{N \times N}\) is a binary matrix with zeros everywhere except the \((i,j)\)th entry.</li>
    <li>\(\delta_{ij} \in \{0,1\}\) is the Kronecker delta.</li>
    <li>\(P^{(i)} \coloneqq \diag(P_{i:}) - P_{i:}^\top P_{i:} \in \mathbb{R}^{N \times N}\).</li>
</ul>
<br>
<br>5. <i>Diagonal Elements</i>: For \(i=j\), the Jacobian element \(J_{ii}\) is:
<br>   \[
   J_{ii} = a X^\top P^{(i)} e_{ii} X + a X^\top P^{(i)} X + P_{ii}.
   \]
<br>
<br>6. <i>Variance Interpretation</i>: The term \(X^\top P^{(i)}X\) is interpreted as the variance of a discrete distribution:
<br>   \[
   X^\top P^{(i)}X  = \textstyle\sum_k P_{ik} x_k^2 - \left(\textstyle\sum_k P_{ik}  x_k\right)^2 = \mathrm{Var}(\mathbb{X}),
   \]
<br>   where \(\mathbb{X}\) is a discrete distribution with support at \(\{x_1,\ldots,x_N \}\) and probability mass function \(\mathbb{P}(\mathbb{X}=x_j)=P_{ij}\).
<br>
<br>7. <i>Positive Semi-Definiteness</i>: It is noted that \(P^{(i)}\) is positive semi-definite (PSD) since \(X^\top P^{(i)} X = \mathrm{Var}(\mathbb{X}) \geq 0\).
<br>
<br>8. <i>Unbounded Jacobian</i>: To show that \(J_{ii}\) is unbounded, consider \(x_i=0\). Then:
<br>   \[
   P_{i:}^\top = \softmax{XAx_i} = \frac{1}{N} \mathds{1},
   \]
<br>   leading to uniform attention. The first term of \(J_{ii}\) disappears, and the last term becomes \(\frac{1}{N} I\). The second term \(a X^\top P^{(i)}X = a \mathrm{Var}(\mathbb{X}_l)\) can be arbitrarily large, making \(J_{ii}\) unbounded.
<br>
<br>Thus, the full Jacobian \(J_f\) is unbounded, showing that \(\verb!DP-MHA!\) is not Lipschitz.
        </div>
    </section>




    <section id="Reference">
        <h3>Reference</h3>
            <p>Title: <a href="/library/papers/895bbef7-305d-4454-850a-7279bb832d9d/index.html"></a></p>
            <p>Authors: []</p>
            <p>URL: <a href="https://arxiv.org/abs/2006.04710">https://arxiv.org/abs/2006.04710</a></p>
    </section>
    <section id="Resources">
        <!-- Bibtex Modal -->
        <div id="bibtexModal" class="modal">
        <div class="modal-content">
            <span class="close" onclick="closeBibtexModal()">&times;</span>
            <p></p>
            <button class="modal-button" onclick="copyBibtex()">Copy</button>
        </div>
        </div>

        <!-- Latex Modal -->
        <div id="latexModal" class="modal">
        <div class="modal-content">
            <span class="close" onclick="closeLatexModal()">&times;</span>
            <p class="tex2jax_ignore"><code>\begin{theorem} \label{thm:dp_not_lipschitz}
\verb!DP-MHA! is not Lipschitz for any vector $p$-norm $\|\cdot\|_p$ with $p \in [1, \infty]$.
\end{theorem}</code></p>
            <button class="modal-button" onclick="copyLatex()">Copy</button>
        </div>
        </div>

        <h3>Resources</h3>    
            <p>
                <b>BibTex</b>:
                    <button onclick="copyBibtex()">Copy</button>
                    <button onclick="openBibtexModel()">Show</button>
            </p>
            <p>
                <b>LaTeX</b>:
                    <button onclick="copyLatex()">Copy</button>
                    <button onclick="openLatexModal()">Show</button>
            </p>
        <script>
            function copyBibtex() {
                var textToCopy = ``;
                navigator.clipboard.writeText(textToCopy).then(function() {
                    //alert('Text copied to clipboard: ' + textToCopy);
                }, function(err) {
                    console.error('Failed to copy text: ', err);
                });
            }
            
            function copyLatex() {
                var textToCopy = `\\begin{theorem} \\label{thm:dp_not_lipschitz}
\\verb!DP-MHA! is not Lipschitz for any vector $p$-norm $\\|\\cdot\\|_p$ with $p \\in [1, \\infty]$.
\\end{theorem}`;
                navigator.clipboard.writeText(textToCopy).then(function() {
                    //alert('Text copied to clipboard: ' + textToCopy);
                }, function(err) {
                    console.error('Failed to copy text: ', err);
                });
            }

            // Bibtex
            function openBibtexModel() {
                document.getElementById('bibtexModal').style.display = 'block';
            }
            function closeBibtexModal() {
                document.getElementById('bibtexModal').style.display = 'none';
            }

            // Latex
            function openLatexModal() {
                document.getElementById('latexModal').style.display = 'block';
            }
            function closeLatexModal() {
                document.getElementById('latexModal').style.display = 'none';
            }
        </script>
    </section>
</section>

    </main>
    <footer>
        <p>&copy; 2024 Theoretical Results on Transformers and Language Models</p>
    </footer>
</body>
</html>
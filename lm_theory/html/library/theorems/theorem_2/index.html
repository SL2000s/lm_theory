<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Theorem 2 - LM Theory</title>
    <link rel="shortcut icon" href="/assets/images/favicon.png">
    <link rel="stylesheet" href="/assets/css/styles.css">
    
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>
    window.MathJax = {
        loader: {load: ['[tex]/mathtools']},
        tex: {
            tags: 'all',  // Automatically number all display equations
            // loader: {load: ['[tex]/autoload']},               // TODO: remove
            // packages: {'[+]': ['autoload', 'mathtools']},     // TODO: remove
            packages: {'[+]': ['mathtools']},
            autoload: {
                coloneqq: ['mathtools']
            },
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            macros: {
                emph: ["\\textit{#1}", 1],
bm: ["\\boldsymbol{\\mathbf{#1}}", 1],
mathds: ["\\mathbf{#1}", 1],
textsl: ["\\textit{#1}", 1],
vspace: ["", 1],
xspace: "",
Tr: "\\operatorname{Tr}",
softmaxOp: "\\operatorname{softmax}",
lip: "\\operatorname{Lip}",
diag: "\\operatorname{diag}",
spaces: "\\hspace{2mm}",
unif: "\\pazocal{U}",
softmax: ["\\softmaxOp\\left(#1\\right)", 1],
norm: ["\\left\\lVert#1\\right\\rVert", 1],
andriy: ["\\textcolor{blue}{[AM: #1]}", 1],
hyunjik: ["\\textcolor{red}{[HK: #1]}", 1],
george: ["\\textcolor{magenta}{[George: #1]}", 1]

            },
            environments: {
                subequations: ["{", "}"]

            },
        },
        options: {
            renderActions: {
                addMenu: []
            },
            ignoreHtmlClass: "tex2jax_ignore",
            skipHtmlTags: ["script", "noscript", "style", "textarea", "pre", "code"]
        }
    };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <header>
        <h1>Theoretical Results on Transformers and Language Models</h1>
        <nav>
            <a href="/">Home</a>
            <a href="/examples">Examples</a>
            <a href="/contact">Contact</a>
            <div class="hover-container">
                <span class="hover-word"><a href="/contribute">Contribute</a></span>
                <div class="dropdown-content">
                    <a href="/contribute/add_statement">Add statement</a>
                    <a href="/contribute/add_paper">Add paper</a>
                </div>
            </div>
            <!-- <a href="/proof_assistant">Proof Assistant</a> -->
            <div class="hover-container">
                <span class="hover-word"><a href="/library">Index</a></span>
                <div class="dropdown-content">
                    <a href="/library">Full Index</a>
                    <a href="/library/papers">Papers</a>
                    <a href="/library/definitions">Definitions</a>
                    <a href="/library/axioms">Axioms</a>
                    <a href="/library/lemmas">Lemmas</a>
                    <a href="/library/theorems">Theorems</a>
                    <a href="/library/corollaries">Corollaries</a>
                </div>
            </div>
        </nav>
    </header>
    <main>
        
<section id="Non-Lipschitzness of DP-MHA">
    <h2>Theorem 2: Non-Lipschitzness of DP-MHA</h2>
    <section id="Statement">
        <h3>Statement</h3>
        <div>
            $\verb!DP-MHA!$ is not Lipschitz for any vector $p$-norm $\|\cdot\|_p$ with $p \in [1, \infty]$.
        </div>
    </section>
    <section id="motivation">
        <h3>Motivation of Statement (AI-Generated)</h3>
        <div>
            The fact that $\verb!DP-MHA!$ is not Lipschitz for any vector $p$-norm $\|\cdot\|_p$ with $p \in [1, \infty]$ is crucial in understanding the limitations of certain algorithms in machine learning and optimization. This result highlights that $\verb!DP-MHA!$ may exhibit sensitivity to small changes in input, which can affect the stability and robustness of the algorithm. It is particularly useful when analyzing the performance and reliability of algorithms in high-dimensional spaces, where ensuring Lipschitz continuity can be challenging.
        </div>
    </section>
        
<section id="OriginalProof">
<h3>Original Proof</h3>
The mapping $f$ can be written as

\begin{equation}
f(X) = PX = \softmax{X A^\top X^\top} X = \begin{bmatrix}
    f_1(X)^\top \\
    \vdots \\
    f_N(X)^\top
\end{bmatrix} \in \mathbb{R}^{N \times D},
\end{equation}
where $A = W^K W^{Q^\top} / \sqrt{D/H} \in \mathbb{R}^{D \times D}$ and
$f_i(X) = \sum_{j=1}^N P_{ij}\mathbf{x}_j$ with $P_{i:}^\top = \softmax{XA\mathbf{x}_i}$.
Hence $f$ can be interpreted as a map of each $\mathbf{x}_i$ to a point in the convex hull of ${\mathbf{x}_1,...,\mathbf{x}_N}$.
Since $f$ is a map from $\mathbb{R}^{N \times D}$ to $\mathbb{R}^{N \times D}$, its Jacobian is
\begin{equation}
    J_f = \begin{bmatrix}
    J_{11} & \dots & J_{1N} \\
    \vdots & \ddots & \vdots \\
    J_{N1} & \dots & J_{NN} \\
    \end{bmatrix}\in \mathbb{R}^{ND \times ND},
\end{equation}
where $J_{ij} = \frac{\partial f_i(X)}{\partial \mathbf{x}_j} \in \mathbb{R}^{D \times D}$. 
By taking partial derivatives we can show that $J_{ij} = X^\top P^{(i)} \left[E_{ji}XA^\top + XA\delta_{ij}\right] + P_{ij}I$
where $E_{ij} \in \mathbb{R}^{N \times N}$ is a binary matrix with zeros everywhere except the $(i,j)$th entry, $\delta_{ij}$ is the Kronecker delta, and $P^{(i)} \coloneqq \diag(P_{i:}) - P_{i:}^\top P_{i:}$.
So for $i=j$:
\begin{align}
J_{ii} &=X^\top P^{(i)}E_{ii}XA^\top + X^\top P^{(i)}XA + P_{ii}I \nonumber \\
&= P_{ii}\left(\mathbf{x}_i - \textstyle\sum_k P_{ik} \mathbf{x}_k\right)\mathbf{x}_i^\top A^\top + X^\top P^{(i)}XA + P_{ii}I. \label{eq:jac_dot_general}
\end{align}
For the last equality, note $E_{ii}X$ has all rows equal to zero except for the $i$th row given by $\mathbf{x}_i^\top$. We can then verify that $X^\top P^{(i)}E_{ii}X$ simplifies to $P_{ii}(\mathbf{x}_i - \sum_k P_{ik} \mathbf{x}_k)\mathbf{x}_i^\top$.

For vector $p$-norms, $\|J_f\|_p$ is bounded if and only if its entries are bounded, by definition of the operator norm. 
The entries of $X^\top P^{(i)}XA$ are bounded for arbitrary $A$ only if the entries of $X^\top P^{(i)}X$ are bounded.
So let us investigate the entries of this $D\times D$ matrix. 
Writing out each term of the matrix, we observe that it is in fact a covariance matrix of a discrete distribution. Specifically:
\begin{equation} \label{eq:cov_general}
    [X^\top P^{(i)}X]_{lm}  = \textstyle\sum_k P_{ik} x_{kl} x_{km} - \left(\textstyle\sum_k P_{ik}  x_{kl}\right)\left(\textstyle\sum_k P_{ik} x_{km}\right) = \mathrm{Cov}(\mathbb{X}_l,\mathbb{X}_m),
\end{equation}
where $\mathbb{X}$ is a discrete distribution with support at the inputs $\{\mathbf{x}_1,\ldots,\mathbf{x}_N \}$ and probability mass function given by their softmax probabilities $\mathbb{P}(\mathbb{X}=\mathbf{x}_j)=P_{ij}$. 
A consequence of this interpretation is that $P^{(i)}$ is $\textit{positive semi-definite}$ (PSD) since for $D=1$, Equation \eqref{eq:cov_general} becomes $X^\top P^{(i)} X = \mathrm{Var}(\mathbb{X}) \geq 0$, with equality if and only if the $\mathbf{x}_j$ are all equal.

We use this observation to show that the terms of $J_{ii}$ are unbounded, and so $\verb!DP-MHA!$ is $\emph{not}$ Lipschitz.
Consider the case $\mathbf{x}_i=0$. Then $P_{i:}^\top = \softmax{XA\mathbf{x}_i} = \frac{1}{N} \mathbf{1}$, i.e.\ we have uniform attention regardless of $\mathbf{x}_{ \neq i}$. 
The first term of $J_{ii}$ in Equation \eqref{eq:jac_dot_general} disappears since $\mathbf{x}_i=\mathbf{0}$, and the last term becomes $\frac{1}{N} I$. For the second term, the entries $[X^\top P^{(i)}X]_{ll} = \mathrm{Var}(\mathbb{X}_l)$ are unbounded since the latter is equal to the sample variance of ${x_{1l},\ldots,x_{Nl}}$, which can be arbitrarily large.

Note that we have shown that single head dot-product self-atttention ($H=1$) is not Lipschitz, but it is clear that this implies multihead self-attention $\verb!DP-MHA!$ is also not Lipschitz, since the output of multihead attention is a linear combination of the outputs of each head.
</section>

    <section id="proofExplanation">
        <h4>Explanation of Proof (AI-Generated)</h4>
        <div>
            The proof involves several steps to demonstrate that the mapping \( f \) is not Lipschitz. Here is a breakdown of the steps:

1. <i></i>Definition of the Mapping \( f \)<i></i>:
   The mapping \( f \) is defined as:
   \[
   f(X) = PX = \softmax{X A^\top X^\top} X = \begin{bmatrix}
       f_1(X)^\top \\
       \vdots \\
       f_N(X)^\top
   \end{bmatrix} \in \mathbb{R}^{N \times D},
   \]
   where \( A = W^K W^{Q^\top} / \sqrt{D/H} \in \mathbb{R}^{D \times D} \) and \( f_i(X) = \sum_{j=1}^N P_{ij}\mathbf{x}_j \) with \( P_{i:}^\top = \softmax{XA\mathbf{x}_i} \).

2. <i></i>Interpretation of \( f \)<i></i>:
   \( f \) maps each \( \mathbf{x}_i \) to a point in the convex hull of \( \{\mathbf{x}_1, \ldots, \mathbf{x}_N\} \).

3. <i></i>Jacobian of \( f \)<i></i>:
   The Jacobian \( J_f \) of \( f \) is given by:
   \[
   J_f = \begin{bmatrix}
       J_{11} & \dots & J_{1N} \\
       \vdots & \ddots & \vdots \\
       J_{N1} & \dots & J_{NN} \\
   \end{bmatrix} \in \mathbb{R}^{ND \times ND},
   \]
   where \( J_{ij} = \frac{\partial f_i(X)}{\partial \mathbf{x}_j} \in \mathbb{R}^{D \times D} \).

4. <i></i>Expression for \( J_{ij} \)<i></i>:
   By taking partial derivatives, it is shown that:
   \[
   J_{ij} = X^\top P^{(i)} \left[E_{ji}XA^\top + XA\delta_{ij}\right] + P_{ij}I,
   \]
   where \( E_{ij} \in \mathbb{R}^{N \times N} \) is a binary matrix, \( \delta_{ij} \) is the Kronecker delta, and \( P^{(i)} \coloneqq \diag(P_{i:}) - P_{i:}^\top P_{i:} \).

5. <i></i>Simplification for \( i = j \)<i></i>:
   For \( i = j \):
   \[
   J_{ii} = P_{ii}\left(\mathbf{x}_i - \textstyle\sum_k P_{ik} \mathbf{x}_k\right)\mathbf{x}_i^\top A^\top + X^\top P^{(i)}XA + P_{ii}I.
   \]

6. <i></i>Boundedness of \( \|J_f\|_p \)<i></i>:
   The entries of \( X^\top P^{(i)}XA \) are bounded if and only if the entries of \( X^\top P^{(i)}X \) are bounded.

7. <i></i>Covariance Matrix Interpretation<i></i>:
   The entries of \( X^\top P^{(i)}X \) form a covariance matrix:
   \[
   [X^\top P^{(i)}X]_{lm} = \textstyle\sum_k P_{ik} x_{kl} x_{km} - \left(\textstyle\sum_k P_{ik}  x_{kl}\right)\left(\textstyle\sum_k P_{ik} x_{km}\right) = \mathrm{Cov}(\mathbb{X}_l,\mathbb{X}_m),
   \]
   where \( \mathbb{X} \) is a discrete distribution with support at the inputs \( \{\mathbf{x}_1, \ldots, \mathbf{x}_N \} \) and probability mass function given by their softmax probabilities.

8. <i></i>Positive Semi-Definiteness<i></i>:
   \( P^{(i)} \) is positive semi-definite (PSD).

9. <i></i>Unboundedness of \( J_{ii} \)<i></i>:
   For \( \mathbf{x}_i = 0 \), the entries of \( J_{ii} \) are unbounded, showing that \( \verb!DP-MHA! \) is not Lipschitz.

10. <i></i>Conclusion<i></i>:
    Since single head dot-product self-attention (\( H = 1 \)) is not Lipschitz, multihead self-attention (\( \verb!DP-MHA! \)) is also not Lipschitz, as the output of multihead attention is a linear combination of the outputs of each head.
        </div>
    </section>




    <section id="Reference">
        <h3>Reference</h3>
            <p>Title: <a href="/library/papers/the_lipschitz_constant_of_self-attention/index.html">The Lipschitz Constant of Self-Attention</a></p>
            <p>Authors: Hyunjik Kim, George Papamakarios, and Andriy Mnih</p>
            <p>URL: <a href="https://arxiv.org/abs/2006.04710">https://arxiv.org/abs/2006.04710</a></p>
    </section>
    <section id="Resources">
        <!-- Bibtex Modal -->
        <div id="bibtexModal" class="modal">
        <div class="modal-content">
            <span class="close" onclick="closeBibtexModal()">&times;</span>
            <p>@misc{kim2021lipschitzconstantselfattention,<br>&emsp;title={The Lipschitz Constant of Self-Attention},<br>&emsp;author={Hyunjik Kim and George Papamakarios and Andriy Mnih},<br>&emsp;year={2021},<br>&emsp;eprint={2006.04710},<br>&emsp;archivePrefix={arXiv},<br>&emsp;primaryClass={stat.ML},<br>&emsp;url={https://arxiv.org/abs/2006.04710}<br>}</p>
            <button class="modal-button" onclick="copyBibtex()">Copy</button>
        </div>
        </div>

        <!-- Latex Modal -->
        <div id="latexModal" class="modal">
        <div class="modal-content">
            <span class="close" onclick="closeLatexModal()">&times;</span>
            <p class="tex2jax_ignore"><code>\begin{theorem} \label{thm:dp_not_lipschitz}
\verb!DP-MHA! is not Lipschitz for any vector $p$-norm $\|\cdot\|_p$ with $p \in [1, \infty]$.
\end{theorem}</code></p>
            <button class="modal-button" onclick="copyLatex()">Copy</button>
        </div>
        </div>

        <h3>Resources</h3>    
            <p>
                <b>BibTex</b>:
                    <button onclick="copyBibtex()">Copy</button>
                    <button onclick="openBibtexModel()">Show</button>
            </p>
            <p>
                <b>LaTeX</b>:
                    <button onclick="copyLatex()">Copy</button>
                    <button onclick="openLatexModal()">Show</button>
            </p>
        <script>
            function copyBibtex() {
                var textToCopy = `@misc{kim2021lipschitzconstantselfattention,
  title={The Lipschitz Constant of Self-Attention},
  author={Hyunjik Kim and George Papamakarios and Andriy Mnih},
  year={2021},
  eprint={2006.04710},
  archivePrefix={arXiv},
  primaryClass={stat.ML},
  url={https://arxiv.org/abs/2006.04710}
}`;
                navigator.clipboard.writeText(textToCopy).then(function() {
                    //alert('Text copied to clipboard: ' + textToCopy);
                }, function(err) {
                    console.error('Failed to copy text: ', err);
                });
            }
            
            function copyLatex() {
                var textToCopy = `\\begin{theorem} \\label{thm:dp_not_lipschitz}
\\verb!DP-MHA! is not Lipschitz for any vector $p$-norm $\\|\\cdot\\|_p$ with $p \\in [1, \\infty]$.
\\end{theorem}`;
                navigator.clipboard.writeText(textToCopy).then(function() {
                    //alert('Text copied to clipboard: ' + textToCopy);
                }, function(err) {
                    console.error('Failed to copy text: ', err);
                });
            }

            // Bibtex
            function openBibtexModel() {
                document.getElementById('bibtexModal').style.display = 'block';
            }
            function closeBibtexModal() {
                document.getElementById('bibtexModal').style.display = 'none';
            }

            // Latex
            function openLatexModal() {
                document.getElementById('latexModal').style.display = 'block';
            }
            function closeLatexModal() {
                document.getElementById('latexModal').style.display = 'none';
            }
        </script>
    </section>
</section>

    </main>
    <footer>
        <p>&copy; 2024 Theoretical Results on Transformers and Language Models</p>
    </footer>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Theorem 9 - LM Theory</title>
    <link rel="stylesheet" href="/assets/css/styles.css">
    
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>
    window.MathJax = {
        loader: {load: ['[tex]/mathtools']},
        tex: {
            tags: 'all',  // Automatically number all display equations
            // loader: {load: ['[tex]/autoload']},               // TODO: remove
            // packages: {'[+]': ['autoload', 'mathtools']},     // TODO: remove
            packages: {'[+]': ['mathtools']},
            autoload: {
                coloneqq: ['mathtools']
            },
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            macros: {
                emph: ["\\textit{#1}", 1],
bm: ["\\boldsymbol{\\mathbf{#1}}", 1],
mathds: ["\\mathbf{#1}", 1],
textsl: ["\\textit{#1}", 1],
vspace: ["", 1],
xspace: "",
data: ["\\textcolor{Maroon}{\\texttt{{#1}}}", 1]

            },
            environments: {
                subequations: ["{", "}"]

            },
        },
        options: {
            renderActions: {
                addMenu: []
            },
            ignoreHtmlClass: "tex2jax_ignore",
            skipHtmlTags: ["script", "noscript", "style", "textarea", "pre", "code"]
        }
    };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <header>
        <h1>Theoretical Results on Transformers and Language Models</h1>
        <nav>
            <a href="/">Home</a>
            <a href="/examples">Examples</a>
            <a href="/contact">Contact</a>
            <a href="/contribute">Contribute</a>
            <div class="hover-container">
                <span class="hover-word"><a href="/library">Index</a></span>
                <div class="dropdown-content">
                    <a href="/library">Full Index</a>
                    <a href="/library/papers">Papers</a>
                    <a href="/library/definitions">Definitions</a>
                    <a href="/library/axioms">Axioms</a>
                    <a href="/library/lemmas">Lemmas</a>
                    <a href="/library/theorems">Theorems</a>
                    <a href="/library/corollaries">Corollaries</a>
                </div>
            </div>
        </nav>
    </header>
    <main>
        
<section id="Stability of Isotropic Attention">
    <h2>Theorem 9: Stability of Isotropic Attention</h2>
    <section id="Statement">
        <h3>Statement</h3>
        <div>
            For isotropic attention:
\begin{equation}
    \epsilon^{\Delta x(q)} \xrightarrow[\epsilon^q\rightarrow0]{\mathrm{perturb~q}} \langle m_t {\tilde k}_t^T \rangle_t \epsilon^q ~~~~~~~~
    
    \epsilon^{\Delta x(k)} \xrightarrow[\epsilon^k_t\rightarrow0]{\mathrm{perturb~k}} \langle {\tilde m}_t {\epsilon^k_t}^T \rangle_t ~q   ~~~~~~~~
    \epsilon^{\Delta x(m)} \xrightarrow[\epsilon^m_t\rightarrow0]{\mathrm{perturb~m}} \langle \epsilon^m_t \rangle_t
\end{equation}

N.B. isotropy requires $k_t=const$ or $q=0$. Lemma 1: the update is stable to noisy $q$ when $k_t=const$, or when $m_t \perp k_t$ (c.f. keys and messages from independent subspaces). Lemma 2: the update is stable to noisy $k_t$ when $q=0$, or when $m_t \perp \epsilon_t^k$. Lemma 3: the update is stable to noisy $m_t$ when $\langle \epsilon^m_t \rangle_t=0$. Other cases propagate linearly.
        </div>
    </section>
    <section id="motivation">
        <h3>Motivation of Statement (AI-Generated)</h3>
        <div>
            Understanding the stability of updates in isotropic attention mechanisms is crucial for designing robust neural networks. This set of lemmas provides conditions under which the updates remain stable despite the presence of noise in the queries ($q$), keys ($k_t$), or messages ($m_t$). Specifically, it tells us that:

1. The update is stable to noisy queries when the keys are constant or when the messages are orthogonal to the keys.
2. The update is stable to noisy keys when the queries are zero or when the messages are orthogonal to the noisy keys.
3. The update is stable to noisy messages when the average noise in the messages is zero.

These insights are particularly useful when implementing attention mechanisms in neural networks, ensuring that the model remains reliable even in the presence of noise.
        </div>
    </section>
        
<section id="OriginalProof">
<h3>Original Proof</h3>
For isotropic attention we have $a_t = \frac{1}{T}$. For perturbations of $q$, the RHS of Eq. (39) [in <a href="https://arxiv.org/pdf/2406.17837#equation.F.39"> original paper</a>] is
\begin{equation}
\begin{split}
    \mathop{\mathbb{E}}_{a_t} \Big[ m_t {\tilde k}_t^T \Big] \epsilon^q ~&=~ \sum_{t} a_t m_t {\tilde k}_t^T \epsilon^q \\
    &=~ \frac{1}{T} \sum_{t=1}^T m_t {\tilde k}_t^T \epsilon^q \\
    &=~ \langle m_t {\tilde k}_t^T \rangle_t \epsilon^q \\
\end{split}
\end{equation}
For lemma 1, we note that $k_t=const$ implies ${\tilde k}_t=0$, and if $m_t \perp k_t$ then $\langle m_t {\tilde k}_t^T \rangle_t = \langle m_t k_t \rangle_t - \langle m_t \rangle_t \langle k_t\rangle_t = Cov(m_t,k_t) = 0$.

For perturbations of $k_t$, the RHS of Eq. (40) [in <a href="https://arxiv.org/pdf/2406.17837#equation.F.40"> original paper</a>] is
\begin{equation}
\begin{split}
    \mathop{\mathbb{E}}_{a_t} \Big[ {\tilde m}_t {\epsilon^k_t}^T \Big] q ~&=~ \frac{1}{T} \sum_{t=1}^T {\tilde m}_t {\epsilon^k_t}^T q \\
    &=~ \langle {\tilde m}_t {\epsilon^k_t}^T \rangle_t q \\
\end{split}
\end{equation}
For lemma 2, this expression evaluates to $0$ if $q=0$, and if $m_t \perp \epsilon_t^k$ then $\langle {\tilde m}_t {\epsilon^k_t}^T \rangle_t = \langle m_t {\epsilon^k_t}^T \rangle_t - \langle m_t \rangle_t \langle {\epsilon^k_t}^T\rangle_t = Cov(m_t,{\epsilon^k_t}^T) = 0$.

For perturbations of $m_t$, the RHS of Eq. (41) [in <a href="https://arxiv.org/pdf/2406.17837#equation.F.41"> original paper</a>] evaluates to
\begin{equation}
    \mathop{\mathbb{E}}_{a_t} \Big[ \epsilon^m_t \Big] ~=~ \frac{1}{T}\sum_{t=1}^T \epsilon^m_t ~=~ \langle \epsilon^m_t \rangle_t
\end{equation}
</section>

    <section id="proofExplanation">
        <h4>Explanation of Proof (AI-Generated)</h4>
        <div>
            To understand the proof, we break it down into several steps, each addressing different perturbations and their implications:
<br>
<br>1. <i></i>Isotropic Attention<i></i>:
<br>   - Given isotropic attention, we have \( a_t = \frac{1}{T} \).
<br>   - For perturbations of \( q \), the right-hand side (RHS) of Eq. (39) [in <a href="https://arxiv.org/pdf/2406.17837#equation.F.39"> original paper</a>] is evaluated as follows:
<br>   \[
   \mathop{\mathbb{E}}_{a_t} \Big[ m_t {\tilde k}_t^T \Big] \epsilon^q = \sum_{t} a_t m_t {\tilde k}_t^T \epsilon^q = \frac{1}{T} \sum_{t=1}^T m_t {\tilde k}_t^T \epsilon^q = \langle m_t {\tilde k}_t^T \rangle_t \epsilon^q
   \]
<br>2. <i></i>Lemma 1<i></i>:
<br>   - If \( k_t = \text{const} \), then \( {\tilde k}_t = 0 \).
<br>   - If \( m_t \perp k_t \), then:
<br>   \[
   \langle m_t {\tilde k}_t^T \rangle_t = \langle m_t k_t \rangle_t - \langle m_t \rangle_t \langle k_t \rangle_t = \text{Cov}(m_t, k_t) = 0
   \]
<br>3. <i></i>Perturbations of \( k_t \)<i></i>:
<br>   - For perturbations of \( k_t \), the RHS of Eq. (40) [in <a href="https://arxiv.org/pdf/2406.17837#equation.F.40"> original paper</a>] is:
<br>   \[
   \mathop{\mathbb{E}}_{a_t} \Big[ {\tilde m}_t {\epsilon^k_t}^T \Big] q = \frac{1}{T} \sum_{t=1}^T {\tilde m}_t {\epsilon^k_t}^T q = \langle {\tilde m}_t {\epsilon^k_t}^T \rangle_t q
   \]
<br>4. <i></i>Lemma 2<i></i>:
<br>   - This expression evaluates to \( 0 \) if \( q = 0 \).
<br>   - If \( m_t \perp \epsilon_t^k \), then:
<br>   \[
   \langle {\tilde m}_t {\epsilon^k_t}^T \rangle_t = \langle m_t {\epsilon^k_t}^T \rangle_t - \langle m_t \rangle_t \langle {\epsilon^k_t}^T \rangle_t = \text{Cov}(m_t, {\epsilon^k_t}^T) = 0
   \]
<br>5. <i></i>Perturbations of \( m_t \)<i></i>:
<br>   - For perturbations of \( m_t \), the RHS of Eq. (41) [in <a href="https://arxiv.org/pdf/2406.17837#equation.F.41"> original paper</a>] evaluates to:
<br>   \[
   \mathop{\mathbb{E}}_{a_t} \Big[ \epsilon^m_t \Big] = \frac{1}{T} \sum_{t=1}^T \epsilon^m_t = \langle \epsilon^m_t \rangle_t
   \]
<br>Each step systematically addresses the perturbations and their effects, leading to the conclusion of the proof.
        </div>
    </section>




    <section id="Reference">
        <h3>Reference</h3>
            <p>Title: <a href="/library/papers/transformer_normalisation_layers_and_the_independence_of_semantic_subspaces/index.html">Transformer Normalisation Layers and the Independence of Semantic Subspaces</a></p>
            <p>Authors: Stephen Menary, Samuel Kaski, and Andre Freitas</p>
            <p>URL: <a href="https://arxiv.org/abs/2406.17837">https://arxiv.org/abs/2406.17837</a></p>
    </section>
    <section id="Resources">
        <!-- Bibtex Modal -->
        <div id="bibtexModal" class="modal">
        <div class="modal-content">
            <span class="close" onclick="closeBibtexModal()">&times;</span>
            <p>@misc{menary2024transformernormalisationlayersindependence,<br>&emsp;title={Transformer Normalisation Layers and the Independence of Semantic Subspaces}, <br>&emsp;author={Stephen Menary and Samuel Kaski and Andre Freitas},<br>&emsp;year={2024},<br>&emsp;eprint={2406.17837},<br>&emsp;archivePrefix={arXiv},<br>&emsp;primaryClass={cs.LG},<br>&emsp;url={https://arxiv.org/abs/2406.17837}<br>}</p>
            <button class="modal-button" onclick="copyBibtex()">Copy</button>
        </div>
        </div>

        <!-- Latex Modal -->
        <div id="latexModal" class="modal">
        <div class="modal-content">
            <span class="close" onclick="closeLatexModal()">&times;</span>
            <p class="tex2jax_ignore"><code>\begin{theorem}
    For isotropic attention:
    \begin{equation}
        \epsilon^{\Delta x(q)} \xrightarrow[\epsilon^q\rightarrow0]{\mathrm{perturb~q}} \langle m_t {\tilde k}_t^T \rangle_t \epsilon^q ~~~~~~~~
        
        \epsilon^{\Delta x(k)} \xrightarrow[\epsilon^k_t\rightarrow0]{\mathrm{perturb~k}} \langle {\tilde m}_t {\epsilon^k_t}^T \rangle_t ~q   ~~~~~~~~
        \epsilon^{\Delta x(m)} \xrightarrow[\epsilon^m_t\rightarrow0]{\mathrm{perturb~m}} \langle \epsilon^m_t \rangle_t
    \end{equation}
    
    
    
    
    
    N.B. isotropy requires $k_t=const$ or $q=0$. Lemma 1: the update is stable to noisy $q$ when $k_t=const$, or when $m_t \perp k_t$ (c.f. keys and messages from independent subspaces). Lemma 2: the update is stable to noisy $k_t$ when $q=0$, or when $m_t \perp \epsilon_t^k$. Lemma 3: the update is stable to noisy $m_t$ when $\langle \epsilon^m_t \rangle_t=0$. Other cases propagate linearly.
\label{theorem: stability: isotropic}
\end{theorem}</code></p>
            <button class="modal-button" onclick="copyLatex()">Copy</button>
        </div>
        </div>

        <h3>Resources</h3>    
            <p>
                <b>BibTex</b>:
                    <button onclick="copyBibtex()">Copy</button>
                    <button onclick="openBibtexModel()">Show</button>
            </p>
            <p>
                <b>LaTeX</b>:
                    <button onclick="copyLatex()">Copy</button>
                    <button onclick="openLatexModal()">Show</button>
            </p>
        <script>
            function copyBibtex() {
                var textToCopy = `@misc{menary2024transformernormalisationlayersindependence,
  title={Transformer Normalisation Layers and the Independence of Semantic Subspaces}, 
  author={Stephen Menary and Samuel Kaski and Andre Freitas},
  year={2024},
  eprint={2406.17837},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2406.17837}
}`;
                navigator.clipboard.writeText(textToCopy).then(function() {
                    //alert('Text copied to clipboard: ' + textToCopy);
                }, function(err) {
                    console.error('Failed to copy text: ', err);
                });
            }
            
            function copyLatex() {
                var textToCopy = `\\begin{theorem}
    For isotropic attention:
    \\begin{equation}
        \\epsilon^{\\Delta x(q)} \\xrightarrow[\\epsilon^q\\rightarrow0]{\\mathrm{perturb~q}} \\langle m_t {\\tilde k}_t^T \\rangle_t \\epsilon^q ~~~~~~~~
        
        \\epsilon^{\\Delta x(k)} \\xrightarrow[\\epsilon^k_t\\rightarrow0]{\\mathrm{perturb~k}} \\langle {\\tilde m}_t {\\epsilon^k_t}^T \\rangle_t ~q   ~~~~~~~~
        \\epsilon^{\\Delta x(m)} \\xrightarrow[\\epsilon^m_t\\rightarrow0]{\\mathrm{perturb~m}} \\langle \\epsilon^m_t \\rangle_t
    \\end{equation}
    
    
    
    
    
    N.B. isotropy requires $k_t=const$ or $q=0$. Lemma 1: the update is stable to noisy $q$ when $k_t=const$, or when $m_t \\perp k_t$ (c.f. keys and messages from independent subspaces). Lemma 2: the update is stable to noisy $k_t$ when $q=0$, or when $m_t \\perp \\epsilon_t^k$. Lemma 3: the update is stable to noisy $m_t$ when $\\langle \\epsilon^m_t \\rangle_t=0$. Other cases propagate linearly.
\\label{theorem: stability: isotropic}
\\end{theorem}`;
                navigator.clipboard.writeText(textToCopy).then(function() {
                    //alert('Text copied to clipboard: ' + textToCopy);
                }, function(err) {
                    console.error('Failed to copy text: ', err);
                });
            }

            // Bibtex
            function openBibtexModel() {
                document.getElementById('bibtexModal').style.display = 'block';
            }
            function closeBibtexModal() {
                document.getElementById('bibtexModal').style.display = 'none';
            }

            // Latex
            function openLatexModal() {
                document.getElementById('latexModal').style.display = 'block';
            }
            function closeLatexModal() {
                document.getElementById('latexModal').style.display = 'none';
            }
        </script>
    </section>
</section>

    </main>
    <footer>
        <p>&copy; 2024 Theoretical Results on Transformers and Language Models</p>
    </footer>
</body>
</html>